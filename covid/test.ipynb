{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9fb96a-72a0-493a-b10c-511744f03d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "from evaluate.visualization import radar_plot\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f835fb6-c464-4d80-bb4f-1ec15086165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_root = \"Data/\"\n",
    "log_prefix = \"log_train_history_\"\n",
    "max_token = 256 # max token from dataset\n",
    "min_token = 100 # min token from dataset\n",
    "type = pd.read_json(f\"{dir_root}test_topic.json\", orient=\"records\")[\"input\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2295ca-67c0-4541-bbf8-14ce24215b14",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calc_cossin(ref, pred):\n",
    "    st = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    sentences_ref = sent_tokenize(ref)\n",
    "    encode_ref = st.encode(sentences_ref)\n",
    "    \n",
    "    sentences_pred = sent_tokenize(pred)\n",
    "    encode_pred = st.encode(sentences_pred)\n",
    "    \n",
    "    cosine_scores = util.cos_sim(encode_pred, encode_ref).numpy()\n",
    "    \n",
    "    return sum([item.max() for item in cosine_scores]) / len(cosine_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0436fe-cb41-42d9-9031-b8169ddeca10",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_best_model(prefix, metric, data):\n",
    "    columns = [prefix + str(round(treshold,1)) + metric for treshold in np.arange(1.0, -0.1, -0.1)]\n",
    "    describe = data[columns].describe().T\n",
    "    val = describe.max()[\"mean\"]\n",
    "    name = describe[describe[\"mean\"] == val].index[0]\n",
    "\n",
    "    return name, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d5395b-f08a-418f-b108-49dd7ef56ec1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def measure(col, samples):\n",
    "    cossins = []\n",
    "    rouge1 = []\n",
    "    rouge2 = []\n",
    "    rougeL = []\n",
    "    rougeLsum = []\n",
    "    meteors = []\n",
    "    bleus = []\n",
    "    bleu_precisions_n1 = []\n",
    "    bleu_precisions_n2 = []\n",
    "    bleu_precisions_n3 = []\n",
    "    bleu_precisions_n4 = []\n",
    "    bleu_brevity_penalty = []\n",
    "    bleu_length_ratio = []\n",
    "    bleu_translation_length = []\n",
    "    bleu_reference_length = []\n",
    "    \n",
    "    rouge = evaluate.load('rouge')\n",
    "    meteor = evaluate.load('meteor')\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    st = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    for i in tqdm(range(len(samples))):\n",
    "        # Cosine sim\n",
    "        cossins.append(calc_cossin(samples[\"output\"].values[i], test_topic[col].values[i]))\n",
    "        \n",
    "        # rouge\n",
    "        r_rouge = rouge.compute(\n",
    "            predictions=[samples[col].values[i]],\n",
    "            references=[samples[\"output\"].values[i]]\n",
    "        )\n",
    "        rouge1.append(r_rouge[\"rouge1\"])\n",
    "        rouge2.append(r_rouge[\"rouge2\"])\n",
    "        rougeL.append(r_rouge[\"rougeL\"])\n",
    "        rougeLsum.append(r_rouge[\"rougeLsum\"])\n",
    "\n",
    "        # meteor\n",
    "        r_meteor = meteor.compute(predictions=[samples[col].values[i]], references=[samples[\"output\"].values[i]])\n",
    "        meteors.append(r_meteor[\"meteor\"])\n",
    "\n",
    "        # bleu\n",
    "        try:\n",
    "            r_bleu = bleu.compute(predictions=[samples[col].values[i]], references=[samples[\"output\"].values[i]])\n",
    "            bleus.append(r_bleu[\"bleu\"])\n",
    "            bleu_precisions_n1.append(r_bleu[\"precisions\"][0])\n",
    "            bleu_precisions_n2.append(r_bleu[\"precisions\"][1])\n",
    "            bleu_precisions_n3.append(r_bleu[\"precisions\"][2])\n",
    "            bleu_precisions_n4.append(r_bleu[\"precisions\"][3])\n",
    "            bleu_brevity_penalty.append(r_bleu[\"brevity_penalty\"])\n",
    "            bleu_length_ratio.append(r_bleu[\"length_ratio\"])\n",
    "            bleu_translation_length.append(r_bleu[\"translation_length\"])\n",
    "            bleu_reference_length.append(r_bleu[\"reference_length\"])\n",
    "        except:\n",
    "            bleus.append(0.0)\n",
    "            bleu_precisions_n1.append(0.0)\n",
    "            bleu_precisions_n2.append(0.0)\n",
    "            bleu_precisions_n3.append(0.0)\n",
    "            bleu_precisions_n4.append(0.0)\n",
    "            bleu_brevity_penalty.append(0.0)\n",
    "            bleu_length_ratio.append(0.0)\n",
    "            bleu_translation_length.append(0.0)\n",
    "            bleu_reference_length.append(0.0)\n",
    "\n",
    "    samples[f\"{col}_cs\"] = cossins\n",
    "    samples[f\"{col}_rouge1\"] = rouge1\n",
    "    samples[f\"{col}_rouge2\"] = rouge2\n",
    "    samples[f\"{col}_rougeL\"] = rougeL\n",
    "    samples[f\"{col}_rougeLsum\"] = rougeLsum\n",
    "    samples[f\"{col}_meteor\"] = meteors\n",
    "    samples[f\"{col}_bleu\"] = bleus\n",
    "    samples[f\"{col}_bleu_precisions_n1\"] = bleu_precisions_n1\n",
    "    samples[f\"{col}_bleu_precisions_n2\"] = bleu_precisions_n2\n",
    "    samples[f\"{col}_bleu_precisions_n3\"] = bleu_precisions_n3\n",
    "    samples[f\"{col}_bleu_precisions_n4\"] = bleu_precisions_n4\n",
    "    samples[f\"{col}_bleu_brevity_penalty\"] = bleu_brevity_penalty\n",
    "    samples[f\"{col}_bleu_length_ratio\"] = bleu_length_ratio\n",
    "    samples[f\"{col}_bleu_translation_length\"] = bleu_translation_length\n",
    "    samples[f\"{col}_bleu_reference_length\"] = bleu_reference_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d093da-39fe-4acf-924b-e0508be76afc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def generate(prompt, tokenizer, model):\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "    )# .to(device)\n",
    "    input_ids = inputs[\"input_ids\"].cuda()    \n",
    "\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=0.6,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.2,\n",
    "    )\n",
    "\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        min_new_tokens=min_token,\n",
    "        max_new_tokens=max_token,\n",
    "        pad_token_id = 0,\n",
    "        eos_token_id = tokenizer.eos_token_id \n",
    "    )\n",
    "\n",
    "    result = []\n",
    "    for s in generation_output.sequences:\n",
    "        result.append(tokenizer.decode(s))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c069f-a704-4bc7-92ed-27d6da89a19d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def run(samples, tokenizer, model):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(samples))):\n",
    "        prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{samples[\"instruction\"].values[i]}\n",
    "### Input:\n",
    "{samples[\"input\"].values[i]}\n",
    "### Response:\"\"\"\n",
    "    \n",
    "        result = generate(prompt, tokenizer, model)\n",
    "        results.append(result[0].split(\"### Response:\\n\")[-1])\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0008b04a-3e3d-469f-aa88-b8a5945a86db",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from sentence_transformers import  util\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class DomainExpert():\n",
    "    def __init__(\n",
    "        self, \n",
    "        top_k=10, \n",
    "        min_new_tokens=0,\n",
    "        max_new_tokens=512,         \n",
    "        num_return_sequences=1,\n",
    "        temperature=0.9,\n",
    "        top_p=0.6,\n",
    "        repetition_penalty=1.2,\n",
    "        do_sample=True,\n",
    "        max_input=4000,\n",
    "        model_st_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_chat_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        devices=\"auto\"):\n",
    "            \n",
    "        self.top_k = top_k\n",
    "        self.min_new_tokens = min_new_tokens\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.num_return_sequences = num_return_sequences\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "        self.repetition_penalty=repetition_penalty\n",
    "        self.do_sample = do_sample\n",
    "        self.model_st_name = model_st_name\n",
    "        self.model_chat_name = model_chat_name\n",
    "        self.devices = devices\n",
    "        self.max_input = 4000\n",
    "                \n",
    "        self.model_st = SentenceTransformer(self.model_st_name)\n",
    "\n",
    "        self.pipeline_chat = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model_chat_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=self.devices,\n",
    "        )\n",
    "\n",
    "        self.tokenizer_chat = transformers.AutoTokenizer.from_pretrained(self.model_chat_name)\n",
    "\n",
    "    def get_number_of_tokens(self, context):\n",
    "        return len(self.tokenizer_chat(context)[\"input_ids\"])\n",
    "    \n",
    "    def sentence_encode(self, sentence):\n",
    "        return self.model_st.encode([sentence])\n",
    "\n",
    "    def mask_prompt(self, question, context = \"\"):\n",
    "        return f\"\"\"[INST]Below is an instruction that describes a task! Write a response that appropriately completes the request. To answer the instruction, use the context if available!\n",
    "Instruction: {question}\n",
    "Context: {context}[/INST]\"\"\"\n",
    "        \n",
    "    def create_prompt(self, question, context):\n",
    "        \"\"\"\n",
    "        Megadja a promptot, hogy mi alapján válaszoljon a modell.\n",
    "        \"\"\"\n",
    "        prompt = \"\"\n",
    "        context_tmp = \"\"\n",
    "        if len(context) > 0:\n",
    "            for text in context:            \n",
    "                context_tmp += f\"{text}\\n\"\n",
    "                prompt_tmp = self.mask_prompt(question, context_tmp)\n",
    "                if len(self.tokenizer_chat(prompt_tmp)[\"input_ids\"]) < self.max_input:\n",
    "                    prompt = prompt_tmp\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            prompt = self.mask_prompt(question)\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def generate_answare(self, question, context):\n",
    "        \"\"\"\n",
    "        Vissza adja a kérdés és a kontextus alapján a választ!\n",
    "        \"\"\"\n",
    "        prompt = self.create_prompt(question, context)\n",
    "        # print(question)\n",
    "        # print(context)\n",
    "        # print(prompt)\n",
    "        sequences = self.pipeline_chat(\n",
    "            prompt,\n",
    "            do_sample = self.do_sample,\n",
    "            top_k = self.top_k,\n",
    "            num_return_sequences = self.num_return_sequences,\n",
    "            min_new_tokens = self.min_new_tokens,\n",
    "            max_new_tokens = self.max_new_tokens,\n",
    "            temperature = self.temperature,\n",
    "            top_p = self.top_p,\n",
    "            repetition_penalty = self.repetition_penalty,\n",
    "            eos_token_id = self.tokenizer_chat.eos_token_id\n",
    "        )\n",
    "\n",
    "        answ = sequences[0]['generated_text'].split(\"[/INST]\")[1][2:].split(\"\\n\")\n",
    "        answ = \"\\n\".join([item for item in answ if len(item) > 0])\n",
    "        # print(answ)        \n",
    "        return answ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acc4a3f-c983-495b-8fa6-0772b5478587",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from sentence_transformers import  util\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class DomainExpertV2():\n",
    "    def __init__(\n",
    "        self, \n",
    "        top_k=10, \n",
    "        min_new_tokens=0,\n",
    "        max_new_tokens=512,         \n",
    "        num_return_sequences=1,\n",
    "        temperature=0.9,\n",
    "        top_p=0.6,\n",
    "        repetition_penalty=1.2,\n",
    "        do_sample=True,\n",
    "        max_input=4096,\n",
    "        model_st_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_chat_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        type=\"Domain Expert\",\n",
    "        devices=\"auto\"):\n",
    "            \n",
    "        self.top_k = top_k\n",
    "        self.min_new_tokens = min_new_tokens\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.num_return_sequences = num_return_sequences\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "        self.repetition_penalty=repetition_penalty\n",
    "        self.do_sample = do_sample\n",
    "        self.model_st_name = model_st_name\n",
    "        self.model_chat_name = model_chat_name\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.type = type\n",
    "        self.devices = devices\n",
    "        self.max_input = max_input - self.max_new_tokens\n",
    "                \n",
    "        self.model_st = SentenceTransformer(self.model_st_name)\n",
    "        self.tokenizer_chat = transformers.AutoTokenizer.from_pretrained(self.tokenizer_name, \n",
    "                                                                         padding=True, \n",
    "                                                                         truncation=True) \n",
    "            \n",
    "        self.pipeline_chat = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model_chat_name,\n",
    "            tokenizer=self.tokenizer_chat,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=self.devices,\n",
    "        )\n",
    "\n",
    "    def get_number_of_tokens(self, context):\n",
    "        return len(self.tokenizer_chat(context)[\"input_ids\"])\n",
    "    \n",
    "    def sentence_encode(self, sentence):\n",
    "        return self.model_st.encode([sentence])\n",
    "\n",
    "    def mask_promt(self, question, context = \"\"):\n",
    "        return f\"\"\"[INST]Below is an instruction that describes a task! Write a response that appropriately completes the request. To answer the instruction, use the context if available!\n",
    "### Instruction: \n",
    "{question}\n",
    "\n",
    "### Input:\n",
    "{self.type}\n",
    "\n",
    "### Context: \n",
    "{context}\n",
    "[/INST]\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "        \n",
    "    def create_prompt(self, question, context):\n",
    "        \"\"\"\n",
    "        Megadja a promptot, hogy mi alapján válaszoljon a modell.\n",
    "        \"\"\"\n",
    "        prompt = \"\"\n",
    "        context_tmp = \"\"\n",
    "        if len(context) > 0:\n",
    "            for text in context:            \n",
    "                context_tmp += f\"{text}\\n\"\n",
    "                # prompt_tmp = f\"\"\"[INST]Summarize and explain the given context in term of given question and start your answare below way 'Based on the information available to me': \n",
    "                # Question: {question}\n",
    "                # Context: {context_tmp}[/INST]\"\"\"    \n",
    "                # prompt_tmp = f\"\"\"[INST]Summarize and explain the given context in terms of the given question and start your answer below way 'Based on the information available to me':\n",
    "                # Question: {question}\n",
    "                # Context: {context_tmp}[/INST]\"\"\"\n",
    "                # prompt_tmp = f\"\"\"[INST]Answer the given question based on the given context.\n",
    "                # Question: {question}\n",
    "                # Context: {context_tmp}[/INST]\"\"\"\n",
    "                prompt_tmp = self.mask_promt(question, context_tmp)\n",
    "                if len(self.tokenizer_chat(prompt_tmp)[\"input_ids\"]) < self.max_input:\n",
    "                    prompt = prompt_tmp\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            prompt = self.mask_promt(question)\n",
    "                \n",
    "        return prompt\n",
    "    \n",
    "    def generate_answare(self, question, context):\n",
    "        \"\"\"\n",
    "        Vissza adja a kérdés és a kontextus alapján a választ!\n",
    "        \"\"\"\n",
    "        prompt = self.create_prompt(question, context)\n",
    "        # print(\"question:\", question)\n",
    "        # print(\"context:\", context)        \n",
    "        # print(\"prompt len\", len(self.tokenizer_chat(prompt)[\"input_ids\"]))\n",
    "        # print(\"prompt:\", prompt)\n",
    "        \n",
    "        sequences = self.pipeline_chat(\n",
    "            prompt,\n",
    "            do_sample = self.do_sample,\n",
    "            top_k = self.top_k,\n",
    "            num_return_sequences = self.num_return_sequences,\n",
    "            min_new_tokens = self.min_new_tokens,\n",
    "            max_new_tokens = self.max_new_tokens,\n",
    "            temperature = self.temperature,\n",
    "            top_p = self.top_p,\n",
    "            repetition_penalty = self.repetition_penalty,\n",
    "            eos_token_id = self.tokenizer_chat.eos_token_id,\n",
    "            pad_token_id = self.tokenizer_chat.eos_token_id\n",
    "        )\n",
    "\n",
    "        # print(sequences)\n",
    "        answ = sequences[0]['generated_text'].split(\"[/INST]\")[1]\n",
    "        answ = answ.replace(\"### Response:\\n\",\"\")\n",
    "        answ = answ.split(\"\\n\")\n",
    "        answ = \"\\n\".join([item for item in answ if len(item) > 0])\n",
    "        # print(answ)\n",
    "        # except:\n",
    "        #     answ = \"\"\n",
    "        # print(answ)\n",
    "        return answ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a69856-8f70-4c71-9671-146b27c32581",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sentence_transformers import  util\n",
    "\n",
    "class ExtractorCOVID():\n",
    "    def __init__(self, path, domain_expert):\n",
    "        self.path = path\n",
    "        self.domain_expert = domain_expert\n",
    "        \n",
    "        self.meta = pd.read_csv(f\"{self.path}/metadata.csv\", low_memory=False)\n",
    "        self.meta = self.meta[~self.meta[\"title\"].isnull()]\n",
    "        self.meta = self.meta[~self.meta[\"abstract\"].isnull()]\n",
    "\n",
    "        self.embedded_vectors = np.load(f\"{self.path}titles.emb\")\n",
    "\n",
    "        with open(f\"{self.path}titles.idx\",\"r\") as f:\n",
    "            self.index_to_vectors = f.read().splitlines()\n",
    "        self.index_to_vectors = [index for index in self.index_to_vectors]\n",
    "\n",
    "    def get_realted_cosine_scores(self):\n",
    "        if len(self.selected_indexes) > 0 and len(self.cosine_scores):\n",
    "            return self.cosine_scores[self.selected_indexes]\n",
    "            \n",
    "    def get_context(self, question=None, treshold=0.75, N=100):\n",
    "        # calculate similarities\n",
    "        self.embedded_question = self.domain_expert.sentence_encode(question)\n",
    "        self.cosine_scores = util.cos_sim(self.embedded_question, self.embedded_vectors)\n",
    "        self.cosine_scores = self.cosine_scores.reshape(len(self.cosine_scores[0])).numpy()\n",
    "    \n",
    "        # Sort similarities\n",
    "        self.selected_indexes = np.argsort(self.cosine_scores)\n",
    "        # Filter limit similarities\n",
    "        self.selected_indexes = self.selected_indexes[::-1][:N]              \n",
    "        # Filter treshold\n",
    "        self.selected_indexes = [self.selected_indexes[i] for i in range(len(self.cosine_scores[self.selected_indexes])) if self.cosine_scores[self.selected_indexes[i]] >= treshold]\n",
    "        # self.cosine_scores = np.argsort(self.cosine_scores)\n",
    "\n",
    "        # find selected elements\n",
    "        self.selected_elements = [self.index_to_vectors[si] for si in self.selected_indexes]        \n",
    "        self.selected_references = self.meta[self.meta[\"cord_uid\"].isin(self.selected_elements)][[\"publish_time\", \"authors\", \"pdf_json_files\", \"pmc_json_files\"]]\n",
    "        \n",
    "        # Create context\n",
    "        self.title = self.meta[self.meta[\"cord_uid\"].isin(self.selected_elements)][\"title\"].values\n",
    "        self.context = self.meta[self.meta[\"cord_uid\"].isin(self.selected_elements)][\"abstract\"].values\n",
    "        \n",
    "        return self.context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8d07c1-4d34-4f64-8dc9-c9e4348b456c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sentence_transformers import  util\n",
    "\n",
    "class ExtractorBase():\n",
    "    def __init__(self, path, domain_expert):\n",
    "        self.path = path\n",
    "        self.domain_expert = domain_expert\n",
    "        \n",
    "        with open(f\"{self.path}qaa.json\") as f:\n",
    "            self.qaa = json.load(f)\n",
    "\n",
    "        with open(f\"{self.path}qaa.idx\",\"r\") as f:\n",
    "            self.index_to_vectors = f.read().splitlines()\n",
    "        self.index_to_vectors = [int(index) for index in self.index_to_vectors]\n",
    "            \n",
    "        self.embedded_vectors = np.load(f\"{self.path}qaa.emb\")\n",
    "\n",
    "    def get_realted_cosine_scores(self):\n",
    "        if len(self.selected_indexes) > 0 and len(self.cosine_scores):\n",
    "            return self.cosine_scores[self.selected_indexes]\n",
    "\n",
    "    def clear_item(self, item):\n",
    "        item = item.replace(\" .\", \"\")\n",
    "        item = item.replace(\"..\", \".\")\n",
    "        return item\n",
    "        \n",
    "    def get_context(self, question=None, treshold=0.75, N=100):\n",
    "        # calculate similarities\n",
    "        self.embedded_question = self.domain_expert.sentence_encode(question)\n",
    "        self.cosine_scores = util.cos_sim(self.embedded_question, self.embedded_vectors)\n",
    "        self.cosine_scores = self.cosine_scores.reshape(len(self.cosine_scores[0])).numpy()\n",
    "    \n",
    "        # Sort similarities\n",
    "        self.selected_indexes = np.argsort(self.cosine_scores)\n",
    "        # Filter limit similarities\n",
    "        self.selected_indexes = self.selected_indexes[::-1][:N]              \n",
    "        # Filter treshold\n",
    "        self.selected_indexes = [self.selected_indexes[i] for i in range(len(self.cosine_scores[self.selected_indexes])) if self.cosine_scores[self.selected_indexes[i]] >= treshold]\n",
    "        # self.cosine_scores = np.argsort(self.cosine_scores)        \n",
    "\n",
    "        # Find elements\n",
    "        self.selected_elements = [self.qaa[self.index_to_vectors[si]] for si in self.selected_indexes]\n",
    "        \n",
    "        # Find duplicated elements\n",
    "        self.duplicated_indexes = pd.DataFrame([se[\"answare\"] for se in self.selected_elements])\n",
    "        self.duplicated_indexes = self.duplicated_indexes[self.duplicated_indexes.duplicated()]\n",
    "        if len(self.duplicated_indexes) > 0:\n",
    "            self.duplicated_indexes = self.duplicated_indexes.groupby(list(self.duplicated_indexes)).apply(lambda x: x.index[0]).tolist()\n",
    "            self.selected_indexes = [self.selected_indexes[i] for i in range(0, len(self.selected_indexes)) if i not in self.duplicated_indexes]\n",
    "            self.selected_elements = [self.qaa[self.index_to_vectors[si]] for si in self.selected_indexes]\n",
    "\n",
    "        # find selected elements        \n",
    "        self.selected_answares = [se[\"answare\"] for se in self.selected_elements]\n",
    "        self.selected_answares = pd.DataFrame(self.selected_answares)\n",
    "        \n",
    "        # Create context\n",
    "        self.context = [self.clear_item(item[0]) for item in self.selected_answares.values]\n",
    "\n",
    "        return self.context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5839ba1b-7ca3-406d-9462-7fe69f5febc8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sentence_transformers import  util\n",
    "\n",
    "class ExtractorBaseF():\n",
    "    def __init__(self, path, domain_expert):\n",
    "        self.max_return = 200 # It surely is more than 4000 tokens\n",
    "        \n",
    "        self.path = path\n",
    "        self.domain_expert = domain_expert \n",
    "        \n",
    "        self.sentences = pd.read_json(f\"{self.path}/sentences.json\")\n",
    "        self.embedded_vectors = np.load(f\"{self.path}sentences.emb\")\n",
    "\n",
    "    def get_realted_cosine_scores(self):\n",
    "        if len(self.selected_indexes) > 0 and len(self.cosine_scores):\n",
    "            return self.cosine_scores[self.selected_indexes]\n",
    "\n",
    "    def get_context(self, question=None, treshold=0.75, N=100):\n",
    "        # calculate similarities\n",
    "        self.embedded_question = self.domain_expert.sentence_encode(question)\n",
    "        self.cosine_scores = util.cos_sim(self.embedded_question, self.embedded_vectors)\n",
    "        self.cosine_scores = self.cosine_scores.reshape(len(self.cosine_scores[0])).numpy()\n",
    "    \n",
    "        # Sort similarities\n",
    "        self.selected_indexes = np.argsort(self.cosine_scores)\n",
    "        # Filter limit similarities\n",
    "        self.selected_indexes = self.selected_indexes[::-1][:N]              \n",
    "        # Filter treshold\n",
    "        self.selected_indexes = [self.selected_indexes[i] for i in range(len(self.cosine_scores[self.selected_indexes])) if self.cosine_scores[self.selected_indexes[i]] >= treshold]\n",
    "        # self.cosine_scores = np.argsort(self.cosine_scores)\n",
    "\n",
    "        self.context = list(self.sentences[self.sentences.index.isin(self.selected_indexes)][\"sentences\"].values[:200])\n",
    "        \n",
    "        self.selected_references = self.sentences[self.sentences.index.isin(self.selected_indexes)]\n",
    "        self.selected_references = self.selected_references.drop(columns=[\"sentences\"])\n",
    "        self.selected_references = self.selected_references[:200]\n",
    "                                                                         \n",
    "        return self.context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9aa4f4-99e5-4b1a-be2c-49d03f23421d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def rag_test(domain_expert, extractor, data, mark, N=100, log_dir=\"Data/\"):\n",
    "\n",
    "    for treshold in np.arange(1.0, -0.1, -0.1):\n",
    "        treshold = round(treshold,1)\n",
    "        answares = []\n",
    "    \n",
    "        # pbar = tqdm(range(len(data[\"instruction\"].values)))\n",
    "        for i in range(len(data[\"instruction\"].values)):   \n",
    "            context = extractor.get_context(data[\"instruction\"].values[i], treshold=treshold, N=100)\n",
    "            # if len(context) == 0:\n",
    "            #     answares.append(\"\")\n",
    "            # else:\n",
    "            answare = domain_expert.generate_answare(\n",
    "                question = data[\"instruction\"].values[i], \n",
    "                context = context)        \n",
    "            answares.append(answare)\n",
    "        \n",
    "            # pbar.set_description(f\"Treshold: {treshold}, context len: {len(context)}\")\n",
    "            out = f\"{datetime.now()} : {treshold} : {len(context)} : {i}\"\n",
    "            print(out)\n",
    "            with open(f\"{log_dir}rag_test_{mark}status.log\",\"a\") as f:\n",
    "                f.write(out + \"\\n\")\n",
    "            \n",
    "        data[mark + str(treshold)] = answares\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d62d50-d0e6-4964-967e-84a41ccec325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config_gptj = AutoConfig.from_pretrained('EleutherAI/gpt-j-6B')\n",
    "print('EleutherAI/gpt-j-6B', config_gptj.max_position_embeddings)\n",
    "\n",
    "config_opt = AutoConfig.from_pretrained('facebook/opt-6.7b')\n",
    "print('facebook/opt-6.7b', config_opt.max_position_embeddings)\n",
    "\n",
    "config_llama = AutoConfig.from_pretrained('huggyllama/llama-7b')\n",
    "print('huggyllama/llama-7b', config_llama.max_position_embeddings)\n",
    "\n",
    "config_llama2 = AutoConfig.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "print('meta-llama/Llama-2-7b-hf', config_llama2.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cace4ff-abf9-49f6-b3fb-f3afefe3d594",
   "metadata": {},
   "source": [
    "## Test RAG vs Find-Tune Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a44c2b5-d23a-4722-8c36-cf05a508dea4",
   "metadata": {},
   "source": [
    "### GPTJ FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ba3da-56a4-422f-809d-ee6b05348441",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_topic = pd.read_json(f\"{dir_root}test_topic.json\", orient=\"records\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Models/gptj-6b-v20231214/checkpoint-1888\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd92e99-bd69-4b97-9264-45870aae90c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run(test_topic, tokenizer, model)\n",
    "test_topic[\"gptj_fn\"] = results\n",
    "test_topic.to_json(f\"{dir_root}/test_topic_gptj_fn.json\", orient=\"records\")\n",
    "test_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2629235c-83d8-46bd-88cc-7fddd3c6750c",
   "metadata": {},
   "source": [
    "### OPT FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5de7d3-3e26-4283-81a0-ca89785d9217",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_topic = pd.read_json(f\"{dir_root}test_topic.json\", orient=\"records\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-6.7b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Models/opt-6.7b-v20231214/checkpoint-3145\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6d3837-982e-484e-90a3-d2adee0a6cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run(test_topic, tokenizer, model)\n",
    "test_topic[\"opt_fn\"] = results\n",
    "test_topic.to_json(f\"{dir_root}/test_topic_opt_fn.json\", orient=\"records\")\n",
    "test_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0c5dc9-0a0c-41cc-afe0-d11108920b82",
   "metadata": {},
   "source": [
    "### LLama FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcc2db7-334e-48a1-b53a-443dafcad021",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_topic = pd.read_json(f\"{dir_root}test_topic.json\", orient=\"records\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggyllama/llama-7b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Models/llama-7b-v20231119/checkpoint-910\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aeefaf-2a82-47bf-a253-1b6d0e27af3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run(test_topic, tokenizer, model)\n",
    "test_topic[\"llama_fn\"] = results\n",
    "test_topic.to_json(f\"{dir_root}/test_topic_llama_fn.json\", orient=\"records\")\n",
    "test_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2fb23c-b14c-40e3-9463-9bb7a47a3e88",
   "metadata": {},
   "source": [
    "### LLama2 FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e7882d-251e-40c3-b6c4-d9ef50a633bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_topic = pd.read_json(f\"{dir_root}test_topic.json\", orient=\"records\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Models/llama-2-7b-hf-v20231217/checkpoint-1888\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b677f40-1846-4f09-bd60-7add1f8552be",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run(test_topic, tokenizer, model)\n",
    "test_topic[\"llama2_fn\"] = results\n",
    "test_topic.to_json(f\"{dir_root}/test_topic_llama2_fn.json\", orient=\"records\")\n",
    "test_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba1aa06-4b1f-40b2-99b0-fc4deb65adeb",
   "metadata": {},
   "source": [
    "### LLama2 RAG with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe7055e-75f4-4411-9dcc-924dac0743e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_expert = DomainExpert(min_new_tokens=min_token, \n",
    "                             max_new_tokens=max_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ca648c-17c4-4fef-bf37-2775a666bb65",
   "metadata": {},
   "source": [
    "#### Sentece based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ce07ba-db4c-4ff1-a507-def6d77c4fb7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test_topic = pd.read_json(f\"{dir_root}test_topic.json\", orient=\"records\")\n",
    "ebf = ExtractorBaseF(path = dir_root, domain_expert = domain_expert)\n",
    "print(ebf.embedded_vectors.shape)\n",
    "test_topic = rag_test(domain_expert, ebf, test_topic, mark = \"llama2_rag_s_\", N=1000)\n",
    "test_topic.to_json(f\"{dir_root}/test_topic_llama2_base_s.json\", orient=\"records\")\n",
    "test_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d8c78-4ea2-4dfa-9e58-92a706c510cd",
   "metadata": {},
   "source": [
    "#### Question based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea60277-e5a7-4064-aa62-fafd0782809b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test_topic = pd.read_json(f\"{dir_root}test_topic.json\", orient=\"records\")\n",
    "eb = ExtractorBase(path = dir_root, domain_expert = domain_expert)\n",
    "print(eb.embedded_vectors.shape)\n",
    "test_topic = rag_test(domain_expert, eb, test_topic, \"llama2_rag_q_\")\n",
    "test_topic.to_json(f\"{dir_root}/test_topic_llama2_base_q.json\", orient=\"records\")\n",
    "test_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd979643-2ef3-4d18-9f91-276bd7377861",
   "metadata": {},
   "source": [
    "### LLama2 RAG with FN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e872e5b-55b3-4f1c-b0b2-585d42371963",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_expert = DomainExpertV2(min_new_tokens=min_token, \n",
    "                               max_new_tokens=max_token,\n",
    "                               type=type,\n",
    "                               max_input=config_llama2.max_position_embeddings,\n",
    "                               model_chat_name=\"Models/llama-2-7b-hf-v20231217/checkpoint-1888\",\n",
    "                               tokenizer_name=\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d7d8dc-d58a-49a8-b75b-a4ed583b73f7",
   "metadata": {},
   "source": [
    "#### Sentece based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a73eb6-b76c-4c3a-953d-f654decf35b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_topic = pd.read_json(f\"{dir_root}test_topic.json\", orient=\"records\")\n",
    "ebf = ExtractorBaseF(path = dir_root, domain_expert = domain_expert)\n",
    "print(ebf.embedded_vectors.shape)\n",
    "test_topic = rag_test(domain_expert, ebf, test_topic, mark = \"llama2_fn_rag_s_\", N=1000)\n",
    "test_topic.to_json(f\"{dir_root}/test_topic_llama2_fn_s.json\", orient=\"records\")\n",
    "test_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fd311a-737e-4d94-a11f-9c26a9f6d4af",
   "metadata": {},
   "source": [
    "#### Question based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce2e45a-d9b5-4e5a-b2cd-b9eb3a5fef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_topic = pd.read_json(f\"{dir_root}test_topic.json\", orient=\"records\")\n",
    "eb = ExtractorBase(path = dir_root, domain_expert = domain_expert)\n",
    "print(eb.embedded_vectors.shape)\n",
    "test_topic = rag_test(domain_expert, eb, test_topic, \"llama2_fn_rag_q_\")\n",
    "test_topic.to_json(f\"{dir_root}/test_topic_llama2_fn_q.json\", orient=\"records\")\n",
    "test_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad41699-abde-4d2b-9a85-961487f98f7d",
   "metadata": {},
   "source": [
    "### LLama2 RAG with Context FN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90fe5de-c7a1-4091-86fd-480e5ee7c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_expert = DomainExpertV2(min_new_tokens=min_token, \n",
    "                               max_new_tokens=max_token,\n",
    "                               type=type,\n",
    "                               max_input=config_llama2.max_position_embeddings,\n",
    "                               model_chat_name=\"Models/llama-2-7b-hf-context-v20231218/checkpoint-3776\",\n",
    "                               tokenizer_name=\"huggyllama/llama-7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aff2987-9c8e-4a7d-90ce-e715780b40ff",
   "metadata": {},
   "source": [
    "#### Sentece based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b6dff3-7a04-421f-b7e6-b037d58c7aa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_topic = pd.read_json(f\"{dir_root}test_topic.json\", orient=\"records\")\n",
    "ebf = ExtractorBaseF(path = dir_root, domain_expert = domain_expert)\n",
    "print(ebf.embedded_vectors.shape)\n",
    "test_topic = rag_test(domain_expert, ebf, test_topic, mark = \"llama2c_rag_s_\", N=1000)\n",
    "test_topic.to_json(f\"{dir_root}/test_topic_llama2c_s.json\", orient=\"records\")\n",
    "test_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5151edd-3bd6-4f5f-b203-aba21b67a31e",
   "metadata": {},
   "source": [
    "#### Question based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe615254-2770-4689-b79d-a2268c52b4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_topic = pd.read_json(f\"{dir_root}test_topic.json\", orient=\"records\")\n",
    "eb = ExtractorBase(path = dir_root, domain_expert = domain_expert)\n",
    "print(eb.embedded_vectors.shape)\n",
    "test_topic = rag_test(domain_expert, eb, test_topic, \"llama2c_rag_q_\")\n",
    "test_topic.to_json(f\"{dir_root}/test_topic_llama2c_q.json\", orient=\"records\")\n",
    "test_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a91dfbb-1430-49f7-b2f5-e2007de62498",
   "metadata": {},
   "source": [
    "### LLama2 Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c37ca8b-0324-4f15-9918-26da6a2c4420",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_topic = pd.read_json(f\"{dir_root}test_topic.json\", orient=\"records\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c554e5fa-2bc5-4738-8c00-89ae1ba63351",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run(test_topic, tokenizer, model)\n",
    "test_topic[\"llama2_base\"] = results\n",
    "test_topic.to_json(f\"{dir_root}/test_topic_llama2_base.json\", orient=\"records\")\n",
    "test_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d555a369-b9cf-4ed8-bb46-7a2287f1c6de",
   "metadata": {},
   "source": [
    "## Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3918962e-7595-4ab0-8bc3-7fb09472a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_topic = pd.read_json(f\"{dir_root}test_topic.json\", orient=\"records\")\n",
    "test_topic_llama2_base_s = pd.read_json(f\"{dir_root}test_topic_llama2_base_s.json\", orient=\"records\")\n",
    "test_topic_llama2_base_q = pd.read_json(f\"{dir_root}test_topic_llama2_base_q.json\", orient=\"records\")\n",
    "test_topic_llama2_fn_s = pd.read_json(f\"{dir_root}test_topic_llama2_fn_s.json\", orient=\"records\")\n",
    "test_topic_llama2_fn_q = pd.read_json(f\"{dir_root}test_topic_llama2_fn_q.json\", orient=\"records\")\n",
    "test_topic_llama2c_s = pd.read_json(f\"{dir_root}test_topic_llama2c_s.json\", orient=\"records\")\n",
    "test_topic_llama2c_q = pd.read_json(f\"{dir_root}test_topic_llama2c_q.json\", orient=\"records\")\n",
    "\n",
    "test_topic_gptj_fn = pd.read_json(f\"{dir_root}/test_topic_gptj_fn.json\", orient=\"records\")\n",
    "test_topic_opt_fn = pd.read_json(f\"{dir_root}/test_topic_opt_fn.json\", orient=\"records\")\n",
    "test_topic_llama_fn = pd.read_json(f\"{dir_root}/test_topic_llama_fn.json\", orient=\"records\")\n",
    "test_topic_llama2_fn = pd.read_json(f\"{dir_root}/test_topic_llama2_fn.json\", orient=\"records\")\n",
    "test_topic_llama2_base = pd.read_json(f\"{dir_root}/test_topic_llama2_base.json\", orient=\"records\")\n",
    "\n",
    "test_topic[\"gptj_fn\"] = test_topic_gptj_fn[\"gptj_fn\"]\n",
    "test_topic[\"opt_fn\"] = test_topic_opt_fn[\"opt_fn\"]\n",
    "test_topic[\"llama_fn\"] = test_topic_llama_fn[\"llama_fn\"]\n",
    "test_topic[\"llama2_fn\"] = test_topic_llama2_fn[\"llama2_fn\"]\n",
    "test_topic[\"llama2_base\"] = test_topic_llama2_base[\"llama2_base\"]\n",
    "\n",
    "for r in [\"llama2_rag_s_\" + str(round(treshold,1)) for treshold in np.arange(1.0, -0.1, -0.1)]:\n",
    "    test_topic[r] = test_topic_llama2_base_s[r]\n",
    "\n",
    "for r in [\"llama2_rag_q_\" + str(round(treshold,1)) for treshold in np.arange(1.0, -0.1, -0.1)]:    \n",
    "    test_topic[r] = test_topic_llama2_base_q[r]\n",
    "\n",
    "for r in [\"llama2_fn_rag_s_\" + str(round(treshold,1)) for treshold in np.arange(1.0, -0.1, -0.1)]:\n",
    "    test_topic[r] = test_topic_llama2_fn_s[r]\n",
    "\n",
    "for r in [\"llama2_fn_rag_q_\" + str(round(treshold,1)) for treshold in np.arange(1.0, -0.1, -0.1)]:\n",
    "    test_topic[r] = test_topic_llama2_fn_q[r]\n",
    "    \n",
    "for r in [\"llama2c_rag_s_\" + str(round(treshold,1)) for treshold in np.arange(1.0, -0.1, -0.1)]:        \n",
    "    test_topic[r] = test_topic_llama2c_s[r]\n",
    "\n",
    "for r in [\"llama2c_rag_q_\" + str(round(treshold,1)) for treshold in np.arange(1.0, -0.1, -0.1)]:    \n",
    "    test_topic[r] = test_topic_llama2c_q[r]\n",
    "\n",
    "test_topic.to_json(f\"{dir_root}/test_topic_gen.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0378aa0d-3304-4e9a-a86f-b09c2aebf225",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_topic_llama2_fn_s = pd.read_json(f\"{dir_root}test_topic_llama2_fn_s.json\", orient=\"records\")\n",
    "test_topic_llama2_fn_q = pd.read_json(f\"{dir_root}test_topic_llama2_fn_q.json\", orient=\"records\")\n",
    "test_topic = pd.read_json(f\"{dir_root}test_topic_measured.json\", orient=\"records\")\n",
    "\n",
    "for r in [\"llama2_fn_rag_s_\" + str(round(treshold,1)) for treshold in np.arange(1.0, -0.1, -0.1)]:\n",
    "    test_topic[r] = test_topic_llama2_fn_s[r]\n",
    "\n",
    "for r in [\"llama2_fn_rag_q_\" + str(round(treshold,1)) for treshold in np.arange(1.0, -0.1, -0.1)]:\n",
    "    test_topic[r] = test_topic_llama2_fn_q[r]\n",
    "\n",
    "test_topic.to_json(f\"{dir_root}/test_topic_measured.json\", orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SuperExpertRAG]",
   "language": "python",
   "name": "conda-env-SuperExpertRAG-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
