{"instruction":{"0":"What is the acronym for UVGI?","1":"What is the most likely cause of Rift Valley fever?","2":"Which country had a large impact on the reduction in COavid-19?","3":"What kind of data do the Laplacian constraints have?","4":"What is a newer application for machine learning?","5":"How many test scores did colleges promise to not penalize?","6":"What type of interpretation strategy was developed in the project?","7":"What is a potential way to safely resume some face to face teaching?","8":"How many more important than intrinsic features of the pandemic are there?","9":"Which is not a requirement of the B-Net, quality extraction or gene translocation?","10":"What has DIMY done to protect the devices and servers?","11":"How many more original messages were reduced compared to summary claims?","12":"How many statistical data does the COVID-19 model use?","13":"What are some of the important aspects that need to be addressed from the onset?","14":"Which type of data is needed to support the theory of the radius valley?","15":"What is AGenT Zero?","16":"How many months did it take for the MERS incident cases to spike from a handful to 260?","17":"How many leafs are in a tree with three and four leafs?","18":"How does the data from these two locations show that they have stronger social ties?","19":"Which mechanism is most efficient within $1.04,_{2}$ (S-cluster)?","20":"What are the two challenges the article describes?","21":"Which model is more suited for extractive question answering?","22":"How many more Sy1s have broad Balmer lines than narrow and broad lines?","23":"How many samples are used in the proposed method?","24":"How many generic and all-encompassing structured structured methodologies are there?","25":"What has the disruption caused by the pandemic called into question?","26":"Which field is the research in which Ethical User stories are being focused on?","27":"Which is more cost-efficient, MPA or MPA?","28":"How many machine learning algorithms does the paper propose?","29":"Which case would have higher migration rate, ICP or IPP?","30":"How do cooperative bandits maximize group regret?","31":"What is the first layer of the governance model?","32":"What was detected by the Laser Interferometer on February 24th?","33":"How is the process of identifying relevant keywords and posts difficult?","34":"How much less does FGS reduce the search effort for tool construction?","35":"Which is the least important aspect of the feedback system?","36":"How many days ahead was the growth rate of COavid-19 infection?","37":"How many types of infections does the CoVid-19 spread?","38":"What is the most reliable way to obtain reliable and accurate nowcast for IGAE?","39":"How many marketing contexts have implications for the transparent model?","40":"How are smart grid systems modeled?","41":"How many deep neural networks are needed to generate faster molecular generative model?","42":"What is the goal of the insurer?","43":"Which did not have a great success in recent years?","44":"How many ways do you combine the effects of a regression model?","45":"How many more software practitioners were surveyed than were surveyed?","46":"Which data is not included in AICov, case and death data or death data?","47":"What is the name of the CXR imaging?","48":"How many months did it take to evaluate Visilant?","49":"Which is the least reliable of the methods?","50":"What is the most important aspect of the model?","51":"Which areas of Italy have the lowest performance?","52":"Which type of reasoning approach is used for reinforcement learning?","53":"Which gender group had higher levels of academic achievement, cis gender women or cis gender men?","54":"Which system's dynamics of the modern grid changes significantly?","55":"What did the men do to try to increase the appeal of a larger audience?","56":"Which of the four states issued the lockdown orders on the same day?","57":"How long did it take to collect the large-scale databases?","58":"Which methods require only partly labeled data?","59":"What is the best way to transfer an online search based model for influenza-like illness?","60":"What are the three major guidelines for the Colombian government?","61":"How many change points does the procedure allow the user to test for?","62":"Which type of machine learning algorithms are most likely to spread fake news?","63":"What is MARE a framework for?","64":"How many more millions of token Penn Treebank are there than the standard 1 million token Penn treebank?","65":"How many percent did vaccination rates rise?","66":"What is a possible use of a server?","67":"Which application would have lower cost, chest X-rays or facial attribute estimation?","68":"What is the infinite memory interacting P.'o}lya contagion network?","69":"How can the problem be solved in linear time?","70":"How many more BBD\/kWh does the system's levelised costs of electricity increase in cost-optimal scenarios than increase in 100% renewable systems?","71":"How many more new cases and deaths were there in the world with one week later intervention compared to new cases?","72":"What is Spark big-data?","73":"How many years are the age-specific COavid-19 associated deaths?","74":"How many ways of analysing the distance and similarity matrices are there?","75":"Which is the most important part of the survey?","76":"How many of the volunteers did not complete the online survey?","77":"How many variants does the DPR have?","78":"How many organizations provide the Zoltar web application?","79":"What is the primary problem we discuss?","80":"How many points is the difference between the highest density posterior interval and the lowest density?","81":"How would multiband observations of black holes be able to constrain the parameter space of modified theories of gravity beyond general relativity?","82":"How many nodes and billions of edges does GraPE have?","83":"Which is a key difference between the large data backbone formed by the IoT and the slowest deployment of the device?","84":"How many years did it take for Bangladesh to experience security threats?","85":"How many more records were collected in the simulation study than the data recorded?","86":"What is the first step towards building a continuous health estimation system?","87":"Which is more likely to be a cause of pandemics, Viral infections or contagious infections?","88":"How many layers of data were used to create the cycle-organ?","89":"Which type of model is used for demand forecasting?","90":"How many more members are there in the data on extremeBB than on the offline forums?","91":"How many neural networks are involved in the project?","92":"How many more US dollars is the saving of a year under current market prices in Saudi Arabia compared to current wind energy prices?","93":"What is the name of the Germanium Material and meteorite Screening Experiment?","94":"What type of situation is trivially non contextual?","95":"How many points is the primary aim of the proposed framework?","96":"How many companies are involved in the COV-19 vaccine market?","97":"What is the primary concern with gradient-based attribution methods?","98":"How can we reduce the pain-staking colour editing task?","99":"How many times have traffic accidents shifted in the past?","100":"What is the purpose of visualizations?","101":"Which is an example of a user who has a university degree in machine learning or related fields, one user or another user?","102":"Which type of simulation is not suitable for the design of a naturally ventilated building?","103":"What are the main areas of AR?","104":"What are the basic principles of quantum computing?","105":"Which alerts do not improve users' accuracy or experience?","106":"How is the dynamic evolution of COVID-19 cased?","107":"What determines the value and difficulty of discovering its answer?","108":"How many people have died from the COVID-19 pandemic?","109":"What is plasma?","110":"How many AI practitioners were conducted in total?","111":"What is the primary concern of this article?","112":"Which is a newer technology, ML or AI?","113":"How many masses connected by a string?","114":"How many ways was the COavid-19 system built?","115":"Which model had local optima, minima or monotonously varies with the mixing rate?","116":"What is one of the most important privacy features of online Social networks?","117":"Which scenario would have less complex data integration, coronavirus data integration or machine log integration?","118":"How many total clips are in the Deep Fake detection model?","119":"What are the main digital public health applications?","120":"What is the EOSCArchitecture report?","121":"Which conditions were included in the study?","122":"How many health conditions can be vastly improved by the collected data?","123":"Which city did not have a CFA?","124":"Which is more likely to propagate in a broad-first manner?","125":"How many monolingual models did the study compare?","126":"Which algorithms would be most difficult to detect on public transport?","127":"How many more unique users on COavid-19 are there than those who mention Nobelist names?","128":"How many X-ray images were collected?","129":"How many cities in India does the data from the analysis show there is significant inequality?","130":"What is the TDA?","131":"Which country had a higher desktop web search share, Germany or Switzerland?","132":"How many months did it take for the RSEs to provide guidance and advice regarding the performance, portability to new architecture, and scalability of selected applications?","133":"What is the lowest estimate of the rate of contact rate?","134":"How many more unique users were there compared to the analysis of over 600,000 Tweets?","135":"Which is larger, the Graph-based Pyramid Global contextual information or the size variation problem?","136":"How many percent is the reduction of cases per 100000 inhabitants?","137":"How many challenging benchmark databases did we use for crowd counting?","138":"How many sessions are the participants involved in the DBN model?","139":"Which graph is highly regular, level 2 or level 3?","140":"What is the input of a quantitative readout?","141":"How many types of vaccines are there in Hungary?","142":"How many countries are on the topological map?","143":"Which type of model is more appropriate for medical images, self-supervised learning or generic self supervised learning?","144":"How many other substellellas have been confirmed by direct imaging around main-sequence stars in Hyades?","145":"Which virus is responsible for the 1957-1958 asian flu pandemic?","146":"How are viruses transmitted?","147":"How many extra dimensions were added to a black hole?","148":"How many samples showed that the proposed Sharp U-Net model did not match the recent state-of-the-art baselines?","149":"Which type of attacks are not vulnerable to?","150":"How is joint training achieved?","151":"How many analogues are used to test the presented measure?","152":"How many networks are there?","153":"How many different ways can the data be quantified?","154":"How do we gather the data on thefly?","155":"Which is a more reliable way to collect diverse experiences, reinforcement learning or videos of humans?","156":"What is the marginal treatment effect?","157":"How many more distinct identified depression users are there than past Tweets?","158":"What is the name of the pandemic?","159":"Which is more flexible, the projected regression or the actual model?","160":"How many electron microscopic images are in the Sars-CoV-2?","161":"What is the estimate error rate for the COavid-19?","162":"Which is less effective, mass spectrometry of intact or viruses?","163":"Which methods were used to collect the data?","164":"How many more points does the BERT model have over the macro-F1?","165":"How can a student improve their assessment tools?","166":"Which city had a longer time frame for the traffic evaluation method?","167":"Are the visual transformations on the virtual avatars more efficiently or less efficiently?","168":"Which is a bad example of artificial intelligence, a chatbot or a dialogue system","169":"How many sets of magnetic resonance images are used to train the network?","170":"How do we estimate the consumption rate of new subscribers who joined just prior to launch?","171":"How many years after the first epidemic coronaviruses did the new coronavirus appear in Wuhan?","172":"What is a key technology component in the evolution towards cognitive radio?","173":"What is the most common type of quantum device?","174":"How does the architecture transform the original multivariate time series into multiple cascading unifier time series?","175":"Which pandemic has caused an increase in mental health issues?","176":"Which is a key factor in reproducibility of scientific pipelines?","177":"Which curtains had a lower amount of concentrated aerosol in the room?","178":"In which stage of the healthcare system will we create an AI doctor?","179":"How many terms satisfying a stochastic imposed gradient condition are there?","180":"How many months after the COVID-19 epidemic was listed as a public health emergency of international concern by the WHO was the screening result obtained?","181":"How many aspects of a healthy GNN are there?","182":"How does our FLO estimator operate?","183":"What is a cyber attack?","184":"What is the name of the model that suggests CBP efficacy may rest on an efficient and distributed global sampling scheme?","185":"How is the policy evaluation process carried out?","186":"Which group is more likely to re-open the economy?","187":"How many more depth sensors are needed to validate our approach?","188":"How many participants in the 16- participants study had a task task execution?","189":"How many more manuallyated videos were in the video dataset than the recorded?","190":"How many entities are there?","191":"Which type of computing would less likely be used in the implementation of the architecture, the cloud-based approach for avatar physics emulation or the legacy computing paradigm?","192":"How many days was Ab gradcon 2021 held?","193":"Which methods are less effective for node classification tasks, graph Convolutional networks (GCN-SE) or graph probability ( GCN-se)?","194":"What type of algebras are included in the generalized Cartan matrix?","195":"What is the underlying network hidden over?","196":"How many search strategies did we compare to other search strategies?","197":"Which categories of metadeta did not report recruitment details or participants compensation?","198":"Which satellite is older, Deimos or Phobos?","199":"Which state's interactions would have favored the emergence of tight consensus, Alabama or Alabama?","200":"How many mobile phones are in the US?","201":"What is the purpose of federated learning?","202":"How many years after the coronavirus appeared in Wuhan did it spread out worldwide?","203":"How many examples of workplace productivity wearables are given?","204":"What has the technology done to the society?","205":"How is the effective magnetic field measured?","206":"What is the purpose of FedMix?","207":"What is the study focused on?","208":"Which type of data are GNNs not good at?","209":"How many more data points does the 2 step algorithm scales compared to the regression problem?","210":"What does XNN stand for?","211":"Which architecture would have less efficient convolution operation, three-tire architecture or one-minute length respiratory signals","212":"Which pandemic is mentioned?","213":"How many of the 9 dataset did not have visual search patterns?","214":"Which encoding parameters were obtained from the Pareto front space?","215":"Which language has a higher score, Malayalam or English?","216":"How many more students use Argo Lite compared to those at Georgia Tech?","217":"How does BCNN improve accuracy?","218":"Which model is more accurate for short-term forecasting horizons?","219":"How many ribbons of polymeric melt are on each other?","220":"How many days does the epidemic double every 2.4?","221":"How is a substantial improvement obtained?","222":"What are the three modalities of the model?","223":"Which is the least difficult to access and extract data from?","224":"How many more infection opportunities will the increasing testing frequency of random samples of the population reduce?","225":"Which type of model would most likely be used to classify healthy persons, 1D or 2D?","226":"How many years has MPTCP been under active development?","227":"How many people have lost their jobs?","228":"Which type of intervention is less common?","229":"How does the model help to predict the future?","230":"What is the main indicator on the extent of testing the population?","231":"What is one of the major reasons science has benefited?","232":"How many ways are we able to solve the BSPRCMMR efficiently?","233":"Which model is most common to fit a linear regression?","234":"How many case studies were used to quantify and understand the relative risk associated with transmission pathways?","235":"How many patients have confirmed COV-19?","236":"How many nations have reduced their levels of tropospheric nitrogen oxides?","237":"Which is a less popular direction in machine learning, deep learning or artificial intelligence?","238":"How many student ANtonia were involved in the study?","239":"Which category of self-harming users is most likely to proliferate on Twitter?","240":"What is the most common mistake researchers make?","241":"Which score is used to select the most useful topics?","242":"How many months does it take for the data collected and analysis to be updated?","243":"What is the most important piece of data in this paper?","244":"What is a shortcoming of the Health system?","245":"How many conditions would a drone detect?","246":"What are tableau used to create?","247":"How many more respondents are concerned about being managed by AI than those who are not?","248":"Which task would have less difficulty in learning algorithms, task or task?","249":"How many years does the data on avocado prices span?","250":"What was designed to be more restrictive?","251":"How many types of sensitivity analyses were conducted?","252":"How many more requests were satisfied in the best case of native deployment than the least successful?","253":"What is the technique for protecting privacy in camera based systems?","254":"How many countries would tracking and controlling the spread of a virus save?","255":"How many sample data are included in the description of Media Cloud?","256":"What branch of physics is the main focus on?","257":"What is the name of the project that is trying to spread laboratory practice among students and teachers?","258":"How many years did it take for the publishing trends to change?","259":"What is the name of the virus that causes respiratory illness?","260":"Which type of learning is less prevalent for information cascade modeling?","261":"How many questions were asked in the survey?","262":"How many percentage points difference is there between the performance of the WCE and the state-of-art methods?","263":"How many more pooling strategies are employed than Dope?","264":"What should NASA do to ensure a more inclusive and sustainable planetary science research community?","265":"How many years did it take for the GDP to drop 11%?","266":"How does the model of wealth densities vary?","267":"What is the new design of the medical mask?","268":"Which ethical factor should be used to monitor social distancing?","269":"How many stages of change point detection are there?","270":"How many cm was the difference between the longest and shortest data?","271":"What is the purpose of the distributed industry 4.0?","272":"What is expected to be the norm of learning globally in the next few years","273":"What is the abbreviation for the basic reproductive number of recent outbreaks?","274":"Which model did not use linear-Gaussian random-walk model?","275":"Which model is less likely to generate a computationally expensive task, agent-based model orDSGE model?","276":"How do we calculate infection rate and recovery rate?","277":"How many confirmed cases are there?","278":"Which use cases are PKG developing in?","279":"How many more frequencies does a 6G application need than a 5G application?","280":"Which of the following is the most accurate statement?","281":"Which groups are included in the existing ecosystem of spatial and spatio-temporal data data analytic?","282":"What does the present paper show?","283":"Which data is the Allen Institute studying?","284":"Which model is trained using the features, Tabnet or Tabnet?","285":"How many tasks does SMedBT significantly improve?","286":"How many times a year is the number of confirmed cases increasing?","287":"What does the model use to handle long and variable time delays?","288":"What does a good corporate reputation buffer?","289":"Which country is the average number of deaths over the last five years?","290":"Which layer is used to carry out the task of the HGR?","291":"How many primary studies were published from 2006 to 2020?","292":"What is U-net mainly used for?","293":"What is the name of the disease that is a pandemic?","294":"Which company's algorithms are more superior for items with a long history?","295":"Which dimensions were examined via a set of metric?","296":"What does SEIRD include?","297":"Which HRI experiment design is designed to allow researchers to conduct HRI Augmented Reality studies without being in a real environment?","298":"Which group had the highest F1, the best classifier or the lowest one?","299":"What is the gross fixed capital formation?","300":"What is the purpose of the distributed denial of service (DDoS)attack vector?","301":"How do we calculate the risk ratio for the marginal probabilities?","302":"How many layers of the network are there?","303":"How long has the Energetic Particle Detector been in operation?","304":"How are face recognition models modified?","305":"What is the assumption we make about the general method?","306":"How many types of predictors does the R package have?","307":"What is the main difference between the one-size-fits-all approach and the two-size tolerance approach?","308":"Which is the least widely used classification model?","309":"Which was a promising approach, sequencing or Thompson sampling?","310":"Which task would be the best to solve, finding the prevalence of harmful MemEs aNd Their tArgets or MOMENTA","311":"How many states does the suIR model divide the epidemic into?","312":"Which is used for forming and interpreting different features from these databases?","313":"How many more senseful embeddings were there on the WiC dataset than on the NES?","314":"What is the contribution from purchasing goods and services to IRAP's carbon footprint?","315":"What is the relationship between the vertical component $W$ and randomness?","316":"Which is the problem with machine learning models?","317":"How many years after parenti published their study did astrophysicists have to relocate?","318":"Which methods are used to handle survival outcome data?","319":"Which concepts lead to false positives, which ones lead to correct diagnosis or false ones?","320":"Which model is optimised to use call data to forecast the number of daily confirmed cases?","321":"What is one of the primary reasons for the increased screen time?","322":"What is the RPA?","323":"What is the main reason behind the APS statement?","324":"How does the COvy-19 pandemic affect our lives?","325":"How many sets of definitions are included in the Top Down Algorithm?","326":"How many months did it take to investigate the dynamics of the clusters over time?","327":"How many parts are in the IBO algorithm?","328":"How many of the tasks that a service robot can perform at home involve navigation skills?","329":"How many people are in the SEIR model?","330":"What is the goal of city planning?","331":"How long does it take to construct a model?","332":"Which model is more complex, the compartmental SIR model or the SIRP model?","333":"Which is losing popularity?","334":"What is the maximum number of patients in a holdout set?","335":"How does the stochastic model for football leagues work?","336":"How many groups were used to construct the dataset?","337":"Which model is more accurate for biochemical research, GNNF or GNNP?","338":"What type of data was collected from the collected data?","339":"Which fires occurred last?","340":"How many key components are included in the compartmental model?","341":"How many methods are used to monitor networks?","342":"How many methods were used to perform this analysis?","343":"What is the main concern of the epidemiological logistics model?","344":"Which country's sportspersons are less likely to publicly criticize politicians?","345":"How many points did our model improve by using a temperature data set?","346":"How many years has the COID-19 epidemic been happening?","347":"How many more times more reach does Perla have than a traditional form-based questionnaire?","348":"Which data sets would most likely not be used in the BOCPD algorithm, real-time basis or synthetic data sets?","349":"Which type of x-ray does not have the greatest magnification factors, cone-beam X-ray transmission imaging or the CBCT?","350":"How do we demonstrate that our model improves performance of existing processes?","351":"Which sub-group of modalities is designed to be inefficient?","352":"How many more viruses were spread in a city than in a network of cities","353":"How many years does the ARD cointegration approach last?","354":"Which is the most accurate interpretation of the data?","355":"Which does not require large-scale data?","356":"What does the literature have to do with AI ethics?","357":"How many devices have the potential to unlock their vast sample handling and processing operation space?","358":"What type of vision do we use to observe hand-object interactions?","359":"Which study had a decreased attention and decision behavior, study 1 or study 2?","360":"How many years has the IIT Kanpur Consulting Group been working on the CO Polo-19 crisis?","361":"How do we know more about high frequency digital asset returns?","362":"How many more mV per molar decade is the dynamic range of 4 orders of magnitude than the linear sensitivity?","363":"What is one of the reasons why face recognition systems are negatively impacted?","364":"What has been increasing in the last few years?","365":"How many days after the first reported case was there a lockdown established?","366":"How many subjects had a lower accuracy than 50%?","367":"What is the goal of economics?","368":"What type of interactions are Emora designed to support?","369":"What is the most likely cause of cyber attacks?","370":"How many predictors did the Cox model have?","371":"What are the two main reasons for the creation of COVID-19?","372":"How many years did it take for the data to be examined?","373":"What type of model is pre-trained language model?","374":"How many features are discernible within traces received at the VoIP destination?","375":"Which experiment had less realistic simulation algorithms, the first or the second?","376":"Which is the most accurate prediction?","377":"Which approach is superior in parking occupancy forecasting?","378":"How many days did the two day workshop on Virtual Meetings last?","379":"Was the study focused on listeners of the radio news broadcast in Awka, Nigeria or the use and gratification theory?","380":"Which is more important, software must comply with the Medical Device Regulation or the Medical device Regulation?","381":"How many teams have registered and contributed to the three task?","382":"What has a promising data source to monitor human behavior?","383":"What are some of the main applications that have been applied to full- Stack clinical applications?","384":"What is the cause of the spread of COavid-19?","385":"How many more people were in Seoul, Republic of Korea than in New Jersey?","386":"Which is cliophysics named after?","387":"How many more tiles are in a 3-layer earth map family than in a one-sided family?","388":"How many factors are included in the closure formula?","389":"How many more times can we capture cross-series correlation between interacting time series?","390":"Which data is included in the joint model?","391":"What is the main problem in Magnetic resonance imaging?","392":"How many languages are included in the new XL-BEL task?","393":"Which search terms were searched for in Europe and America?","394":"How is the pandemic spread?","395":"How many years does data corresponds to the period from 28 of June to 13 of December 2020","396":"What pandemic is the COVID-19 pandemic?","397":"How many more people were able to accurately estimate the number of people than the conventional method?","398":"How many methods have been used to diagnose COV-19?","399":"How many more high-quality designs were there than standard methods?","400":"Which two methods are used to report the results?","401":"Which is more difficult to discover, novel inhibitor molecules for emerging drug- targets proteins or potential ligands?","402":"What is it important to provide users with?","403":"What is the minimum value for the long-term behavior of the disease?","404":"What is the main concern of the report?","405":"Which is the most current device for the device?","406":"How many users were used in the large-scale data analysis?","407":"Which is more sensitive to data quality and quantity, federated learning or game theory?","408":"What is the name of the problem in the literature?","409":"Which pandemic has caused the rise of inaccurate information?","410":"What is the process of making a dataset fit-for-use and archiveable?","411":"What is a disadvantage of including protected attributes in prediction models?","412":"How many sub-corpora of questions and answer are there?","413":"What is a major problem with data science?","414":"What pandemic was caused by the virus 2?","415":"Which field is the latest trending research in the field of AI and Data Science?","416":"What is the diffusive epidemic process?","417":"How many databases does DBLP contain?","418":"What is the main benefit of the modular implementation?","419":"What is the difference between Rapid antigen detection and PCR tests?","420":"Which news aggregator has a lower chance of developing apophenia?","421":"What is the minimum amount of effective effective charges?","422":"How many more ordinary differential equations are there for the number of nodes and edges than for the network model?","423":"Which group had a higher overall performance?","424":"What is the minimum for complete convergence?","425":"Which news network does this paper focus on?","426":"How do CNNs get more robust?","427":"What do we aim to develop in our courses?","428":"Which is a less reliable method of decontamination, ultraviolet C (UVC) radiation or occupancy detection?","429":"Which scheme can adapt to any existing encoder, configuration to deal with different encoding requirements or the network condition","430":"How many top 5 European soccer coaches are there?","431":"How many different models are presented in the model?","432":"How many different stages does a conventional study of fluid simulation involve?","433":"Which is the most likely reason for the increase in traffic accidents?","434":"How many evolutionary algorithms have been introduced?","435":"What is the relative risk for g1,g2?","436":"What is the process of renewing ties called?","437":"Which task would the task of stance detection benefit from, feature based information or ensembling?","438":"How many clients did we distribute the data among?","439":"Which was used to detect motion interference, radio-frequency or IR-U WB?","440":"Which of the four obligate Stem Loops has a higher fold-back duplex, the 3-UTR or the 4-UTran?","441":"Which model is most likely not documented in the context of cryptos?","442":"How many test dataset did we conduct?","443":"Which application would most likely not support parameter estimation, the one we propose or the other one we proposed?","444":"How many more times can FastGAT reduce the computational time and memory requirements than it can reduce?","445":"What is the general reproduction number of the model?","446":"How is the PSGR model first constructed?","447":"How many features are needed to make the trolley collection capable of collecting trolleys?","448":"How many degrees of freedom are the randomness allowed to possess?","449":"Who is more likely to upskill in general, users with postsecondary degrees or users with high school degrees?","450":"How many months did the daily data on confirmed cases of COV-19 last?","451":"What is the name of the counterfeit coin puzzle?","452":"What was the percentage of the dose reduction in ULD chest CT?","453":"How many components does the IID assumption have?","454":"How many published contact trace apps are there?","455":"How many spacetime dimensions are required to support the expansion of the momentum amplituhedron?","456":"How many more local gradients are there than local gradulations?","457":"Which city has a lower user ratio?","458":"Which is a major threat to the financial system?","459":"Which area is used for creating the mask?","460":"What is a factor of two smaller than the lowest values?","461":"Which observation is described as `anisotropic quenching'?","462":"Which model would have lower eigen value of the mixing matrix, SIR or standard SIR?","463":"How many dimensions are there in the reported theory?","464":"How many broad categories does I sort into?","465":"How many participants are in the Revisal Extraction task?","466":"Which distribution had a lower likelihood of failure?","467":"How many of the selected publications were not critically appraised vis-a-vis the key themes?","468":"How many more studies were selected out of n=605 collected from the academic databases Ei Compendex, ERIC, and In Spec than were carried out in two","469":"Which group is more likely to be affected by the introduction of e-cigarettes?","470":"How many countries did the study take place?","471":"How many of the 23 studied have strain amplitudes that are lower than the limits calculated from their electromagnetically measured spin-down rates?","472":"Which model is used to study asymptotic behavior of solutions of the migration epidemics models?","473":"What has the growth of online Digital\/social media allowed people to do?","474":"Which type of news is more contagious?","475":"What group is at the highest risk for the pandemic?","476":"How many groups are there?","477":"How can the energy cost of the vehicles be recovered?","478":"How do we test a vaccination strategy that prioritizes the members of large families?","479":"What type of risk do small businesses face?","480":"How is the kinetic transport model solved?","481":"Which pandemic is the SARS-CoV-2 infectious outbreak?","482":"Which group had a lower chance of changing their behavior, multi-exposure or extreme change groups?","483":"Which is the most important part of the universe?","484":"What are the three types of public health policies?","485":"Which was the first machine learning model to predict survival odds on Titanic?","486":"Which data sources would most likely be used in a semi-supervised setting, labeled or unlabelled?","487":"Which library offers concise syntax to write down processing pipelines?","488":"How many databases are generated for the two aircraft configurations?","489":"How many explanations are there for the principles-to-practices gap?","490":"Which type of model is used to analyze GNNs?","491":"Which distribution is used for the odds of positive cases, Pareto distribution or Generalized Pare To distribution?","492":"What is the minimum value of the value of g(k)$?","493":"Which is an example of a combination of attributes, a young person with gray hair or women with beards?","494":"Given a model has a set number of values for which there may be infinitely many values, such parameters are called what?","495":"What type of model is used to predict an infection curve for Cuba?","496":"What is the main concern about urban networks?","497":"Which methods are used to calculate high density?","498":"What are some use cases that are important to the architecture?","499":"What is the abbreviation for the SIRD model?","500":"How many lattice gas versions of the model were applied on hybrid lattices?","501":"How many states are assumed to be Markovian?","502":"How many days was the peak for the new data?","503":"Which questions do we need to answer in this review?","504":"How many more observations does the model have in the tails than in the linear model?","505":"Which news site had a higher readers engagement, news publishers or fake news sites?","506":"What are the key words that the research paper lists?","507":"How many search engines does our final search engine have?","508":"Which type of architecture is not ideal for the Vision Transformer?","509":"What is the third of the third DIHARD challenge?","510":"What is the most important aspect of the VB?","511":"How many metropolitan areas in the United States are in the new statistical model?","512":"Which pandemic is an example of a tool that can be used for communication between genomics and infectious disease epidemiology?","513":"What are some ways we can mitigate the spread of fake information?","514":"How many more percent of the best decision tree model achieve sensitivity and specificity than specificity?","515":"What is the current method of contact trace?","516":"What model is described as being deterministarily detergent?","517":"How many years after the pandemic outbreak did vaccines become accessible?","518":"How many years did the pandemic last?","519":"What type of model is used for data analysis?","520":"Which is less desirable, selective optic detection of Sars-CoV-2 aerosol or the integrated Air Quality Monitoring System?","521":"How many times have the cavities studied been limited to cw accelerating fields?","522":"What is the purpose of the Age of Incorrect Information?","523":"How many years did it take for the number of databases described in db.org to grow from 500K to 30M?","524":"How many points difference is there between the positions and heights of a first peak?","525":"How many proteins are active?","526":"How can we make FOSS findable, accessible, and reusable?","527":"Which is the least important data source for the researchers?","528":"Which is a popular model for statistics and machine learning?","529":"How many countries have to take quarantine measures?","530":"How many small-to middle-scale cities were epidemic hotspots?","531":"How long is the lag for the asymptomatic infection?","532":"What is the main purpose of the Echo-Ch Chambers?","533":"Which modes of coughs, fever, and shortness of breath are less prevalent?","534":"Which task has gained popularity in the natural language processing field?","535":"What is the name of the AI-ABA platform?","536":"Which is a fundamental problem in bioinformatics, requiring to reconstruct a source genome from an assembly graph built from a set of reads (short strings sequenced from the","537":"What is the name of the two-phase dynamic contagion process?","538":"Which model could be used to estimate the droplet evaporation rate?","539":"How many groups can adaptive sampling protocols be estimated in?","540":"How many regular seasons are in the NBA?","541":"Which is an integral part of a digital twin?","542":"Which type of ventilators are low-cost, low-complexity signal processing algorithm, pressure-cycled pneumatic ventilator, or commercial ventil","543":"How many things are critical for medical supply and treatment?","544":"What are some of the issues we have discussed with the ACT concept?","545":"What is the minimum number of infected individuals?","546":"How many spectral types are there?","547":"Which galaxy has a lower heat and gas reservoir, the secondary galaxy or the central region?","548":"How is the impact of the mobile network operator evaluated?","549":"How many preprocessing techniques were introduced?","550":"What is the assumed growth rate of generation time?","551":"How many papers are in the statement?","552":"How many years is the time span for the prediction model?","553":"What type of conditions can be sparse or non-existent?","554":"What is Schuss well known for?","555":"What was the EC's response to the paper?","556":"Which classifier has a higher accuracy, Random Forest or Random Forest?","557":"What pandemic affected the transition to remote labs?","558":"What are some of the factors that predict treatment without being balanced?","559":"How many months does the Weibo-COV take to complete?","560":"How many of the five bounce forward resilience preconditions are not local expertise?","561":"How many scans were in the multi-national database?","562":"How many more cases of COavid-19 are there than articles from Asia?","563":"What are the conditions for the basic reproduction number $M?","564":"How many frequency ranges are used in the data?","565":"Which algorithm is less data-efficient, the LS+ or the L+?","566":"How many paths refer to a supervised path?","567":"Which model was introduced first, SAM or Schemaueller?","568":"How many points difference is there between the rate and extent ofdistress propagation?","569":"Which type of model would have lower average Shannon entropy, HSea or 2D random matrix?","570":"What are the features that we perform to exploit the information?","571":"What is the name of the TCCNet sub modules?","572":"How many percent of the population will be reintegrated to the public space?","573":"What type of ANN is wellsuited to deal with economic time-series?","574":"How many months did the first wave last?","575":"Which methods do we use to assess dietary intake?","576":"How much of an increase in the MD can be due to the addition of a single edge?","577":"How many groups of users did the human-to-robot handover experiment with?","578":"What is the main reason for the lack of potential benefits of reinforcement learning?","579":"What are the central goals of the research?","580":"Which group had a higher public awareness?","581":"Which two models were used to verify the impact of energy trade on energy trade?","582":"What sector of the secondary educated workforce is more monopsonistic than the rest of the labour market?","583":"How many groups of atoms are represented by a triangle","584":"What is the social cost of reduction of the COidal-19 pandemic?","585":"What type of system is mHealth?","586":"What is the new metric for the economic risk of a given airspace element?","587":"How many predominant political parties are there?","588":"Which approach is better for link prediction, PoLo or Hetionet?","589":"What is the minimum amount of people could be infected at the peak?","590":"How many different mechanisms are involved in the relationship between tonality and emotional valence?","591":"Which system will serve as effective tools to assist physicians?","592":"Which methods cannot accurately predict the post-stenting aneurysmal flow field due to inhomogeneous FD wire distributions on anatomogeneous arteries?","593":"How many more accuracy does the proposed CNN model yield for COavid-19 than for CO translator-19?","594":"Which is the most likely cause of COV-19?","595":"What model is not different from the SIR model?","596":"How many methods are used to estimate parameter?","597":"How many total units of telescopes does Goto-4 have?","598":"How many points difference is there between the sensitivity and specificity of the analysis?","599":"Which type of growth patterns are most common?","600":"How many responses were received to the survey?","601":"How many web pages did the authors base their research on?","602":"Which is a general theory, temperature variations or excess deaths?","603":"Which e-contest had a lower ranking?","604":"How many more developers validate and confirm that the information that we provide helps developers in the performance of the software systems?","605":"Which case studies were not successful?","606":"What was the effect of exposure on case growth consistently higher in?","607":"What are the two groups that can be recruited or defect from their subpopulation?","608":"What is the topological feature characterized by?","609":"How many limitations are there in the calculation of pandemic intensity?","610":"How many countries are included in the model?","611":"What could the demand of goods produced by the sector expand or expand?","612":"What is the primary function of the policy graph?","613":"How many more volumes of CXR were compared to the four benchmarking CXRs?","614":"Which pandemic is the worst in India?","615":"How many computing professors were interviewed?","616":"How many months after COavid-19 was declared a pandemic did it take for the public to take active responses?","617":"Which is the least important aspect of the TSE framework?","618":"Which are the LSTs designed for?","619":"Which type of card game will have more random effects when played?","620":"What are some of the methods and procedures that have been deployed?","621":"How is the Bad system designed?","622":"What is the main concern about the extended school closures?","623":"What is a complex problem?","624":"How many months after the first positive case was confirmed in China was the second positive case confirmed?","625":"How many years is the time span of the four pandemic waves collected?","626":"How many more important techniques are presented than modern tools?","627":"is there a test for point of care?","628":"How many months is the program that the architecture supports?","629":"Which model is less effective in containing epidemics, BC or eigen vector centrality?","630":"Which model was most accurate in predicting future cases?","631":"Which is a mobile health app, MHealth or EMA?","632":"Which solution is assumed to be comprised of two masses?","633":"What are some examples of pathogens?","634":"How many percent difference is there between the best and worst methods?","635":"How many participants were not included in the study?","636":"Which model can generate molecules with high generalization ability, deep neural network or basic neural network?","637":"How many isotopes are there for the PC-L3?","638":"How many viruses are present in a bivirus network?","639":"How many types of machine learning classifiers are there?","640":"Which device would be more difficult to identify, portable X-ray devices or portable devices?","641":"How many models do we link the Bayesian causal inference literature to?","642":"Which countries have partially released lockdown measures?","643":"In what order do the spectral densities of two point functions develop?","644":"How many percent of the F1-score does the model not have?","645":"What is the H.E.S.S.'s mission?","646":"How many separate codebooks are used to calculate robustness?","647":"What is the aim of the RSS?","648":"How many more accuracy is achieved in solutions of high and low turbidity than in solutions with a small scattering cross-section?","649":"How many BLEU points did the English-Korean language-agnostic pre- training methods have?","650":"What are the two methods to solve the heat potential method?","651":"Which of the following is the most affected mode of transportation?","652":"What is the name of the software that is capable of automatically processing, processing, and modeling questions pages?","653":"Which model is designed to protect from data breaches?","654":"How many AGN light curves were not modeled with VRAE architecture?","655":"What is the main component?","656":"What is the most efficient way of utilizing existing systems and external data?","657":"How many more accuracy does the Lesion Encoder framework have over the accuracy of OS prediction on the validate data set?","658":"Which is used to generate a low-level radiation exposure and less harmful to health, low- dose computed tomography (LDCT)?","659":"How many percentage points did global CO$2$ emissions increase from December 2020 to 2019?","660":"How many methods were proposed to facilitate the interpretation of classification results?","661":"Which of the surveyed proposals is the most important?","662":"Which model is less performant than imitation learning?","663":"What is the main reason for the disconnect between models and models?","664":"Which methods are at the leading edge of technologies used for classifying objects?","665":"What does the COVID-19 pandemic examine?","666":"Which side of the study is the numerical side?","667":"Which type of technology is used to support high-speed transmission of massive multimedia medical data?","668":"How many methods are used to augment and replace traditional visualization methods?","669":"Which is an example of data parallelism, machine learning or experiment parallelism?","670":"Which model would most likely use the syntactic structure of the text, joint model or LSTM based model?","671":"In binary networked public Goods game, who must decide if she participates in a public project?","672":"How many factors are governed by probabilities?","673":"Which is a less likely scenario, scenario in which the load of fast charging stations would result in increased peak load demand or scenario in Italian grid?","674":"Which is the most likely reason for failure?","675":"How many users and typing patterns were involved in the experiments?","676":"Which was the higher performance boost, the private test set or the data augmentation technique?","677":"What is the main concern of the paper?","678":"Which data validate the qualitative features of the model?","679":"Which type of regulation is better, DiSH-trend or hybrid regulation?","680":"How many reactions did the \/r\/dataisbeautiful community have?","681":"Which two regions can be used to generalize the COV-19 infection?","682":"Which test is more powerful, the permutation test or the conventional test?","683":"Which group had a smaller data base, participants or participants?","684":"Which model is used to model rates and proportions?","685":"How many participants were asked to compare the interaction modes based on their judgement?","686":"What changed the NAC's summer research program?","687":"How many more micrometers is the diameter of the particles than the diameter in the survey?","688":"Which of the three main parts of the solar simulation was designed and constructed in-house?","689":"How long does it take for the turnaround results to be valid?","690":"How many states does our system collect?","691":"What is the aim of the study?","692":"Which agent would most likely not cooperate with the Sender, Sender or receiver?","693":"How many different Vaccine Hesitancy Framings were there?","694":"What is the highest possible generalization across these splits?","695":"How many percent of the Twitter users are not news about anti- Asian hate crimes?","696":"How many methods are used to detect poses in the landing phase?","697":"What is the purpose of the study?","698":"How many less severe cases were there than non-severe cases?","699":"Which scenario would most likely have stable sources of production, Nash Equ diseased or Stackelberg equilibrium?","700":"How many cathegories are there?","701":"What is a complex and costly process?","702":"Which is a major societal problem?","703":"What is the acronym for the novel coronavirus disease?","704":"How many months will it take for the global health crisis to become even more acute?","705":"How many days does it take to generate 10-day ahead forecasts of the likely number of infected cases and deaths?","706":"What is one of the most pressing issues in the COavid-19 pandemic?","707":"What does each new bus route have?","708":"What is the name of the hypothesis behind the celebrated smoothed analysis?","709":"What are the two main goals of the planned staggered release policy?","710":"How many more times higher is the frequently cited false negative rate compared to the frequently cite false negative?","711":"Which group is least affected by the COVID epidemic?","712":"How many peaks in intensity are there?","713":"Which model is better at predicting the number of Covid-19 deaths?","714":"How many apps were not subject to compatibility checks?","715":"What is the most important aspect of SE research?","716":"How many years after the latest edition of ADG was held in Nanning?","717":"If monotonicity fails, would it be difficult or difficult to guarantee the convergence of the above- mentioned classes of matrices?","718":"How many conditions are the ED trying to simulate?","719":"Which type of electrolytes are suitable for RFB?","720":"Which data set has the highest F1-score?","721":"How do we estimate common factors?","722":"Which model is used to approximate the posterior distributions of the states given the observations?","723":"How many more Tweets are there in the dataset than are manually recorded?","724":"How many percent of the total volume of a unregulated exchange is wash trading?","725":"How many more noise monitoring stations recorded high noise levels for more that 60% of the time before the lockdown?","726":"Which group had more people ask questions and sharing ideas, under represented or majority groups?","727":"Which model has superior soft-tissue contrast?","728":"How many years does the crisis period last?","729":"What are the main drivers of WTI futures?","730":"How is the analysis of the surface web marketplace performed?","731":"How many sampling algorithms were used to investigate the COV-19 vaccination data?","732":"What is a major obstacle to maintaining herd immunity?","733":"How many tools are needed to measure the restitution coefficient?","734":"Are students exposed more or less to doorways?","735":"What type of ontology was developed to analyze the impact of the Covid19 pandemic on the Indian economy?","736":"How many years does the City Nature Challenges last?","737":"Which is larger, the AugESC or the original ESConv?","738":"How many models are involved in the review?","739":"What is the hypothesis in the hypothesis?","740":"How many Higgs doublet models have been searched for?","741":"How many more people are in the Michaelis- Menten enzyme-substrate reaction model than in the other model?","742":"Which year is the simulation prediction from Ghana for?","743":"Which case would require a set social distance to be violated, first or second?","744":"How many years was the study conducted?","745":"What is a challenge in big data?","746":"How many percent higher was the F1 score on the Sars-CoV-2 than the COavid19-CT?","747":"What pandemic is mentioned?","748":"What does the abbreviation PIQL stand for?","749":"Which type of face recognition system is more vulnerable?","750":"How many more accuracy does Random Forest have over other generic fake news classification models?","751":"How many elements are commonly used in microfluidics?","752":"Which will have lower computational costs?","753":"What does XD stand for?","754":"What did Jean Gaudart study?","755":"How many more training sessions were there than visual models?","756":"How many months after the outbreak of COavid-19 has been declared a pandemic was it declared a global emergency?","757":"What is the basic purpose of PriLock?","758":"How can the modified logics be presented?","759":"Which model would less likely have lower SpO$2$, the convolutional neural network based noncontact spO$_2$ measurement or the state-of","760":"Which distributions have a higher F1-score, XLNet or XLNet?","761":"What is one of the main approaches to control and slow down the spread of COVID-19?","762":"Which model performed better for short term forecasting, long term forecasting or long term prediction?","763":"What are the three types of employment?","764":"What is the most important aspect of the BTD model?","765":"How many observing runs of the Advanced Virgo detectors were there?","766":"Which tool is used to examine the nature of the information?","767":"How many people follow the project?","768":"What is the primary task of early classification?","769":"What was the average concentration of the most common pollutants?","770":"What pandemic was happening in March?","771":"How many cases of COavid-19 are there?","772":"How many effective field theories are there?","773":"How many teams did not have a positive effect on team performance?","774":"Which of the following will reduce social interactions?","775":"How many days after the implementation of the national lockdown was the net reproduction number dropped?","776":"Which type of networks have more complex topologies?","777":"Which was used to classify the data in the CXR?","778":"How many daily cases did we have in Iran?","779":"How many types of information are not always ambiguous?","780":"Which VAP has lower resource cost?","781":"How does the knowledge graph explain the connections between information and the media?","782":"What is the model of the model on the real line with subgaussian mixing distribution?","783":"Which is a dynamical atmospheric region, ABL or BL?","784":"What is the name of the regulatory authority?","785":"Which model doesn't use time-series filtering to overcome the low signal-to-noise ratio typical of complex systems data?","786":"What does ICHEP 2020 mark?","787":"What type of microarray analysis was used?","788":"What is the main difference between information management and quantum information management?","789":"How many networks are included in the study?","790":"Which type of networks were used to evaluate our method?","791":"Which type of threat is most likely to spread through one's life via natural disasters?","792":"What type of model do we use to characterize the quantal response equilibrium of the proposed setting?","793":"Which country has a lower number of active coronavirus cases?","794":"Which are the three endpoint damages?","795":"How many people are affected by Dengue?","796":"Which was higher accuracy, accuracy or recall?","797":"What is the purpose of RT-PCR testing?","798":"What is the critical \"boundary line\"?","799":"Which is a drug that is designed to treat male pattern hair loss?","800":"Did the information we recorded increase or decrease the chance of creating a misinformation filter bubble?","801":"What do viruses have?","802":"What is the name of the technique that we exploit a mathematical programming technique?","803":"What is the aim of the quantum computing course?","804":"What is a realistic requirement that does not sacrifice immediate or short-term reproducibility?","805":"What is the transmission rate of the new containment regime?","806":"How many of the top Android parental control apps were not designed for the purpose of promoting children's online safety?","807":"How many years did it take for the data collected to be publicly available?","808":"What is a key issue for knowledge sharing in agile ISD organizations?","809":"Which is a difficult difficulty to understand?","810":"Which of the following is the most important aspect of the mechanism?","811":"What does the COV-19 study?","812":"How many more participants were in the initial pilot application than in the general public?","813":"How many English articles were published in the systematic literature review?","814":"What does FSS stand for?","815":"What type of resistivity measurements suggest dominant quasi two dimensional Fermi surfaces?","816":"What is the Private Re CaP?","817":"What is the new control-theoretic paradigm for?","818":"What is absent in the stochastic fluctuation model?","819":"How many months does the effect of COavid-19 persist?","820":"How many years did it take for the bipartite news-user networks to be analyzed?","821":"What does the paper discuss?","822":"Which country will have a higher social interactions, Italy or France?","823":"What is the main disadvantage of the AI system?","824":"How many countries are included in the data for the Ebola outbreak?","825":"What are the main privacy concerns?","826":"How many percent difference is there between the levels-1 and level-2 human spreader proteins?","827":"How many more percent of the proposed network did the CheXNet reach than the test accuracy?","828":"How can we avoid detection of the diffusion source?","829":"What is the name of the model we use to describe a formal approach based on graphical causal models?","830":"How many situations of homogeneous and nonhomogeneous population were considered?","831":"What is the maximum possible trade off between the number of users and the total samples per user required for private mean estimation?","832":"Which model has a lower accuracy, linearSVC or burst?","833":"What type of network is the world wide air transportation network?","834":"What is the maximum acceptable value of daily growth rate?","835":"What is an example of a context in which governments adopt, procure, and use algorithmic systems?","836":"Which methods have a lower sensitivity, RT-PCR or CT imaging?","837":"How does Real-FND work?","838":"Which is the most important asset in the digital world?","839":"Which themes would most likely limit women's participation in science and engineering careers?","840":"What is the Kerr black-hole not a solution for?","841":"How many stars are in the range range from 1 to 5?","842":"What is the name of the tool PyHard uses to produce a hardness embedding of a dataset?","843":"Which network is less expensive, AirTag orFind My?","844":"What is the name of the virus?","845":"Which drugs had the highest binding energies?","846":"Which graph is larger, Nalanda graph or Nalando index system?","847":"How do we show that our knowledge and intuition on external topics are aligned with our knowledge?","848":"What is the purpose of the SEI3rd model?","849":"How many technical definitions of algorithmic fair machine learning have been proposed?","850":"Which antenna has a lower experimental accuracy, the observed or the manufacturer radii?","851":"How many topics specific dictionaries were included in COV-19Base?","852":"Which model had a lower yield in China, the FL or the UK?","853":"What is the purpose of the dashboards?","854":"Which is the largest Twitter bot detection benchmark to date?","855":"How many more terminology definition pairs does Graphine cover than any other subdiscipline?","856":"What is the main reason why the Kermack-McKendrick model was constructed?","857":"What is the purpose of the vaccination campaign?","858":"What is the purpose of the iCSN?","859":"Which two authors presented their results in 2021?","860":"How many countries are affected by the virus?","861":"What does the project have the ability to extract?","862":"What is the main focus area of the COV-19 infodemic?","863":"How many basic performance metrics were used to identify the five major European football leagues' natural clusters?","864":"How many religions have a higher risk than BAME?","865":"What is a subhalos?","866":"Which method would most likely be accurate for large areas, the Jackknife method or synthetic estimation method?","867":"How many countries have followed different policy actions?","868":"Which pandemic has affected the world the most?","869":"How many categories are the vulnerable population divided into?","870":"How many local rural teachers were not involved?","871":"Which algorithm is more accurate, RFE-GB or gradient boost?","872":"How many losses are there?","873":"What is the purpose of the MBIC?","874":"How many objectives does the cost model have?","875":"Which type of networks are more suited for static structures, nodes only or networks with static structures?","876":"How many months after France was the infection Fatality rate 4 \u00a31%?","877":"Which is harder to calculate, energy consumption or energy consumption?","878":"How many patients were used for the machine learning model?","879":"Which has triggered fewer health complications?","880":"What is the ARD?","881":"How many skills are in VASH?","882":"What is fiberStars?","883":"What is the nature of the Bohr Weisskopf effect?","884":"Which is more suitable for the storage policy, AGV-assisted mixed-shelves storage policy or the traditional picker-to-parts warehousing system?","885":"Which is an established topic in natural images for years, few work is attended to histology images or normal samples?","886":"What pandemic is mentioned?","887":"What is the name of the nanostructures that are capable of channeling particles without staging?","888":"How many papers had no form of empirical evaluation?","889":"Which has a lower anxiety score, beer or hoarding-related subjects?","890":"How is the local model distributed?","891":"How many methods are used to collect data?","892":"Which is larger, the injected mass or the entrained mass?","893":"Which is a more complex example of partial observation, partial observation or Coarse observation?","894":"What are the parameters of stochastic models?","895":"What is the main factor in the transition rate?","896":"What model is the bohmion method applied to?","897":"How many types of thought processes were there?","898":"What is the tool set for operating with them at scale?","899":"Which are the most affected countries?","900":"What is an alternative approach to this?","901":"Which is more likely to be used as a COavid-19 inhibitor, Calotropin or Mpro\/3CLpro?","902":"How many patients did not have symptoms after the infection?","903":"How does the paper aim to detect COV-19 patients?","904":"What is a possible violation of the FL protocol?","905":"Which model is less expressive and allows for plugging in the actual contact networks?","906":"How many countries do not have daily case numbers?","907":"Which technique would most likely use free space bench-top geometries?","908":"Which is the most efficient way to utilize achieved parallelism?","909":"What is the primary reason behind the difference in traditional clusters?","910":"How many percentage points difference is there between the sentiment about WFH and the sentiment of the public?","911":"What are zines?","912":"Which is a harder problem, the ability to determine what parts of objects and surfaces people touch or the cross view projection consistency?","913":"What is the minimum amount of CO2 reduction per million?","914":"What is the HEP?","915":"What is a valuable asset?","916":"How many parameters do we need to predict to predict the new infections?","917":"What is the most pressing issue for privacy systems?","918":"Which has a higher risk score, Jakarta or Jakarta?","919":"What is the critical index?","920":"What is the focus of the exploration?","921":"How many months did it take for the confirmed cases of COV-19 to present consistent ocular pathological symbols?","922":"What is the aim of the study?","923":"How many more days was the daily employee self-reported symptoms collected using an automated text messaging system?","924":"Which algorithms are used to increase sample efficiency of Off Policy RL?","925":"Which account had a lower chance of survival in 2019\/ 2020 Australian bushfire?","926":"Which model shows the fraction of dark matter in primordial black holes?","927":"How many variations of the model are there?","928":"How many political figures were mined from the 10 billion political figures?","929":"What is a major issue in the study?","930":"Which compression ratio is achieved by homogeneous diffusion inpainting?","931":"What is the main idea?","932":"How many categories of information are there in the tractor2 twitter model?","933":"How many students were enrolled in the four public universities in Pakistan?","934":"Which market has a lower social inclusion, short and low-density routes or leisure routes?","935":"How many drugs achieved antiviral effects?","936":"How many types of events are there?","937":"How many different ways does the V-Cube View algorithm differ from the V=Cube View?","938":"How long has it been since the evolution of COv-19 been recognized?","939":"What is the purpose of the data valuation approaches?","940":"How many percent of conversations are not in the guide?","941":"What is the most likely cause of the energy grid vulnerability?","942":"Which is the natural extension of product-moment covariance?","943":"Which model is better in offline tests, the one we propose or the one that performs better offline tests?","944":"How many subjects had a lower sensitivity than the sensitivity?","945":"What is the name of the proposed currency?","946":"Which planet will have a lower wind mass loss rate?","947":"How many components are required to be in a circular economy?","948":"How many themes are there in the four main non-opioid related networks?","949":"What is the most reliable domain?","950":"What is the maximum matching in $G$?","951":"What is the main cause of rising air pollution levels?","952":"How many times did the high-contrast imaging search for Pa$\\ beta$ line emission from protoplanets in the PDS-70 system fail?","953":"How many years did the data from the two National Federation and Health Surveyys in Nigeria last?","954":"How many campuses was the system deployed on?","955":"What is the standard model for detergent SIS dynamics?","956":"Which materials were suitable for masks meeting the N95 NIOSH standard?","957":"Which pollutants were found at more than 65% of monitoring sites?","958":"What is the probability density function?","959":"How many cities in the US had crime count data from January 2018 through December 2020?","960":"How many possible states are there?","961":"Which approach is most likely to be adopted?","962":"Which pandemic has highlighted the importance of heterogeneity?","963":"How many dimensions does GACELA have?","964":"How many more confirmed cases were there in the 11th of March than in the 12th of march?","965":"How many more people support the anti-COVID-19 measure than oppose it?","966":"How many positions are in the Molecular CloudS?","967":"Which model is more robust against offenders actively trying to evade detection, CNN model or the best-selling model?","968":"What is the name of the epidemic model?","969":"Which is more likely to be promoted, conspiracy or anti-social message?","970":"Which search has there not been a strong indication for new physics?","971":"What is the Rademacher symbol for?","972":"What is the scientific name of the novel coronavirus?","973":"How many different types of games are mentioned?","974":"How many different diseases does contact trace manage?","975":"What is the name of the pandemic?","976":"How many primary care encounters and free-text reason-for-visit entries were used to train SmartTriage?","977":"How do we generalize the class imbalance algorithm?","978":"How do the experiences that created personal mini-tours for specific others differ from the experiences created by the Museum?","979":"Will activity recipes be generated for good sleep quality for each behavior mode within each cluster?","980":"Which methods have less reliable prediction performance?","981":"What was the missing map?","982":"How many more ideas stimulation is there than reduced creativity?","983":"How many levels of knowledge acquisition are there in Bloom's taxonomy?","984":"How many more points are in the model generated from the three recurrent neural network than in the experimental protocol?","985":"Which type of interaction is the HRI?","986":"Which is more expensive, bluetooth smart or smart bluetooth?","987":"Which type of face mask is least effective?","988":"How did our solutions rank in the prediction challenge?","989":"What is the goal of the problem?","990":"How many important modules do we propose for the hyperbolic temporal graph network (HT GN)?","991":"Which two factors contributed to the increase in CO2 emissions?","992":"What are the three main components of the architecture method?","993":"What is a strategy that uses early detection and prompt isolation?","994":"What do we build to integrate the Susceptible-exposed-Infectious- forgot epidemiology model?","995":"How many microparticles are estimated to have small speeds?","996":"Which state's serology studies are more reliable, California or New York?","997":"How many methods are used to extract features from implied volatage data?","998":"How many main population areas speaker of indigenous language in Veracruz?","999":"How many more records are used to develop the new sensor model than the old one?"},"output":{"0":"The physical basis and algorithm of the ultraviolet germicidal irradiation (UVGI) scientific calculator are presented herein. The algorithm was implemented in the web application UVGI Scientific Calculator (found at https:\/\/covid-19-clean.org\/calculator-journal-report\/) that has been used to facilitate the engineering and design of air-purifying equipment being developed for use in hospitals, commercial and residential applications. Presented examples of the calculations illustrate how different factors of the construction of a UVGI air-purifier may influence the degree of virus inactivation.","1":"Sensitivity Analysis (SA) is a useful tool to measure the impact of changes in model parameters on the infection dynamics, particularly to quantify the expected efficacy of disease control strategies. SA has only been applied to epidemic models at the population level, ignoring the effect of within-host virus-with-immune-system interactions on the disease spread. Connecting the scales from individual to population can help inform drug and vaccine development. Thus the value of understanding the impact of immunological parameters on epidemiological quantities. Here we consider an age-since-infection structured vector-host model, in which epidemiological parameters are formulated as functions of within-host virus and antibody densities, governed by an ODE system. We then use SA for these immuno-epidemiological models to investigate the impact of immunological parameters on population-level disease dynamics such as basic reproduction number, final size of the epidemic or the infectiousness at different phases of an outbreak. As a case study, we consider Rift Valley Fever Disease (RFVD) utilizing parameter estimations from prior studies. SA indicates that 1% increase in within-host pathogen growth rate can lead up to 8% increase in R0; up to 1% increase in steady-state infected host abundance, and up to 4% increase in infectiousness of hosts when the reproduction number R0 is larger than one. These significant increases in population-scale disease quantities suggest that control strategies that reduce the within-host pathogen growth can be important in reducing disease prevalence.","2":"In this paper, we estimate the impact of national lockdown on COVID-19 related total and daily deaths, per million people, in select European countries. In particular, we compare countries that imposed a nationwide lockdown (Treatment group); Belgium, Denmark, France, Germany, Italy, Norway, Spain, United Kingdom (UK), and the US, to Sweden (Control group) that did not impose national lockdown using a changes-in-changes (CIC) estimation model. The key advantage of the CIC model as compared to the standard difference-in-difference model is that CIC allows for mean and variance of the outcomes to change over time in the absence of any policy intervention, and CIC accounts for endogeneity in the choice of policy intervention. Our results indicate that in contrast to Sweden, which did not impose a national lockdown, Germany, and to some extent, the US were the two countries where nationwide lockdown had a significant impact on the reduction in COVID-19 related total and daily deaths per million people. In Norway and Denmark, there was no significant impact on total and daily deaths per million people relative to Sweden. Whereas in other countries; Belgium, France, Italy, Spain, and the UK, the effect of the lockdown was in the opposite direction, that is, they experienced significantly higher COVID-19 related total and daily deaths per million people, post the lockdown as compared to Sweden. Our results suggest that the impact of nationwide lockdown on COVID-19 related total and daily deaths per million people varied from one country to another.","3":"We investigate the problem of learning undirected graphical models under Laplacian structural constraints from the point of view of financial market data. We show that Laplacian constraints have meaningful physical interpretations related to the market index factor and to the conditional correlations between stocks. Those interpretations lead to a set of guidelines that users should be aware of when estimating graphs in financial markets. In addition, we propose algorithms to learn undirected graphs that account for stylized facts and tasks intrinsic to financial data such as non-stationarity and stock clustering.","4":"In recent years, there has been a rapid growth in the application of machine learning techniques that leverage aviation data collected from commercial airline operations to improve safety. Anomaly detection and predictive maintenance have been the main targets for machine learning applications. However, this paper focuses on the identification of precursors, which is a relatively newer application. Precursors are events correlated with adverse events that happen prior to the adverse event itself. Therefore, precursor mining provides many benefits including understanding the reasons behind a safety incident and the ability to identify signatures, which can be tracked throughout a flight to alert the operators of the potential for an adverse event in the future. This work proposes using the multiple-instance learning (MIL) framework, a weakly supervised learning task, combined with carefully designed binary classifier leveraging a Multi-Head Convolutional Neural Network-Recurrent Neural Network (MHCNN-RNN) architecture. Multi-class classifiers are then created and compared, enabling the prediction of different adverse events for any given flight by combining binary classifiers, and by modifying the MHCNN-RNN to handle multiple outputs. Results obtained showed that the multiple binary classifiers perform better and are able to accurately forecast high speed and high path angle events during the approach phase. Multiple binary classifiers are also capable of determining the aircraft's parameters that are correlated to these events. The identified parameters can be considered precursors to the events and may be studied\/tracked further to prevent these events in the future.","5":"Due to the Covid-19 pandemic, more than 500 US-based colleges and universities went\"test-optional\"for admissions and promised that they would not penalize applicants for not submitting test scores, part of a longer trend to rethink the role of testing in college admissions. However, it remains unclear how (and whether) a college can simultaneously use test scores for those who submit them, while not penalizing those who do not--and what that promise even means. We formalize these questions, and study how a college can overcome two challenges with optional testing: $\\textit{strategic applicants}$ (when those with low test scores can pretend to not have taken the test), and $\\textit{informational gaps}$ (it has more information on those who submit a test score than those who do not). We find that colleges can indeed do so, if and only if they are able to use information on who has test access and are willing to randomize admissions.","6":"During a research project in which we developed a machine learning (ML) driven visualization system for non-ML experts, we reflected on interpretability research in ML, computer-supported collaborative work and human-computer interaction. We found that while there are manifold technical approaches, these often focus on ML experts and are evaluated in decontextualized empirical studies. We hypothesized that participatory design research may support the understanding of stakeholders' situated sense-making in our project, yet, found guidance regarding ML interpretability inexhaustive. Building on philosophy of technology, we formulated explanation strategies as an empirical-analytical lens explicating how technical explanations mediate the contextual preferences concerning people's interpretations. In this paper, we contribute a report of our proof-of-concept use of explanation strategies to analyze a co-design workshop with non-ML experts, methodological implications for participatory design research, design implications for explanations for non-ML experts and suggest further investigation of technological mediation theories in the ML interpretability space.","7":"COVID-19 has significantly affected universities, forcing many courses to be delivered entirely online. As countries bring the pandemic under control, a potential way to safely resume some face-to-face teaching is the synchronous hybrid classroom, in which physically and remotely attending students are taught simultaneously. This comes with challenges, however, including the risk that remotely attending students perceive a 'gap' between their engagement and that of their physical peers. In this experience report, we describe how an interactive programming course was adapted to hybrid delivery in a way that mitigated this risk. Our solution centred on the use of a professional communication platform - Slack - to equalise participation opportunities and to facilitate peer learning. Furthermore, to mitigate 'Zoom fatigue', we implemented a semi-flipped classroom, covering concepts in videos and using shorter lessons to consolidate them. Finally, we critically reflect on the results of a student survey and our own experiences of implementing the solution.","8":"Italy has been one of the first countries timewise strongly impacted by the COVID-19 pandemic. The adoption of social distancing and heavy lockdown measures is posing a heavy burden on the population and the economy. The timing of the measures has crucial policy-making implications. Using publicly available data for the pandemic progression in Italy, we quantitatively assess the effect of the intervention time on the pandemic expansion, with a methodology that combines a generalized susceptible-exposed-infectious-recovered (SEIR) model together with statistical learning methods. The modeling shows that the lockdown has strongly deviated the pandemic trajectory in Italy. However, the difference between the forecasts and real data up to 20 April 2020 can be explained only by the existence of a time lag between the actual issuance date and the full effect of the measures. To understand the relative importance of intervention with respect to other factors, a thorough uncertainty quantification of the model predictions is performed. Global sensitivity indices show that the the time of intervention is 4 times more relevant than quarantine, and eight times more important than intrinsic features of the pandemic such as protection and infection rates. The relevance of their interactions is also quantified and studied.","9":"Temporary changes in electrical resistance of a nanopore sensor caused by translocating target analytes are recorded as a sequence of pulses on current traces. Prevalent algorithms for feature extraction in pulse-like signals lack objectivity because empirical amplitude thresholds are user-defined to single out the pulses from the noisy background. Here, we use deep learning for feature extraction based on a bi-path network (B-Net). After training, the B-Net acquires the prototypical pulses and the ability of both pulse recognition and feature extraction without a priori assigned parameters. The B-Net performance is evaluated on generated datasets and further applied to experimental data of DNA and protein translocation. The B-Net results show remarkably small relative errors and stable trends. The B-Net is further shown capable of processing data with a signal-to-noise ratio equal to one, an impossibility for threshold-based algorithms. The developed B-Net is generic for pulse-like signals beyond pulsed nanopore currents.","10":"The infection rate of COVID-19 and lack of an approved vaccine has forced governments and health authorities to adopt lockdowns, increased testing, and contact tracing to reduce the spread of the virus. Digital contact tracing has become a supplement to the traditional manual contact tracing process. However, although there have been a number of digital contact tracing apps proposed and deployed, these have not been widely adopted owing to apprehensions surrounding privacy and security. In this paper, we propose a blockchain-based privacy-preserving contact tracing protocol,\"Did I Meet You\"(DIMY), that provides full-lifecycle data privacy protection on the devices themselves as well as on the back-end servers, to address most of the privacy concerns associated with existing protocols. We have employed Bloom filters to provide efficient privacy-preserving storage, and have used the Diffie-Hellman key exchange for secret sharing among the participants. We show that DIMY provides resilience against many well known attacks while introducing negligible overheads. DIMY's footprint on the storage space of clients' devices and back-end servers is also significantly lower than other similar state of the art apps.","11":"Researchers have been investigating automated solutions for fact-checking in a variety of fronts. However, current approaches often overlook the fact that the amount of information released every day is escalating, and a large amount of them overlap. Intending to accelerate fact-checking, we bridge this gap by grouping similar messages and summarizing them into aggregated claims. Specifically, we first clean a set of social media posts (e.g., tweets) and build a graph of all posts based on their semantics; Then, we perform two clustering methods to group the messages for further claim summarization. We evaluate the summaries both quantitatively with ROUGE scores and qualitatively with human evaluation. We also generate a graph of summaries to verify that there is no significant overlap among them. The results reduced 28,818 original messages to 700 summary claims, showing the potential to speed up the fact-checking process by organizing and selecting representative claims from massive disorganized and redundant messages.","12":"In this work, we examine a novel forecasting approach for COVID-19 case prediction that uses Graph Neural Networks and mobility data. In contrast to existing time series forecasting models, the proposed approach learns from a single large-scale spatio-temporal graph, where nodes represent the region-level human mobility, spatial edges represent the human mobility based inter-region connectivity, and temporal edges represent node features through time. We evaluate this approach on the US county level COVID-19 dataset, and demonstrate that the rich spatial and temporal information leveraged by the graph neural network allows the model to learn complex dynamics. We show a 6% reduction of RMSLE and an absolute Pearson Correlation improvement from 0.9978 to 0.998 compared to the best performing baseline models. This novel source of information combined with graph based deep learning approaches can be a powerful tool to understand the spread and evolution of COVID-19. We encourage others to further develop a novel modeling paradigm for infectious disease based on GNNs and high resolution mobility data.","13":"The rapid spread of COVID-19 infections on a global level has highlighted the need for accurate, transparent and timely information regarding collective mobility patterns to inform de-escalation strategies as well as to provide forecasting capacity for re-escalation policies aiming at addressing further waves of the virus. Such information can be extracted using aggregate anonymised data from innovative sources such as mobile positioning data. This paper presents lessons learnt and results of a unique Business-to-Government (B2G) initiative between several Mobile Network Operators in Europe and the European Commission. Mobile positioning data have supported policy makers and practitioners with evidence and data-driven knowledge to understand and predict the spread of the disease, the effectiveness of the containment measures, their socio-economic impacts while feeding scenarios at EU scale and in a comparable way across countries. The challenges of this data sharing initiative are not limited to data quality, harmonisation, and comparability across countries, however important they are. Equally essential aspects that need to be addressed from the onset are related to data privacy, security, fundamental rights and commercial sensitivity.","14":"The radius valley, a bifurcation in the size distribution of small, close-in exoplanets, is hypothesized to be a signature of planetary atmospheric loss. Such an evolutionary phenomenon should depend on the age of the star-planet system. In this work, we study the temporal evolution of the radius valley using two independent determinations of host star ages among the California-Kepler Survey (CKS) sample. We find evidence for a wide and nearly empty void of planets in the period-radius diagram at the youngest system ages ($\\lesssim$2-3 Gyr) represented in the CKS sample. We show that the orbital period dependence of the radius valley among the younger CKS planets is consistent with that found among those planets with asteroseismically determined host star radii. Relative to previous studies of preferentially older planets, the radius valley determined among the younger planetary sample is shifted to smaller radii. This result is compatible with an atmospheric loss timescale on the order of gigayears for progenitors of the largest observed super-Earths. In support of this interpretation, we show that the planet sizes which appear to be unrepresented at ages $\\lesssim$2-3 Gyr are likely to correspond to planets with rocky compositions. Our results suggest the size distribution of close-in exoplanets, and the precise location of the radius valley, evolves over gigayears.","15":"Multiple-choice questions (MCQs) offer the most promising avenue for skill evaluation in the era of virtual education and job recruiting, where traditional performance-based alternatives such as projects and essays have become less viable, and grading resources are constrained. The automated generation of MCQs would allow assessment creation at scale. Recent advances in natural language processing have given rise to many complex question generation methods. However, the few methods that produce deployable results in specific domains require a large amount of domain-specific training data that can be very costly to acquire. Our work provides an initial foray into MCQ generation under high data-acquisition cost scenarios by strategically emphasizing paraphrasing the question context (compared to the task). In addition to maintaining semantic similarity between the question-answer pairs, our pipeline, which we call AGenT Zero, consists of only pre-trained models and requires no fine-tuning, minimizing data acquisition costs for question generation. AGenT Zero successfully outperforms other pre-trained methods in fluency and semantic similarity. Additionally, with some small changes, our assessment pipeline can be generalized to a broader question and answer space, including short answer or fill in the blank questions.","16":"In a paper of August 2013, I discussed the so-called SuperSpreader (SS) epidemic model and emphasized that it has dynamics differing greatly from the more-familiar uniform (or Poisson) textbook model. In that paper, SARS in 2003 was the representative instance and it was suggested that MERS may be another. In April 2014, MERS incident cases showed a spectacular spike (going from a handful in the previous April to more than 260 in that month of 2014) reminiscent of a figure I published nine months earlier. Here I refit the two-level and several variant SS models to incident data from January 1, 2013--April 30, 2014 and conclude that MERS will go pandemic (all other factors remaining the same). In addition, I discuss a number of model-realism and fitting methodology issues relevant to analysing SS epidemics.","17":"One regards spaces of trees as stratified spaces, to study distributions of phylogenetic trees. Stratified spaces with may have cycles, however spaces of trees with a fixed number of leafs are contractible. Spaces of trees with three leafs, in particular, are spiders with three legs. One gives an elementary proof of the stickiness of intrinsic sample means on spiders. One also represents four leafs tree data in terms of an associated Petersen graph. One applies such ideas to analyze RNA sequences of SARS-CoV-2 from multiple sources, by building samples of trees and running nonparametric statistics for intrinsic means on tree spaces with three and four leafs. SARS-CoV-2 are also used to built trees with leaves consisting in addition to other related coronaviruses.","18":"We use anonymized and aggregated data from Facebook to show that areas with stronger social ties to two early COVID-19\"hotspots\"(Westchester County, NY, in the U.S. and Lodi province in Italy) generally have more confirmed COVID-19 cases as of March 30, 2020. These relationships hold after controlling for geographic distance to the hotspots as well as for the income and population density of the regions. These results suggest that data from online social networks may prove useful to epidemiologists and others hoping to forecast the spread of communicable diseases such as COVID-19.","19":"In the Galactic center nuclear star cluster, bright late-type stars exhibit a flat or even a decreasing surface-density profile, while fainter late-type stars maintain a cusp-like profile. Historically, the lack of red giants in the Galactic center was discovered via the drop in the strength of the CO absorption bandhead by Kris Sellgren et al. (1990), later followed by the stellar number counts based on the high angular resolution near-infrared observations. Several mechanisms were put forward that could have led to the preferential depletion of bright red giants: star-star collisions, tidal stripping, star-accretion disc collisions, or an infall of a massive cluster or a secondary black hole. Here we propose a novel scenario for the bright red-giant depletion based on the collisions between red giants and the nuclear jet, which was likely active in the Galactic center a few million years ago and could have led to the formation of the large-scale $\\gamma$-ray Fermi bubbles. The process of the jet-induced ablation of red giants appears to be most efficient within $\\sim 0.04\\,{\\rm pc}$ (S-cluster), while at larger distances it was complemented by star-accretion disc collisions and at smaller scales, tidal stripping operated. These three mechanisms likely operated simultaneously and created an apparent core of late-type stars within $\\sim 0.5\\,{\\rm pc}$.","20":"This article brings together two distinct, but related perspectives on playful museum experiences: Critical play and hybrid design. The article explores the challenges involved in combining these two perspectives, through the design of two hybrid museum experiences that aimed to facilitate critical play with\/in the collections of the Museum of Yugoslavia and the highly contested heritage they represent. Based on reflections from the design process as well as feedback from test users we describe a series of challenges: Challenging the norms of visitor behaviour, challenging the role of the artefact, and challenging the curatorial authority. In conclusion we outline some possible design strategies to address these challenges.","21":"Question Answering (QA) is a task in natural language processing that has seen considerable growth after the advent of transformers. There has been a surge in QA datasets that have been proposed to challenge natural language processing models to improve human and existing model performance. Many pre-trained language models have proven to be incredibly effective at the task of extractive question answering. However, generalizability remains as a challenge for the majority of these models. That is, some datasets require models to reason more than others. In this paper, we train various pre-trained language models and fine-tune them on multiple question answering datasets of varying levels of difficulty to determine which of the models are capable of generalizing the most comprehensively across different datasets. Further, we propose a new architecture, BERT-BiLSTM, and compare it with other language models to determine if adding more bidirectionality can improve model performance. Using the F1-score as our metric, we find that the RoBERTa and BART pre-trained models perform the best across all datasets and that our BERT-BiLSTM model outperforms the baseline BERT model.","22":"The optical spectra of 3,896 Seyfert~1 (Sy1) galaxies detected with WISE at z<0.4 were analyzed for evidence of outflows. In 37% of the Sy1s the outflows appear as broad, blue-shifted, of the [OIII]5007, with a mean maximum velocity V_max ~1014 km\/s$, consistent with AGN winds. For each Sy1 we deduced that black hole (BH) mass, bolometric luminosity, Eddington ratio and power-law index of the continuum, which we compared with the star formation rate (SFR) and host morphology. Having separated our sample in two spectroscopic subgroups, Sy1s with only broad Balmer lines (Sy1B) and with both narrow and broad (Sy1N) lines and distinguishing those that show as outflow (Sy1Bw and Sy1Nw), we report the following differences: 1) the BH mass is systematically higher and the power-law steeper in the Sy1B-Sy1Bw than in the Sy1N-Sy1Nw, 2) V_max is higher in the Sy1Bw than in the Sy1Nw, correlated in both groups with the BH mass and bolometric luminosity, 3) the Eddington ratio and SFR are higher in the Sy1 with outflows, and 4) the specific star formation rates (sSFRs) of the Sy1s are normal for their morphology and mass; typical of early-type spiral galaxies in the green valley, far from the quenched regime. From these results we conclude that in Sy1s AGN winds are triggered by higher accretion rates and probably radiatively launched, and there is no clear evidence of an effect on the star formation.","23":"We propose a new method for clustering of functional data using a $k$-means framework. We work within the elastic functional data analysis framework, which allows for decomposition of the overall variation in functional data into amplitude and phase components. We use the amplitude component to partition functions into shape clusters using an automated approach. To select an appropriate number of clusters, we additionally propose a novel Bayesian Information Criterion defined using a mixture model on principal components estimated using functional Principal Component Analysis. The proposed method is motivated by the problem of posterior exploration, wherein samples obtained from Markov chain Monte Carlo algorithms are naturally represented as functions. We evaluate our approach using a simulated dataset, and apply it to a study of acute respiratory infection dynamics in San Luis Potos\\'{i}, Mexico.","24":"To fight against the evolution of malware and its development, the specific methodologies that are applied by the malware analysts are crucial. Yet, this is something often overlooked in the relevant bibliography or in the formal and informal training of the relevant professionals. There are only two generic and all-encompassing structured methodologies for Malware Analysis (MA) - SAMA and MARE. The question is whether they are adequate and there is no need for another one or whether there is no such need at all. This paper will try to answer the above and it will contribute in the following ways: it will present, compare and dissect those two malware analysis methodologies, it will present their capacity for analysing modern malware by applying them on a random modern specimen and finally, it will conclude on whether there is a procedural optimization for malware analysis over the evolution of these two methodologies.","25":"The disruption caused by the pandemic has called into question industrial norms and created an opportunity to reimagine the future of work. We discuss how this period of opportunity may be leveraged to bring about a future in which the workforce thrives rather than survives. Any coherent plan of such breadth must address the interaction of multiple technological, social, economic, and environmental systems. A shared language that facilitates communication across disciplinary boundaries can bring together stakeholders and facilitate a considered response. The origin story of cybernetics and the ideas posed therein serve to illustrate how we may better understand present complex challenges, to create a future of work that places human values at its core.","26":"Digitalization and Smart systems are part of our everyday lives today. So far the development has been rapid and all the implications that comes after the deployment has not been able to foresee or even assess during the development, especially when ethics or trustworthiness is concerned. Artificial Intelligence (AI) and Autonomous Systems (AS) are the direction that software systems are taking today. It is witnessed in banks, stores, internet and it is proceeding to transportation as well as on traveling. Autonomous maritime industry has also taking this direction when taking under development in digitalization on fairway and port terminals. AI ethics has advanced profoundly since the machine learning develop during the last decade and is now being implemented in AI development and workflow of software engineers. It is not an easy task and tools are needed to make the ethical assessment easier. This paper will review a research in an industrial setting, where Ethically Aligned Design practice, Ethical User Stories are used to transfer ethical requirements to ethical user stories to form practical solutions for project use. This project is in the field of maritime industry and concentrates on digitalization of port terminals and this particular paper focuses on the passenger flow. Results are positive towards the practice of Ethical User Stories, drawn from a large empirical data set.","27":"Pooled testing is widely used for screening for viral or bacterial infections with low prevalence when individual testing is not cost-efficient. Pooled testing with qualitative assays that give binary results has been well-studied. However, characteristics of pooling with quantitative assays were mostly demonstrated using simulations or empirical studies. We investigate properties of three pooling strategies with quantitative assays: traditional two-stage mini-pooling (MP) (Dorfman, 1943), mini-pooling with deconvolution algorithm (MPA) (May et al., 2010), and marker-assisted MPA (mMPA) (Liu et al., 2017). MPA and mMPA test individuals in a sequence after a positive pool and implement a deconvolution algorithm to determine when testing can cease to ascertain all individual statuses. mMPA uses information from other available markers to determine an optimal order for individual testings. We derive and compare the general statistical properties of the three pooling methods. We show that with a proper pool size, MP, MPA, and mMPA can be more cost-efficient than individual testing, and mMPA is superior to MPA and MP. For diagnostic accuracy, mMPA and MPA have higher specificity and positive predictive value but lower sensitivity and negative predictive value than MP and individual testing. Included in this paper are applications to various simulations and an application for HIV treatment monitoring.","28":"As the world is becoming more dependent on the internet for information exchange, some overzealous journalists, hackers, bloggers, individuals and organizations tend to abuse the gift of free information environment by polluting it with fake news, disinformation and pretentious content for their own agenda. Hence, there is the need to address the issue of fake news and disinformation with utmost seriousness. This paper proposes a methodology for fake news detection and reporting through a constraint mechanism that utilizes the combined weighted accuracies of four machine learning algorithms.","29":"We investigate spreading and recovery of disease in a square lattice, and in particular, emphasize the role of the initial distribution of infected patches in the network, on the progression of an endemic and initiation of a recovery process, if any, due to migration of both the susceptible and infected hosts. The disease starts in the lattice with three possible initial distribution patterns of infected and infection-free sites, infected core patches (ICP), infected peripheral patches (IPP) and randomly distributed infected patches (RDIP). Our results show that infection spreads monotonically in the lattice with increasing migration without showing any sign of recovery in the ICP case. In the IPP case, it follows a similar monotonic progression with increasing migration, however, a self-organized healing process starts for higher migration, leading the lattice to full recovery at a critical rate of migration. Encouragingly, for the initial RDIP arrangement, chances of recovery are much higher with a lower rate of critical migration. An eigenvalue based semi-analytical study is made to determine the critical migration rate for realizing a stable infection-free lattice. The initial fraction of infected patches and the force of infection play significant roles in the self-organized recovery. They follow an exponential law, for the RDIP case, that governs the recovery process. For the frustrating case of ICP arrangement, we propose a random rewiring of links in the lattice allowing long-distance migratory paths that effectively initiate a recovery process. Global prevalence of infection thereby declines and progressively improves with the rewiring probability that follows a power law with the critical migration and leads to the birth of emergent infection-free networks.","30":"In cooperative bandits, a framework that captures essential features of collective sequential decision making, agents can minimize group regret, and thereby improve performance, by leveraging shared information. However, sharing information can be costly, which motivates developing policies that minimize group regret while also reducing the number of messages communicated by agents. Existing cooperative bandit algorithms obtain optimal performance when agents share information with their neighbors at \\textit{every time step}, i.e., full communication. This requires $\\Theta(T)$ number of messages, where $T$ is the time horizon of the decision making process. We propose \\textit{ComEx}, a novel cost-effective communication protocol in which the group achieves the same order of performance as full communication while communicating only $O(\\log T)$ number of messages. Our key step is developing a method to identify and only communicate the information crucial to achieving optimal performance. Further we propose novel algorithms for several benchmark cooperative bandit frameworks and show that our algorithms obtain \\textit{state-of-the-art} performance while consistently incurring a significantly smaller communication cost than existing algorithms.","31":"Data privacy is a trending topic in the internet era. Given such importance, many challenges emerged in order to collect, manage, process, and publish data. In this sense, personal data have got attention, and many regulations emerged, such as GDPR in the European Union and LGPD in Brazil. This regulation model aims to protect users' data from misusage and leakage and allow users to request an explanation from companies when needed. In pandemic situations, such as the COVID-19 and Ebola outbreak, the action related to sharing health data between different organizations is\/ was crucial to develop a significant movement to avoid the massive infection and decrease the number of deaths. However, the data subject, i.e., the users, should have the right to request the purpose of data use, anonymization, and data deletion. In this sense, permissioned blockchain technology emerges to empower users to get their rights providing data ownership, transparency, and security through an immutable, unified, and distributed database ruled by smart contracts. The governance model discussed in blockchain applications is usually regarding the first layer governance, i.e., public and permissioned models. However, this discussion is too superficial, and they do not cover compliance with the data regulations. Therefore, in order to organize the relationship between data owners and the stakeholders, i.e., companies and governmental entities, we developed a second layer data governance model for permissioned blockchains based on the Governance Analytical Framework principles applied in pandemic situations preserving the users' privacy and their duties. From the law perspective, we based our model on the UE GDPR in regard to data privacy concerns.","32":"On 2020 February 24, during their third observing run (\"O3\"), the Laser Interferometer Gravitational-wave Observatory and Virgo Collaboration (LVC) detected S200224ca: a candidate gravitational wave (GW) event produced by a binary black hole (BBH) merger. This event was one of the best-localized compact binary coalescences detected in O3 (with 50%\/90% error regions of 13\/72 deg$^2$), and so the Neil Gehrels Swift Observatory performed rapid near-UV\/X-ray follow-up observations. Swift-XRT and UVOT covered approximately 79.2% and 62.4% (respectively) of the GW error region, making S200224ca the BBH event most thoroughly followed-up in near-UV (u-band) and X-ray to date. No likely EM counterparts to the GW event were found by the Swift BAT, XRT, or UVOT, nor by other observatories. Here we report on the results of our searches for an EM counterpart, both in the BAT data near the time of the merger, and in follow-up UVOT\/XRT observations. We also discuss the upper limits we can place on EM radiation from S200224ca, and the implications these limits have on the physics of BBH mergers. Namely, we place a shallow upper limit on the dimensionless BH charge, $\\hat{q}<1.4 \\times10^{-4}$, and an upper limit on the isotropic-equivalent energy of a blast wave $E<4.1\\times10^{51}$ erg (assuming typical GRB parameters).","33":"The COVID-19 pandemic has been accompanied by an `infodemic' -- of accurate and inaccurate health information across social media. Detecting misinformation amidst dynamically changing information landscape is challenging; identifying relevant keywords and posts is arduous due to the large amount of human effort required to inspect the content and sources of posts. We aim to reduce the resource cost of this process by introducing a weakly-supervised iterative graph-based approach to detect keywords, topics, and themes related to misinformation, with a focus on COVID-19. Our approach can successfully detect specific topics from general misinformation-related seed words in a few seed texts. Our approach utilizes the BERT-based Word Graph Search (BWGS) algorithm that builds on context-based neural network embeddings for retrieving misinformation-related posts. We utilize Latent Dirichlet Allocation (LDA) topic modeling for obtaining misinformation-related themes from the texts returned by BWGS. Furthermore, we propose the BERT-based Multi-directional Word Graph Search (BMDWGS) algorithm that utilizes greater starting context information for misinformation extraction. In addition to a qualitative analysis of our approach, our quantitative analyses show that BWGS and BMDWGS are effective in extracting misinformation-related content compared to common baselines in low data resource settings. Extracting such content is useful for uncovering prevalent misconceptions and concerns and for facilitating precision public health messaging campaigns to improve health behaviors.","34":"Robots in the real world should be able to adapt to unforeseen circumstances. Particularly in the context of tool use, robots may not have access to the tools they need for completing a task. In this paper, we focus on the problem of tool construction in the context of task planning. We seek to enable robots to construct replacements for missing tools using available objects, in order to complete the given task. We introduce the Feature Guided Search (FGS) algorithm that enables the application of existing heuristic search approaches in the context of task planning, to perform tool construction efficiently. FGS accounts for physical attributes of objects (e.g., shape, material) during the search for a valid task plan. Our results demonstrate that FGS significantly reduces the search effort over standard heuristic search approaches by approximately 93% for tool construction.","35":"This paper is concerned with the design of a human-in-the-loop system for deployment on a smart pedelec (e-bike). From the control-theoretic perspective, the goal is not only to use the power assistance of the e-bike to reject disturbances along the route but also to manage the possibly competitive interactions between a human and the motor intervention. Managing the competitive\/cooperative nature of the interactions is crucial for applications in which we wish to control physical aspects of the cycling behavior (e.g. heart rate and breathing rate). The basis of the control is a pitchfork bifurcation system, modeling the interactions, augmented using ideas from gain-scheduling. In vivo experiments have been conducted, showing the effectiveness of the proposed control strategy.","36":"This study develops a framework for quantification of the impact of changes in population mobility due to social distancing on the COVID-19 infection growth rate. Using the Susceptible-Infected-Recovered (SIR) epidemiological model we establish that under some mild assumptions the growth rate of COVID-19 deaths is a time-delayed approximation of the growth rate of COVID-19 infections. We then hypothesize that the growth rate of COVID-19 infections is a function of population mobility, which leads to a statistical model that predicts the growth rate of COVID-19 deaths as a delayed function of population mobility. The parameters of the statistical model directly reveal the growth rate of infections, the mobility-dependent transmission rate, the mobility-independent recovery rate, and the critical mobility, below which COVID-19 growth rate becomes negative. We fitted the proposed statistical model on publicly available data from 14 countries where daily death counts exceeded 100 for more than 3 days as of May 6th, 2020. The publicly available Google Mobility Index (GMI) was used as a measure of population mobility at the country level. Our results show that the growth rate of COVID-19 deaths can be accurately estimated 20 days ahead as a quadratic function of the transit category of GMI (adjusted R-squared = 0.784). The estimated 95% confidence interval for the critical mobility is in the range between 36.1% and 47.6% of the pre-COVID-19 mobility. This result indicates that a significant reduction in population mobility is needed to reverse the growth of COVID-19 epidemic. Moreover, the quantitative relationship established herein suggests that a readily available, population-level metric such as GMI can be a useful indicator of the course of COVID-19 epidemic.","37":"This article presents a new model to predict the evolution of infective diseases under uncertainty or low-quality information, just as it has happened in the initial scenario during the CoVid-19 spread in China and Europe. The model has been used to predict the death rate in Spain but can be used to predict the demand of ICUs or mechanical ventilators under different restraint policies. The main novelty of the model is that it keeps track of the date of infection of a single individual and uses stochastic distributions to aggregate individuals who share the same date of infection. In addition, it uses two types of infections, mild and serious, with a different recovery time. These features are implemented in a set of differential equations which determine the number of Carriers, Infections, Recoveries, Hospitalized and Deaths. Comparison with real data shows good agreement.","38":"In this paper, we present a new approach based on dynamic factor models (DFMs) to perform nowcasts for the percentage annual variation of the Mexican Global Economic Activity Indicator (IGAE in Spanish). The procedure consists of the following steps: i) build a timely and correlated database by using economic and financial time series and real-time variables such as social mobility and significant topics extracted by Google Trends; ii) estimate the common factors using the two-step methodology of Doz et al. (2011); iii) use the common factors in univariate time-series models for test data; and iv) according to the best results obtained in the previous step, combine the statistically equal better nowcasts (Diebold-Mariano test) to generate the current nowcasts. We obtain timely and accurate nowcasts for the IGAE, including those for the current phase of drastic drops in the economy related to COVID-19 sanitary measures. Additionally, the approach allows us to disentangle the key variables in the DFM by estimating the confidence interval for both the factor loadings and the factor estimates. This approach can be used in official statistics to obtain preliminary estimates for IGAE up to 50 days before the official results.","39":"Recent advancements in computational power and algorithms have enabled unabridged data (e.g., raw images or audio) to be used as input in some models (e.g., deep learning). However, the black box nature of such models reduces their likelihood of adoption by marketing scholars. Our paradigm of analysis, the Transparent Model of Unabridged Data (TMUD), enables researchers to investigate the inner workings of such black box models by incorporating an ex ante filtration module and an ex post experimentation module. We empirically demonstrate the TMUD by investigating the role of facial components and sexual dimorphism in face perceptions, which have implications for four marketing contexts: advertisement (perceptions of approachability, trustworthiness, and competence), brand (perceptions of whether a face represents a brand's typical customer), category (perceptions of whether a face represents a category's typical customer), and customer persona (perceptions of whether a face represents the persona of a brand's customer segment). Our results reveal new and useful findings that enrich the existing literature on face perception, most of which is based on abridged attributes (e.g., width of mouth). The TMUD has great potential to be a useful paradigm for generating theoretical insights and may encourage more marketing researchers and practitioners to use unabridged data.","40":"In the literature, smart grids are modeled as cyber-physical power systems without considering the computational social aspects. However, end-users are playing a key role in their operation and response to disturbances via demand response and distributed energy resources. Therefore, due to the critical role of active and passive end-users and the intermittency of renewable energy, smart grids must be planned and operated by considering the computational social aspects in addition to the technical aspects. The level of cooperation, flexibility, and other social features of the various stakeholders, including consumers, prosumers, and microgrids, affect the system efficiency, reliability, and resilience. In this paper, we design an artificial society simulating the interaction between power systems and the social communities that they serve via agent-based modeling inspired by Barsade's theory on the emotional spread. The simulation results show a decline in the consumers' and prosumers' satisfaction levels induced by a shortage of electricity. It also shows the effects of social diffusion via the Internet and mass media on the satisfaction level. In view of the importance of computational social science for power system applications and the limited number of publications devoted to it, we provide a list of research topics that need to be achieved to enhance the reliability and resilience of power systems' operation and planning.","41":"Personalized medicine is expected to maximize the intended drug effects and minimize side effects by treating patients based on their genetic profiles. Thus, it is important to generate drugs based on the genetic profiles of diseases, especially in anticancer drug discovery. However, this is challenging because the vast chemical space and variations in cancer properties require a huge time resource to search for proper molecules. Therefore, an efficient and fast search method considering genetic profiles is required for de novo molecular design of anticancer drugs. Here, we propose a faster molecular generative model with genetic algorithm and tree search for cancer samples (FasterGTS). FasterGTS is constructed with a genetic algorithm and a Monte Carlo tree search with three deep neural networks: supervised learning, self-trained, and value networks, and it generates anticancer molecules based on the genetic profiles of a cancer sample. When compared to other methods, FasterGTS generated cancer sample-specific molecules with general chemical properties required for cancer drugs within the limited numbers of samplings. We expect that FasterGTS contributes to the anticancer drug generation.","42":"This paper studies a dynamic optimal reinsurance and dividend-payout problem for an insurer in a finite time horizon. The goal of the insurer is to maximize its expected cumulative discounted dividend payouts until bankruptcy or maturity which comes earlier. The insurer is allowed to dynamically choose reinsurance contracts over the whole time horizon. This is a mixed singular-classical control problem and the corresponding Hamilton-Jacobi-Bellman equation is a variational inequality with fully nonlinear operator and with gradient constraint. The $C^{2,1}$ smoothness of the value function and a comparison principle for its gradient function are established by penalty approximation method. We find that the surplus-time space can be divided into three non-overlapping regions by a risk-magnitude-and-time-dependent reinsurance barrier and a time-dependent dividend-payout barrier. The insurer should be exposed to higher risk as surplus increases; exposed to all the risks once surplus upward crosses the reinsurance barrier; and pay out all reserves in excess of the dividend-payout barrier. The localities of these regions are explicitly estimated.","43":"Signal-to-Noise Ratios (SNRs) and the Shannon-Hartley channel capacity are metrics that help define the loss of known information while transferring data through a noisy channel. These metrics cannot be used for quantifying the opposite process: the harvesting of new information. Correlation functions and correlation coefficients do play an important role in collecting new information from noisy sources. However, Bershad and Rockmore [1974] based their formulas on contradictory a priori assumptions in Real-space and in Fourier-space. Their formulations were subsequently copied literally to the practical science of electron microscopy, where those a priori assumptions now distort most quality metrics in Cryo-EM. Cryo-EM became a great success in recent years [Wiley Award 2017; Nobel prize for Chemistry 2017] and became the method of choice for revealing structures of biological complexes like ribosomes, viruses, or corona-virus spikes, vitally important during the current COVID-19 pandemic. Those early misconceptions now interfere with the objective comparison of independently obtained results. We found that the roots of these problems significantly pre-date those 1970s publications and were already inherent in the original SNR definitions. We here propose novel metrics to assess the amount of information harvested in an experiment, information which is measured in bits. These new metrics assess the total amount of information collected on an object, as well as the information density distribution within that object. The new metrics can be applied everywhere where data is collected, processed, compressed, or compared. As an example, we compare the structures of two recently published SARS-CoV-2 spike proteins. We also introduce new metrics for transducer-quality assessment in many sciences including: cryo-EM, biomedical imaging, microscopy, signal processing, photography, tomography, etc.","44":"To capture the death rates and strong weekly, biweekly and probably monthly patterns in the Canada COVID-19, we utilize the generalized additive models in the absence of direct statistically based measurement of infection rates. By examining the death rates of Canada in general and Quebec, Ontario and Alberta in particular, one can easily figured out that there are substantial overdispersion relative to the Poisson so that the negative binomial distribution is an appropriate choice for the analysis. Generalized additive models (GAMs) are one of the main modeling tools for data analysis. GAMs can efficiently combine different types of fixed, random and smooth terms in the linear predictor of a regression model to account for different types of effects. GAMs are a semi-parametric extension of the generalized linear models (GLMs), used often for the case when there is no a priori reason for choosing a particular response function such as linear, quadratic, etc. and need the data to 'speak for themselves'. GAMs do this via the smoothing functions and take each predictor variable in the model and separate it into sections delimited by 'knots', and then fit polynomial functions to each section separately, with the constraint that there are no links at the knots - second derivatives of the separate functions are equal at the knots.","45":"Automated program repair is an emerging technology that seeks to automatically rectify bugs and vulnerabilities using learning, search, and semantic analysis. Trust in automatically generated patches is necessary for achieving greater adoption of program repair. Towards this goal, we survey more than 100 software practitioners to understand the artifacts and setups needed to enhance trust in automatically generated patches. Based on the feedback from the survey on developer preferences, we quantitatively evaluate existing test-suite based program repair tools. We find that they cannot produce high-quality patches within a top-10 ranking and an acceptable time period of 1 hour. The developer feedback from our qualitative study and the observations from our quantitative examination of existing repair tools point to actionable insights to drive program repair research. Specifically, we note that producing repairs within an acceptable time-bound is very much dependent on leveraging an abstract search space representation of a rich enough search space. Moreover, while additional developer inputs are valuable for generating or ranking patches, developers do not seem to be interested in a significant human-in-the-loop interaction.","46":"The COVID-19 pandemic has profound global consequences on health, economic, social, political, and almost every major aspect of human life. Therefore, it is of great importance to model COVID-19 and other pandemics in terms of the broader social contexts in which they take place. We present the architecture of AICov, which provides an integrative deep learning framework for COVID-19 forecasting with population covariates, some of which may serve as putative risk factors. We have integrated multiple different strategies into AICov, including the ability to use deep learning strategies based on LSTM and even modeling. To demonstrate our approach, we have conducted a pilot that integrates population covariates from multiple sources. Thus, AICov not only includes data on COVID-19 cases and deaths but, more importantly, the population's socioeconomic, health and behavioral risk factors at a local level. The compiled data are fed into AICov, and thus we obtain improved prediction by integration of the data to our model as compared to one that only uses case and death data.","47":"Computer-aided diagnosis has become a necessity for accurate and immediate coronavirus disease 2019 (COVID-19) detection to aid treatment and prevent the spread of the virus. Compared to other diagnosis methodologies, chest X-ray (CXR) imaging is an advantageous tool since it is fast, low-cost, and easily accessible. Thus, CXR has a great potential not only to help diagnose COVID-19 but also to track the progression of the disease. Numerous studies have proposed to use Deep Learning techniques for COVID-19 diagnosis. However, they have used very limited CXR image repositories for evaluation with a small number, a few hundreds, of COVID-19 samples. Moreover, these methods can neither localize nor grade the severity of COVID-19 infection. For this purpose, recent studies proposed to explore the activation maps of deep networks. However, they remain inaccurate for localizing the actual infestation making them unreliable for clinical use. This study proposes a novel method for the joint localization, severity grading, and detection of COVID-19 from CXR images by generating the so-called infection maps that can accurately localize and grade the severity of COVID-19 infection. To accomplish this, we have compiled the largest COVID-19 dataset up to date with 2951 COVID-19 CXR images, where the annotation of the ground-truth segmentation masks is performed on CXRs by a novel collaborative expert human-machine approach. Furthermore, we publicly release the first CXR dataset with the ground-truth segmentation masks of the COVID-19 infected regions. A detailed set of experiments show that state-of-the-art segmentation networks can learn to localize COVID-19 infection with an F1-score of 85.81%, that is significantly superior to the activation maps created by the previous methods. Finally, the proposed approach achieved a COVID-19 detection performance with 98.37% sensitivity and 99.16% specificity.","48":"The daily routine of criminal investigators consists of a thorough analysis of highly complex and heterogeneous data of crime cases. Such data can consist of case descriptions, testimonies, criminal networks, spatial and temporal information, and virtually any other data that is relevant for the case. Criminal investigators work under heavy time pressure to analyze the data for relationships, propose and verify several hypotheses, and derive conclusions, while the data can be incomplete or inconsistent and is changed and updated throughout the investigation, as new findings are added to the case. Based on a four-year intense collaboration with criminalists, we present a conceptual design for a visual tool supporting the investigation workflow and Visilant, a web-based tool for the exploration and analysis of criminal data guided by the proposed design. Visilant aims to support namely the exploratory part of the investigation pipeline, from case overview, through exploration and hypothesis generation, to the case presentation. Visilant tracks the reasoning process and as the data is changing, it informs investigators which hypotheses are affected by the data change and should be revised. The tool was evaluated by senior criminology experts within two sessions and their feedback is summarized in the paper. Additional supplementary material contains the technical details and exemplary case study.","49":"The introduction of online marketplace platforms has led to the advent of new forms of flexible, on-demand (or 'gig') work. Yet, most prior research concerning the experience of gig workers examines delivery or crowdsourcing platforms, while the experience of the large numbers of workers who undertake educational labour in the form of tutoring gigs remains understudied. To address this, we use a computational grounded theory approach to analyse tutors' discussions on Reddit. This approach consists of three phases including data exploration, modelling and human-centred interpretation. We use both validation and human evaluation to increase the trustworthiness and reliability of the computational methods. This paper is a work in progress and reports on the first of the three phases of this approach.","50":"Background: In medical imaging, prior studies have demonstrated disparate AI performance by race, yet there is no known correlation for race on medical imaging that would be obvious to the human expert interpreting the images. Methods: Using private and public datasets we evaluate: A) performance quantification of deep learning models to detect race from medical images, including the ability of these models to generalize to external environments and across multiple imaging modalities, B) assessment of possible confounding anatomic and phenotype population features, such as disease distribution and body habitus as predictors of race, and C) investigation into the underlying mechanism by which AI models can recognize race. Findings: Standard deep learning models can be trained to predict race from medical images with high performance across multiple imaging modalities. Our findings hold under external validation conditions, as well as when models are optimized to perform clinically motivated tasks. We demonstrate this detection is not due to trivial proxies or imaging-related surrogate covariates for race, such as underlying disease distribution. Finally, we show that performance persists over all anatomical regions and frequency spectrum of the images suggesting that mitigation efforts will be challenging and demand further study. Interpretation: We emphasize that model ability to predict self-reported race is itself not the issue of importance. However, our findings that AI can trivially predict self-reported race -- even from corrupted, cropped, and noised medical images -- in a setting where clinical experts cannot, creates an enormous risk for all model deployments in medical imaging: if an AI model secretly used its knowledge of self-reported race to misclassify all Black patients, radiologists would not be able to tell using the same data the model has access to.","51":"Governments and organizations design performance-based research funding systems (PBFRS) for strategic aims, such as to selectively allocate scarce resources and stimulate research efficiency. In this work we analyze the relative change in research productivity of Italian universities after the introduction of such a system, featuring financial and reputational incentives. Using a bibliometric approach, we compare the relative research performance of universities before and after introduction of PBFRS, at the overall, discipline and field levels. The findings show convergence in the universities' performance, due above all to the remarkable improvement of the lowest performers. Geographically, the universities of the south (versus central and northern Italy) achieved the greatest improvement in relative performance. The methodology, and results, should be of use to university management and policy-makers.","52":"The graph structure of biomedical data differs from those in typical knowledge graph benchmark tasks. A particular property of biomedical data is the presence of long-range dependencies, which can be captured by patterns described as logical rules. We propose a novel method that combines these rules with a neural multi-hop reasoning approach that uses reinforcement learning. We conduct an empirical study based on the real-world task of drug repurposing by formulating this task as a link prediction problem. We apply our method to the biomedical knowledge graph Hetionet and show that our approach outperforms several baseline methods.","53":"In this study, we examine a set of primary data collected from 484 students enrolled in a large public university in the Mid-Atlantic United States region during the early stages of the COVID-19 pandemic. The data, called Ties data, included students' demographic and support network information. The support network data comprised of information that highlighted the type of support, (i.e. emotional or educational; routine or intense). Using this data set, models for predicting students' academic achievement, quantified by their self-reported GPA, were created using Chi-Square Automatic Interaction Detection (CHAID), a decision tree algorithm, and cforest, a random forest algorithm that uses conditional inference trees. We compare the methods' accuracy and variation in the set of important variables suggested by each algorithm. Each algorithm found different variables important for different student demographics with some overlap. For White students, different types of educational support were important in predicting academic achievement, while for non-White students, different types of emotional support were important in predicting academic achievement. The presence of differing types of routine support were important in predicting academic achievement for cisgender women, while differing types of intense support were important in predicting academic achievement for cisgender men.","54":"As renewable resources gradually replace conventional generation based synchronous machines, the dynamics of the modern grid changes significantly and the system synchronous inertia decreases substantially. This transformation poses severe challenges for power system stability; for instance, it may lead to larger initial rate of change of frequency and increase frequency excursions. However, new opportunities also arise as novelconverter control techniques, so-called grid-forming strategies, show higher efficiency and faster response than conventional synchronous generators. They mainly involve virtual inertia (VI) emulation to mimic the behavior of synchronous machines. In this study, a state-space model for the power system network is developed with VI as a frequency regulation method. A reduced model based-norm algorithm (RMHA) considering the Fiedler mode impact is proposed in this paper to optimize the allocation of VI devices and improve power system frequency stability. Finally, case studies conducted on the IEEE 24-bus system demonstrate the efficacy of the proposed RMHA approach.","55":"In March 2021, Turkey withdrew from The Istanbul Convention, a human-rights treaty that addresses violence against women, citing issues with the convention's implicit recognition of sexual and gender minorities. In this work, we trace disinformation campaigns related to the Istanbul Convention and its associated Turkish law that circulate on divorced men's rights Facebook groups. We find that these groups adjusted the narrative and focus of the campaigns to appeal to a larger audience, which we refer to as\"tactical reframing.\"Initially, the men organized in a grass-roots manner to campaign against the Turkish law that was passed to codify the convention, focusing on one-sided custody of children and indefinite alimony. Later, they reframed their campaign and began attacking the Istanbul Convention, highlighting its acknowledgment of homosexuality. This case study highlights how disinformation campaigns can be used to weaponize homophobia in order to limit the rights of women. To the best of our knowledge, this is the first case study that analyzes a narrative reframing in the context of a disinformation campaign on social media.","56":"The COVID-19 pandemic has posed an unprecedented challenge to individuals around the globe. To mitigate the spread of the virus, many states in the U.S. issued lockdown orders to urge their residents to stay at their homes, avoid get-togethers, and minimize physical interactions. While many offline workers are experiencing significant challenges performing their duties, digital technologies have provided ample tools for individuals to continue working and to maintain their productivity. Although using digital platforms to build resilience in remote work is effective, other aspects of remote work (beyond the continuation of work) should also be considered in gauging true resilience. In this study, we focus on content creators, and investigate how restrictions in individual's physical environment impact their online content creation behavior. Exploiting a natural experimental setting wherein four states issued state-wide lockdown orders on the same day whereas five states never issued a lockdown order, and using a unique dataset collected from a short video-sharing social media platform, we study the impact of lockdown orders on content creators' behaviors in terms of content volume, content novelty, and content optimism. We combined econometric methods (difference-in-differences estimations of a matched sample) with machine learning-based natural language processing to show that on average, compared to the users residing in non-lockdown states, the users residing in lockdown states create more content after the lockdown order enforcement. However, we find a decrease in the novelty level and optimism of the content generated by the latter group. Our findings have important contributions to the digital resilience literature and shed light on managers' decision-making process related to the adjustment of employees' work mode in the long run.","57":"The outbreak of the COVID-19 pandemic has changed our lives in unprecedented ways. In the face of the projected catastrophic consequences, many countries have enacted social distancing measures in an attempt to limit the spread of the virus. Under these conditions, the Web has become an indispensable medium for information acquisition, communication, and entertainment. At the same time, unfortunately, the Web is being exploited for the dissemination of potentially harmful and disturbing content, such as the spread of conspiracy theories and hateful speech towards specific ethnic groups, in particular towards Chinese people since COVID-19 is believed to have originated from China. In this paper, we make a first attempt to study the emergence of Sinophobic behavior on the Web during the outbreak of the COVID-19 pandemic. We collect two large-scale datasets from Twitter and 4chan's Politically Incorrect board (\/pol\/) over a time period of approximately five months and analyze them to investigate whether there is a rise or important differences with regard to the dissemination of Sinophobic content. We find that COVID-19 indeed drives the rise of Sinophobia on the Web and that the dissemination of Sinophobic content is a cross-platform phenomenon: it exists on fringe Web communities like \\dspol, and to a lesser extent on mainstream ones like Twitter. Also, using word embeddings over time, we characterize the evolution and emergence of new Sinophobic slurs on both Twitter and \/pol\/. Finally, we find interesting differences in the context in which words related to Chinese people are used on the Web before and after the COVID-19 outbreak: on Twitter we observe a shift towards blaming China for the situation, while on \/pol\/ we find a shift towards using more (and new) Sinophobic slurs.","58":"The increasing digitization of medical imaging enables machine learning based improvements in detecting, visualizing and segmenting lesions, easing the workload for medical experts. However, supervised machine learning requires reliable labelled data, which is is often difficult or impossible to collect or at least time consuming and thereby costly. Therefore methods requiring only partly labeled data (semi-supervised) or no labeling at all (unsupervised methods) have been applied more regularly. Anomaly detection is one possible methodology that is able to leverage semi-supervised and unsupervised methods to handle medical imaging tasks like classification and segmentation. This paper uses a semi-exhaustive literature review of relevant anomaly detection papers in medical imaging to cluster into applications, highlight important results, establish lessons learned and give further advice on how to approach anomaly detection in medical imaging. The qualitative analysis is based on google scholar and 4 different search terms, resulting in 120 different analysed papers. The main results showed that the current research is mostly motivated by reducing the need for labelled data. Also, the successful and substantial amount of research in the brain MRI domain shows the potential for applications in further domains like OCT and chest X-ray.","59":"Online search data is routinely used to monitor the prevalence of infectious diseases, such as influenza. Previous work has focused on supervised learning solutions, where ground truth data, in the form of historical syndromic surveillance reports, can be used to train machine learning models. However, no sufficient data -- in terms of accuracy and time span -- exist to apply such approaches for monitoring the emerging COVID-19 infectious disease pandemic caused by a novel coronavirus (SARS-CoV-2). Therefore, unsupervised, or semi-supervised solutions should be sought. Recent outcomes have shown that it is possible to transfer an online search based model for influenza-like illness from a source to a target country without using ground truth data for the target location. The transferred model's accuracy depends on choosing search queries and their corresponding weights wisely, via a transfer learning methodology, for the target location. In this work, we draw a parallel to previous findings and attempt to develop an unsupervised model for COVID-19 by: (i) carefully choosing search queries that refer to related symptoms as identified by a survey from the National Health Service in the United Kingdom (UK), and (ii) weighting them based on their reported ratio of occurrence in people infected by COVID-19. Furthermore, understanding that online searches may be also driven by concern rather than infections, we devise a preliminary approach that attempts to minimise this part of the signal by incorporating a basic news media coverage metric in association with confirmed COVID-19 cases. Finally, we propose a transfer learning method for mapping supervised COVID-19 models from a country to another, in an effort to transfer knowledge from areas where the disease has a more extended progression.","60":"We use an enhanced methodology combining specific forms of AI techniques, opinion mining and artificial mathematical intelligence (AMI), with public data on the spread of the coronavirus SARS-CoV-2 and the incidence of COVID-19 disease in Colombia during the first three months since the first reported positive case. The results obtained, together with conceptual tools coming from the global taxonomy of fundamental cognitive mechanisms emerging in AMI and with suitable contextual information from Colombian public health and mainstream social media, allowed us to stating specific preventive guidelines for a better restructuring of initial safe and stable life conditions in Colombia, and in an extended manner in similar Latin American Countries. More specifically, we describe three major guidelines: 1) regular creative visualization and effective planning, 2) the continuous use of constructive linguistic frameworks, and 3) frequent and moderate use of kinesthetic routines. They should be understood as effective tools from a cognitive and behavioural perspective, rather than from a biological one. Even more, the first two guidelines should be acknowledged in integral cooperation with the third one regarding the global effect of COVID-19 in human beings as a whole, this includes the mind and body.","61":"We investigate several rank-based change-point procedures for the covariance operator in a sequence of observed functions, called FKWC change-point procedures. Our methods allow the user to test for one change-point, to test for an epidemic period, or to detect an unknown amount of change-points in the data. Our methodology combines functional data depth values with the traditional Kruskal Wallis test statistic. By taking this approach we have no need to estimate the covariance operator, which makes our methods computationally cheap. For example, our procedure can identify multiple change-points in $O(n\\log n)$ time. Our procedure is fully non-parametric and is robust to outliers through the use of data depth ranks. We show that when $n$ is large, our methods have simple behaviour under the null hypothesis.We also show that the FKWC change-point procedures are $n^{-1\/2}$-consistent. In addition to asymptotic results, we provide a finite sample accuracy result for our at-most-one change-point estimator. In simulation, we compare our methods against several others. We also present an application of our methods to intraday asset returns and f-MRI scans.","62":"Fake news on social media is increasingly regarded as one of the most concerning issues. Low cost, simple accessibility via social platforms, and a plethora of low-budget online news sources are some of the factors that contribute to the spread of false news. Most of the existing fake news detection algorithms are solely focused on the news content only but engaged users prior posts or social activities provide a wealth of information about their views on news and have significant ability to improve fake news identification. Graph Neural Networks are a form of deep learning approach that conducts prediction on graph-described data. Social media platforms are followed graph structure in their representation, Graph Neural Network are special types of neural networks that could be usually applied to graphs, making it much easier to execute edge, node, and graph-level prediction. Therefore, in this paper, we present a comparative analysis among some commonly used machine learning algorithms and Graph Neural Networks for detecting the spread of false news on social media platforms. In this study, we take the UPFD dataset and implement several existing machine learning algorithms on text data only. Besides this, we create different GNN layers for fusing graph-structured news propagation data and the text data as the node feature in our GNN models. GNNs provide the best solutions to the dilemma of identifying false news in our research.","63":"Supply Chains (SCs) are subject to disruptive events that potentially hinder the operational performance. Disruption Management Process (DMP) relies on the analysis of integrated heterogeneous data sources such as production scheduling, order management and logistics to evaluate the impact of disruptions on the SC. Existing approaches are limited as they address DMP process steps and corresponding data sources in a rather isolated manner which hurdles the systematic handling of a disruption originating anywhere in the SC. Thus, we propose MARE a semantic disruption management and resilience evaluation framework for integration of data sources included in all DMP steps, i.e. Monitor\/Model, Assess, Recover and Evaluate. MARE, leverages semantic technologies i.e. ontologies, knowledge graphs and SPARQL queries to model and reproduce SC behavior under disruptive scenarios. Also, MARE includes an evaluation framework to examine the restoration performance of a SC applying various recovery strategies. Semantic SC DMP, put forward by MARE, allows stakeholders to potentially identify the measures to enhance SC integration, increase the resilience of supply networks and ultimately facilitate digitalization.","64":"Many NLP applications, such as biomedical data and technical support, have 10-100 million tokens of in-domain data and limited computational resources for learning from it. How should we train a language model in this scenario? Most language modeling research considers either a small dataset with a closed vocabulary (like the standard 1 million token Penn Treebank), or the whole web with byte-pair encoding. We show that for our target setting in English, initialising and freezing input embeddings using in-domain data can improve language model performance by providing a useful representation of rare words, and this pattern holds across several different domains. In the process, we show that the standard convention of tying input and output embeddings does not improve perplexity when initializing with embeddings trained on in-domain data.","65":"In order to slow the spread of the CoViD-19 pandemic, governments around the world have enacted a wide set of policies limiting the transmission of the disease. Initially, these focused on non-pharmaceutical interventions; more recently, vaccinations and large-scale rapid testing have started to play a major role. The objective of this study is to explain the quantitative effects of these policies on determining the course of the pandemic, allowing for factors like seasonality or virus strains with different transmission profiles. To do so, the study develops an agent-based simulation model, which is estimated using data for the second and the third wave of the CoViD-19 pandemic in Germany. The paper finds that during a period where vaccination rates rose from 5% to 40%, large-scale rapid testing had the largest effect on reducing infection numbers. Frequent large-scale rapid testing should remain part of strategies to contain CoViD-19; it can substitute for many non-pharmaceutical interventions that come at a much larger cost to individuals, society, and the economy.","66":"Federal administrative tax data are invaluable for research, but because of privacy concerns, access to these data is typically limited to select agencies and a few individuals. An alternative to sharing microlevel data are validation servers, which allow individuals to query statistics without accessing the confidential data. This paper studies the feasibility of using differentially private (DP) methods to implement such a server. We provide an extensive study on existing DP methods for releasing tabular statistics, means, quantiles, and regression estimates. We also include new methodological adaptations to existing DP regression algorithms for using new data types and returning standard error estimates. We evaluate the selected methods based on the accuracy of the output for statistical analyses, using real administrative tax data obtained from the Internal Revenue Service Statistics of Income (SOI) Division. Our findings show that a validation server would be feasible for simple statistics but would struggle to produce accurate regression estimates and confidence intervals. We outline challenges and offer recommendations for future work on validation servers. This is the first comprehensive statistical study of DP methodology on a real, complex dataset, that has significant implications for the direction of a growing research field.","67":"This work proposes a novel deep neural network (DNN) architecture, Implicit Segmentation Neural Network (ISNet), to solve the task of image segmentation followed by classification. It substitutes the common pipeline of two DNNs with a single model. We designed the ISNet for high flexibility and performance: it allows virtually any classification neural network architecture to analyze a common image as if it had been previously segmented. Furthermore, in relation to the unmodified classifier, the ISNet does not cause any increment in computational cost at run-time. We test the architecture with two applications: COVID-19 detection in chest X-rays, and facial attribute estimation. We implement an ISNet based on a DenseNet121 classifier, and compare the model to a U-net (performing lung\/face segmentation) followed by a DenseNet121, and to a standalone DenseNet121. The new architecture matched the other DNNs in facial attribute estimation. Moreover, it strongly surpassed them in COVID-19 detection, according to an external test dataset. The ISNet precisely ignored the image regions outside of the lungs or faces. Therefore, in COVID-19 detection it reduced the effects of background bias and shortcut learning, and it improved security in facial attribute estimation. ISNet presents an accurate, fast, and light methodology. The successful implicit segmentation, considering two largely diverse fields, highlights the architecture's general applicability.","68":"We introduce a new model for contagion spread using a network of interacting finite memory two-color P\\'{o}lya urns, which we refer to as the finite memory interacting P\\'{o}lya contagion network. The urns interact in the sense that the probability of drawing a red ball (which represents an infection state) for a given urn, not only depends on the ratio of red balls in that urn but also on the ratio of red balls in the other urns in the network, hence accounting for the effect of spatial contagion. The resulting network-wide contagion process is a discrete-time finite-memory ($M$th order) Markov process, whose transition probability matrix is determined. The stochastic properties of the network contagion Markov process are analytically examined, and for homogeneous system parameters, we characterize the limiting state of infection in each urn. For the non-homogeneous case, given the complexity of the stochastic process, and in the same spirit as the well-studied SIS models, we use a mean-field type approximation to obtain a discrete-time dynamical system for the finite memory interacting P\\'{o}lya contagion network. Interestingly, for $M=1$, we obtain a linear dynamical system which exactly represents the corresponding Markov process. For $M>1$, we use mean-field approximation to obtain a nonlinear dynamical system. Furthermore, noting that the latter dynamical system admits a linear variant (realized by retaining its leading linear terms), we study the asymptotic behavior of the linear systems for both memory modes and characterize their equilibrium. Finally, we present simulation studies to assess the quality of the approximation purveyed by the linear and non-linear dynamical systems.","69":"We consider the NP-complete problem of tracking paths in a graph, first introduced by Banik et. al. [3]. Given an undirected graph with a source $s$ and a destination $t$, find the smallest subset of vertices whose intersection with any $s-t$ path results in a unique sequence. In this paper, we show that this problem remains NP-complete when the graph is planar and we give a 4-approximation algorithm in this setting. We also show, via Courcelle's theorem, that it can be solved in linear time for graphs of bounded-clique width, when its clique decomposition is given in advance.","70":"The high dependence on imported fuels and the potential for both climate change mitigation and economic diversification make Barbados' energy system particularly interesting for detailed transformation analysis. An open source energy system model is presented here for the analysis of a future Barbadian energy system. The model was applied in a scenario analysis, using a greenfield approach, to investigate cost-optimal and 100% renewable energy system configurations. Within the scenarios, the electrification of private passenger vehicles and cruise ships through shore-to-ship power supply was modelled to assess its impact on the energy system and the necessary investment in storage. Results show that for most scenarios of a system in 2030, a renewable energy share of over 80% is achieved in cost-optimal cases, even with a growing demand. The system's levelised costs of electricity range from 0.17 to 0.36 BBD\/kWh in the cost-optimal scenarios and increase only moderately for 100% renewable systems. Under the reasonable assumption of decreasing photovoltaic investment costs, system costs of a 100% system may be lower than the current costs. The results show that pumped hydro-storage is a no-regret option for the Barbadian power system design. Overall, the results highlight the great potential of renewable energy as well as the technical and economic feasibility of a 100% renewable energy system for Barbados.","71":"When the Covid-19 pandemic enters dangerous new phase, whether and when to take aggressive public health interventions to slow down the spread of COVID-19. To develop the artificial intelligence (AI) inspired methods for real-time forecasting and evaluating intervention strategies to curb the spread of Covid-19 in the World. A modified auto-encoder for modeling the transmission dynamics of the epidemics is developed and applied to the surveillance data of cumulative and new Covid-19 cases and deaths from WHO, as of March 16, 2020. The average errors of 5-step forecasting were 2.5%. The total peak number of cumulative cases and new cases, and the maximum number of cumulative cases in the world with later intervention (comprehensive public health intervention is implemented 4 weeks later) could reach 75,249,909, 10,086,085, and 255,392,154, respectively. The case ending time was January 10, 2021. However, the total peak number of cumulative cases and new cases and the maximum number of cumulative cases in the world with one week later intervention were reduced to 951,799, 108,853 and 1,530,276, respectively. Duration time of the Covid-19 spread would be reduced from 356 days to 232 days. The case ending time was September 8, 2020. We observed that delaying intervention for one month caused the maximum number of cumulative cases to increase 166.89 times, and the number of deaths increase from 53,560 to 8,938,725. We will face disastrous consequences if immediate action to intervene is not taken.","72":"The Internet of Things, crowdsourcing, social media, public authorities, and other sources generate bigger and bigger data sets. Big and open data offers many benefits for emergency management, but also pose new challenges. This chapter will review the sources of big data and their characteristics. We then discuss potential benefits of big data for emergency management along with the technological and societal challenges it poses. We review central technologies for big-data storage and processing in general, before presenting the Spark big-data engine in more detail. Finally, we review ethical and societal threats that big data pose.","73":"The COVID-19 pandemic has caused severe public health consequences in the United States. In this study, we use a hierarchical Bayesian model to estimate the age-specific COVID-19 attributable deaths over time in the United States. The model is specified by a novel non-parametric spatial approach, a low-rank Gaussian Process (GP) projected by regularised B-splines. We show that this projection defines a new GP with attractive smoothness and computational efficiency properties, derive its kernel function, and discuss the penalty terms induced by the projected GP. Simulation analyses and benchmark results show that the spatial approach performs better than standard B-splines and Bayesian P-splines and equivalently well as a standard GP, for considerably lower runtimes. The B-splines projected GP priors that we develop are likely an appealing addition to the arsenal of Bayesian regularising priors. We apply the model to weekly, age-stratified COVID-19 attributable deaths reported by the US Centers for Disease Control, which are subject to censoring and reporting biases. Using the B-splines projected GP, we can estimate longitudinal trends in COVID-19 associated deaths across the US by 1-year age bands. These estimates are instrumental to calculate age-specific mortality rates, describe variation in age-specific deaths across the US, and for fitting epidemic models. Here, we couple the model with age-specific vaccination rates to show that lower vaccination rates in younger adults aged 18-64 are associated with significantly stronger resurgences in COVID-19 deaths, especially in Florida and Texas. These results underscore the critical importance of medically able individuals of all ages to be vaccinated against COVID-19 in order to limit fatal outcomes.","74":"This paper proposes a new method for determining similarity and anomalies between time series, most practically effective in large collections of (likely related) time series, with a particular focus on measuring distances between structural breaks within such a collection. We consolidate and generalise a class of semi-metric distance measures, which we term MJ distances. Experiments on simulated data demonstrate that our proposed family of distances uncover similarity within collections of time series more effectively than measures such as the Hausdorff and Wasserstein metrics. Although our class of distances do not necessarily satisfy the triangle inequality requirement of a metric, we analyse the transitivity properties of respective distance matrices in various contextual scenarios. There, we demonstrate a trade-off between robust performance in the presence of outliers, and the triangle inequality property. We show in experiments using real data that the contrived scenarios that severely violate the transitivity property rarely exhibit themselves in real data; instead, our family of measures satisfies all the properties of a metric most of the time. We illustrate three ways of analysing the distance and similarity matrices, via eigenvalue analysis, hierarchical clustering, and spectral clustering. The results from our hierarchical and spectral clustering experiments on simulated data demonstrate that the Hausdorff and Wasserstein metrics may lead to erroneous inference as to which time series are most similar with respect to their structural breaks, while our semi-metrics provide an improvement.","75":"Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions; and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.","76":"The COVID-19 pandemic has birthed a wealth of information through many publicly accessible sources, such as news outlets and social media. However, gathering and understanding the content can be difficult due to inaccuracies or inconsistencies between the different sources. To alleviate this challenge in Australia, a team of 48 student volunteers developed an open-source COVID-19 information dashboard to provide accurate, reliable, and real-time COVID-19 information for Australians. The students developed this software while working under legislative restrictions that required social isolation. The goal of this study is to characterize the experiences of the students throughout the project. We conducted an online survey completed by 39 of the volunteering students contributing to the COVID-19 dashboard project. Our results indicate that playing a positive role in the COVID-19 crisis and learning new skills and technologies were the most cited motivating factors for the students to participate in the project. While working on the project, some students struggled to maintain a work-life balance due to working from home. However, the students generally did not express strong sentiment towards general project challenges. The students expressed more strongly that data collection was a significant challenge as it was difficult to collect reliable, accurate, and up-to-date data from various government sources. The students have been able to mitigate these challenges by establishing a systematic data collection process in the team, leveraging frequent and clear communication through text, and appreciating and encouraging each other's efforts. By participating in the project, the students boosted their technical (e.g., front-end development) and non-technical (e.g., task prioritization) skills. Our study discusses several implications for students, educators, and policymakers.","77":"Recent years have witnessed the rapid accumulation of massive electronic medical records (EMRs), which highly support the intelligent medical services such as drug recommendation. However, prior arts mainly follow the traditional recommendation strategies like collaborative filtering, which usually treat individual drugs as mutually independent, while the latent interactions among drugs, e.g., synergistic or antagonistic effect, have been largely ignored. To that end, in this paper, we target at developing a new paradigm for drug package recommendation with considering the interaction effect within drugs, in which the interaction effects could be affected by patient conditions. Specifically, we first design a pre-training method based on neural collaborative filtering to get the initial embedding of patients and drugs. Then, the drug interaction graph will be initialized based on medical records and domain knowledge. Along this line, we propose a new Drug Package Recommendation (DPR) framework with two variants, respectively DPR on Weighted Graph (DPR-WG) and DPR on Attributed Graph (DPR-AG) to solve the problem, in which each the interactions will be described as signed weights or attribute vectors. In detail, a mask layer is utilized to capture the impact of patient condition, and graph neural networks (GNNs) are leveraged for the final graph induction task to embed the package. Extensive experiments on a real-world data set from a first-rate hospital demonstrate the effectiveness of our DPR framework compared with several competitive baseline methods, and further support the heuristic study for the drug package generation task with adequate performance.","78":"Forecasting has emerged as an important component of informed, data-driven decision-making in a wide array of fields. We introduce a new data model for probabilistic predictions that encompasses a wide range of forecasting settings. This framework clearly defines the constituent parts of a probabilistic forecast and proposes one approach for representing these data elements. The data model is implemented in Zoltar, a new software application that stores forecasts using the data model and provides standardized API access to the data. In one real-time case study, an instance of the Zoltar web application was used to store, provide access to, and evaluate real-time forecast data on the order of 10$^7$ rows, provided by over 20 international research teams from academia and industry making forecasts of the COVID-19 outbreak in the US. Tools and data infrastructure for probabilistic forecasts, such as those introduced here, will play an increasingly important role in ensuring that future forecasting research adheres to a strict set of rigorous and reproducible standards.","79":"We begin this thesis with an extensive pedagogical introduction aimed at clarifying the foundations of the hierarchy problem. After introducing effective field theory, we discuss renormalization at length from a variety of perspectives. We focus on conceptual understanding and connections between approaches, while providing a plethora of examples for clarity. With that background we can then clearly understand the hierarchy problem, which is reviewed primarily by introducing and refuting common misconceptions thereof. We next discuss some of the beautiful classic frameworks to approach the issue. However, we argue that the LHC data have qualitatively modified the issue into `The Loerarchy Problem'---how to generate an IR scale without accompanying visible structure---and we discuss recent work on this approach. In the second half, we present some of our own work in these directions, beginning with explorations of how the Neutral Naturalness approach motivates novel signatures of electroweak naturalness at a variety of physics frontiers. Finally, we propose a New Trail for Naturalness and suggest that the physical breakdown of EFT, which gravity demands, may be responsible for the violation of our EFT expectations at the LHC.","80":"We present a test of the statistical method introduced by Bernard F. Shutz in 1986 using only gravitational waves to infer the Hubble constant (H$_0$) from GW190814, the first first high-probability neutron-star--black-hole (NS-BH) merger candidate detected by the Laser Interferometer Gravitational Wave Observatory (LIGO) and the Virgo interferometer. We apply a baseline test of this method to the binary neutron star (BNS) merger GW170817 and find H$_0 = 70^{+35.0}_{-18.0}$ km s$^{-1}$ Mpc$^{-1}$ (maximum a posteriori and 68.3% highest density posterior interval) for a galaxy $B$-band luminosity threshold of $L_B \\geq 0.001 L_B^*$ with a correction for catalog incompleteness. Repeating the calculation for GW190814, we obtain H$_0 = 67^{+41.0}_{-26.0}$ km s$^{-1}$ Mpc$^{-1}$ and H$_0 = 71^{+34.0}_{-30.0}$ km s$^{-1}$ Mpc$^{-1}$ for $L_B \\geq 0.001 L_B^*$ and $L_B \\geq 0.626 L_B^*$, respectively. Combining the posteriors for both events yields H$_0 = 70^{+29.0}_{-18.0}$ km s$^{-1}$ Mpc$^{-1}$, demonstrating the improvement on constraints when using multiple gravitational wave events. We also confirm the results of other works that adopt this method, showing that increasing the $L_B$ threshold enhances the posterior structure and slightly shifts the distribution's peak to higher H$_0$ values. We repeat the joint inference using the low-spin PhenomPNRT and combined (SEOBNRv4PHM + IMRPhenomPv3HM) posterior samples for GW170817 and GW190814, respectively, achieving a tighter constraint of H$_0 = 69^{+29.0}_{-14.0}$ km s$^{-1}$ Mpc$^{-1}$.","81":"In this Letter we show that multiband observations of stellar-mass binary black holes by the next generation of ground-based observatories (3G) and the space-based Laser Interferometer Space Antenna (LISA) would facilitate a comprehensive test of general relativity by simultaneously measuring all the post-Newtonian (PN) coefficients. Multiband observations would measure most of the known PN phasing coefficients to an accuracy below a few percent---two orders-of-magnitude better than the best bounds achievable from even `golden' binaries in the 3G or LISA bands. Such multiparameter bounds would play a pivotal role in constraining the parameter space of modified theories of gravity beyond general relativity.","82":"Graph Representation Learning methods have enabled a wide range of learning problems to be addressed for data that can be represented in graph form. Nevertheless, several real world problems in economy, biology, medicine and other fields raised relevant scaling problems with existing methods and their software implementation, due to the size of real world graphs characterized by millions of nodes and billions of edges. We present GraPE, a software resource for graph processing and random walk based embedding, that can scale with large and high-degree graphs and significantly speed up-computation. GraPE comprises specialized data structures, algorithms, and a fast parallel implementation that displays everal orders of magnitude improvement in empirical space and time complexity compared to state of the art software resources, with a corresponding boost in the performance of machine learning methods for edge and node label prediction and for the unsupervised analysis of graphs.GraPE is designed to run on laptop and desktop computers, as well as on high performance computing clusters","83":"The new era of the Internet of Things (IoT) is changing our urban lives in every way. With the support of recent IoT technologies, IoT-enabled smart cities are capable of building extensive data networks incorporating a large number of heterogeneous devices participating in the data collection and decision-making process. This massive data backbone formed by IoT devices allows systems to perform real-time environmental monitoring and actuating. Coupled with high flexibility and ease of deployment, IoT-based solutions can be easily adopted for different application scenarios. However, there are many challenges that system designers would need to consider before employing large-scale IoT deployment. Our current infrastructure will have to quickly adapt to handle the big data in our systems, supply interoperability for cross-functional performance, and provide the cognitive capabilities for system intelligence. Most importantly, we must consider a new spectrum of security challenges arising from the new context. In this paper, we aim to provide general security guidelines for IoT-enabled smart city developments by 1) presenting some of the latest innovations in IoT-enabled smart cities, and highlighting common security challenges and research opportunities; 2) reviewing recent cryptographic security implementations for IoT-enabled smart cities; 3) using the Activity-Network-Things architecture to analyze major security challenges in IoT deployments and proposing a set of specialized security requirements for IoT-enabled smart cities; 4) providing a discussion on the potential prospect for IoT and IoT security.","84":"Conventional wisdom suggests that large-scale refugees pose security threats to the host community or state. With massive influx of Rohingyas in Bangladesh in 2017 resulting a staggering total of 1.6 million Rohingyas, a popular discourse emerged that Bangladesh would face severe security threats. This article investigates the security experience of Bangladesh in case of Rohingya influx over a three-year period, August 2017 to August 2020. The research question I intend to address is, 'has Bangladesh experienced security threat due to massive Rohingya influx?' If so in what ways? I test four security threat areas: societal security, economic security, internal security, and public security. I have used newspaper content analysis over past three years along with interview data collected from interviewing local people in coxs bazar area where the Rohingya camps are located. To assess if the threats are low level, medium level, or high level, I investigated both the frequency of reports and the way they are interpreted. I find that Bangladesh did not experience any serious security threats over the last three years. There are some criminal activities and offenses, but these are only low-level security threat at best. My research presents empirical evidence that challenges conventional assertions that refugees are security threats or challenges to the host states.","85":"Pandemic and epidemic diseases such as CoVID-19, SARS-CoV2, and Ebola have spread to multiple countries and infected thousands of people. Such diseases spread mainly through person-to-person contacts. Health care authorities recommend contact tracing procedures to prevent the spread to a vast population. Although several mobile applications have been developed to trace contacts, they typically require collection of privacy-intrusive information such as GPS locations, and the logging of privacy-sensitive data on a third party server, or require additional infrastructure such as WiFi APs with known locations. In this paper, we introduce CONTAIN, a privacy-oriented mobile contact tracing application that does not rely on GPS or any other form of infrastructure-based location sensing, nor the continuous logging of any other personally identifiable information on a server. The goal of CONTAIN is to allow users to determine with complete privacy if they have been within a short distance, specifically, Bluetooth wireless range, of someone that is infected, and potentially also when. We identify and prove the privacy guarantees provided by our approach. Our simulation study utilizing an empirical trace dataset (Asturies) involving 100 mobile devices and around 60000 records shows that users can maximize their possibility of identifying if they were near an infected user by turning on the app during active times.","86":"Knowing the state of our health at every moment in time is critical for advances in health science. Using data obtained outside an episodic clinical setting is the first step towards building a continuous health estimation system. In this paper, we explore a system that allows users to combine events and data streams from different sources to retrieve complex biological events, such as cardiovascular volume overload. These complex events, which have been explored in biomedical literature and which we call interface events, have a direct causal impact on relevant biological systems. They are the interface through which the lifestyle events influence our health. We retrieve the interface events from existing events and data streams by encoding domain knowledge using an event operator language.","87":"Viral infections are among the main reasons for serious pandemics and contagious infections; hence, they cause thousands of fatalities and economic losses annually. In the case of COVID-19, world economies have shut down for months, and physical distancing along with drastic changes in the social behavior of many humans has generated many issues for all countries. Thus, a rapid, low-cost, and sensitive viral detection method is critical to upgrade the living standards of humans while exploiting biomedicine, environmental science, bioresearch, and biosecurity. The emergence of various carbon-based nanomaterials such as carbon nanotubes, graphene, and carbon nanoparticles provided a great possibility for researchers to develop a new and wide variety of biosensors. In particular, graphene has become a promising tool for biosensor fabrication owing to its many interesting properties, such as its exceptional conductivity, ultrahigh electron mobility, and excellent thermal conductivity. This paper provides an overall perspective of graphene-based biosensors and a critical review of the recent advances in graphene-based biosensors that are used to detect different types of viruses, such as Ebola, Zika, and influenza.","88":"COVID-19 has become a global pandemic and is still posing a severe health risk to the public. Accurate and efficient segmentation of pneumonia lesions in CT scans is vital for treatment decision-making. We proposed a novel unsupervised approach using cycle consistent generative adversarial network (cycle-GAN) which automates and accelerates the process of lesion delineation. The workflow includes lung volume segmentation,\"synthetic\"healthy lung generation, infected and healthy image subtraction, and binary lesion mask creation. The lung volume volume was firstly delineated using a pre-trained U-net and worked as the input for the later network. The cycle-GAN was developed to generate synthetic\"healthy\"lung CT images from infected lung images. After that, the pneumonia lesions are extracted by subtracting the synthetic\"healthy\"lung CT images from the\"infected\"lung CT images. A median filter and K-means clustering were then applied to contour the lesions. The auto segmentation approach was validated on two public datasets (Coronacases and Radiopedia). The Dice coefficients reached 0.748 and 0.730, respectively, for the Coronacases and Radiopedia datasets. Meanwhile, the precision and sensitivity for lesion segmentationdetection are 0.813 and 0.735 for the Coronacases dataset, and 0.773 and 0.726 for the Radiopedia dataset. The performance is comparable to existing supervised segmentation networks and outperforms previous unsupervised ones. The proposed unsupervised segmentation method achieved high accuracy and efficiency in automatic COVID-19 lesion delineation. The segmentation result can serve as a baseline for further manual modification and a quality assurance tool for lesion diagnosis. Furthermore, due to its unsupervised nature, the result is not influenced by physicians' experience which otherwise is crucial for supervised methods.","89":"The inclusion of intermittent and renewable energy sources has increased the importance of demand forecasting in power systems. Smart meters can play a critical role in demand forecasting due to the measurement granularity they provide. Despite their virtue, smart meters used for forecasting face some constraints as consumers' privacy concerns, reluctance of utilities and vendors to share data with competitors or third parties, and regulatory constraints. This paper examines a collaborative machine learning method, federated learning extended with privacy preserving techniques for short-term demand forecasting using smart meter data as a solution to the previous constraints. The combination of privacy preserving techniques and federated learning enables to ensure consumers' confidentiality concerning both their data, the models generated using it (Differential Privacy), and the communication mean (Secure Aggregation). To evaluate this paper's collaborative secure federated learning setting, we explore current literature to select the baseline for our simulations and evaluation. We simulate and evaluate several scenarios that explore how traditional centralized approaches could be projected in the direction of a decentralized, collaborative and private system. The results obtained over the evaluations provided decent performance and in a privacy setting using differential privacy almost perfect privacy budgets (1.39,$10e^{-5}$) and (2.01,$10e^{-5}$) with a negligible performance compromise.","90":"Online extremism is a growing and pernicious problem, and increasingly linked to real-world violence. We introduce a new resource to help research and understand it: ExtremeBB is a structured textual dataset containing nearly 44M posts made by more than 300K registered members on 12 different online extremist forums, enabling both qualitative and quantitative large-scale analyses of historical trends going back two decades. It enables us to trace the evolution of different strands of extremist ideology; to measure levels of toxicity while exploring and developing the tools to do so better; to track the relationships between online subcultures and external political movements such as MAGA and to explore links with misogyny and violence, including radicalisation and recruitment. To illustrate a few potential uses, we apply statistical and data-mining techniques to analyse the online extremist landscape in a variety of ways, from posting patterns through topic modelling to toxicity and the membership overlap across different communities. A picture emerges of communities working as support networks, with complex discussions over a wide variety of topics. The discussions of many topics show a level of disagreement which challenges the perception of homogeneity among these groups. These two features of mutual support and a wide range of attitudes lead us to suggest a more nuanced policy approach than simply shutting down these websites. Censorship might remove the support that lonely and troubled individuals are receiving, and fuel paranoid perceptions that the world is against them, though this must be balanced with other benefits of de-platforming. ExtremeBB can help develop a better understanding of these sub-cultures which may lead to more effective interventions; it also opens up the prospect of research to monitor the effectiveness of any interventions that are undertaken.","91":"Wearing a mask has proven to be one of the most effective ways to prevent the transmission of SARS-CoV-2 coronavirus. However, wearing a mask poses challenges for different face recognition tasks and raises concerns about the performance of masked face presentation detection (PAD). The main issues facing the mask face PAD are the wrongly classified bona fide masked faces and the wrongly classified partial attacks (covered by real masks). This work addresses these issues by proposing a method that considers partial attack labels to supervise the PAD model training, as well as regional weighted inference to further improve the PAD performance by varying the focus on different facial areas. Our proposed method is not directly linked to specific network architecture and thus can be directly incorporated into any common or custom-designed network. In our work, two neural networks (DeepPixBis and MixFaceNet) are selected as backbones. The experiments are demonstrated on the collaborative real mask attack (CRMA) database. Our proposed method outperforms established PAD methods in the CRMA database by reducing the mentioned shortcomings when facing masked faces. Moreover, we present a detailed step-wise ablation study pointing out the individual and joint benefits of the proposed concepts on the overall PAD performance.","92":"Fast and accurate hourly forecasts of wind speed and power are crucial in quantifying and planning the energy budget in the electric grid. Modeling wind at a high resolution brings forth considerable challenges given its turbulent and highly nonlinear dynamics. In developing countries, where wind farms over a large domain are currently under construction or consideration, this is even more challenging given the necessity of modeling wind over space as well. In this work, we propose a machine learning approach to model the nonlinear hourly wind dynamics in Saudi Arabia with a domain-specific choice of knots to reduce the spatial dimensionality. Our results show that for locations highlighted as wind abundant by a previous work, our approach results in an 11% improvement in the two-hour-ahead forecasted power against operational standards in the wind energy sector, yielding a saving of nearly one million US dollars over a year under current market prices in Saudi Arabia.","93":"The GeMSE (Germanium Material and meteorite Screening Experiment) facility operates a low-background HPGe crystal in an underground laboratory with a moderate rock overburden of 620$\\,$m.w.e. in Switzerland. It has been optimized for continuous remote operation. A multi-layer passive shielding, a muon veto, and a boil-off nitrogen purge line inside the measurement cavity minimize the instrument's background rate, which decreased by 33% to (164$\\pm$2)$\\,$counts\/day (100-2700$\\,$keV) after five years of underground operation. This agrees with the prediction based on the expected decay of short-lived isotopes. A fit to the known background components, modeled via a precise simulation of the detector, shows that the GeMSE background is now muon-dominated. We also present updates towards a more accurate detection efficiency calculation for the screened samples: the thickness of the crystal's outer dead-layer is precisely determined and the efficiency can now be easily calculated for any sample geometry. The advantage of this feature is showcased via the determination of the $^{40}$K content in the screening of a complex-shaped object: a banana.","94":"This is a non-technical introduction into theory of contextuality. More precisely, it presents the basics of a theory of contextuality called Contextuality-by-Default (CbD). One of the main tenets of CbD is that the identity of a random variable is determined not only by its content (that which is measured or responded to) but also by contexts, systematically recorded conditions under which the variable is observed; and the variables in different contexts possess no joint distributions. I explain why this principle has no paradoxical consequences, and why it does not support the holistic\"everything depends on everything else\"view. Contextuality is defined as the difference between two differences: (1) the difference between content-sharing random variables when taken in isolation, and (2) the difference between the same random variables when taken within their contexts. Contextuality thus defined is a special form of context-dependence rather than a synonym for the latter. The theory applies to any empirical situation describable in terms of random variables. Deterministic situations are trivially noncontextual in CbD, but some of them can be described by systems of epistemic random variables, in which random variability is replaced with epistemic uncertainty. Mathematically, such systems are treated as if they were ordinary systems of random variables.","95":"The paper develops a stochastic Agent-Based Model (ABM) mimicking the spread of infectious diseases in geographical domains. The model is designed to simulate the spatiotemporal spread of SARS-CoV2 disease, known as COVID-19. Our SARS-CoV2-based ABM framework (CoV-ABM) simulates the spread at any geographical scale, ranging from a village to a country and considers unique characteristics of SARS-CoV2 viruses such as its persistence in the environment. Therefore, unlike other simulators, CoV-ABM computes the density of active viruses inside each location space to get the virus transmission probability for each agent. It also uses the local census and health data to create health and risk factor profiles for each individual. The proposed model relies on a flexible timestamp scale to optimize the computational speed and the level of detail. In our framework each agent represents a person interacting with the surrounding space and other adjacent agents inside the same space. Moreover, families stochastic daily tasks are formulated to get tracked by the corresponding family members. The model also formulates the possibility of meetings for each subset of friendships and relatives. The main aim of the proposed framework is threefold: to illustrate the dynamics of SARS-CoV diseases, to identify places which have a higher probability to become infection hubs and to provide a decision-support system to design efficient interventions in order to fight against pandemics. The framework employs SEIHRD dynamics of viral diseases with different intervention scenarios. The paper simulates the spread of COVID-19 in the State of Delaware, United States, with near one million stochastic agents. The results achieved over a period of 15 weeks with a timestamp of 1 hour show which places become the hubs of infection. The paper also illustrates how hospitals get overwhelmed as the outbreak reaches its pick.","96":"According to the World Health Organization, development of the COVID-19 vaccine is occurring in record time. Administration of the vaccine has started the same year as the declaration of the COVID-19 pandemic. The United Nations emphasized the importance of providing COVID-19 vaccines as\"a global public good\", which is accessible and affordable world-wide. Pricing the COVID-19 vaccines is a controversial topic. We use optimization and game theoretic approaches to model the COVID-19 U.S. vaccine market as a duopoly with two manufacturers Pfizer-BioNTech and Moderna. The results suggest that even in the context of very high production and distribution costs, the government can negotiate prices with the manufacturers to keep public sector prices as low as possible while meeting demand and ensuring each manufacturer earns a target profit. Furthermore, these prices are consistent with those currently predicted in the media.","97":"Visual explanation methods have an important role in the prognosis of the patients where the annotated data is limited or unavailable. There have been several attempts to use gradient-based attribution methods to localize pathology from medical scans without using segmentation labels. This research direction has been impeded by the lack of robustness and reliability. These methods are highly sensitive to the network parameters. In this study, we introduce a robust visual explanation method to address this problem for medical applications. We provide an innovative visual explanation algorithm for general purpose and as an example application, we demonstrate its effectiveness for quantifying lesions in the lungs caused by the Covid-19 with high accuracy and robustness without using dense segmentation labels. This approach overcomes the drawbacks of commonly used Grad-CAM and its extended versions. The premise behind our proposed strategy is that the information flow is minimized while ensuring the classifier prediction stays similar. Our findings indicate that the bottleneck condition provides a more stable severity estimation than the similar attribution methods.","98":"We present a simple primary colour editing method for consumer product images. We show that by using colour correction and colour blending, we can automate the pain-staking colour editing task and save time for consumer colour preference researchers. To improve the colour harmony between the primary colour and its complementary colours, our algorithm also tunes the other colours in the image. Preliminary experiment has shown some promising results compared with a state-of-the-art method and human editing.","99":"The COVID-19 pandemic has resulted in significant social and economic impacts throughout the world. In addition to the health consequences, the impacts on traffic behaviors have also been sudden and dramatic. We have analyzed how the road traffic safety of New York City, Los Angeles, and Boston in the U.S. have been impacted by the pandemic and corresponding local government orders and restrictions. To be specific, we have studied the accident hotspots' distributions before and after the outbreak of the pandemic and found that traffic accidents have shifted in both location and time compared to previous years. In addition, we have studied the road network characteristics in those hotspot regions with the hope to understand the underlying cause of the hotspot shifts.","100":"Digital systems for analyzing human communication data have become prevalent in recent years. Intelligence analysis of communications data in investigative journalism, criminal intelligence, and law present particularly interesting cases, as they must take into account the often highly sensitive properties of the underlying operations and data. At the same time, these are areas where increasingly automated, sophisticated approaches systems can be particularly relevant, especially in terms of Big Data manageability. However, by the shifting of responsibilities, this also poses dangers. In addition to privacy concerns, these dangers relate to uncertain or poor data quality, leading to discrimination and potentially misleading insights. Visual analytics combines machine learning methods with interactive visual interfaces to enable human sense- and decision-making. This technique can be key for designing and operating meaningful interactive communication analysis systems that consider these ethical challenges. In this interdisciplinary work, a joint endeavor of computer scientists, ethicists, and scholars in Science&Technology Studies, we investigate and evaluate opportunities and risks involved in using Visual analytics approaches for communication analysis in intelligence applications in particular. We introduce, at first, the common technological systems used in communication analysis, further discussing the domain-specific ethical implications, tensions, and risks involved. We then make the case of how tailored Visual Analytics approaches may reduce and mitigate the described problems, both theoretically and through practical examples. We show that finding Visual Analytics design solutions for ethical issues is not a mere optimization task, but balancing out and negotiating these trade-offs has, as we argue, to be an integral aspect of the system design process from the outset.","101":"The successful application of machine learning (ML) methods becomes increasingly dependent on their interpretability or explainability. Designing explainable ML systems is instrumental to ensuring transparency of automated decision-making that targets humans. The explainability of ML methods is also an essential ingredient for trustworthy artificial intelligence. A key challenge in ensuring explainability is its dependence on the specific human user (\"explainee\"). The users of machine learning methods might have vastly different background knowledge about machine learning principles. One user might have a university degree in machine learning or related fields, while another user might have never received formal training in high-school mathematics. This paper applies information-theoretic concepts to develop a novel measure for the subjective explainability of the predictions delivered by a ML method. We construct this measure via the conditional entropy of predictions, given a user signal. This user signal might be obtained from user surveys or biophysical measurements. Our main contribution is the explainable empirical risk minimization (EERM) principle of learning a hypothesis that optimally balances between the subjective explainability and risk. The EERM principle is flexible and can be combined with arbitrary machine learning models. We present several practical implementations of EERM for linear models and decision trees. Numerical experiments demonstrate the application of EERM to detecting the use of inappropriate language on social media.","102":"Natural ventilation is gaining popularity in response to an increasing demand for a sustainable and healthy built environment, but the design of a naturally ventilated building can be challenging due to the inherent variability in the operating conditions that determine the natural ventilation flow. Large-eddy simulations (LES) have significant potential as an analysis method for natural ventilation flow, since they can provide an accurate prediction of turbulent flow at any location in the computational domain. However, the simulations can be computationally expensive, and few validation and sensitivity studies have been reported. The objectives of this study are to validate LES of wind-driven cross-ventilation and to quantify the sensitivity of the solution to the grid resolution and the inflow boundary conditions. We perform LES for an isolated building with two openings, using three different grid resolutions and two different inflow conditions with varying turbulence intensities. Predictions of the ventilation rate are compared to a reference wind-tunnel experiment available from literature, and we also quantify the age of air and ventilation efficiency. The results show that a sufficiently fine grid resolution is needed to provide accurate predictions of the detailed flow pattern and the age of air, while the inflow condition is found to affect the standard deviation of the instantaneous ventilation rate. However, for the cross-ventilation case modeled in this paper, the prediction of the mean ventilation flow rate is very robust, showing negligible sensitivity to the grid resolution or the inflow characteristics.","103":"Augmented reality (AR) is one of the relatively old, yet trending areas in the intersection of computer vision and computer graphics with numerous applications in several areas, from gaming and entertainment, to education and healthcare. Although it has been around for nearly fifty years, it has seen a lot of interest by the research community in the recent years, mainly because of the huge success of deep learning models for various computer vision and AR applications, which made creating new generations of AR technologies possible. This work tries to provide an overview of modern augmented reality, from both application-level and technical perspective. We first give an overview of main AR applications, grouped into more than ten categories. We then give an overview of around 100 recent promising machine learning based works developed for AR systems, such as deep learning works for AR shopping (clothing, makeup), AR based image filters (such as Snapchat's lenses), AR animations, and more. In the end we discuss about some of the current challenges in AR domain, and the future directions in this area.","104":"The efficient calculation of the centrality or\"hierarchy\"of nodes in a network has gained great relevance in recent years due to the generation of large amounts of data. The eigenvector centrality is quickly becoming a good metric for centrality due to both its simplicity and fidelity. In this work we lay the foundations for the calculation of eigenvector centrality using quantum computational paradigms such as quantum annealing and gate-based quantum computing. The problem is reformulated as a quadratic unconstrained binary optimization (QUBO) that can be solved on both quantum architectures. The results focus on correctly identifying a given number of the most important nodes in numerous networks given by our QUBO formulation of eigenvector centrality on both the D-Wave and IBM quantum computers.","105":"An important goal in the field of human-AI interaction is to help users more appropriately trust AI systems' decisions. A situation in which the user may particularly benefit from more appropriate trust is when the AI receives anomalous input or provides anomalous output. To the best of our knowledge, this is the first work towards understanding how anomaly alerts may contribute to appropriate trust of AI. In a formative mixed-methods study with 4 radiologists and 4 other physicians, we explore how AI alerts for anomalous input, very high and low confidence, and anomalous saliency-map explanations affect users' experience with mockups of an AI clinical decision support system (CDSS) for evaluating chest x-rays for pneumonia. We find evidence suggesting that the four anomaly alerts are desired by non-radiologists, and the high-confidence alerts are desired by both radiologists and non-radiologists. In a follow-up user study, we investigate how high- and low-confidence alerts affect the accuracy and thus appropriate trust of 33 radiologists working with AI CDSS mockups. We observe that these alerts do not improve users' accuracy or experience and discuss potential reasons why.","106":"We study the dynamic evolution of COVID-19 cased by the Omicron variant via a fractional susceptible-exposedinfected-removed (SEIR) model. Preliminary data suggest that the symptoms of Omicron infection are not prominent and the transmission is therefore more concealed, which causes a relatively slow increase in the detected cases of the new infected at the beginning of the pandemic. To characterize the specific dynamics, the Caputo-Hadamard fractional derivative is adopted to refined the classical SEIR model. Based on the reported data, we infer the fractional order, timedependent parameters, as well as unobserved dynamics of the fractional SEIR model via fractional physics-informed neural networks (fPINNs). Then, we make short-time predictions using the learned fractional SEIR model.","107":"Is more novel research always desirable? We develop a model in which knowledge shapes society's policies and guides the search for discoveries. Researchers select a question and how intensely to study it. The novelty of a question determines both the value and the difficulty of discovering its answer. We show that the benefits of discoveries are non-monotone in novelty. Through a dynamic externality, moonshots -- research on questions more novel than myopically optimal -- can improve the evolution of knowledge. Incentivizing moonshots requires promising ex-post rewards. However, even a myopic funder combines rewards with ex-ante cost reductions to increase research effort.","108":"As of July 31, 2020, the COVID-19 pandemic has over 17 million reported cases, causing more than 667,000 deaths. Countries irrespective of economic status have succumbed to this pandemic. Many aspects of the lives, including health, economy, freedom of movement have been negatively affected by the coronavirus outbreak. Numerous strategies have been taken in order to prevent the outbreak. Some countries took severe resections in the form of full-scale lockdown, while others took a moderate approach of dealing with the pandemics, for example, mass testing, prohibiting large-scale public gatherings, restricting international travels. South America adopted primarily the lockdown strategies due to inadequate economy and health care support. Since the social interactions between the people are primarily affected by the lockdown, psychological distress, e.g. anxiety, stress, fear are supposedly affecting the South American population in a severe way. This paper aims to explore the impact of lockdown over the psychological aspect of the people of all the Spanish speaking South American capitals. We have utilized infodemiology approach by employing large-scale Twitter data-set over 33 million feeds in order to understand people's interaction over the months of this on-going coronavirus pandemic. Our result is surprising: at the beginning of the pandemic, people demonstrated strong emotions (i.e. anxiety, worry, fear) which declined over time even though the actual pandemic is worsening by having more positive cases, and inflicting more deaths. This leads us to speculate that the South American population is adapting to this pandemic thus improving the overall psychological distress.","109":"Plasma, the fourth and most pervasive state of matter in the visible universe, is a fascinating medium that is connected to the beginning of our universe itself. Man-made plasmas are at the core of many technological advances that include the fabrication of semiconductor devices, which enabled the modern computer and communication revolutions. The introduction of low temperature, atmospheric pressure plasmas to the biomedical field has ushered a new revolution in the healthcare arena that promises to introduce plasma-based therapies to combat some thorny and long-standing medical challenges. This paper presents an overview of where research is at today and discusses innovative concepts and approaches to overcome present challenges and take the field to the next level. It is written by a team of experts who took an in-depth look at the various biomedical applications, made critical analysis, and proposed ideas and concepts that should help the research community focus their efforts on clear and practical steps necessary to keep the field advancing for decades to come.","110":"Various tools and practices have been developed to support practitioners in identifying, assessing, and mitigating fairness-related harms caused by AI systems. However, prior research has highlighted gaps between the intended design of these tools and practices and their use within particular contexts, including gaps caused by the role that organizational factors play in shaping fairness work. In this paper, we investigate these gaps for one such practice: disaggregated evaluations of AI systems, intended to uncover performance disparities between demographic groups. By conducting semi-structured interviews and structured workshops with thirty-three AI practitioners from ten teams at three technology companies, we identify practitioners' processes, challenges, and needs for support when designing disaggregated evaluations. We find that practitioners face challenges when choosing performance metrics, identifying the most relevant direct stakeholders and demographic groups on which to focus, and collecting datasets with which to conduct disaggregated evaluations. More generally, we identify impacts on fairness work stemming from a lack of engagement with direct stakeholders or domain experts, business imperatives that prioritize customers over marginalized groups, and the drive to deploy AI systems at scale.","111":"Forecasting has always been at the forefront of decision making and planning. The uncertainty that surrounds the future is both exciting and challenging, with individuals and organisations seeking to minimise risks and maximise utilities. The large number of forecasting applications calls for a diverse set of forecasting methods to tackle real-life challenges. This article provides a non-systematic review of the theory and the practice of forecasting. We provide an overview of a wide range of theoretical, state-of-the-art models, methods, principles, and approaches to prepare, produce, organise, and evaluate forecasts. We then demonstrate how such theoretical concepts are applied in a variety of real-life contexts. We do not claim that this review is an exhaustive list of methods and applications. However, we wish that our encyclopedic presentation will offer a point of reference for the rich work that has been undertaken over the last decades, with some key insights for the future of forecasting theory and practice. Given its encyclopedic nature, the intended mode of reading is non-linear. We offer cross-references to allow the readers to navigate through the various topics. We complement the theoretical concepts and applications covered by large lists of free or open-source software implementations and publicly-available databases.","112":"Chatbots have revolutionized the way humans interact with computer systems and they have substituted the use of service agents, call-center representatives etc. Fitness industry has always been a growing industry although it has not adapted to the latest technologies like AI, ML and cloud computing. In this paper, we propose an idea to develop a chatbot for fitness management using IBM Watson and integrate it with a web application. We proposed using Natural Language Processing (NLP) and Natural Language Understanding (NLU) along with frameworks of IBM Cloud Watson provided for the Chatbot Assistant. This software uses a serverless architecture to combine the services of a professional by offering diet plans, home exercises, interactive counseling sessions, fitness recommendations.","113":"The looping pendulum is a simple physical system consisting of two masses connected by a string that passes over a rod. We derive equations of motion for the looping pendulum using Newtonian mechanics, and show that these equations can be solved numerically to give a good description of the system's dynamics. The numerical solution captures complex aspects of the looping pendulum's behavior, and is in good agreement with the experimental results.","114":"Justifying draconian measures during the Covid-19 pandemic was difficult not only because of the restriction of individual rights, but also because of its economic impact. The objective of this work is to present a machine learning approach to identify regions that should implement similar health policies. For that end, we successfully developed a system that gives a notion of economic impact given the prediction of new incidental cases through unsupervised learning and time series forecasting. This system was built taking into account computational restrictions and low maintenance requirements in order to improve the system's resilience. Finally this system was deployed as part of a web application for simulation and data analysis of COVID-19, in Colombia, available at (https:\/\/covid19.dis.eafit.edu.co).","115":"We study how homophily of human physical interactions affects the efficacy of digital proximity tracing. Analytical results show a non monotonous dependence of the reproduction number with respect to the mixing rate between individuals that adopt the contact tracing app and the ones that do not. Furthermore, we find regimes in which the attack rate has local optima, minima or monotonously varies with the mixing rate. We corroborate our findings with Monte Carlo simulations on a primary-school network. This study provides a mathematical basis to better understand how homophily in health behavior shapes the dynamics of epidemics.","116":"Access-Control Lists (ACLs) (a.k.a. friend lists) are one of the most important privacy features of Online Social Networks (OSNs) as they allow users to restrict the audience of their publications. Nevertheless, creating and maintaining custom ACLs can introduce a high cognitive burden on average OSNs users since it normally requires assessing the trustworthiness of a large number of contacts. In principle, community detection algorithms can be leveraged to support the generation of ACLs by mapping a set of examples (i.e. contacts labelled as untrusted) to the emerging communities inside the user's ego-network. However, unlike users' access-control preferences, traditional community-detection algorithms do not take the homophily characteristics of such communities into account (i.e. attributes shared among members). Consequently, this strategy may lead to inaccurate ACL configurations and privacy breaches under certain homophily scenarios. This work investigates the use of community-detection algorithms for the automatic generation of ACLs in OSNs. Particularly, it analyses the performance of the aforementioned approach under different homophily conditions through a simulation model. Furthermore, since private information may reach the scope of untrusted recipients through the re-sharing affordances of OSNs, information diffusion processes are also modelled and taken explicitly into account. Altogether, the removal of gatekeeper nodes is further explored as a strategy to counteract unwanted data dissemination.","117":"Data is the king in the age of AI. However data integration is often a laborious task that is hard to automate. Schema change is one significant obstacle to the automation of the end-to-end data integration process. Although there exist mechanisms such as query discovery and schema modification language to handle the problem, these approaches can only work with the assumption that the schema is maintained by a database. However, we observe diversified schema changes in heterogeneous data and open data, most of which has no schema defined. In this work, we propose to use deep learning to automatically deal with schema changes through a super cell representation and automatic injection of perturbations to the training data to make the model robust to schema changes. Our experimental results demonstrate that our proposed approach is effective for two real-world data integration scenarios: coronavirus data integration, and machine log integration.","118":"Deepfakes are a recent off-the-shelf manipulation technique that allows anyone to swap two identities in a single video. In addition to Deepfakes, a variety of GAN-based face swapping methods have also been published with accompanying code. To counter this emerging threat, we have constructed an extremely large face swap video dataset to enable the training of detection models, and organized the accompanying DeepFake Detection Challenge (DFDC) Kaggle competition. Importantly, all recorded subjects agreed to participate in and have their likenesses modified during the construction of the face-swapped dataset. The DFDC dataset is by far the largest currently and publicly available face swap video dataset, with over 100,000 total clips sourced from 3,426 paid actors, produced with several Deepfake, GAN-based, and non-learned methods. In addition to describing the methods used to construct the dataset, we provide a detailed analysis of the top submissions from the Kaggle contest. We show although Deepfake detection is extremely difficult and still an unsolved problem, a Deepfake detection model trained only on the DFDC can generalize to real\"in-the-wild\"Deepfake videos, and such a model can be a valuable analysis tool when analyzing potentially Deepfaked videos. Training, validation and testing corpuses can be downloaded from https:\/\/ai.facebook.com\/datasets\/dfdc.","119":"Data collection and processing via digital public health technologies are being promoted worldwide by governments and private companies as strategic remedies for mitigating the COVID-19 pandemic and loosening lockdown measures. However, the ethical and legal boundaries of deploying digital tools for disease surveillance and control purposes are unclear, and a rapidly evolving debate has emerged globally around the promises and risks of mobilizing digital tools for public health. To help scientists and policymakers navigate technological and ethical uncertainty, we present a typology of the primary digital public health applications currently in use. Namely: proximity and contact tracing, symptom monitoring, quarantine control, and flow modeling. For each, we discuss context-specific risks, cross-sectional issues, and ethical concerns. Finally, in recognition of the need for practical guidance, we propose a navigation aid for policymakers made up of ten steps for the ethical use of digital public health tools.","120":"The goal of this document is to openly contribute with our comments to the EOSCArchitecture report: Scholarly Infrastructures for Research Software (SIRS), and thus, to participate in the European Open Science Cloud (EOSC) architecture design.","121":"The presence of people in an urban area throughout the day -- often called 'urban vitality' -- is one of the qualities world-class cities aspire to the most, yet it is one of the hardest to achieve. Back in the 1970s, Jane Jacobs theorized urban vitality and found that there are four conditions required for the promotion of life in cities: diversity of land use, small block sizes, the mix of economic activities, and concentration of people. To build proxies for those four conditions and ultimately test Jane Jacobs's theory at scale, researchers have had to collect both private and public data from a variety of sources, and that took decades. Here we propose the use of one single source of data, which happens to be publicly available: Sentinel-2 satellite imagery. In particular, since the first two conditions (diversity of land use and small block sizes) are visible to the naked eye from satellite imagery, we tested whether we could automatically extract them with a state-of-the-art deep-learning framework and whether, in the end, the extracted features could predict vitality. In six Italian cities for which we had call data records, we found that our framework is able to explain on average 55% of the variance in urban vitality extracted from those records.","122":"Conversational commerce, first pioneered by Apple's Siri, is the first of may applications based on always-on artificial intelligence systems that decide on its own when to interact with the environment, potentially collecting 24x7 longitudinal training data that is often Personally Identifiable Information (PII). A large body of scholarly papers, on the order of a million according to a simple Google Scholar search, suggests that the treatment of many health conditions, including COVID-19 and dementia, can be vastly improved by this data if the dataset is large enough as it has happened in other domains (e.g. GPT3). In contrast, current dominant systems are closed garden solutions without wake neutrality and that can't fully exploit the PII data they have because of IRB and Cohues-type constraints. We present a voice browser-and-server architecture that aims to address these two limitations by offering wake neutrality and the possibility to handle PII aiming to maximize its value. We have implemented this browser for the collection of speech samples and have successfully demonstrated it can capture over 200.000 samples of COVID-19 coughs. The architecture we propose is designed so it can grow beyond our kind into other domains such as collecting sound samples from vehicles, video images from nature, ingestible robotics, multi-modal signals (EEG, EKG,...), or even interacting with other kinds such as dogs and cats.","123":"Local delivery platforms are collaborative undertakings where local stores offer instant-delivery to local customers ordering their products online. Offering such delivery services both cost-efficiently and reliably is one of the main challenges for local delivery platforms, as they face a complex, dynamic, stochastic dynamic pickup-and-delivery problem. Orders need to be consolidated to increase the efficiency of the delivery operations and thereby enable a high service guarantee towards the customer and stores. But, waiting for consolidation opportunities may jeopardize delivery service reliability in the future, and thus requires anticipating future demand. This paper introduces a generic approach to balance the consolidation potential and delivery urgency of orders. Specifically, it presents a newly developed parameterized Cost-Function Approximation (CFA) approach that modifies a set-packing formulation with two parameters. This CFA approach not only anticipates future demand but also utilizes column generation to search the large decision space related to pickup-and-delivery problems fast. Inspired by a motivating application in the city of Groningen, the Netherlands, numerical experiments show that our CFA approach strongly increases perceived customer satisfaction while lowering the total travel time of the vehicles compared to various benchmark policies. Furthermore, our CFA also reduces the percentage of late deliveries, and their lateness, to a minimum. Finally, our approach may assist managers in practice to manage the non-trivial balance between consolidation opportunity and delivery urgency.","124":"With the emergence and rapid proliferation of social media platforms and social networking sites, recent years have witnessed a surge of misinformation spreading in our daily life. Drawing on a large-scale dataset which covers more than 1.4M posts and 18M comments, we investigate the propagation of two distinct narratives--(i) conspiracy information, whose claims are generally unsubstantiated and thus referred as misinformation to some extent, and (ii) scientific information, whose origins are generally readily identifiable and verifiable--in an online social media platform. We find that conspiracy cascades tend to propagate in a multigenerational branching process while science cascades are more likely to grow in a breadth-first manner. Specifically, conspiracy information triggers larger cascades, involves more users and generations, persists longer, is more viral and bursty than science information. Content analysis reveals that conspiracy cascades contain more negative words and emotional words which convey anger, fear, disgust, surprise and trust. We also find that conspiracy cascades are more concerned with political and controversial topics. After applying machine learning models, we achieve an AUC score of nearly 90% in discriminating conspiracy from science narratives using the constructed features. We also find that conspiracy cascades are more likely to be controlled by a broader set of users than science cascades. Although political affinity is thought to affect the consumption of misinformation, there is very few evidence that political orientation of the information source plays a role during the propagation of conspiracy information. Our study provides complementing evidence to current misinformation research and has practical policy implications to stem the propagation and mitigate the influence of misinformation online.","125":"This paper presents the Multilingual COVID-19 Analysis Method (CMTA) for detecting and observing the spread of misinformation about this disease within texts. CMTA proposes a data science (DS) pipeline that applies machine learning models for processing, classifying (Dense-CNN) and analyzing (MBERT) multilingual (micro)-texts. DS pipeline data preparation tasks extract features from multilingual textual data and categorize it into specific information classes (i.e., 'false', 'partly false', 'misleading'). The CMTA pipeline has been experimented with multilingual micro-texts (tweets), showing misinformation spread across different languages. To assess the performance of CMTA and put it in perspective, we performed a comparative analysis of CMTA with eight monolingual models used for detecting misinformation. The comparison shows that CMTA has surpassed various monolingual models and suggests that it can be used as a general method for detecting misinformation in multilingual micro-texts. CMTA experimental results show misinformation trends about COVID-19 in different languages during the first pandemic months.","126":"Social distancing in public spaces has become an essential aspect in helping to reduce the impact of the COVID-19 pandemic. Exploiting recent advances in machine learning, there have been many studies in the literature implementing social distancing via object detection through the use of surveillance cameras in public spaces. However, to date, there has been no study of social distance measurement on public transport. The public transport setting has some unique challenges, including some low-resolution images and camera locations that can lead to the partial occlusion of passengers, which make it challenging to perform accurate detection. Thus, in this paper, we investigate the challenges of performing accurate social distance measurement on public transportation. We benchmark several state-of-the-art object detection algorithms using real-world footage taken from the London Underground and bus network. The work highlights the complexity of performing social distancing measurement on images from current public transportation onboard cameras. Further, exploiting domain knowledge of expected passenger behaviour, we attempt to improve the quality of the detections using various strategies and show improvement over using vanilla object detection alone.","127":"The COVID-19 pandemic has resulted in a slew of misinformation, often described as an\"infodemic\". Whereas previous research has focused on the propagation of unreliable sources as a main vehicle of misinformation, the present study focuses on exploring the role of scientists whose views oppose the scientific consensus. Using Nobelists in Physiology and Medicine as a proxy for scientific consensus, we analyze two separate datasets: 15.8K tweets by 13.1K unique users on COVID-19 vaccines specifically, and 208K tweets by 151K unique users on COVID-19 broadly which mention the Nobelist names. Our analyses reveal that dissenting scientists are amplified by a factor of 426 relative to true scientific consensus in the context of COVID-19 vaccines, and by a factor of 43 in the context of COVID-19 generally. Although more popular accounts tend to mention consensus-abiding scientists more, our results suggest that this false consensus is driven by higher engagement with dissent-mentioning tweets. Furthermore, false consensus mostly occurs due to traffic spikes following highly popularized statements of dissenting scientists. We find that dissenting voices are mainly discussed in French, English-speaking, Turkish, Brazilian, Argentine, Indian, and Japanese misinformation clusters. This research suggests that social media platforms should prioritize the exposure of consensus-abiding scientists as a vehicle of reversing false consensus and addressing misinformation stemming from seemingly credible sources.","128":"Background and Objective: Artificial intelligence (AI) methods coupled with biomedical analysis has a critical role during pandemics as it helps to release the overwhelming pressure from healthcare systems and physicians. As the ongoing COVID-19 crisis worsens in countries having dense populations and inadequate testing kits like Brazil and India, radiological imaging can act as an important diagnostic tool to accurately classify covid-19 patients and prescribe the necessary treatment in due time. With this motivation, we present our study based on deep learning architecture for detecting covid-19 infected lungs using chest X-rays. Dataset: We collected a total of 2470 images for three different class labels, namely, healthy lungs, ordinary pneumonia, and covid-19 infected pneumonia, out of which 470 X-ray images belong to the covid-19 category. Methods: We first pre-process all the images using histogram equalization techniques and segment them using U-net architecture. VGG-16 network is then used for feature extraction from the pre-processed images which is further sampled by SMOTE oversampling technique to achieve a balanced dataset. Finally, the class-balanced features are classified using a support vector machine (SVM) classifier with 10-fold cross-validation and the accuracy is evaluated. Result and Conclusion: Our novel approach combining well-known pre-processing techniques, feature extraction methods, and dataset balancing method, lead us to an outstanding rate of recognition of 98% for COVID-19 images over a dataset of 2470 X-ray images. Our model is therefore fit to be utilized in healthcare facilities for screening purposes.","129":"Along with the rapid growth and rise to prominence of food delivery platforms, concerns have also risen about the terms of employment of the gig workers underpinning this growth. Our analysis on data derived from a real-world food delivery platform across three large cities from India show that there is significant inequality in the money delivery agents earn. In this paper, we formulate the problem of fair income distribution among agents while also ensuring timely food delivery. We establish that the problem is not only NP-hard but also inapproximable in polynomial time. We overcome this computational bottleneck through a novel matching algorithm called FairFoody. Extensive experiments over real-world food delivery datasets show FairFoody imparts up to 10 times improvement in equitable income distribution when compared to baseline strategies, while also ensuring minimal impact on customer experience.","130":"The Census TopDown Algorithm (TDA) is a disclosure avoidance system using differential privacy for privacy-loss accounting. The algorithm ingests the final, edited version of the 2020 Census data and the final tabulation geographic definitions. The algorithm then creates noisy versions of key queries on the data, referred to as measurements, using zero-Concentrated Differential Privacy. Another key aspect of the TDA are invariants, statistics that the Census Bureau has determined, as matter of policy, to exclude from the privacy-loss accounting. The TDA post-processes the measurements together with the invariants to produce a Microdata Detail File (MDF) that contains one record for each person and one record for each housing unit enumerated in the 2020 Census. The MDF is passed to the 2020 Census tabulation system to produce the 2020 Census Redistricting Data (P.L. 94-171) Summary File. This paper describes the mathematics and testing of the TDA for this purpose.","131":"We conduct a comparative analysis of desktop web search behaviour of users from Germany (n=558) and Switzerland (n=563) based on a combination of web tracking and survey data. We find that web search accounts for 13% of all desktop browsing, with the share being higher in Switzerland than in Germany. We find that in over 50% of cases users clicked on the first search result, with over 97% of all clicks being made on the first page of search outputs. Most users rely on Google when conducting searches, and users preferences for other engines are related to their demographics. We also test relationships between user demographics and daily number of searches, average share of search activities among tracked events by user as well as the tendency to click on higher- or lower-ranked results. We find differences in such relationships between the two countries that highlights the importance of comparative research in this domain. Further, we observe differences in the temporal patterns of web search use between women and men, marking the necessity of disaggregating data by gender in observational studies regarding online information behaviour.","132":"We present the collaborative model of ESiWACE2 Services, where Research Software Engineers (RSEs) from the Netherlands eScience Center (NLeSC) and Atos offer their expertise to climate and earth system modeling groups across Europe. Within 6-month collaborative projects, the RSEs intend to provide guidance and advice regarding the performance, portability to new architectures, and scalability of selected applications. We present the four awarded projects as examples of this funding structure.","133":"Aim of this manuscript is to show a simple method to infer the time-course of new COVID-19 infections (the most important information in order to establish the effect of containment strategies) from available aggregated data, such as number of deaths and hospitalizations. The method, that was used for HIV-AIDS and was named `back-calculation', relies on good estimates of the distribution of the delays between infection and the observed events; assuming that the epidemic follows a simple SIR model with a known generation interval, we can then estimate the parameters that define the time-varying contact rate through maximum likelihood. We show the application of the method to data from Italy and several of its region; it is found that $R_0$ had decreased consistently below 1 around March 20, and in the beginning of April it was between 0.5 and 0.8 in the whole Italy and in most regions.","134":"The #MeToo movement on Twitter has drawn attention to the pervasive nature of sexual harassment and violence. While #MeToo has been praised for providing support for self-disclosures of harassment or violence and shifting societal response, it has also been criticized for exemplifying how women of color have been discounted for their historical contributions to and excluded from feminist movements. Through an analysis of over 600,000 tweets from over 256,000 unique users, we examine online #MeToo conversations across gender and racial\/ethnic identities and the topics that each demographic emphasized. We found that tweets authored by white women were overrepresented in the movement compared to other demographics, aligning with criticism of unequal representation. We found that intersected identities contributed differing narratives to frame the movement, co-opted the movement to raise visibility in parallel ongoing movements, employed the same hashtags both critically and supportively, and revived and created new hashtags in response to pivotal moments. Notably, tweets authored by black women often expressed emotional support and were critical about differential treatment in the justice system and by police. In comparison, tweets authored by white women and men often highlighted sexual harassment and violence by public figures and weaved in more general political discussions. We discuss the implications of work for digital activism research and design including suggestions to raise visibility by those who were under-represented in this hashtag activism movement. Content warning: this article discusses issues of sexual harassment and violence.","135":"Coronavirus Disease 2019 (COVID-19) has rapidly spread in 2020, emerging a mass of studies for lung infection segmentation from CT images. Though many methods have been proposed for this issue, it is a challenging task because of infections of various size appearing in different lobe zones. To tackle these issues, we propose a Graph-based Pyramid Global Context Reasoning (Graph-PGCR) module, which is capable of modeling long-range dependencies among disjoint infections as well as adapt size variation. We first incorporate graph convolution to exploit long-term contextual information from multiple lobe zones. Different from previous average pooling or maximum object probability, we propose a saliency-aware projection mechanism to pick up infection-related pixels as a set of graph nodes. After graph reasoning, the relation-aware features are reversed back to the original coordinate space for the down-stream tasks. We further construct multiple graphs with different sampling rates to handle the size variation problem. To this end, distinct multi-scale long-range contextual patterns can be captured. Our Graph-PGCR module is plug-and-play, which can be integrated into any architecture to improve its performance. Experiments demonstrated that the proposed method consistently boost the performance of state-of-the-art backbone architectures on both of public and our private COVID-19 datasets.","136":"How do matching of spouses and the nature of work jointly shape the distribution of COVID-19 health risks? To address this question, I study the association between the incidence of COVID-19 and the degree of spousal sorting into occupations that differ by contact intensity at the workplace. The mechanism, that I explore, implies that the higher degree of positive spousal sorting mitigates intra-household contagion and this translates into a smaller number of individuals exposed to COVID-19 risk. Using the U.S. data at the state level, I argue that spousal sorting is an important factor for understanding the disparities in the prevalence of COVID-19 during the early stages of the pandemic. First, I document that it creates about two-thirds of the U.S. dual-earner couples that are exposed to higher COVID-19 health risk due to within-household transmission. Moreover, I uncover substantial heterogeneity in the degree of spousal sorting by state. Next, for the first week of April 2020, I estimate that a one standard deviation increase in the measure of spousal sorting is associated with a 30% reduction in the total number of cases per 100000 inhabitants and a 39.3% decline in the total number of deaths per 100000 inhabitants. Furthermore, I find substantial temporal heterogeneity as the coefficients decline in magnitude over time. My results speak to the importance of policies that allow mitigating intra-household contagion.","137":"Crowd counting aims to predict the number of people and generate the density map in the image. There are many challenges, including varying head scales, the diversity of crowd distribution across images and cluttered backgrounds. In this paper, we propose a multi-scale context aggregation network (MSCANet) based on single-column encoder-decoder architecture for crowd counting, which consists of an encoder based on a dense context-aware module (DCAM) and a hierarchical attention-guided decoder. To handle the issue of scale variation, we construct the DCAM to aggregate multi-scale contextual information by densely connecting the dilated convolution with varying receptive fields. The proposed DCAM can capture rich contextual information of crowd areas due to its long-range receptive fields and dense scale sampling. Moreover, to suppress the background noise and generate a high-quality density map, we adopt a hierarchical attention-guided mechanism in the decoder. This helps to integrate more useful spatial information from shallow feature maps of the encoder by introducing multiple supervision based on semantic attention module (SAM). Extensive experiments demonstrate that the proposed approach achieves better performance than other similar state-of-the-art methods on three challenging benchmark datasets for crowd counting. The code is available at https:\/\/github.com\/KingMV\/MSCANet","138":"Longitudinal interaction studies with Socially Assistive Robots are crucial to ensure that the robot is relevant for long-term use and its perceptions are not prone to the novelty effect. In this paper, we present a dynamic Bayesian network (DBN) to capture the longitudinal interactions participants had with a teleoperated robot coach (RC) delivering mindfulness sessions. The DBN model is used to study complex, temporal interactions between the participants self-reported personality traits, weekly baseline wellbeing scores, session ratings, and facial AUs elicited during the sessions in a 5-week longitudinal study. DBN modelling involves learning a graphical representation that facilitates intuitive understanding of how multiple components contribute to the longitudinal changes in session ratings corresponding to the perceptions of the RC, and participants relaxation and calm levels. The learnt model captures the following within and between sessions aspects of the longitudinal interaction study: influence of the 5 personality dimensions on the facial AU states and the session ratings, influence of facial AU states on the session ratings, and the influences within the items of the session ratings. The DBN structure is learnt using first 3 time points and the obtained model is used to predict the session ratings of the last 2 time points of the 5-week longitudinal data. The predictions are quantified using subject-wise RMSE and R2 scores. We also demonstrate two applications of the model, namely, imputation of missing values in the dataset and estimation of longitudinal session ratings of a new participant with a given personality profile. The obtained DBN model thus facilitates learning of conditional dependency structure between variables in the longitudinal data and offers inferences and conceptual understanding which are not possible through other regression methodologies.","139":"A graph $X$ is defined inductively to be $(a_0,\\dots,a_{n-1})$-regular if $X$ is $a_0$-regular and for every vertex $v$ of $X$, the sphere of radius $1$ around $v$ is an $(a_1,\\dots,a_{n-1})$-regular graph. Such a graph $X$ is said to be highly regular (HR) of level $n$ if $a_{n-1}\\neq 0$. Chapman, Linial and Peled studied HR-graphs of level 2 and provided several methods to construct families of graphs which are expanders\"globally and locally\". They ask whether such HR-graphs of level 3 exist. In this paper we show how the theory of Coxeter groups, and abstract regular polytopes and their generalisations, can lead to such graphs. Given a Coxeter system $(W,S)$ and a subset $M$ of $S$, we construct highly regular quotients of the 1-skeleton of the associated Wythoffian polytope $\\mathcal{P}_{W,M}$, which form an infinite family of expander graphs when $(W,S)$ is indefinite and $\\mathcal{P}_{W,M}$ has finite vertex links. The regularity of the graphs in this family can be deduced from the Coxeter diagram of $(W,S)$. The expansion stems from applying superapproximation to the congruence subgroups of the linear group $W$. This machinery gives a rich collection of families of HR-graphs, with various interesting properties, and in particular answers affirmatively the question asked by Chapman, Linial and Peled.","140":"We propose Tapestry, a novel approach to pooled testing with application to COVID-19 testing with quantitative Polymerase Chain Reaction (PCR) that can result in shorter testing time and conservation of reagents and testing kits. Tapestry combines ideas from compressed sensing and combinatorial group testing with a novel noise model for PCR. Unlike Boolean group testing algorithms, the input is a quantitative readout from each test, and the output is a list of viral loads for each sample. While other pooling techniques require a second confirmatory assay, Tapestry obtains individual sample-level results in a single round of testing. When testing $n$ samples with $t$ tests, as many as $k= O(t \/ \\log n)$ infected samples can be identified at clinically-acceptable false positive and false negative rates. This makes Tapestry viable even at prevalence rates as high as 10\\%. Tapestry has been validated in simulations as well as in wet lab experiments with oligomers. Clinical trials with Covid-19 samples are underway. An accompanying Android application Byom Smart Testing which makes the Tapestry protocol straightforward to implement in testing centres is available for free download.","141":"Many countries have secured larger quantities of COVID-19 vaccines than their populace is willing to take. This abundance and variety of vaccines created a historical moment to understand vaccine hesitancy better. Never before were more types of vaccines available for an illness and the intensity of vaccine-related public discourse is unprecedented. Yet, the heterogeneity of hesitancy by vaccine types has been neglected so far, even though factual or believed vaccine characteristics and patient attributes are known to influence acceptance. We address this problem by analysing acceptance and assessment of five vaccine types using information collected with a nationally representative survey at the end of the third wave of the COVID-19 pandemic in Hungary, where a unique portfolio of vaccines were available to the public in large quantities. Our special case enables us to quantify revealed preferences across vaccine types since one could evaluate a vaccine unacceptable and even could reject an assigned vaccine to wait for another type. We find that the source of information that respondents trust characterizes their attitudes towards vaccine types differently and leads to divergent vaccine hesitancy. Believers of conspiracy theories were significantly more likely to evaluate the mRNA vaccines (Pfizer and Moderna) unacceptable while those who follow the advice of politicians evaluate vector-based (AstraZeneca and Sputnik) or whole-virus vaccines (Sinopharm) acceptable with higher likelihood. We illustrate that the rejection of non-desired and re-selection of preferred vaccines fragments the population by the mRNA versus other type of vaccines while it generally improves the assessment of the received vaccine. These results highlight that greater variance of available vaccine types and individual free choice are desirable conditions that can widen the acceptance of vaccines in societies.","142":"At the beginning of 2020 the world has seen the initial outbreak of COVID-19, a disease caused by SARS-CoV2 virus in China. The World Health Organization (WHO) declared this disease as a pandemic on March 11 2020. As the disease spread globally, it becomes difficult to tract the transmission dynamics of this disease in all countries, as they may differ in geographical, demographic and strategical aspects. In this short note, the author proposes the utilization of a type of neural network to generate a global topological map for these dynamics, in which countries that share similar dynamics are mapped adjacently, while countries with significantly different dynamics are mapped far from each other. The author believes that this kind of topological map can be useful for further analyzing and comparing the correlation between the diseases dynamics with strategies to mitigate this global crisis in an intuitive manner. Some initial experiments with with time series of patients numbers in more than 240 countries are explained in this note.","143":"Supervised learning method requires a large volume of annotated datasets. Collecting such datasets is time-consuming and expensive. Until now, very few annotated COVID-19 imaging datasets are available. Although self-supervised learning enables us to bootstrap the training by exploiting unlabeled data, the generic self-supervised methods for natural images do not sufficiently incorporate the context. For medical images, a desirable method should be sensitive enough to detect deviation from normal-appearing tissue of each anatomical region; here, anatomy is the context. We introduce a novel approach with two levels of self-supervised representation learning objectives: one on the regional anatomical level and another on the patient-level. We use graph neural networks to incorporate the relationship between different anatomical regions. The structure of the graph is informed by anatomical correspondences between each patient and an anatomical atlas. In addition, the graph representation has the advantage of handling any arbitrarily sized image in full resolution. Experiments on large-scale Computer Tomography (CT) datasets of lung images show that our approach compares favorably to baseline methods that do not account for the context. We use the learnt embedding to quantify the clinical progression of COVID-19 and show that our method generalizes well to COVID-19 patients from different hospitals. Qualitative results suggest that our model can identify clinically relevant regions in the images.","144":"We present the direct-imaging discovery of a substellar companion in orbit around a Sun-like star member of the Hyades open cluster. So far, no other substellar companions have been unambiguously confirmed via direct imaging around main-sequence stars in Hyades. The star HIP 21152 is an accelerating star as identified by the astrometry from the Gaia and Hipparcos satellites. We have detected the companion, HIP 21152 B, in multi-epoch using the high-contrast imaging from SCExAO\/CHARIS and Keck\/NIRC2. We have also obtained the stellar radial-velocity data from the Okayama 188cm telescope. The CHARIS spectroscopy reveals that HIP 21152 B's spectrum is consistent with the L\/T transition, best fit by an early T dwarf. Our orbit modeling determines the semi-major axis and the dynamical mass of HIP 21152 B to be 17.0$^{+7.1}_{-3.5}$ au and 27.5$^{+8.7}_{-5.3}$ $M_{\\rm{Jup}}$, respectively. The mass ratio of HIP 21152 B relative to its host is $\\approx$2\\%, near the planet\/brown dwarf boundary suggested from recent surveys. Mass estimates inferred from luminosity evolution models are slightly higher (33--42 $M_{\\rm{Jup}}$). With a dynamical mass and a well-constrained age due to the system's Hyades membership, HIP 21152 B will become a critical benchmark in understanding the formation, evolution, and atmosphere of a substellar object as a function of mass and age. Our discovery is yet another key proof-of-concept for using precision astrometry to select direct imaging targets.","145":"Viral transmission pathways have profound implications for public safety; thus it is imperative to establish an accurate and complete understanding of viable infectious avenues. Whether SARS-CoV-2 is airborn is currently uncertain. While mounting evidence suggests it could be transmitted via the air in confined spaces, the airborn transmission mode has not yet been demonstrated. Here we show that the quantitative analysis of several reported superspreading events points towards aerosol mediated transmission of SARS-CoV-2. Aerosolized virus emitted by an infected person in an enclosed space will accumulate until virion emission and virion destabilization are balanced, resulting in a steady-state concentration $C_{\\infty}$. The timescale to accumulation leads to significantly enhanced exposure when virus-carrying aereosol droplets are inhaled for longer duration co-occupancy. Reported superspreading events are found to trace out a single value of the calculated virion exposure, suggesting a universal minimum infective dose (MID) via aerosol. The MID implied by our analysis is comparable to the measured MIDs for influenza A (H2N2), the virus responsible for the 1957-1958 asian flu pandemic. Our model suggests that the likelihood for aerosol-mediated transmission reduces significantly when there is filtration at a rate exceeding the destabilization rate of aerosolized SARS-CoV-2.","146":"It is well known that several viruses, as well as SARS-CoV-2, can be transmitted through airborne diffusion of saliva micro-droplets. For this reason many reserach groups have been devoted their efforts in order to gain new insight into the transport of fluids and particles originted from human respiratory tracts. This paper aims to provide a contribution to the numerical modelling of bio-aerosols. In particular, the well-known problem around the safety distance to be held for avoiding virus transmission in the absence of external wind is further investigated. Thus, new indexes capable of evaluating the contamination risk are introduced and the possibility to inactivate virus particles by means of an external UV-C radiation source is studied. For this purpose, a new model which takes into account biological inactivation deriving from UV-C exposure in a Eulerian-Lagrangian framework is presented.","147":"The presence of extra dimensions generically modify the spacetime geometry of a rotating black hole, by adding an additional hair, besides the mass $M$ and the angular momentum $J$, known as the `tidal charge' parameter, $\\beta$. In a braneworld scenario with one extra spatial dimension, the extra dimension is expected to manifest itself through -- (a) negative values of $\\beta$, and (b) modified gravitational perturbations. This in turn would affect the quasi-normal modes of rotating black holes. We numerically solve the perturbed gravitational field equations using the continued fractions method and determine the quasi-normal mode spectra for the braneworld black hole. We find that increasingly negative values of $\\beta$ correspond to a diminishing imaginary part of the quasi-normal mode, or equivalently, an increasing damping time. Using the publicly available data of the properties of the remnant black hole in the gravitational wave signal GW150914, we check for consistency between the predicted values (for a given $\\beta$) of the frequency and damping time of the least-damped $\\ell=2,m=2$ quasi-normal mode and measurements of these quantities using other independent techniques. We find that it is highly unlikely for the tidal charge, $\\beta \\lesssim -0.05$, providing a conservative limit on the tidal charge parameter. Implications and future directions are discussed.","148":"The U-Net architecture, built upon the fully convolutional network, has proven to be effective in biomedical image segmentation. However, U-Net applies skip connections to merge semantically different low- and high-level convolutional features, resulting in not only blurred feature maps, but also over- and under-segmented target regions. To address these limitations, we propose a simple, yet effective end-to-end depthwise encoder-decoder fully convolutional network architecture, called Sharp U-Net, for binary and multi-class biomedical image segmentation. The key rationale of Sharp U-Net is that instead of applying a plain skip connection, a depthwise convolution of the encoder feature map with a sharpening kernel filter is employed prior to merging the encoder and decoder features, thereby producing a sharpened intermediate feature map of the same size as the encoder map. Using this sharpening filter layer, we are able to not only fuse semantically less dissimilar features, but also to smooth out artifacts throughout the network layers during the early stages of training. Our extensive experiments on six datasets show that the proposed Sharp U-Net model consistently outperforms or matches the recent state-of-the-art baselines in both binary and multi-class segmentation tasks, while adding no extra learnable parameters. Furthermore, Sharp U-Net outperforms baselines that have more than three times the number of learnable parameters.","149":"Deployment of optical network infrastructure and network services is growing exponentially for beyond 5G networks. Since the uptake of e-commerce and e-services has seen unprecedented serge in recent months due to the global COVID-19 pandemic era, the security of such transactions in optical communication has gained much importance. Optical fiber communication networks are vulnerable to several types of security threats, such as single point failure, wormhole attacks, and sybil attacks. Therefore, blockchain is a promising solution to protect confidential information against attacks and helps in achieving trusted network architecture by creating a distributed ledger platform. Recently, blockchain has received much attention because of its decentralized and distributed ledger technology. Hence, blockchain has also been employed to protect network against such attacks. However, blockchain technology's security relies on the platform of computational complexity, and because of the evolution of quantum computers, it will become insecure in the near future. Therefore, for enhancing blockchain security, research focus on combining quantum key distribution (QKD) with blockchain. This new technology is known as quantum-secured blockchain. The article describes the attacks in optical networks and provides a solution to protect network against security attacks by employing quantum-secured blockchain in optical networks. It provides a brief overview of blockchain technology with its security loopholes and focuses on QKD, which makes blockchain technology more robust against quantum-attacks. Next, the article provides a broad view of quantum-secured blockchain and presents the network architecture for future research and development of secure and trusted optical communication networks using quantum-secured blockchain.","150":"Federated learning allows multiple parties to collaboratively train a joint model without sharing local data. This enables applications of machine learning in settings of inherently distributed, undisclosable data such as in the medical domain. In practice, joint training is usually achieved by aggregating local models, for which local training objectives have to be in expectation similar to the joint (global) objective. Often, however, local datasets are so small that local objectives differ greatly from the global objective, resulting in federated learning to fail. We propose a novel approach that intertwines model aggregations with permutations of local models. The permutations expose each local model to a daisy chain of local datasets resulting in more efficient training in data-sparse domains. This enables training on extremely small local datasets, such as patient data across hospitals, while retaining the training efficiency and privacy benefits of federated learning.","151":"We propose a copula-based measure of asymmetry between the lower and upper tail probabilities of bivariate distributions. The proposed measure has a simple form and possesses some desirable properties as a measure of asymmetry. The limit of the proposed measure as the index goes to the boundary of its domain can be expressed in a simple form under certain conditions on copulas. A sample analogue of the proposed measure for a sample from a copula is presented and its weak convergence to a Gaussian process is shown. Another sample analogue of the presented measure, which is based on a sample from a distribution on $\\mathbb{R}^2$, is given. Simple methods for interval estimation and nonparametric testing based on the two sample analogues are presented. As an example, the presented measure is applied to daily returns of S&P500 and Nikkei225.","152":"From generating never-before-seen images to domain adaptation, applications of Generative Adversarial Networks (GANs) spread wide in the domain of vision and graphics problems. With the remarkable ability of GANs in learning the distribution and generating images of a particular class, they can be used for semi-supervised classification tasks. However, the problem is that if two classes of images share similar characteristics, the GAN might learn to generalize and hinder the classification of the two classes. In this paper, we use various images from MNIST and Fashion-MNIST datasets to illustrate how similar images cause the GAN to generalize, leading to the poor classification of images. We propose a modification to the traditional training of GANs that allows for improved multi-class classification in similar classes of images in a semi-supervised learning framework.","153":"When the novel coronavirus disease SARS-CoV2 (COVID-19) was officially declared a pandemic by the WHO in March 2020, the scientific community had already braced up in the effort of making sense of the fast-growing wealth of data gathered by national authorities all over the world. However, despite the diversity of novel theoretical approaches and the comprehensiveness of many widely established models, the official figures that recount the course of the outbreak still sketch a largely elusive and intimidating picture. Here we show unambiguously that the dynamics of the COVID-19 outbreak belongs to the simple universality class of the SIR model and extensions thereof. Our analysis naturally leads us to establish that there exists a fundamental limitation to any theoretical approach, namely the unpredictable non-stationarity of the testing frames behind the reported figures. However, we show how such bias can be quantified self-consistently and employed to mine useful and accurate information from the data. In particular, we describe how the time evolution of the reporting rates controls the occurrence of the apparent epidemic peak, which typically follows the true one in countries that were not vigorous enough in their testing at the onset of the outbreak. The importance of testing early and resolutely appears as a natural corollary of our analysis, as countries that tested massively at the start clearly had their true peak earlier and less deaths overall.","154":"We develop online graph learning algorithms from streaming network data. Our goal is to track the (possibly) time-varying network topology, and effect memory and computational savings by processing the data on-the-fly as they are acquired. The setup entails observations modeled as stationary graph signals generated by local diffusion dynamics on the unknown network. Moreover, we may have a priori information on the presence or absence of a few edges as in the link prediction problem. The stationarity assumption implies that the observations' covariance matrix and the so-called graph shift operator (GSO -- a matrix encoding the graph topology) commute under mild requirements. This motivates formulating the topology inference task as an inverse problem, whereby one searches for a sparse GSO that is structurally admissible and approximately commutes with the observations' empirical covariance matrix. For streaming data said covariance can be updated recursively, and we show online proximal gradient iterations can be brought to bear to efficiently track the time-varying solution of the inverse problem with quantifiable guarantees. Specifically, we derive conditions under which the GSO recovery cost is strongly convex and use this property to prove that the online algorithm converges to within a neighborhood of the optimal time-varying batch solution. Numerical tests illustrate the effectiveness of the proposed graph learning approach in adapting to streaming information and tracking changes in the sought dynamic network.","155":"Reinforcement learning is a powerful framework for robots to acquire skills from experience, but often requires a substantial amount of online data collection. As a result, it is difficult to collect sufficiently diverse experiences that are needed for robots to generalize broadly. Videos of humans, on the other hand, are a readily available source of broad and interesting experiences. In this paper, we consider the question: can we perform reinforcement learning directly on experience collected by humans? This problem is particularly difficult, as such videos are not annotated with actions and exhibit substantial visual domain shift relative to the robot's embodiment. To address these challenges, we propose a framework for reinforcement learning with videos (RLV). RLV learns a policy and value function using experience collected by humans in combination with data collected by robots. In our experiments, we find that RLV is able to leverage such videos to learn challenging vision-based skills with less than half as many samples as RL methods that learn from scratch.","156":"This paper studies identification of the marginal treatment effect (MTE) when a binary treatment variable is misclassified. We show under standard assumptions that the MTE is identified as the derivative of the conditional expectation of the observed outcome given the true propensity score, which is partially identified. We characterize the identified set for this propensity score, and then for the MTE. We use our MTE bounds to derive bounds on other commonly used parameters in the literature. We show that our bounds are tighter than the existing bounds for the local average treatment effect. We illustrate the practical relevance of our derived bounds through some numerical and empirical results.","157":"The COVID-19 pandemic has severely affected people's daily lives and caused tremendous economic loss worldwide. However, its influence on people's mental health conditions has not received as much attention. To study this subject, we choose social media as our main data resource and create by far the largest English Twitter depression dataset containing 2,575 distinct identified depression users with their past tweets. To examine the effect of depression on people's Twitter language, we train three transformer-based depression classification models on the dataset, evaluate their performance with progressively increased training sizes, and compare the model's\"tweet chunk\"-level and user-level performances. Furthermore, inspired by psychological studies, we create a fusion classifier that combines deep learning model scores with psychological text features and users' demographic information and investigate these features' relations to depression signals. Finally, we demonstrate our model's capability of monitoring both group-level and population-level depression trends by presenting two of its applications during the COVID-19 pandemic. We hope this study can raise awareness among researchers and the general public of COVID-19's impact on people's mental health.","158":"The world is under an ongoing pandemic, COVID-19, of a scale last seen a century ago. Contact tracing is one of the most critical and highly effective tools for containing and breaking the chain of infections especially in the case of infectious respiratory diseases like COVID-19. Thanks to the technological progress in our times, we now have digital mobile applications like the Corona-Warn-App for digital contact tracing. However, due to the invasive nature of contact tracing, it is very important to preserve the privacy of the users. Privacy preservation is important for increasing trust in the app and subsequently enabling its widespread usage in a privacy-valuing population. In this paper, we present a visual simulation of the working of the Corona-Warn-App to demonstrate how the privacy of its users is preserved, how they're notified of infectious contacts and how it helps in containing the spread of COVID-19.","159":"We present a novel class of projected methods, to perform statistical analysis on a data set of probability distributions on the real line, with the 2-Wasserstein metric. We focus in particular on Principal Component Analysis (PCA) and regression. To define these models, we exploit a representation of the Wasserstein space closely related to its weak Riemannian structure, by mapping the data to a suitable linear space and using a metric projection operator to constrain the results in the Wasserstein space. By carefully choosing the tangent point, we are able to derive fast empirical methods, exploiting a constrained B-spline approximation. As a byproduct of our approach, we are also able to derive faster routines for previous work on PCA for distributions. By means of simulation studies, we compare our approaches to previously proposed methods, showing that our projected PCA has similar performance for a fraction of the computational cost and that the projected regression is extremely flexible even under misspecification. Several theoretical properties of the models are investigated and asymptotic consistency is proven. Two real world applications to Covid-19 mortality in the US and wind speed forecasting are discussed.","160":"SARS-CoV-2 has characteristics of wide contagion and quick velocity of propagation. To analysis the visual information of it, we build a SARS-CoV-2 image dataset with 48 electron microscopic images and also prepare their ground truth images. Furthermore, we extract multiple classical features and novel deep learning features to describe the visual information of SARS-CoV-2. Finally, it is proved that the visual features of the SARS-CoV-2 images which are observed under the electron microscopic can be extracted and analysed.","161":"We present Coronavirus disease 2019 (COVID-19) statistics in China dataset: daily statistics of the COVID-19 outbreak in China at the city\/county level. For each city\/country, we include the six most important numbers for epidemic research: daily new infections, accumulated infections, daily new recoveries, accumulated recoveries, daily new deaths, and accumulated deaths. We cross validate the dataset and the estimate error rate is about 0.04%. We then give several examples to show how to trace the spreading in particular cities or provinces, and also contrast the development of COVID-19 in all cities in China at the early, middle and late stages. We hope this dataset can help researchers around the world better understand the spreading dynamics of COVID-19 at a regional level, to inform intervention and mitigation strategies for policymakers.","162":"Mass spectrometry of intact nanoparticles and viruses can serve as a potent characterization tool for material science and biophysics. Inaccessible by widespread commercial techniques, the mass of single nanoparticles and viruses (>10MDa) can be readily measured by NEMS (Nanoelectromechanical Systems) based Mass Spectrometry, where charged and isolated analyte particles are generated by Electrospray Ionization (ESI) in air and transported onto the NEMS resonator for capture and detection. However, the applicability of NEMS as a practical solution is hindered by their miniscule surface area, which results in poor limit-of-detection and low capture efficiency values. Another hindrance is the necessity to house the NEMS inside complex vacuum systems, which is required in part to focus analytes towards the miniscule detection surface of the NEMS. Here, we overcome both limitations by integrating an ion lens onto the NEMS chip. The ion lens is composed of a polymer layer, which charges up by receiving part of the ions incoming from the ESI tip and consequently starts to focus the analytes towards an open window aligned with the active area of the NEMS electrostatically. With this integrated system, we have detected the mass of gold and polystyrene nanoparticles under ambient conditions and with two orders-of-magnitude improvement in capture efficiency compared to the state-of-the-art. We then applied this technology to obtain the mass spectrum of SARS-CoV-2 and BoHV-1 virions. With the increase in analytical throughput, the simplicity of the overall setup and the operation capability under ambient conditions, the technique demonstrates that NEMS Mass Spectrometry can be deployed for mass detection of engineered nanoparticles and biological samples efficiently.","163":"The COVID-19 pandemic has caused changes in terms of traditional teaching globally. In Kosova context, the Universities have found the transition from teaching in class to online classes quite challenging. This study investigates the transformation process from in-campus classes to online classes from the technical perspective within five Higher Education Institutions (HEI) in Kosovo. The data was collected using the qualitative methods and its analysis followed the 3C Lichtman approach. The results show that each of the Universities followed a different approach, by using either their limited premises infrastructure or using additional cloud infrastructure.","164":"We introduce the well-established social scientific concept of social solidarity and its contestation, anti-solidarity, as a new problem setting to supervised machine learning in NLP to assess how European solidarity discourses changed before and after the COVID-19 outbreak was declared a global pandemic. To this end, we annotate 2.3k English and German tweets for (anti-)solidarity expressions, utilizing multiple human annotators and two annotation approaches (experts vs.\\ crowds). We use these annotations to train a BERT model with multiple data augmentation strategies. Our augmented BERT model that combines both expert and crowd annotations outperforms the baseline BERT classifier trained with expert annotations only by over 25 points, from 58\\% macro-F1 to almost 85\\%. We use this high-quality model to automatically label over 270k tweets between September 2019 and December 2020. We then assess the automatically labeled data for how statements related to European (anti-)solidarity discourses developed over time and in relation to one another, before and during the COVID-19 crisis. Our results show that solidarity became increasingly salient and contested during the crisis. While the number of solidarity tweets remained on a higher level and dominated the discourse in the scrutinized time frame, anti-solidarity tweets initially spiked, then decreased to (almost) pre-COVID-19 values before rising to a stable higher level until the end of 2020.","165":"Proof Blocks is a software tool which enables students to write proofs by dragging and dropping prewritten proof lines into the correct order. These proofs can be graded completely automatically, enabling students to receive rapid feedback on how they are doing with their proofs. When constructing a problem, the instructor specifies the dependency graph of the lines of the proof, so that any correct arrangement of the lines can receive full credit. This innovation can improve assessment tools by increasing the types of questions we can ask students about proofs, and can give greater access to proof knowledge by increasing the amount that students can learn on their own with the help of a computer.","166":"Because of the complexity of urban transportation networks and the temporal changes in traffic conditions, it is difficult to assess real-time traffic situations. However, the development of information terminals has made it easier to obtain personal mobility information. In this study, we propose methods for evaluating the mobility of people in a city using global positioning system data. There are two main methods for evaluating movement. One is to create a temporal network from real data and check the change in travel time according to time zones or seasons. Temporal networks are difficult to evaluate because of their time complexity, and in this study, we proposed an evaluation method using the probability density function of travel time. The other method is to define a time-dependent traveling salesman problem and find an efficient traveling route by finding the shortest path. By creating a time-dependent traveling salesman problem in an existing city and solving it, a traveler can choose an efficient route by considering traffic conditions at different times of the day. We used 2 months of data from Kyoto City to conduct a traffic evaluation as a case study.","167":"With rapid developments in consumer-level head-mounted displays and computer graphics, immersive VR has the potential to take online and remote learning closer to real-world settings. However, the effects of such digital transformations on learners, particularly for VR, have not been evaluated in depth. This work investigates the interaction-related effects of sitting positions of learners, visualization styles of peer-learners and teachers, and hand-raising behaviors of virtual peer-learners on learners in an immersive VR classroom, using eye tracking data. Our results indicate that learners sitting in the back of the virtual classroom may have difficulties extracting information. Additionally, we find indications that learners engage with lectures more efficiently if virtual avatars are visualized with realistic styles. Lastly, we find different eye movement behaviors towards different performance levels of virtual peer-learners, which should be investigated further. Our findings present an important baseline for design decisions for VR classrooms.","168":"Today is the era of intelligence in machines. With the advances in Artificial Intelligence, machines have started to impersonate different human traits, a chatbot is the next big thing in the domain of conversational services. A chatbot is a virtual person who is capable to carry out a natural conversation with people. They can include skills that enable them to converse with the humans in audio, visual, or textual formats. Artificial intelligence conversational entities, also called chatbots, conversational agents, or dialogue system, are an excellent example of such machines. Obtaining the right information at the right time and place is the key to effective disaster management. The term\"disaster management\"encompasses both natural and human-caused disasters. To assist citizens, our project is to create a COVID Assistant to provide the need of up to date information to be available 24 hours. With the growth in the World Wide Web, it is quite intelligible that users are interested in the swift and relatedly correct information for their hunt. A chatbot can be seen as a question-and-answer system in which experts provide knowledge to solicit users. This master thesis is dedicated to discuss COVID Assistant chatbot and explain each component in detail. The design of the proposed chatbot is introduced by its seven components: Ontology, Web Scraping module, DB, State Machine, keyword Extractor, Trained chatbot, and User Interface.","169":"High-resolution magnetic resonance images can provide fine-grained anatomical information, but acquiring such data requires a long scanning time. In this paper, a framework called the Fused Attentive Generative Adversarial Networks(FA-GAN) is proposed to generate the super-resolution MR image from low-resolution magnetic resonance images, which can reduce the scanning time effectively but with high resolution MR images. In the framework of the FA-GAN, the local fusion feature block, consisting of different three-pass networks by using different convolution kernels, is proposed to extract image features at different scales. And the global feature fusion module, including the channel attention module, the self-attention module, and the fusion operation, is designed to enhance the important features of the MR image. Moreover, the spectral normalization process is introduced to make the discriminator network stable. 40 sets of 3D magnetic resonance images (each set of images contains 256 slices) are used to train the network, and 10 sets of images are used to test the proposed method. The experimental results show that the PSNR and SSIM values of the super-resolution magnetic resonance image generated by the proposed FA-GAN method are higher than the state-of-the-art reconstruction methods.","170":"Subscription services face a difficult problem when estimating the causal impact of content launches on acquisition. Customers buy subscriptions, not individual pieces of content, and once subscribed they may consume many pieces of content in addition to the one(s) that drew them to the service. In this paper, we propose a scalable methodology to estimate the incremental acquisition impact of content launches in a subscription business model when randomized experimentation is not feasible. Our approach uses simple assumptions to transform the problem into an equivalent question: what is the expected consumption rate for new subscribers who did not join due to the content launch? We estimate this counterfactual rate using the consumption rate of new subscribers who joined just prior to launch, while making adjustments for variation related to subscriber attributes, the in-product experience, and seasonality. We then compare our counterfactual consumption to the actual rate in order to back out an acquisition estimate. Our methodology provides top-line impact estimates at the content \/ day \/ region grain. Additionally, to enable subscriber-level attribution, we present an algorithm that assigns specific individual accounts to add up to the top-line estimate. Subscriber-level attribution is derived by solving an optimization problem to minimize the number of subscribers attributed to more than one piece of content, while maximizing the average propensity to be incremental for subscribers attributed to each piece of content. Finally, in the absence of definitive ground truth, we present several validation methods which can be used to assess the plausibility of impact estimates generated by these methods.","171":"The new coronavirus (2019-nCoV or SARS-CoV2), inducing the current pandemic disease (COVID-19) and causing pneumoniae in humans, is dramatically increasing in epidemic scale since its first appearance in Wuhan, China, in December 2019. The first infection from epidemic coronaviruses in 2003 fostered the spread of an overwhelming amount of related scientific efforts. The manifold aspects that have been raised, as well as their redundancy offer precious information that has been underexploited and needs to be critically re-evaluated, appropriately used and offered to the whole community, from scientists, to medical doctors, stakeholders and common people. These efforts will favour a holistic view on the comprehension, prevention and development of strategies (pharmacological, clinical etc) as well as common intervention against the new coronavirus spreading. Here we describe a model that emerged from our analysis that was focused on the Renin Angiotensin System (RAS) and the possible routes linking it to the viral infection. because the infection is mediated by the viral receptor on human cell membranes Angiotensin Converting Enzyme (ACE2), which is a key component in RAS signalling. The model depicts the main pathways determining the disease and the molecular framework for its establishment, and can help to shed light on mechanisms involved in the infection. It promptly gives an answer to some of the controversial, and still open, issues concerning predisposing conditions and medical treatments that protect from or favour the severity of the disease (such as the use of ACE inhibitors or ARBs\/sartans), or to the sex related biases in the affected population. The model highlights novel opportunities for further investigations, diagnosis and appropriate intervention to understand and fight COVID19.","172":"Multiple-input multiple-output orthogonal frequency-division multiplexing (MIMO-OFDM) is a key technology component in the evolution towards cognitive radio (CR) in next-generation communication in which the accuracy of timing and frequency synchronization significantly impacts the overall system performance. In this paper, we propose a novel scheme leveraging extreme learning machine (ELM) to achieve high-precision synchronization. Specifically, exploiting the preamble signals with synchronization offsets, two ELMs are incorporated into a traditional MIMO-OFDM system to estimate both the residual symbol timing offset (RSTO) and the residual carrier frequency offset (RCFO). The simulation results show that the performance of the proposed ELM-based synchronization scheme is superior to the traditional method under both additive white Gaussian noise (AWGN) and frequency selective fading channels. Furthermore, comparing with the existing machine learning based techniques, the proposed method shows outstanding performance without the requirement of perfect channel state information (CSI) and prohibitive computational complexity. Finally, the proposed method is robust in terms of the choice of channel parameters (e.g., number of paths) and also in terms of\"generalization ability\"from a machine learning standpoint.","173":"Sharing correlated random variables is a resource for a number of information theoretic tasks such as privacy amplification, simultaneous message passing, secret sharing and many more. In this article, we show that to establish such a resource called shared randomness, quantum systems provide an advantage over their classical counterpart. Precisely, we show that appropriate albeit fixed measurements on a shared two-qubit state can generate correlations which cannot be obtained from any possible state on two classical bits. In a resource theoretic set-up, this feature of quantum systems can be interpreted as an advantage in winning a two players co-operative game, which we call the `non-monopolize social subsidy' game. It turns out that the quantum states leading to the desired advantage must possess non-classicality in the form of quantum discord. On the other hand, while distributing such sources of shared randomness between two parties via noisy channels, quantum channels with zero capacity as well as with classical capacity strictly less than unity perform more efficiently than the perfect classical channel. Protocols presented here are noise-robust and hence should be realizable with state-of-the-art quantum devices.","174":"This paper presents a novel deep learning architecture for short term load forecasting of building energy loads. The architecture is based on a simple base learner and multiple boosting systems that are modelled as a single deep neural network. The architecture transforms the original multivariate time series into multiple cascading univariate time series. Together with sparse interactions, parameter sharing and equivariant representations, this approach makes it possible to combat against overfitting while still achieving good presentation power with a deep network architecture. The architecture is evaluated in several short-term load forecasting tasks with energy data from an office building in Finland. The proposed architecture outperforms state-of-the-art load forecasting model in all the tasks.","175":"The recent COVID-19 pandemic has caused unprecedented impact across the globe. We have also witnessed millions of people with increased mental health issues, such as depression, stress, worry, fear, disgust, sadness, and anxiety, which have become one of the major public health concerns during this severe health crisis. For instance, depression is one of the most common mental health issues according to the findings made by the World Health Organisation (WHO). Depression can cause serious emotional, behavioural and physical health problems with significant consequences, both personal and social costs included. This paper studies community depression dynamics due to COVID-19 pandemic through user-generated content on Twitter. A new approach based on multi-modal features from tweets and Term Frequency-Inverse Document Frequency (TF-IDF) is proposed to build depression classification models. Multi-modal features capture depression cues from emotion, topic and domain-specific perspectives. We study the problem using recently scraped tweets from Twitter users emanating from the state of New South Wales in Australia. Our novel classification model is capable of extracting depression polarities which may be affected by COVID-19 and related events during the COVID-19 period. The results found that people became more depressed after the outbreak of COVID-19. The measures implemented by the government such as the state lockdown also increased depression levels. Further analysis in the Local Government Area (LGA) level found that the community depression level was different across different LGAs. Such granular level analysis of depression dynamics not only can help authorities such as governmental departments to take corresponding actions more objectively in specific regions if necessary but also allows users to perceive the dynamics of depression over the time.","176":"HPC is an enabling platform for AI. The introduction of AI workloads in the HPC applications basket has non-trivial consequences both on the way of designing AI applications and on the way of providing HPC computing. This is the leitmotif of the convergence between HPC and AI. The formalized definition of AI pipelines is one of the milestones of HPC-AI convergence. If well conducted, it allows, on the one hand, to obtain portable and scalable applications. On the other hand, it is crucial for the reproducibility of scientific pipelines. In this work, we advocate the StreamFlow Workflow Management System as a crucial ingredient to define a parametric pipeline, called\"CLAIRE COVID-19 Universal Pipeline,\"which is able to explore the optimization space of methods to classify COVID-19 lung lesions from CT scans, compare them for accuracy, and therefore set a performance baseline. The universal pipeline automatizes the training of many different Deep Neural Networks (DNNs) and many different hyperparameters. It, therefore, requires a massive computing power, which is found in traditional HPC infrastructure thanks to the portability-by-design of pipelines designed with StreamFlow. Using the universal pipeline, we identified a DNN reaching over 90% accuracy in detecting COVID-19 lesions in CT scans.","177":"Airborne transmission of disease is of concern in many indoor spaces. Here, aerosol dispersion and removal in an unoccupied 4-bed hospital room was characterized using a transient aerosol tracer experiment for 38 experiments covering 4 configurations of air purifiers and 3 configurations of curtains. NaCl particle (mass mean aerodynamic diameter $\\sim 3 \\mu m$) concentrations were measured around the room following an aerosol release. Particle transport across the room was 1.5 - 4 minutes which overlaps with the characteristic times for significant viral deactivation and gravitational settling of larger particles. Concentrations were close to spatially uniform except very near the source. Short curtains had no consistent effects on concentrations at the non-source patient locations while floor-length curtains reduced concentrations slightly depending on the purifier configuration. The aerosol decay rate was in most cases higher than expected from the clean air delivery rate, but the reduction in steady-state concentrations resulting from air purifiers was less than suggested by the decay rates. Apparently a substantial (and configuration-dependent) fraction of the aerosol is removed immediately and this effect is not captured by the decay rate. Overall, the combination of curtains and purifiers is likely to reduce disease transmission in multipatient hospital rooms.","178":"We are facing a global healthcare crisis today as the healthcare cost is ever climbing, but with the aging population, government fiscal revenue is ever dropping. To create a more efficient and effective healthcare system, three technical challenges immediately present themselves: healthcare access, healthcare equity, and healthcare efficiency. An autonomous mobile clinic solves the healthcare access problem by bringing healthcare services to the patient by the order of the patient's fingertips. Nevertheless, to enable a universal autonomous mobile clinic network, a three-stage technical roadmap needs to be achieved: In stage one, we focus on solving the inequity challenge in the existing healthcare system by combining autonomous mobility and telemedicine. In stage two, we develop an AI doctor for primary care, which we foster from infancy to adulthood with clean healthcare data. With the AI doctor, we can solve the inefficiency problem. In stage three, after we have proven that the autonomous mobile clinic network can truly solve the target clinical use cases, we shall open up the platform for all medical verticals, thus enabling universal healthcare through this whole new system.","179":"In this paper we consider optimization problems with stochastic composite objective function subject to (possibly) infinite intersection of constraints. The objective function is expressed in terms of expectation operator over a sum of two terms satisfying a stochastic bounded gradient condition, with or without strong convexity type properties. In contrast to the classical approach, where the constraints are usually represented as intersection of simple sets, in this paper we consider that each constraint set is given as the level set of a convex but not necessarily differentiable function. Based on the flexibility offered by our general optimization model we consider a stochastic subgradient method with random feasibility updates. At each iteration, our algorithm takes a stochastic proximal (sub)gradient step aimed at minimizing the objective function and then a subsequent subgradient step minimizing the feasibility violation of the observed random constraint. We analyze the convergence behavior of the proposed algorithm for diminishing stepsizes and for the case when the objective function is convex or has a quadratic functional growth, unifying the nonsmooth and smooth cases. We prove sublinear convergence rates for this stochastic subgradient algorithm, which are known to be optimal for subgradient methods on this class of problems. When the objective function has a linear least-square form and the constraints are polyhedral, it is shown that the algorithm converges linearly. Numerical evidence supports the effectiveness of our method in real problems.","180":"The COVID-19 epidemic was listed as a public health emergency of international concern by the WHO on January 30, 2020. To curb the secondary spread of the epidemic, many public places were equipped with thermal imagers to check the body temperature. However, the COVID-19 pneumonia has concealed symptoms: the first symptom may not be fever, and can be shortness of breath. During epidemic prevention, many people tend to wear masks. Therefore, in this demo paper, we proposed a portable non-contact healthy screening system for people wearing masks, which can simultaneously obtain body temperature and respiration state. This system consists of three modules viz. thermal image collection module, health indicator calculation module and health assessment module. In this system, the thermal video of human faces is first captured through a portable thermal imaging camera. Then, body temperature and respiration state are extracted from the video and are imported into the following health assessment module. Finally, the screening result can be obtained. The results of preliminary experiments show that this syetem can give an accurate screening result within 15 seconds. This system can be applied to many application scenarios such as community and campus.","181":"Graph neural networks (GNNs) have emerged as a series of competent graph learning methods for diverse real-world scenarios, ranging from daily applications like recommendation systems and question answering to cutting-edge technologies such as drug discovery in life sciences and n-body simulation in astrophysics. However, task performance is not the only requirement for GNNs. Performance-oriented GNNs have exhibited potential adverse effects like vulnerability to adversarial attacks, unexplainable discrimination against disadvantaged groups, or excessive resource consumption in edge computing environments. To avoid these unintentional harms, it is necessary to build competent GNNs characterised by trustworthiness. To this end, we propose a comprehensive roadmap to build trustworthy GNNs from the view of the various computing technologies involved. In this survey, we introduce basic concepts and comprehensively summarise existing efforts for trustworthy GNNs from six aspects, including robustness, explainability, privacy, fairness, accountability, and environmental well-being. Additionally, we highlight the intricate cross-aspect relations between the above six aspects of trustworthy GNNs. Finally, we present a thorough overview of trending directions for facilitating the research and industrialisation of trustworthy GNNs.","182":"Successful applications of InfoNCE and its variants have popularized the use of contrastive variational mutual information (MI) estimators in machine learning. While featuring superior stability, these estimators crucially depend on costly large-batch training, and they sacrifice bound tightness for variance reduction. To overcome these limitations, we revisit the mathematics of popular variational MI bounds from the lens of unnormalized statistical modeling and convex optimization. Our investigation not only yields a new unified theoretical framework encompassing popular variational MI bounds but also leads to a novel, simple, and powerful contrastive MI estimator named as FLO. Theoretically, we show that the FLO estimator is tight, and it provably converges under stochastic gradient descent. Empirically, our FLO estimator overcomes the limitations of its predecessors and learns more efficiently. The utility of FLO is verified using an extensive set of benchmarks, which also reveals the trade-offs in practical MI estimation.","183":"Blockchains and cryptocurrencies disrupted the conversion of energy into a medium of exchange. Numerous applications for blockchains and cryptocurrencies are now envisioned for purposes ranging from inventory control to banking applications. Naturally, in order to mine in an economically viable way, regions where energy is plentiful and cheap, e.g., close to hydroelectric plants, are sought. The possibility of converting energy into cash, however, also opens up opportunities for a new kind of cyber attack aimed at illegally mining cryptocurrencies by stealing energy. In this work, we indicate, using data from January and February of 2018 from our university, that such a threat is real, and present a projection of the gains derived from these attacks.","184":"COVID-19 is now a global pandemic, and an effective vaccine may be many months away. Over 100 years ago, Spanish flu fatalities were attenuated when doctors began treating patients with blood plasma donated by recovered (or convalesced) survivors. Passive immunity transfer via administration of convalesced blood product (CBP) appears to represent a readily available and promising avenue for mitigating mortalities, expediting recovery time, and even prophylaxis against the SARS-CoV-2 virus. Here, we review challenges to CBP efficacy, and present a graph theoretical model of transmission dynamics that identifies evolving hubs of COVID-19 cases. Importantly, this model suggests that CBP efficacy may rest on an efficient and distributed global sampling scheme as opposed to CBP pooled from local donors alone.","185":"We propose PrYVeCT, a private-yet-verifiable contact tracing system. PrYVeCT works also as an authorization framework allowing for the definition of fine-grained policies, which a certain facility can define and apply to better model its own access rules. Users are authorized to access the facility only when they exhibit a contact trace that complies with the policy. The policy evaluation process is carried out without disclosing the personal data of the user. At the same time, each user can prove to a third party (e.g., a public authority) that she received a certain authorization. PrYVeCT takes advantage of oblivious automata evaluation to implement a privacy-preserving policy enforcement mechanism.","186":"Investigating and classifying sentiments of social media users towards an item, situation, and system are very popular among the researchers. However, they rarely discuss the underlying socioeconomic factor associations for such sentiments. This study attempts to explore the factors associated with positive and negative sentiments of the people about reopening the economy, in the United States (US) amidst the COVID-19 global crisis. It takes into consideration the situational uncertainties (i.e., changes in work and travel pattern due to lockdown policies), economic downturn and associated trauma, and emotional factors such as depression. To understand the sentiment of the people about the reopening economy, Twitter data was collected, representing the 51 states including Washington DC of the US. State-wide socioeconomic characteristics of the people, built environment data, and the number of COVID-19 related cases were collected and integrated with Twitter data to perform the analysis. A binary logit model was used to identify the factors that influence people toward a positive or negative sentiment. The results from the logit model demonstrate that family households, people with low education levels, people in the labor force, low-income people, and people with higher house rent are more interested in reopening the economy. In contrast, households with a high number of members and high income are less interested to reopen the economy. The model can correctly classify 56.18% of the sentiments. The Pearson chi2 test indicates that overall this model has high goodness-of-fit. This study provides a clear indication to the policymakers where to allocate resources and what policy options they can undertake to improve the socioeconomic situations of the people and mitigate the impacts of pandemics in the current situation and as well as in the future.","187":"The recent pandemic emergency raised many challenges regarding the countermeasures aimed at containing the virus spread, and constraining the minimum distance between people resulted in one of the most effective strategies. Thus, the implementation of autonomous systems capable of monitoring the so-called social distance gained much interest. In this paper, we aim to address this task leveraging a single RGB frame without additional depth sensors. In contrast to existing single-image alternatives failing when ground localization is not available, we rely on single image depth estimation to perceive the 3D structure of the observed scene and estimate the distance between people. During the setup phase, a straightforward calibration procedure, leveraging a scale-aware SLAM algorithm available even on consumer smartphones, allows us to address the scale ambiguity affecting single image depth estimation. We validate our approach through indoor and outdoor images employing a calibrated LiDAR + RGB camera asset. Experimental results highlight that our proposal enables sufficiently reliable estimation of the inter-personal distance to monitor social distancing effectively. This fact confirms that despite its intrinsic ambiguity, if appropriately driven single image depth estimation can be a viable alternative to other depth perception techniques, more expensive and not always feasible in practical applications. Our evaluation also highlights that our framework can run reasonably fast and comparably to competitors, even on pure CPU systems. Moreover, its practical deployment on low-power systems is around the corner.","188":"In ad-hoc human-robot collaboration (HRC), humans and robots work on a task without pre-planning the robot's actions prior to execution; instead, task allocation occurs in real-time. However, prior research has largely focused on task allocations that are pre-planned - there has not been a comprehensive exploration or evaluation of techniques where task allocation is adjusted in real-time. Inspired by HCI research on territoriality and proxemics, we propose a design space of novel task allocation techniques including both explicit techniques, where the user maintains agency, and implicit techniques, where the efficiency of automation can be leveraged. The techniques were implemented and evaluated using a tabletop HRC simulation in VR. A 16-participant study, which presented variations of a collaborative block stacking task, showed that implicit techniques enable efficient task completion and task parallelization, and should be augmented with explicit mechanisms to provide users with fine-grained control.","189":"Most current multi-object trackers focus on short-term tracking, and are based on deep and complex systems that often cannot operate in real-time, making them impractical for video-surveillance. In this paper we present a long-term, multi-face tracking architecture conceived for working in crowded contexts where faces are often the only visible part of a person. Our system benefits from advances in the fields of face detection and face recognition to achieve long-term tracking, and is particularly unconstrained to the motion and occlusions of people. It follows a tracking-by-detection approach, combining a fast short-term visual tracker with a novel online tracklet reconnection strategy grounded on rank-based face verification. The proposed rank-based constraint favours higher inter-class distance among tracklets, and reduces the propagation of errors due to wrong reconnections. Additionally, a correction module is included to correct past assignments with no extra computational cost. We present a series of experiments introducing novel specialized metrics for the evaluation of long-term tracking capabilities, and publicly release a video dataset with 10 manually annotated videos and a total length of 8' 54\". Our findings validate the robustness of each of the proposed modules, and demonstrate that, in these challenging contexts, our approach yields up to 50% longer tracks than state-of-the-art deep learning trackers.","190":"Text mining and information extraction for the medical domain has focused on scientific text generated by researchers. However, their direct access to individual patient experiences or patient-doctor interactions can be limited. Information provided on social media, e.g., by patients and their relatives, complements the knowledge in scientific text. It reflects the patient's journey and their subjective perspective on the process of developing symptoms, being diagnosed and offered a treatment, being cured or learning to live with a medical condition. The value of this type of data is therefore twofold: Firstly, it offers direct access to people's perspectives. Secondly, it might cover information that is not available elsewhere, including self-treatment or self-diagnoses. Named entity recognition and relation extraction are methods to structure information that is available in unstructured text. However, existing medical social media corpora focused on a comparably small set of entities and relations and particular domains, rather than putting the patient into the center of analyses. With this paper we contribute a corpus with a rich set of annotation layers following the motivation to uncover and model patients' journeys and experiences in more detail. We label 14 entity classes (incl. environmental factors, diagnostics, biochemical processes, patients' quality-of-life descriptions, pathogens, medical conditions, and treatments) and 20 relation classes (e.g., prevents, influences, interactions, causes) most of which have not been considered before for social media data. The publicly available dataset consists of 2,100 tweets with approx. 6,000 entity and 3,000 relation annotations. In a corpus analysis we find that over 80 % of documents contain relevant entities. Over 50 % of tweets express relations which we consider essential for uncovering patients' narratives about their journeys.","191":"The Metaverse is a virtual environment where users are represented by avatars to navigate a virtual world, which has strong links with the physical one. State-of-the-art Metaverse architectures rely on a cloud-based approach for avatar physics emulation and graphics rendering computation. Such centralized design is unfavorable as it suffers from several drawbacks caused by the long latency required for cloud access, such as low quality visualization. To solve this issue, in this paper, we propose a Fog-Edge hybrid computing architecture for Metaverse applications that leverage an edge-enabled distributed computing paradigm, which makes use of edge devices computing power to fulfil the required computational cost for heavy tasks such as collision detection in virtual universe and computation of 3D physics in virtual simulation. The computational cost related to an entity in the Metaverse such as collision detection or physics emulation are performed at the end-device of the associated physical entity. To prove the effectiveness of the proposed architecture, we simulate a distributed social metaverse application. Simulation results shows that the proposed architecture can reduce the latency by 50% when compared with the legacy cloud-based Metaverse applications.","192":"The Astrobiology Graduate Conference (AbGradCon) is an annual conference both organized for and by early career researchers, postdoctoral fellows, and students as a way to train the next generation of astrobiologists and develop a robust network of cohorts moving forward. AbGradCon 2021 was held virtually on September 14-17, 2021, hosted by the Earth-Life Science Institute (ELSI) of Tokyo Institute of Technology after postponement of the in-person event in 2020 due to the COVID-19 pandemic. The meeting consisted of presentations by 120 participants from a variety of fields, two keynote speakers, and other career building events and workshops. Here, we report on the organizational and executional aspects of AbGradCon 2021, including the meeting participant demographics, various digital aspects introduced specifically for a virtual edition of the meeting, and the abstract submission and evaluation process. The abstract evaluation process of AbGradCon 2021 is unique in that all evaluations are done by the peers of the applicants, and as astrobiology is inherently a broad discipline, the abstract evaluation process revealed a number of trends related to multidisciplinarity of the astrobiology field. We believe that meetings like AbGradCon can provide a unique opportunity for students and early career researchers in astrobiology to experience community building, inter- and multidisciplinary collaboration, and career training and would be a welcome sight in other fields as well. We hope that this report provides inspiration and a basic roadmap for organizing future conferences in any field with similar goals.","193":"Graph Convolutional Networks (GCNs) are a popular method from graph representation learning that have proved effective for tasks like node classification tasks. Although typical GCN models focus on classifying nodes within a static graph, several recent variants propose node classification in dynamic graphs whose topologies and node attributes change over time, e.g., social networks with dynamic relationships, or literature citation networks with changing co-authorships. These works, however, do not fully address the challenge of flexibly assigning different importance to snapshots of the graph at different times, which depending on the graph dynamics may have more or less predictive power on the labels. We address this challenge by proposing a new method, GCN-SE, that attaches a set of learnable attention weights to graph snapshots at different times, inspired by Squeeze and Excitation Net (SE-Net). We show that GCN-SE outperforms previously proposed node classification methods on a variety of graph datasets. To verify the effectiveness of the attention weight in determining the importance of different graph snapshots, we adapt perturbation-based methods from the field of explainable machine learning to graphical settings and evaluate the correlation between the attention weights learned by GCN-SE and the importance of different snapshots over time. These experiments demonstrate that GCN-SE can in fact identify different snapshots' predictive power for dynamic node classification.","194":"Gei\\ss-Leclerc-Schr\\\"oer [Invent. Math. 209 (2017)] has introduced a notion of generalized preprojective algebra associated with a generalized Cartan matrix and its symmetrizer. This class of algebra realizes a crystal structure on the set of maximal dimensional irreducible components of the nilpotent variety [Selecta Math. (N.S.) 24 (2018)]. For general finite types, we give stratifications of these components via partial orders of torsion classes in module categories of generalized preprojective algebras in terms of Weyl groups. In addition, we realize Mirkovi\\'c-Vilonen polytopes from generic modules of these components, and give an identification as crystals between the set of Mirkovi\\'c-Vilonen polytopes and the set of maximal dimensional irreducible components. This generalizes results of Baumann-Kamnitzer [Represent. Theory 16 (2012)] and Baumann-Kamnitzer-Tingley [Publ. Math. Inst. Hautes \\'Etudes Sci. 120 (2014)].","195":"Information diffusion, spreading of infectious diseases, and spreading of rumors are fundamental processes occurring in real-life networks. In many practical cases, one can observe when nodes become infected, but the underlying network, over which a contagion or information propagates, is hidden. Inferring properties of the underlying network is important since these properties can be used for constraining infections, forecasting, viral marketing, and so on. Moreover, for many applications, it is sufficient to recover only coarse high-level properties of this network rather than all its edges. This article conducts a systematic and extensive analysis of the following problem: Given only the infection times, find communities of highly interconnected nodes. This task significantly differs from the well-studied community detection problem since we do not observe a graph to be clustered. We carry out a thorough comparison between existing and new approaches on several large datasets and cover methodological challenges specific to this problem. One of the main conclusions is that the most stable performance and the most significant improvement on the current state-of-the-art are achieved by our proposed simple heuristic approaches agnostic to a particular graph structure and epidemic model. We also show that some well-known community detection algorithms can be enhanced by including edge weights based on the cascade data.","196":"Finding optimal parameter configurations for tunable GPU kernels is a non-trivial exercise for large search spaces, even when automated. This poses an optimization task on a non-convex search space, using an expensive to evaluate function with unknown derivative. These characteristics make a good candidate for Bayesian Optimization, which has not been applied to this problem before. However, the application of Bayesian Optimization to this problem is challenging. We demonstrate how to deal with the rough, discrete, constrained search spaces, containing invalid configurations. We introduce a novel contextual variance exploration factor, as well as new acquisition functions with improved scalability, combined with an informed acquisition function selection mechanism. By comparing the performance of our Bayesian Optimization implementation on various test cases to the existing search strategies in Kernel Tuner, as well as other Bayesian Optimization implementations, we demonstrate that our search strategies generalize well and consistently outperform other search strategies by a wide margin.","197":"Study reproducibility and generalizability of results to broadly inclusive populations is crucial in any research. Previous meta-analyses in HRI have focused on the consistency of reported information from papers in various categories. However, members of the HRI community have noted that much of the information needed for reproducible and generalizable studies is not found in published papers. We address this issue by surveying the reported study metadata over the past three years (2019 through 2021) of the main proceedings of the International Conference on Human-Robot Interaction (HRI) as well as alt.HRI. Based on the analysis results, we propose a set of recommendations for the HRI community that follow the longer-standing reporting guidelines from human-computer interaction (HCI), psychology, and other fields most related to HRI. Finally, we examine three key areas for user study reproducibility: recruitment details, participant compensation, and participant gender. We find a lack of reporting within each of these study metadata categories: of the 236 studies, 139 studies failed to report recruitment method, 118 studies failed to report compensation, and 62 studies failed to report gender data. This analysis therefore provides guidance about specific types of needed reporting improvements for HRI.","198":"We numerically explore the possibility that the large orbital inclination of the martian satellite Deimos originated in an orbital resonance with an ancient inner satellite of Mars more massive than Phobos. We find that Deimos's inclination can be reliably generated by outward evolution of a martian satellite that is about 20 times more massive than Phobos through the 3:1 mean-motion resonance with Deimos at 3.3 Mars radii. This outward migration, in the opposite direction from tidal evolution within the synchronous radius, requires interaction with a past massive ring of Mars. Our results therefore strongly support the cyclic martian ring-satellite hypothesis of Hesselbrock and Minton (2017). Our findings, combined with the model of Hesselbrock and Minton (2017), suggest that the age of the surface of Deimos is about 3.5-4 Gyr, and require Phobos to be significantly younger.","199":"Covid-19 vaccines are widely available in the United States, yet our Covid-19 vaccination rates have remained far below 100%. Not only that, but CDC data shows that even in places where vaccine acceptance was proportionally high at the outset of the Covid-19 vaccination effort, that willingness has not necessarily translated into high rates of vaccination over the subsequent months. We model how such a shift could have arisen, using parameters in agreement with data from the state of Alabama. The simulations suggest that in Alabama, local interactions would have favored the emergence of tight consensus around the initial majority view, which was to accept the Covid-19 vaccine. Yet this is not what happened. We therefore add to our model the impact of mega-influencers such as mass media, the governor of the state, etc. Our simulations show that a single vaccine-hesitant mega-influencer, reaching a large fraction of the population, can indeed cause the consensus to shift radically, from acceptance to hesitancy. Surprisingly this is true even when the mega-influencer only reaches individuals who are already somewhat inclined to agree with them, and under the conservative assumption that individuals give no more weight to the mega-influencer than they would give to a single one of their friends or neighbors. Our simulations also suggest that a competing mega-influencer with the opposite view can shift the mean population opinion back, but cannot restore the tightness of consensus around that view. Our code and data are distributed in the ODyN (Opinion Dynamic Networks) library available at https:\/\/github.com\/annahaensch\/ODyN.","200":"We present an Extended Kalman Filter framework for system identification and control of a stochastic high-dimensional epidemic model. The scale and severity of the COVID-19 emergency have highlighted the need for accurate forecasts of the state of the pandemic at a high resolution. Mechanistic compartmental models are widely used to produce such forecasts and assist in the design of control and relief policies. Unfortunately, the scale and stochastic nature of many of these models often makes the estimation of their parameters difficult. With the goal of calibrating a high dimensional COVID-19 model using low-level mobility data, we introduce a method for tractable maximum likelihood estimation that combines tools from Bayesian inference with scalable optimization techniques from machine learning. The proposed approach uses automatic backward-differentiation to directly compute the gradient of the likelihood of COVID-19 incidence and death data. The likelihood of the observations is estimated recursively using an Extended Kalman Filter and can be easily optimized using gradient-based methods to compute maximum likelihood estimators. Our compartmental model is trained using GPS mobility data that measures the mobility patterns of millions of mobile phones across the United States. We show that, after calibrating against incidence and deaths data from the city of Philadelphia, our model is able to produce an accurate 30-day forecast of the evolution of the pandemic.","201":"Big data has remarkably evolved over the last few years to realize an enormous volume of data generated from newly emerging services and applications and a massive number of Internet-of-Things (IoT) devices. The potential of big data can be realized via analytic and learning techniques, in which the data from various sources is transferred to a central cloud for central storage, processing, and training. However, this conventional approach faces critical issues in terms of data privacy as the data may include sensitive data such as personal information, governments, banking accounts. To overcome this challenge, federated learning (FL) appeared to be a promising learning technique. However, a gap exists in the literature that a comprehensive survey on FL for big data services and applications is yet to be conducted. In this article, we present a survey on the use of FL for big data services and applications, aiming to provide general readers with an overview of FL, big data, and the motivations behind the use of FL for big data. In particular, we extensively review the use of FL for key big data services, including big data acquisition, big data storage, big data analytics, and big data privacy preservation. Subsequently, we review the potential of FL for big data applications, such as smart city, smart healthcare, smart transportation, smart grid, and social media. Further, we summarize a number of important projects on FL-big data and discuss key challenges of this interesting topic along with several promising solutions and directions.","202":"The coronavirus which appeared in December 2019 in Wuhan has spread out worldwide and caused the death of more than 280,000 people (as of May, 11 2020). Since February 2020, doubts were raised about the numbers of confirmed cases and deaths reported by the Chinese government. In this paper, we examine data available from China at the city and provincial levels and we compare them with Canadian provincial data, US state data and French regional data. We consider cumulative and daily numbers of confirmed cases and deaths and examine these numbers through the lens of their first two digits and in particular we measure departures of these first two digits to the Newcomb-Benford distribution, often used to detect frauds. Our finding is that there is no evidence that cumulative and daily numbers of confirmed cases and deaths for all these countries have different first or second digit distributions. We also show that the Newcomb-Benford distribution cannot be rejected for these data.","203":"The workplace influences the safety, health, and productivity of workers at multiple levels. To protect and promote total worker health, smart hardware, and software tools have emerged for the identification, elimination, substitution, and control of occupational hazards. Wearable devices enable constant monitoring of individual workers and the environment, whereas connected worker solutions provide contextual information and decision support. Here, the recent trends in commercial workplace technologies to monitor and manage occupational risks, injuries, accidents, and diseases are reviewed. Workplace safety wearables for safe lifting, ergonomics, hazard identification, sleep monitoring, fatigue management, and heat and cold stress are discussed. Examples of workplace productivity wearables for asset tracking, augmented reality, gesture and motion control, brain wave sensing, and work stress management are given. Workplace health wearables designed for work-related musculoskeletal disorders, functional movement disorders, respiratory hazards, cardiovascular health, outdoor sun exposure, and continuous glucose monitoring are shown. Connected worker platforms are discussed with information about the architecture, system modules, intelligent operations, and industry applications. Predictive analytics provide contextual information about occupational safety risks, resource allocation, equipment failure, and predictive maintenance. Altogether, these examples highlight the ground-level benefits of real-time visibility about frontline workers, work environment, distributed assets, workforce efficiency, and safety compliance","204":"Advancements in digital technologies have a bootstrapping effect. The past fifty years of technological innovations from the computer architecture community have brought innovations and orders-of-magnitude efficiency improvements that engender use cases that were not previously possible -- stimulating novel application domains and increasing uses and deployments at an ever-faster pace. Consequently, computing technologies have fueled significant economic growth, creating education opportunities, enabling access to a wider and more diverse spectrum of information, and, at the same time, connecting people of differing needs in the world together. Technology must be offered that is inclusive of the world's physical, cultural, and economic diversity, and which is manufactured, used, and recycled with environmental sustainability at the forefront. For the next decades to come, we envision significant cross-disciplinary efforts to build a circular development cycle by placing pervasive connectivity, sustainability, and demographic inclusion at the design forefront in order to sustain and expand the benefits of a technologically rich society. We hope this work will inspire our computing community to take broader and more holistic approaches when developing technological solutions to serve people from different parts of the world.","205":"Nambu-Goldstone bosons, or axions, may be ubiquitous. Some of the axions may have small masses and thus serve as mediators of long-range forces. In this paper, we study the force mediated by an extremely light axion, $\\phi$, between the visible sector and the dark sector, where dark matter lives. Since nature does not preserve the CP symmetry, the coupling between dark matter and $\\phi$ is generically CP-violating. In this case, the induced force is extremely long-range and behaves as an effective magnetic field. If the force acts on electrons or nucleons, the spins of them on Earth precess around a fixed direction towards the galactic center. This provides an experimental opportunity for $\\phi$ with mass, $m_\\phi$, and decay constant, $f_\\phi$, satisfying $m_\\phi\\lesssim 10^{-25}\\,$ eV, $f_\\phi\\lesssim 10^{14}\\,$GeV if the daily modulation of the effective magnetic field signals in magnetometers is measured by using the coherent averaging method. The effective magnetic field induced by an axionic compact object, such as an axion domain wall, is also discussed.","206":"Federated Learning (FL) allows edge devices (or clients) to keep data locally while simultaneously training a shared high-quality global model. However, current research is generally based on an assumption that the training data of local clients have ground-truth. Furthermore, FL faces the challenge of statistical heterogeneity, i.e., the distribution of the client's local training data is non-independent identically distributed (non-IID). In this paper, we present a robust semi-supervised FL system design, where the system aims to solve the problem of data availability and non-IID in FL. In particular, this paper focuses on studying the labels-at-server scenario where there is only a limited amount of labeled data on the server and only unlabeled data on the clients. In our system design, we propose a novel method to tackle the problems, which we refer to as Federated Mixing (FedMix). FedMix improves the naive combination of FL and semi-supervised learning methods and designs parameter decomposition strategies for disjointed learning of labeled, unlabeled data, and global models. To alleviate the non-IID problem, we propose a novel aggregation rule based on the frequency of the client's participation in training, namely the FedFreq aggregation algorithm, which can adjust the weight of the corresponding local model according to this frequency. Extensive evaluations conducted on CIFAR-10 dataset show that the performance of our proposed method is significantly better than those of the current baseline. It is worth noting that our system is robust to different non-IID levels of client data.","207":"Live streaming has become increasingly popular. Our study focuses on the emerging Chinese female audiences who send virtual gifts to young male streamers. We observe a reversed entertainer-viewer gender relationship. We aim to study why they watch young male streamers, why they send gifts, and their relationships with these streamers.","208":"Graph Neural Networks (GNNs) have made rapid developments in the recent years. Due to their great ability in modeling graph-structured data, GNNs are vastly used in various applications, including high-stakes scenarios such as financial analysis, traffic predictions, and drug discovery. Despite their great potential in benefiting humans in the real world, recent study shows that GNNs can leak private information, are vulnerable to adversarial attacks, can inherit and magnify societal bias from training data and lack interpretability, which have risk of causing unintentional harm to the users and society. For example, existing works demonstrate that attackers can fool the GNNs to give the outcome they desire with unnoticeable perturbation on training graph. GNNs trained on social networks may embed the discrimination in their decision process, strengthening the undesirable societal bias. Consequently, trustworthy GNNs in various aspects are emerging to prevent the harm from GNN models and increase the users' trust in GNNs. In this paper, we give a comprehensive survey of GNNs in the computational aspects of privacy, robustness, fairness, and explainability. For each aspect, we give the taxonomy of the related methods and formulate the general frameworks for the multiple categories of trustworthy GNNs. We also discuss the future research directions of each aspect and connections between these aspects to help achieve trustworthiness.","209":"The accuracy and complexity of machine learning algorithms based on kernel optimization are limited by the set of kernels over which they are able to optimize. An ideal set of kernels should: admit a linear parameterization (for tractability); be dense in the set of all kernels (for robustness); be universal (for accuracy). The recently proposed Tesselated Kernels (TKs) is currently the only known class which meets all three criteria. However, previous algorithms for optimizing TKs were limited to classification and relied on Semidefinite Programming (SDP) - limiting them to relatively small datasets. By contrast, the 2-step algorithm proposed here scales to 10,000 data points and extends to the regression problem. Furthermore, when applied to benchmark data, the algorithm demonstrates significant improvement in performance over Neural Nets and SimpleMKL with similar computation time.","210":"This paper summarizes our endeavors in the past few years in terms of explaining image classifiers, with the aim of including negative results and insights we have gained. The paper starts with describing the explainable neural network (XNN), which attempts to extract and visualize several high-level concepts purely from the deep network, without relying on human linguistic concepts. This helps users understand network classifications that are less intuitive and substantially improves user performance on a difficult fine-grained classification task of discriminating among different species of seagulls. Realizing that an important missing piece is a reliable heatmap visualization tool, we have developed I-GOS and iGOS++ utilizing integrated gradients to avoid local optima in heatmap generation, which improved the performance across all resolutions. During the development of those visualizations, we realized that for a significant number of images, the classifier has multiple different paths to reach a confident prediction. This has lead to our recent development of structured attention graphs (SAGs), an approach that utilizes beam search to locate multiple coarse heatmaps for a single image, and compactly visualizes a set of heatmaps by capturing how different combinations of image regions impact the confidence of a classifier. Through the research process, we have learned much about insights in building deep network explanations, the existence and frequency of multiple explanations, and various tricks of the trade that make explanations work. In this paper, we attempt to share those insights and opinions with the readers with the hope that some of them will be informative for future researchers on explainable deep learning.","211":"Deep learning technology has been widely used in edge computing. However, pandemics like covid-19 require deep learning capabilities at mobile devices (detect respiratory rate using mobile robotics or conduct CT scan using a mobile scanner), which are severely constrained by the limited storage and computation resources at the device level. To solve this problem, we propose a three-tier architecture, including robot layers, edge layers, and cloud layers. We adopt this architecture to design a non-contact respiratory monitoring system to break down respiratory rate calculation tasks. Experimental results of respiratory rate monitoring show that the proposed approach in this paper significantly outperforms other approaches. It is supported by computation time costs with 2.26 ms per frame, 27.48 ms per frame, 0.78 seconds for convolution operation, similarity calculation, processing one-minute length respiratory signals, respectively. And the computation time costs of our three-tier architecture are less than that of edge+cloud architecture and cloud architecture. Moreover, we use our three-tire architecture for CT image diagnosis task decomposition. The evaluation of a CT image dataset of COVID-19 proves that our three-tire architecture is useful for resolving tasks on deep learning networks by edge equipment. There are broad application scenarios in smart hospitals in the future.","212":"Growing polarization of the news media has been blamed for fanning disagreement, controversy and even violence. Early identification of polarized topics is thus an urgent matter that can help mitigate conflict. However, accurate measurement of topic-wise polarization is still an open research challenge. To address this gap, we propose Partisanship-aware Contextualized Topic Embeddings (PaCTE), a method to automatically detect polarized topics from partisan news sources. Specifically, utilizing a language model that has been finetuned on recognizing partisanship of the news articles, we represent the ideology of a news corpus on a topic by corpus-contextualized topic embedding and measure the polarization using cosine distance. We apply our method to a dataset of news articles about the COVID-19 pandemic. Extensive experiments on different news sources and topics demonstrate the efficacy of our method to capture topical polarization, as indicated by its effectiveness of retrieving the most polarized topics.","213":"In this work, we present RadioTransformer, a novel visual attention-driven transformer framework, that leverages radiologists' gaze patterns and models their visuo-cognitive behavior for disease diagnosis on chest radiographs. Domain experts, such as radiologists, rely on visual information for medical image interpretation. On the other hand, deep neural networks have demonstrated significant promise in similar tasks even where visual interpretation is challenging. Eye-gaze tracking has been used to capture the viewing behavior of domain experts, lending insights into the complexity of visual search. However, deep learning frameworks, even those that rely on attention mechanisms, do not leverage this rich domain information. RadioTransformer fills this critical gap by learning from radiologists' visual search patterns, encoded as 'human visual attention regions' in a cascaded global-focal transformer framework. The overall 'global' image characteristics and the more detailed 'local' features are captured by the proposed global and focal modules, respectively. We experimentally validate the efficacy of our student-teacher approach for 8 datasets involving different disease classification tasks where eye-gaze data is not available during the inference phase.","214":"The dissertation proposes the use of a multi-objective optimization framework for designing and selecting among enhanced GOP configurations in video compression standards. The proposed methods achieve fine optimization over a set of general modes that include: (i) maximum video quality, (ii) minimum bitrate, (iii) maximum encoding rate (previously minimum encoding time mode) and (iv) can be shown to improve upon the YouTube\/Netflix default encoder mode settings over a set of opposing constraints to guarantee satisfactory performance. The dissertation describes the implementation of a codec-agnostic approach using different video coding standards (x265, VP9, AV1) on a wide range of videos derived from different video datasets. The results demonstrate that the optimal encoding parameters obtained from the Pareto front space can provide significant bandwidth savings without sacrificing video quality. This is achieved by the use of effective regression models that allow for the selection of video encoding settings that are jointly optimal in the encoding time, bitrate, and video quality space. The dissertation applies the proposed methods to x265, VP9, AV1 and using new GOP configurations in x265, delivering over 40% of the optimal encodings in two standard reference videos.","215":"In recent years, several systems have been developed to regulate the spread of negativity and eliminate aggressive, offensive or abusive contents from the online platforms. Nevertheless, a limited number of researches carried out to identify positive, encouraging and supportive contents. In this work, our goal is to identify whether a social media post\/comment contains hope speech or not. We propose three distinct models to identify hope speech in English, Tamil and Malayalam language to serve this purpose. To attain this goal, we employed various machine learning (support vector machine, logistic regression, ensemble), deep learning (convolutional neural network + long short term memory) and transformer (m-BERT, Indic-BERT, XLNet, XLM-Roberta) based methods. Results indicate that XLM-Roberta outdoes all other techniques by gaining a weighted $f_1$-score of $0.93$, $0.60$ and $0.85$ respectively for English, Tamil and Malayalam language. Our team has achieved $1^{st}$, $2^{nd}$ and $1^{st}$ rank in these three tasks respectively.","216":"Graph data have become increasingly common. Visualizing them helps people better understand relations among entities. Unfortunately, existing graph visualization tools are primarily designed for single-person desktop use, offering limited support for interactive web-based exploration and online collaborative analysis. To address these issues, we have developed Argo Lite, a new in-browser interactive graph exploration and visualization tool. Argo Lite enables users to publish and share interactive graph visualizations as URLs and embedded web widgets. Users can explore graphs incrementally by adding more related nodes, such as highly cited papers cited by or citing a paper of interest in a citation network. Argo Lite works across devices and platforms, leveraging WebGL for high-performance rendering. Argo Lite has been used by over 1,000 students at Georgia Tech's Data and Visual Analytics class. Argo Lite may serve as a valuable open-source tool for advancing multiple CIKM research areas, from data presentation, to interfaces for information systems and more.","217":"Binarized neural networks, or BNNs, show great promise in edge-side applications with resource limited hardware, but raise the concerns of reduced accuracy. Motivated by the complex neural networks, in this paper we introduce complex representation into the BNNs and propose Binary complex neural network -- a novel network design that processes binary complex inputs and weights through complex convolution, but still can harvest the extraordinary computation efficiency of BNNs. To ensure fast convergence rate, we propose novel BCNN based batch normalization function and weight initialization function. Experimental results on Cifar10 and ImageNet using state-of-the-art network models (e.g., ResNet, ResNetE and NIN) show that BCNN can achieve better accuracy compared to the original BNN models. BCNN improves BNN by strengthening its learning capability through complex representation and extending its applicability to complex-valued input data. The source code of BCNN will be released on GitHub.","218":"We compare the pseudo-real-time forecasting performance of two factor models for a large set of macroeconomic and financial time series: (i) The standard principal-component model used by Stock and Watson (2002) (ii) The factor model with martingale difference errors introduced by Lee and Shao (2018). The factor model with martingale difference error (FMMDE) makes it possible to retrieve a transformation of the original series so that the resulting variables can be partitioned into distinct groups according to whether they are conditionally mean independent upon past information or not. In terms of prediction, this feature of the model allows us to achieve optimal results (in the mean squared error sense) as dimension reduction is performed. Conducting a pseudo-real-time forecasting exercise based on a large dataset of macroeconomic and financial monthly time series for the U.S. economy, the results obtained from the empirical study suggest that FMMDE performs comparably to SW for short-term forecasting horizons. In contrast, for long-term forecasting horizons, FMMDE displays superior performance. These results are particularly evident for Output&Income, Labor Market, and Consumption sectors.","219":"A thermodynamic framework has been developed for a class of amorphous polymers used in fused deposition modeling (FDM), in order to predict the residual stresses and the accompanying distortion of the geometry of the printed part (warping). When a polymeric melt is cooled, the inhomogeneous distribution of temperature causes spatially varying volumetric shrinkage resulting in the generation of residual stresses. Shrinkage is incorporated into the framework by introducing an isotropic volumetric expansion\/contraction in the kinematics of the body. We show that the parameter for shrinkage also appears in the systematically derived rate-type constitutive relation for the stress. The solidification of the melt around the glass transition temperature is emulated by drastically increasing the viscosity of the melt. In order to illustrate the usefulness and efficacy of the derived constitutive relation, we consider four ribbons of polymeric melt stacked on each other such as those extruded using a flat nozzle: each layer laid instantaneously and allowed to cool for one second before another layer is laid on it. Each layer cools, shrinks and warps until a new layer is laid, at which time the heat from the newly laid layer flows and heats up the bottom layers. The residual stresses of the existing and newly laid layers readjust to satisfy equilibrium. Such mechanical and thermal interactions amongst layers result in a complex distribution of residual stresses. The plane strain approximation predicts nearly equibiaxial tensile stress conditions in the core region of the solidified part, implying that a pre-existing crack in that region is likely to propagate and cause failure of the part during service. The free-end of the interface between the first and the second layer is subjected to the largest magnitude of combined shear and tension in the plane with a propensity for delamination.","220":"The novel coronavirus (2019-nCoV) is a recently emerged human pathogen that has spread widely since January 2020. Initially, the basic reproductive number, R0, was estimated to be 2.2 to 2.7. Here we provide a new estimate of this quantity. We collected extensive individual case reports and estimated key epidemiology parameters, including the incubation period. Integrating these estimates and high-resolution real-time human travel and infection data with mathematical models, we estimated that the number of infected individuals during early epidemic double every 2.4 days, and the R0 value is likely to be between 4.7 and 6.6. We further show that quarantine and contact tracing of symptomatic individuals alone may not be effective and early, strong control measures are needed to stop transmission of the virus.","221":"COVID-19 outbreaks have proven to be very difficult to isolate and extinguish before they spread out. An important reason behind this might be that epidemiological barriers consisting in stopping symptomatic people are likely to fail because of the contagion time before onset, mild cases and\/or asymptomatics carriers. Motivated by these special COVID-19 features, we study a scheme for containing an outbreak in a city that consists in adding an extra firewall block between the outbreak and the rest of the city. We implement a coupled compartment model with stochastic noise to simulate a localized outbreak that is partially isolated and analyze its evolution with and without firewall for different plausible model parameters. We explore how further improvements could be achieved if the epidemic evolution would trigger policy changes for the flux and\/or lock-down in the different blocks. Our results show that a substantial improvement is obtained by merely adding an extra block between the outbreak and the bulk of the city.","222":"Protein-protein interactions (PPIs) are essentials for many biological processes where two or more proteins physically bind together to achieve their functions. Modeling PPIs is useful for many biomedical applications, such as vaccine design, antibody therapeutics, and peptide drug discovery. Pre-training a protein model to learn effective representation is critical for PPIs. Most pre-training models for PPIs are sequence-based, which naively adopt the language models used in natural language processing to amino acid sequences. More advanced works utilize the structure-aware pre-training technique, taking advantage of the contact maps of known protein structures. However, neither sequences nor contact maps can fully characterize structures and functions of the proteins, which are closely related to the PPI problem. Inspired by this insight, we propose a multimodal protein pre-training model with three modalities: sequence, structure, and function (S2F). Notably, instead of using contact maps to learn the amino acid-level rigid structures, we encode the structure feature with the topology complex of point clouds of heavy atoms. It allows our model to learn structural information about not only the backbones but also the side chains. Moreover, our model incorporates the knowledge from the functional description of proteins extracted from literature or manual annotations. Our experiments show that the S2F learns protein embeddings that achieve good performances on a variety of PPIs tasks, including cross-species PPI, antibody-antigen affinity prediction, antibody neutralization prediction for SARS-CoV-2, and mutation-driven binding affinity change prediction.","223":"Knowledge discovery is one of the main goals of Artificial Intelligence. This Knowledge is usually stored in databases spread in different environments, being a tedious (or impossible) task to access and extract data from them. To this difficulty we must add that these datasources may contain private data, therefore the information can never leave the source. Privacy Preserving Machine Learning (PPML) helps to overcome this difficulty, employing cryptographic techniques, allowing knowledge discovery while ensuring data privacy. K-means is one of the data mining techniques used in order to discover knowledge, grouping data points in clusters that contain similar features. This paper focuses in Privacy Preserving Machine Learning applied to K-means using recent protocols from the field of criptography. The algorithm is applied to different scenarios where data may be distributed either horizontally or vertically.","224":"The COVID-19 pandemic poses challenges for continuing economic activity while reducing health risk to individuals and preventing uncontrolled outbreaks. These challenges can be mitigated by extensive testing. We study testing policies that optimize a fixed budget of tests within a single institution (e.g., business, school, nursing home, etc.) by varying the number of batches that the tests are split into. We prove that, in an exponential spread model and for reasonable parameter values, the expected size of an outbreak at initial detection is smaller when random subgroups of the population are tested frequently, as opposed to periodic testing of the entire population. We also simulate the effect of different policies in a network SEIR model taking into account factors such as variable connectivity between individuals, incubation period, and inaccurate testing results. We show that under a broad set of early-outbreak scenarios, given a certain budget of tests, increasing testing frequency of random samples of the population will reduce the societal risk, defined as the number of infection opportunities until first detection. For example, testing a quarter of the institution members every week is generally better than testing the entire institution every month. In fact, in many settings, sufficiently frequent testing (combined with mitigation once an outbreak is detected) can decrease the risk even compared to the baseline when the institution is closed and testing is not conducted. The bottom-line is a simple policy prescription for institutions: distribute the total tests over several batches instead of using them all at once.","225":"Healthcare is one of the most important aspects of human life. Heart disease is known to be one of the deadliest diseases which is hampering the lives of many people around the world. Heart disease must be detected early so the loss of lives can be prevented. The availability of large-scale data for medical diagnosis has helped developed complex machine learning and deep learning-based models for automated early diagnosis of heart diseases. The classical approaches have been limited in terms of not generalizing well to new data which have not been seen in the training set. This is indicated by a large gap in training and test accuracies. This paper proposes a novel deep learning architecture using a 1D convolutional neural network for classification between healthy and non-healthy persons to overcome the limitations of classical approaches. Various clinical parameters are used for assessing the risk profile in the patients which helps in early diagnosis. Various techniques are used to avoid overfitting in the proposed network. The proposed network achieves over 97% training accuracy and 96% test accuracy on the dataset. The accuracy of the model is compared in detail with other classification algorithms using various performance parameters which proves the effectiveness of the proposed architecture.","226":"Multipath TCP (MPTCP) extends traditional TCP to enable simultaneous use of multiple connection endpoints at the source and destination. MPTCP has been under active development since its standardization in 2013, and more recently in February 2020, MPTCP was upstreamed to the Linux kernel. In this paper, we provide an in-depth analysis of MPTCPv0 in the Internet and the first analysis of MPTCPv1 to date. We probe the entire IPv4 address space and an IPv6 hitlist to detect MPTCP-enabled systems operational on port 80 and 443. Our scans reveal a steady increase in MPTCPv0-capable IPs, reaching 13k+ on IPv4 (2$\\times$ increase in one year) and 1k on IPv6 (40$\\times$ increase). MPTCPv1 deployment is comparatively low with $\\approx$100 supporting hosts in IPv4 and IPv6, most of which belong to Apple. We also discover a substantial share of seemingly MPTCP-capable hosts, an artifact of middleboxes mirroring TCP options. We conduct targeted HTTP(S) measurements towards select hosts and find that middleboxes can aggressively impact the perceived quality of applications utilizing MPTCP. Finally, we analyze two complementary traffic traces from CAIDA and MAWI to shed light on the real-world usage of MPTCP. We find that while MPTCP usage has increased by a factor of 20 over the past few years, its traffic share is still quite low.","227":"The sudden onset of the coronavirus (SARS-CoV-2) pandemic has resulted in tremendous loss of human life and economy in more than 210 countries and territories around the world. While self-protections such as wearing mask, sheltering in place and quarantine polices and strategies are necessary for containing virus transmission, tens of millions people in the U.S. have lost their jobs due to the shutdown of businesses. Therefore, how to reopen the economy safely while the virus is still circulating in population has become a problem of significant concern and importance to elected leaders and business executives. In this study, mathematical modeling is employed to quantify the profit generation and the infection risk simultaneously from the point of view of a business entity. Specifically, an ordinary differential equation model was developed to characterize disease transmission and infection risk. An algebraic equation is proposed to determine the net profit that a business entity can generate after reopening and take into account the costs associated of several protection\/quarantine guidelines. All model parameters were calibrated based on various data and information sources. Sensitivity analyses and case studies were performed to illustrate the use of the model in practice.","228":"When an unprecedented infectious disease with high mortality and transmissibility emerges, immediate usage of vaccines or medicines is hardly available. Thus, many health authorities rely on non-pharmaceutical interventions through traceable fixed contacts. However, in reality, there is an additional type of transmission routes to the regular and fixed contacts: the random anonymous infection cases where non-pharmaceutical interventions are hardly feasible. In our study, such realistic situations are implemented by the susceptible-infected-recovered model with isolation on multiplex networks. The multiplex networks are composed of a fixed interaction layer and a layer with time-varying random interactions to represent the different types of disease spreading routes. The multiplex networks represent the combinations of the quenched disorder and annealed disorder. Here, we suggest a preemptive isolation protocol which isolates the second nearest neighbors of the hospitalized individuals and compare it with one of the most popular protocol adopted by many health organizations over the globe. From numerical simulations we find that our preemptive measure significantly reduces both the final epidemic size and the number of the isolated per unit time. Our finding suggests a better non-pharmaceutical intervention which can be adopted to various types of diseases even though the contact tracing is only partially available.","229":"This two-part paper presents a new approach to predictive analysis for social processes. In Part I, we begin by identifying a class of social processes which are simultaneously important in applications and difficult to predict using existing methods. It is shown that these processes can be modeled within a multi-scale, stochastic hybrid system framework that is sociologically sensible, expressive, illuminating, and amenable to formal analysis. Among other advantages, the proposed modeling framework enables proper characterization of the interplay between the intrinsic aspects of a social process (e.g., the appeal of a political movement) and the social dynamics which are its realization; this characterization is key to successful social process prediction. The utility of the modeling methodology is illustrated through a case study involving the global SARS epidemic of 2002-2003. Part II of the paper then leverages this modeling framework to develop a rigorous, computationally tractable approach to social process predictive analysis.","230":"The spread of the novel coronavirus across various countries is wide and rapid. The number of confirmed cases and the reproduction number are some of the epidemiological parameters utilized in scientific studies for the analysis and prediction of the viral transmission. The positive rate, an indicator on the extent of testing the population, aids in understanding the severity of the infection in a given geographic location. The positive rate for selected countries has been considered in this study to construct ARIMA based statistical models. The goodness of fit of the models are verified by the investigation of residuals, Box-Luang test and the forecast error values. The positive rates forecasted by the ARIMA models are utilized to investigate the scope for implementation of relaxations in social distancing measures in some countries and the necessity to tighten the rules further in some other countries.","231":"Historically, science has benefited greatly through the mobility of researchers, whether it has been due to large-scale conflict, the search for new opportunities or a lack thereof. Today's world of strict global immigration policies, exacerbated by the COVID-19 pandemic, places inordinate hurdles on the mobility of all researchers, let alone quantum ones. Exorbitant Visa fees, the difficulty of navigating a foreign immigration system, lack of support for researchers' families, and explicit government policy targeting selected groups of immigrants are all examples of things that have severely impacted the ability of quantum researchers to cross both physical and scientific borders. Here we clearly identify some key problems affecting quantum researcher mobility and discuss examples of good practice on the governmental, institutional, and societal level that have helped, or might help, overcome these hurdles. The adoption of such practices worldwide can ensure that quantum scientists can reach their fullest potential, irrespective of where they were born.","232":"This paper investigates a variant of the dial-a-ride problem (DARP), namely Risk-aware DARP (RDARP). Our RDARP extends the DARP by (1) minimizing a weighted sum of travel cost and disease-transmission risk exposure for onboard passengers and (2) introducing a maximum cumulative exposure risk constraint for each vehicle to ensure a trip with lower external exposure. Both extensions require that the risk exposure of each onboard passenger is propagated in a minimized fashion while satisfying other existing constraints of the DARP. To fully describe the RDARP, we first provide a three-index arc-based formulation and reformulate the RDARP into an equivalent min-max trip-based formulation, which is solved by an exact Branch-Cut-and-Price (BCP) algorithm. For each node in the branching tree, we adopt the column generation method by decomposing the problem into a master problem and a subproblem. The latter takes the form of an elementary shortest path problem with resource constraints and minimized maximum risk (ESPPRCMMR). To solve the ESPPRCMMR efficiently, we develop a new labeling algorithm and establish families of resource extension functions in compliance with the risk-related resources. We adopt a real-world paratransit trip dataset to generate the RDARP instances ranging from 17 to 55 heterogeneous passengers with 3 to 13 vehicles during the morning and afternoon periods. Computational results show that our BCP algorithm can optimally solve all small- and medium-sized instances (32 or fewer passengers) within 5 minutes. For the large instances (39 to 55 passengers), our BCP algorithm can solve 23 of 30 instances optimally within a time limit of one hour.","233":"School accountability systems increasingly hold schools to account for their performances using value-added models purporting to measure the effects of schools on student learning. The most common approach is to fit a linear regression of student current achievement on student prior achievement, where the school effects are the school means of the predicted residuals. In the literature, further adjustments are usually made for student sociodemographics and sometimes school composition and 'non-malleable' characteristics. However, accountability systems typically make fewer adjustments: for transparency to end users, because data is unavailable or of insufficient quality, or for ideological reasons. There is therefore considerable interest in understanding the extent to which simpler models give similar school effects to more theoretically justified but complex models. We explore these issues via a case study and empirical analysis of England's 'Progress 8' secondary school accountability system.","234":"The Covid-19 pandemic has taken millions of lives, demonstrating the tragedy and disruption of respiratory diseases, and how difficult they can be to manage. However, there is still significant debate in the scientific community as to which transmission pathways are most significant and how settings and behaviour affect risk of infection, which all have implications for which mitigation strategies are most effective. This study presents a general model to estimate the rate of viral transfer between individuals, objects, and the air. The risk of infection to individuals in a setting is then computed considering the behaviour and interactions of individuals between themselves and the environment in the setting, survival times of the virus on different surface types and in the air, and mitigating interventions (ventilation, hand disinfection, surface cleaning, etc.). The model includes discrete events such as touch events, individuals entering\/leaving the setting, and cleaning events. We demonstrate the model capabilities on three case studies to quantify and understand the relative risk associated with the different transmission pathways and the effectiveness of mitigation strategies in different settings. The results show the importance of considering all transmission pathways and their interactions, with each scenario displaying different dominant pathways depending on the setting and behaviours of individuals therein. The flexible model, which is freely available, can be used to quickly simulate the spread of any respiratory virus via the modelled transmission pathways and the efficacy of potential mitigation strategies in any enclosed setting by making reasonable assumptions regarding the behaviour of its occupants. It is hoped that the model can be used to inform sensible decision-making regarding viral infection mitigations that are targeted to specific settings and pathogens.","235":"A novel coronavirus disease 2019 (COVID-19) was detected and has spread rapidly across various countries around the world since the end of the year 2019, Computed Tomography (CT) images have been used as a crucial alternative to the time-consuming RT-PCR test. However, pure manual segmentation of CT images faces a serious challenge with the increase of suspected cases, resulting in urgent requirements for accurate and automatic segmentation of COVID-19 infections. Unfortunately, since the imaging characteristics of the COVID-19 infection are diverse and similar to the backgrounds, existing medical image segmentation methods cannot achieve satisfactory performance. In this work, we try to establish a new deep convolutional neural network tailored for segmenting the chest CT images with COVID-19 infections. We firstly maintain a large and new chest CT image dataset consisting of 165,667 annotated chest CT images from 861 patients with confirmed COVID-19. Inspired by the observation that the boundary of the infected lung can be enhanced by adjusting the global intensity, in the proposed deep CNN, we introduce a feature variation block which adaptively adjusts the global properties of the features for segmenting COVID-19 infection. The proposed FV block can enhance the capability of feature representation effectively and adaptively for diverse cases. We fuse features at different scales by proposing Progressive Atrous Spatial Pyramid Pooling to handle the sophisticated infection areas with diverse appearance and shapes. We conducted experiments on the data collected in China and Germany and show that the proposed deep CNN can produce impressive performance effectively.","236":"Tropospheric nitrogen dioxide (NO$_2$) concentrations are strongly affected by anthropogenic activities. Using space-based measurements of tropospheric NO$_2$, here we investigate the responses of tropospheric NO$_2$ to the 2019 novel coronavirus (COVID-19) over China, South Korea, and Italy. We find noticeable reductions of tropospheric NO$_2$ columns due to the COVID-19 controls by more than 40% over E. China, South Korea, and N. Italy. The 40% reductions of tropospheric NO$_2$ are coincident with intensive lockdown events as well as up to 20% reductions in anthropogenic nitrogen oxides (NO$_x$) emissions. The perturbations in tropospheric NO$_2$ diminished accompanied with the mitigation of COVID-19 pandemic, and finally disappeared within around 50-70 days after the starts of control measures over all three nations, providing indications for the start, maximum, and mitigation of intensive controls. This work exhibits significant influences of lockdown measures on atmospheric environment, highlighting the importance of satellite observations to monitor anthropogenic activity changes.","237":"In recent, deep learning has become the most popular direction in machine learning and artificial intelligence. However, preparation of training data is often a bottleneck in the lifecycle of deploying a deep learning model for production or research. Reusing models for inferencing a dataset can greatly save the human costs required for training data creation. Although there exist a number of model sharing platform such as TensorFlow Hub, PyTorch Hub, DLHub, most of these systems require model uploaders to manually specify the details of each model and model downloaders to screen keyword search results for selecting a model. They are in lack of an automatic model searching tool. This paper proposes an end-to-end process of searching related models for serving based on the similarity of the target dataset and the training datasets of the available models. While there exist many similarity measurements, we study how to efficiently apply these metrics without pair-wise comparison and compare the effectiveness of these metrics. We find that our proposed adaptivity measurement which is based on Jensen-Shannon (JS) divergence, is an effective measurement, and its computation can be significantly accelerated by using the technique of locality sensitive hashing.","238":"This work investigates the use of interactively updated label suggestions to improve upon the efficiency of gathering annotations on the task of opinion mining in German Covid-19 social media data. We develop guidelines to conduct a controlled annotation study with social science students and find that suggestions from a model trained on a small, expert-annotated dataset already lead to a substantial improvement - in terms of inter-annotator agreement(+.14 Fleiss' $\\kappa$) and annotation quality - compared to students that do not receive any label suggestions. We further find that label suggestions from interactively trained models do not lead to an improvement over suggestions from a static model. Nonetheless, our analysis of suggestion bias shows that annotators remain capable of reflecting upon the suggested label in general. Finally, we confirm the quality of the annotated data in transfer learning experiments between different annotator groups. To facilitate further research in opinion mining on social media data, we release our collected data consisting of 200 expert and 2,785 student annotations.","239":"Since the advent of online social media platforms such as Twitter and Facebook, useful health-related studies have been conducted using the information posted by online participants. Personal health-related issues such as mental health, self-harm and depression have been studied because users often share their stories on such platforms. Online users resort to sharing because the empathy and support from online communities are crucial in helping the affected individuals. A preliminary analysis shows how contents related to non-suicidal self-injury (NSSI) proliferate on Twitter. Thus, we use Twitter to collect relevant data, analyse, and proffer ways of supporting users prone to NSSI behaviour. Our approach utilises a custom crawler to retrieve relevant tweets from self-reporting users and relevant organisations interested in combating self-harm. Through textual analysis, we identify six major categories of self-harming users consisting of inflicted, anti-self-harm, support seekers, recovered, pro-self-harm and at risk. The inflicted category dominates the collection. From an engagement perspective, we show how online users respond to the information posted by self-harm support organisations on Twitter. By noting the most engaged organisations, we apply a useful technique to uncover the organisations' strategy. The online participants show a strong inclination towards online posts associated with mental health related attributes. Our study is based on the premise that social media can be used as a tool to support proactive measures to ease the negative impact of self-harm. Consequently, we proffer ways to prevent potential users from engaging in self-harm and support affected users through a set of recommendations. To support further research, the dataset will be made available for interested researchers.","240":"False assumptions about sex and gender are deeply embedded in the medical system, including that they are binary, static, and concordant. Machine learning researchers must understand the nature of these assumptions in order to avoid perpetuating them. In this perspectives piece, we identify three common mistakes that researchers make when dealing with sex\/gender data:\"sex confusion\", the failure to identity what sex in a dataset does or doesn't mean;\"sex obsession\", the belief that sex, specifically sex assigned at birth, is the relevant variable for most applications; and\"sex\/gender slippage\", the conflation of sex and gender even in contexts where only one or the other is known. We then discuss how these pitfalls show up in machine learning studies based on electronic health record data, which is commonly used for everything from retrospective analysis of patient outcomes to the development of algorithms to predict risk and administer care. Finally, we offer a series of recommendations about how machine learning researchers can produce both research and algorithms that more carefully engage with questions of sex\/gender, better serving all patients, including transgender people.","241":"Topic Modelling is one of the most prevalent text analysis technique used to explore and retrieve collection of documents. The evaluation of the topic model algorithms is still a very challenging tasks due to the absence of gold-standard list of topics to compare against for every corpus. In this work, we present a specificity score based on keywords properties that is able to select the most informative topics. This approach helps the user to focus on the most informative topics. In the experiments, we show that we are able to compress the state-of-the-art topic modelling results of different factors with an information loss that is much lower than the solution based on the recent coherence score presented in literature.","242":"Public sentiment (the opinion, attitude or feeling that the public expresses) is a factor of interest for government, as it directly influences the implementation of policies. Given the unprecedented nature of the COVID-19 crisis, having an up-to-date representation of public sentiment on governmental measures and announcements is crucial. While the staying-at-home policy makes face-to-face interactions and interviews challenging, analysing real-time Twitter data that reflects public opinion toward policy measures is a cost-effective way to access public sentiment. In this paper, we collect streaming data using the Twitter API starting from the COVID-19 outbreak in the Netherlands in February 2020, and track Dutch general public reactions on governmental measures and announcements. We provide temporal analysis of tweet frequency and public sentiment over the past four months. We also identify public attitudes towards the Dutch policy on wearing face masks in a case study. By presenting those preliminary results, we aim to provide visibility into the social media discussions around COVID-19 to the general public, scientists and policy makers. The data collection and analysis will be updated and expanded over time.","243":"The ongoing digital transformation of the medical sector requires solutions that are convenient and efficient for all stakeholders while protecting patients' sensitive data. One example involving both patients and health professionals that has already attracted design-oriented research are medical prescriptions. However, current implementations of electronic prescriptions typically create centralized data silos, leaving user data vulnerable to cybersecurity incidents and impeding interoperability. Research has also proposed decentralized solutions based on blockchain technology as an alternative, but privacy-related challenges have either been ignored or shifted to complex or yet non-standardized solutions so far. This paper presents a design and implementation of a system for the exchange of electronic prescriptions based on the combination of two blockchains and a digital wallet app. Our solution combines the bilateral, verifiable, and privacy-focused exchange of information between doctors, patients, and pharmacies based on a verifiable credential with a token-based, anonymized double-spending check. Our qualitative and quantitative evaluations suggest that this architecture can improve existing approaches to electronic prescription management by offering patients control over their data by design, a sufficient level of performance and scalability, and interoperability with emerging digital identity management solutions for users, businesses, and institutions.","244":"'Fragmentation in care' continuum is often considered as a shortcoming of Health system whereas, 'Integration of care' is widely acclaimed as a viable solution to fragmentation. In last two decades, Information and communication technologies (ICTs), by virtue of their ability to integrate information for action, has been extensively used in addressing many public health problems like malnutrition. Tackling the public health challenge of malnutrition demands attention to interconnectedness and interactions between multiple systems. In this paper, using a case study of an ICT application used by community workers for malnutrition management in Karnataka, we argue that lack of recognition of interconnectedness and interactions among stakeholders and context can pose a challenge to integration of care. ICTs can be key enablers to overcome fragmentation, provided it recognizes the inherent complexities of malnutrition and its management. We argue that for an effective ICT enabled integration of Severe Acute Malnutrition (SAM) management, a thorough understanding of perspectives of multiple stakeholders together with rich picture of the contextual dynamics should not be ignored at design and implementation phase.","245":"This is the paper for the first place winning solution of the Drone vs. Bird Challenge, organized by AVSS 2021. As the usage of drones increases with lowered costs and improved drone technology, drone detection emerges as a vital object detection task. However, detecting distant drones under unfavorable conditions, namely weak contrast, long-range, low visibility, requires effective algorithms. Our method approaches the drone detection problem by fine-tuning a YOLOv5 model with real and synthetically generated data using a Kalman-based object tracker to boost detection confidence. Our results indicate that augmenting the real data with an optimal subset of synthetic data can increase the performance. Moreover, temporal information gathered by object tracking methods can increase performance further.","246":"Our earlier publications showed semantic tableau admits partial exceptions to the Second Incompleteness Theorem where a formalism recognizes its self consistency and views multiplication as a 3-way relation (rather than as a total function). We now show these boundary-case evasions will collapse if the Law of the Excluded Middle is treated by tableau as a schema of logical axioms (instead of as derived theorems).","247":"Human resource management technologies have moved from biometric surveillance to emotional artificial intelligence (AI) that monitor employees' engagement and productivity, analyze video interviews and CVs of job applicants. The rise of the US$20 billion emotional AI industry will transform the future workplace. Yet, besides no international consensus on the principles or standards for such technologies, there is a lack of cross-cultural research on future job seekers' attitude toward such use of AI technologies. This study collects a cross-sectional dataset of 1,015 survey responses of international students from 48 countries and 8 regions worldwide. A majority of the respondents (52%) are concerned about being managed by AI. Following the hypothetico-deductivist philosophy of science, we use the MCMC Hamiltonian approach and conduct a detailed comparison of 10 Bayesian network models with the PSIS-LOO method. We consistently find having a higher income, being male, majoring in business, and\/or self-rated familiarity with AI correlate with a more positive view of emotional AI in the workplace. There is also a stark cross-cultural and cross-regional difference. Our analysis shows people from economically less developed regions (Africa, Oceania, Central Asia) tend to exhibit less concern for AI managers. And for East Asian countries, 64% of the Japanese, 56% of the South Korean, and 42% of the Chinese professed the trusting attitude. In contrast, an overwhelming majority of 75% of the European and Northern American possesses the worrying\/neutral attitude toward being managed by AI. Regarding religion, Muslim students correlate with the most concern toward emotional AI in the workplace. When religiosity is higher, the correlation becomes stronger for Muslim and Buddhist students.","248":"With the spreading of hate speech on social media in recent years, automatic detection of hate speech is becoming a crucial task and has attracted attention from various communities. This task aims to recognize online posts (e.g., tweets) that contain hateful information. The peculiarities of languages in social media, such as short and poorly written content, lead to the difficulty of learning semantics and capturing discriminative features of hate speech. Previous studies have utilized additional useful resources, such as sentiment hashtags, to improve the performance of hate speech detection. Hashtags are added as input features serving either as sentiment-lexicons or extra context information. However, our close investigation shows that directly leveraging these features without considering their context may introduce noise to classifiers. In this paper, we propose a novel approach to leverage sentiment hashtags to enhance hate speech detection in a natural language inference framework. We design a novel framework SRIC that simultaneously performs two tasks: (1) semantic relation inference between online posts and sentiment hashtags, and (2) sentiment classification on these posts. The semantic relation inference aims to encourage the model to encode sentiment-indicative information into representations of online posts. We conduct extensive experiments on two real-world datasets and demonstrate the effectiveness of our proposed framework compared with state-of-the-art representation learning models.","249":"The purpose of our paper is to analyze the dataset from Hass Avocado Board (HAB). The data features historical data on avocado prices and sales volume in multiple cities, states, and regions of the United States, ranging from 2015 to 2020. The paper consists of a mapped and calculated statistical analysis of the data over an established period. Using the cloud visualization tool SAP Analytics Cloud (SAC) to import and clean the data, we will complete comprehensive investigations and employ class lecture information as needed. Then, we will present insights using visualization, and time-series analysis. Our research intends to reveal insights into consumer buying statistics, such as the most popular type of avocado, preferences of organic to conventional and seasonality trends. The research is relevant as health-conscious trends have become increasingly popular, and avocado purchases indicate this.","250":"We study the problem usually referred to as group testing in the context of COVID-19. Given n samples collected from patients, how should we select and test mixtures of samples to maximize information and minimize the number of tests? Group testing is a well-studied problem with several appealing solutions, but recent biological studies impose practical constraints for COVID-19 that are incompatible with traditional methods. Furthermore, existing methods use unnecessarily restrictive solutions, which were devised for settings with more memory and compute constraints than the problem at hand. This results in poor utility. In the new setting, we obtain strong solutions for small values of n using evolutionary strategies. We then develop a new method combining Bloom filters with belief propagation to scale to larger values of n (more than 100) with good empirical results. We also present a more accurate decoding algorithm that is tailored for specific COVID-19 settings. This work demonstrates the practical gap between dedicated algorithms and well-known generic solutions. Our efforts results in a new and practical multiplex method yielding strong empirical performance without mixing more than a chosen number of patients into the same probe. Finally, we briefly discuss adaptive methods, casting them into the framework of adaptive sub-modularity.","251":"In this paper we develop statistical methods for causal inference in epidemics. Our focus is in estimating the effect of social mobility on deaths in the Covid-19 pandemic. We propose a marginal structural model motivated by a modified version of a basic epidemic model. We estimate the counterfactual time series of deaths under interventions on mobility. We conduct several types of sensitivity analyses. We find that the data support the idea that reduced mobility causes reduced deaths, but the conclusion comes with caveats. There is evidence of sensitivity to model misspecification and unmeasured confounding which implies that the size of the causal effect needs to be interpreted with caution. While there is little doubt the the effect is real, our work highlights the challenges in drawing causal inferences from pandemic data.","252":"Internet of Vehicles (IoV) is a cornerstone building block of smart cities to provide better traffic safety and mobile infotainment. Recently, improved efficiency in WLAN-based dense scenarios has become widespread through Wi-Fi 6, a license-free spectrum technology that can complement the cellular-based infrastructure for IoV. In addition, Named Data Networking (NDN) is a promising Internet architecture to accomplish content distribution in dynamic IoV scenarios. However, NDN deployments, i.e., native (clean-slate) and overlay (running on top of IP stack), require further investigation of their performance over wireless networks, particularly regarding the IoV scenario. This paper performs a comparative simulation-based study of these NDN deployments over Wi-Fi 6 for IoV using real vehicular traces. To the best of our knowledge, this is the first effort that extends ndnSIM 2 with an overlay-based NDN implementation and that compares it with the native approach. Results show that the overlay-based NDN consistently outperforms the native one, reaching around 99% of requests satisfied, against only 42.35% in the best case of native deployment.","253":"Many critical applications rely on cameras to capture video footage for analytical purposes. This has led to concerns about these cameras accidentally capturing more information than is necessary. In this paper, we propose a deep learning approach towards protecting privacy in camera-based systems. Instead of specifying specific objects (e.g. faces) are privacy sensitive, our technique distinguishes between salient (visually prominent) and non-salient objects based on the intuition that the latter is unlikely to be needed by the application.","254":"Today, tracking and controlling the spread of a virus is a crucial need for almost all countries. Doing this early would save millions of lives and help countries keep a stable economy. The easiest way to control the spread of a virus is to immediately inform the individuals who recently had close contact with the diagnosed patients. However, to achieve this, a centralized authority (e.g., a health authority) needs detailed location information from both healthy individuals and diagnosed patients. Thus, such an approach, although beneficial to control the spread of a virus, results in serious privacy concerns, and hence privacy-preserving solutions are required to solve this problem. Previous works on this topic either (i) compromise privacy (especially privacy of diagnosed patients) to have better efficiency or (ii) provide unscalable solutions. In this work, we propose a technique based on private set intersection between physical contact histories of individuals (that are recorded using smart phones) and a centralized database (run by a health authority) that keeps the identities of the positive diagnosed patients for the disease. Proposed solution protects the location privacy of both healthy individuals and diagnosed patients and it guarantees that the identities of the diagnosed patients remain hidden from other individuals. Notably, proposed scheme allows individuals to receive warning messages indicating their previous contacts with a positive diagnosed patient. Such warning messages will help them realize the risk and isolate themselves from other people. We make sure that the warning messages are only observed by the corresponding individuals and not by the health authority. We also implement the proposed scheme and show its efficiency and scalability via simulations.","255":"We present the first full description of Media Cloud, an open source platform based on crawling hyperlink structure in operation for over 10 years, that for many uses will be the best way to collect data for studying the media ecosystem on the open web. We document the key choices behind what data Media Cloud collects and stores, how it processes and organizes these data, and its open API access as well as user-facing tools. We also highlight the strengths and limitations of the Media Cloud collection strategy compared to relevant alternatives. We give an overview two sample datasets generated using Media Cloud and discuss how researchers can use the platform to create their own datasets.","256":"The article below is based on lectures delivered to new students remotely in the course of orientation. It presents the quantum theory tree from its inception a century ago till today. The main focus is on the nuclear physics - HEP branch.","257":"Even if laboratory practice is essential for all scientific branches of knowledge, it is often neglected at High School, due to lack of time and\/or resources. To establish a closer contact between school and experimental sciences, the University Sapienza of Roma and the Istituto Nazionale di Fisica Nucleare (INFN) launched the Lab2Go project, with the goal of spreading laboratory practice among students and teachers in high schools.","258":"Our ordinary life changed quite a bit in March of 2020 due to the global Covid-19 pandemic. While spring time in general well awaited and regarded as a synonym for rejuvenation the spring of 2020 brought lock-down, curfew, home office and digital education to the lives of many. The particle physics community was not an exception: research institutes and universities introduced home office and digital lecturing and all workshops, conferences and summer schools were canceled, got postponed or took place online. Using publicly available data from the INSPIRE and arXiv databases we investigate the effects of this dramatic change of life to the publishing trends of the high-energy physics community with an emphasis on particle phenomenology and theory. To get insights we gather information about publishing trends in the last 20 years, and analyse it in detail.","259":"The novel Coronavirus (COVID-19) is an infectious disease caused by a new virus called COVID-19 or 2019-nCoV that first identified in Wuhan, China. The disease causes respiratory illness (such as the flu) with other symptoms such as a cough, fever, and in more severe cases, difficulty breathing. This new Coronavirus seems to be very infectious and has spread quickly and globally. In this work, information about COVID-19 is provided and the situation in Arab countries and territories regarding the COVID-19 strike is presented. The next few weeks main expectations is also given.","260":"Supervised learning, while prevalent for information cascade modeling, often requires abundant labeled data in training, and the trained model is not easy to generalize across tasks and datasets. It often learns task-specific representations, which can easily result in overfitting for downstream tasks. Recently, self-supervised learning is designed to alleviate these two fundamental issues in linguistic and visual tasks. However, its direct applicability for information cascade modeling, especially graph cascade related tasks, remains underexplored. In this work, we present Contrastive Cascade Graph Learning (CCGL), a novel framework for information cascade graph learning in a contrastive, self-supervised, and task-agnostic way. In particular, CCGL first designs an effective data augmentation strategy to capture variation and uncertainty by simulating the information diffusion in graphs. Second, it learns a generic model for graph cascade tasks via self-supervised contrastive pre-training using both unlabeled and labeled data. Third, CCGL learns a task-specific cascade model via fine-tuning using labeled data. Finally, to make the model transferable across datasets and cascade applications, CCGL further enhances the model via distillation using a teacher-student architecture. We demonstrate that CCGL significantly outperforms its supervised and semi-supervised counterparts for several downstream tasks.","261":"To stimulate the intellectual curiosity of elementary school students, we conducted a workshop in distance education aimed at exploring the microscopic world inside a cell. In this workshop, elementary school students motivated to learn more on the subject of science analyzed movies of the Brownian motion of micrometer-sized particles suspended in water, using an open-source software, Tracker. These students then performed two-dimensional(2D)-random walk experiments using a dice game sheet to examine the physical mechanism of Brownian motion. After the workshop, we conducted a questionnaire-based survey. Many participants answered that the contents were difficult but interesting, suggesting that our workshop was very efficient to stimulate the curiosity of motivated students.","262":"Wireless Capsule Endoscopy (WCE) presented in 2001 as one of the key approaches to observe the entire gastrointestinal (GI) tract, generally the small bowels. It has been used to detect diseases in the gastrointestinal tract. Endoscopic image analysis is still a required field with many open problems. The quality of many images it produced is rather unacceptable due to the nature of this imaging system, which causes some issues to prognosticate by physicians and computer-aided diagnosis. In this paper, a novel technique is proposed to improve the quality of images captured by the WCE. More specifically, it enhanced the brightness, contrast, and preserve the color information while reducing its computational complexity. Furthermore, the experimental results of PSNR and SSIM confirm that the error rate in this method is near to the ground and negligible. Moreover, the proposed method improves intensity restricted average local entropy (IRMLE) by 22%, color enhancement factor (CEF) by 10%, and can keep the lightness of image effectively. The performances of our method have better visual quality and objective assessments in compare to the state-of-art methods.","263":"Testing individuals for the presence of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the pathogen causing the coronavirus disease 2019 (COVID-19), is crucial for curtailing transmission chains. Moreover, rapidly testing many potentially infected individuals is often a limiting factor in controlling COVID-19 outbreaks. Hence, pooling strategies, wherein individuals are grouped and tested simultaneously, are employed. We present a novel pooling strategy that implements D-Optimal Pooling Experimental design (DOPE). DOPE defines optimal pooled tests as those maximizing the mutual information between data and infection states. We estimate said mutual information via Monte-Carlo sampling and employ a discrete optimization heuristic for maximizing it. DOPE outperforms common pooling strategies both in terms of lower error rates and fewer tests utilized. DOPE holds several additional advantages: it provides posterior distributions of the probability of infection, rather than only binary classification outcomes; it naturally incorporates prior information of infection probabilities and test error rates; and finally, it can be easily extended to include other, newly discovered information regarding COVID-19. Hence, we believe that implementation of Bayesian D-optimal experimental design holds a great promise for the efforts of combating COVID-19 and other future pandemics.","264":"We review some key issues pertaining to NASA's Research and Analysis programs, and offer recommended actions to mitigate or resolve these issues. In particular, we recommended that NASA increases funding to support a healthy selection rate (~40%) for R&A programs, which underpin much scientific discovery with NASA mission data, and on which the majority of the U.S. planetary science community relies (either in part or wholly). We also recommend additional actions NASA can take to ensure a more equitable and sustainable planetary science research community in the U.S., including supporting the next generations of planetary researchers, working to minimize biases in peer review, and reducing the burden of scientists as they prepare R&A proposals.","265":"The COVID-19 pandemic has caused more than 8 million confirmed cases and 500,000 death to date. In response to this emergency, many countries have introduced a series of social-distancing measures including lockdowns and businesses' temporary shutdowns, in an attempt to curb the spread of the infection. Accordingly, the pandemic has been generating unprecedent disruption on practically every aspect of society. This paper demonstrates that high-frequency electricity market data can be used to estimate the causal, short-run impact of COVID-19 on the economy. In the current uncertain economic conditions, timeliness is essential. Unlike official statistics, which are published with a delay of a few months, with our approach one can monitor virtually every day the impact of the containment policies, the extent of the recession and measure whether the monetary and fiscal stimuli introduced to address the crisis are being effective. We illustrate our methodology on daily data for the Italian day-ahead power market. Not surprisingly, we find that the containment measures caused a significant reduction in economic activities and that the GDP at the end of in May 2020 is still about 11% lower that what it would have been without the outbreak.","266":"We develop a mathematical framework to study the economic impact of infectious diseases by integrating epidemiological dynamics with a kinetic model of wealth exchange. The multi-agent description leads to study the evolution over time of a system of kinetic equations for the wealth densities of susceptible, infectious and recovered individuals, whose proportions are driven by a classical compartmental model in epidemiology. Explicit calculations show that the spread of the disease seriously affects the distribution of wealth, which, unlike the situation in the absence of epidemics, can converge towards a stationary state with a bimodal form. Furthermore, simulations confirm the ability of the model to describe different phenomena characteristics of economic trends in situations compromised by the rapid spread of an epidemic, such as the unequal impact on the various wealth classes and the risk of a shrinking middle class.","267":"In this brief report we propose a new design of a medical mask with a plasma layer, which provides both additional air filtration from microdrops, bacteria and viruses due to the electrostatic effect and self-disinfecting of surfaces by a pulsed barrier discharge. The key features of the mask are the mutual arrangement of the layers, the direction of air flows and the synchronization of the discharge with respiration, which ensures the safe wearing of the mask and high degree of protection against pathogenic microorganisms.","268":"Social distancing has been proven as an effective measure against the spread of the infectious COronaVIrus Disease 2019 (COVID-19). However, individuals are not used to tracking the required 6-feet (2-meters) distance between themselves and their surroundings. An active surveillance system capable of detecting distances between individuals and warning them can slow down the spread of the deadly disease. Furthermore, measuring social density in a region of interest (ROI) and modulating inflow can decrease social distancing violation occurrence chance. On the other hand, recording data and labeling individuals who do not follow the measures will breach individuals' rights in free-societies. Here we propose an Artificial Intelligence (AI) based real-time social distancing detection and warning system considering four important ethical factors: (1) the system should never record\/cache data, (2) the warnings should not target the individuals, (3) no human supervisor should be in the detection\/warning loop, and (4) the code should be open-source and accessible to the public. Against this backdrop, we propose using a monocular camera and deep learning-based real-time object detectors to measure social distancing. If a violation is detected, a non-intrusive audio-visual warning signal is emitted without targeting the individual who breached the social distancing measure. Also, if the social density is over a critical value, the system sends a control signal to modulate inflow into the ROI. We tested the proposed method across real-world datasets to measure its generality and performance. The proposed method is ready for deployment, and our code is open-sourced.","269":"Piecewise stationarity is a widely adopted assumption for modelling non-stationary time series. However fitting piecewise stationary vector autoregressive (VAR) models to high-dimensional data is challenging as the number of parameters increases as a quadratic of the dimension. Recent approaches to address this have imposed sparsity assumptions on the parameters of the VAR model, but such assumptions have been shown to be inadequate when datasets exhibit strong (auto)correlations. We propose a piecewise stationary time series model that accounts for pervasive serial and cross-sectional correlations through a factor structure, and only assumes that any remaining idiosyncratic dependence between variables can be modelled by a sparse VAR model. We propose an accompanying two-stage change point detection methodology which fully addresses the challenges arising from not observing either the factors or the idiosyncratic VAR process directly. Its consistency in estimating both the total number and the locations of the change points in the latent components, is established under conditions considerably more general than those in the existing literature. We demonstrate the competitive performance of the proposed methodology on simulated datasets and an application to US blue chip stocks data.","270":"Non-invasive in vivo sensing of the lung with light would help diagnose and monitor pulmonary disorders (caused by e.g. COVID-19, emphysema, immature lung tissue in infants). We investigated the possibility to probe the lung with time domain diffuse optics, taking advantage of the increased depth (few cm) reached by photons detected after a long (few ns) propagation time. An initial study on 5 healthy volunteers included time-resolved broadband diffuse optical spectroscopy measurements at 3 cm source-detector distance over the 600-1100 nm range, and long-distance (6-9 cm) measurements at 820 nm performed during a breathing protocol. The interpretation of the in vivo data with a simplified homogeneous model yielded a maximum probing depth of 2.6-3.9 cm, suitable to reach the lung. Also, signal changes related to the inspiration act were observed, especially at high photon propagation times. Yet, intra- and inter-subject variability and inconsistencies, possibly alluring to competing scattering and absorption effects, prevented a simple interpretation. Aspects to be further investigated to gain a deeper insight are discussed.","271":"There has been an intense concern for security alternatives because of the recent rise of cyber attacks, mainly targeting critical systems such as industry, medical, or energy ecosystem. Though the latest industry infrastructures largely depend on AI-driven maintenance, the prediction based on corrupted data undoubtedly results in loss of life and capital. Admittedly, an inadequate data-protection mechanism can readily challenge the security and reliability of the network. The shortcomings of the conventional cloud or trusted certificate-driven techniques have motivated us to exhibit a unique Blockchain-based framework for a secure and efficient industry 4.0 system. The demonstrated framework obviates the long-established certificate authority after enhancing the consortium Blockchain that reduces the data processing delay, and increases cost-effective throughput. Nonetheless, the distributed industry 4.0 security model entails cooperative trust than depending on a single party, which in essence indulges the costs and threat of the single point of failure. Therefore, multi-signature technique of the proposed framework accomplishes the multi-party authentication, which confirms its applicability for the real-time and collaborative cyber-physical system.","272":"The rise of the Internet of Everything lifestyle in the last decade has had a significant impact on the increased emergence and adoption of online learning in almost all countries across the world. E-learning 3.0 is expected to become the norm of learning globally in almost all sectors in the next few years. The pervasiveness of the Semantic Web powered by the Internet of Everything lifestyle is expected to play a huge role towards seamless and faster adoption of the emerging paradigms of E-learning 3.0. Therefore, this paper presents an exploratory study to analyze multimodal components of Semantic Web behavior data to investigate the emergence of online learning in different countries across the world. The work specifically involved investigating relevant web behavior data to interpret the 5 W's and 1 H - Who, What, When Where, Why, and How related to online learning. Based on studying the E-learning Index of 2021, the study was performed for all the countries that are member states of the Organization for Economic Cooperation and Development. The results presented and discussed help to interpret the emergence of online learning in each of these countries in terms of the associated public perceptions, queries, opinions, behaviors, and perspectives. Furthermore, to support research and development in this field, we have published the web behavior-based Big Data related to online learning that was mined for all these 38 countries, in the form of a dataset, which is avail-able at https:\/\/dx.doi.org\/10.21227\/xbvs-0198.","273":"Incidence - Cumulative Cases (ICC) curves are introduced and shown to provide a simple framework for parameter identification in the case of the most elementary epidemiological model, consisting of susceptible, infected, and removed compartments. This novel methodology is used to estimate the basic reproductive number of recent outbreaks, including the ongoing COVID-19 epidemic.","274":"Conditional particle filters (CPFs) are powerful smoothing algorithms for general nonlinear\/non-Gaussian hidden Markov models. However, CPFs can be inefficient or difficult to apply with diffuse initial distributions, which are common in statistical applications. We propose a simple but generally applicable auxiliary variable method, which can be used together with the CPF in order to perform efficient inference with diffuse initial distributions. The method only requires simulatable Markov transitions that are reversible with respect to the initial distribution, which can be improper. We focus in particular on random-walk type transitions which are reversible with respect to a uniform initial distribution (on some domain), and autoregressive kernels for Gaussian initial distributions. We propose to use on-line adaptations within the methods. In the case of random-walk transition, our adaptations use the estimated covariance and acceptance rate adaptation, and we detail their theoretical validity. We tested our methods with a linear-Gaussian random-walk model, a stochastic volatility model, and a stochastic epidemic compartment model with time-varying transmission rate. The experimental findings demonstrate that our method works reliably with little user specification, and can be substantially better mixing than a direct particle Gibbs algorithm that treats initial states as parameters.","275":"Agent-Based Models (ABM) are computational scenario-generators, which can be used to predict the possible future outcomes of the complex system they represent. To better understand the robustness of these predictions, it is necessary to understand the full scope of the possible phenomena the model can generate. Most often, due to high-dimensional parameter spaces, this is a computationally expensive task. Inspired by ideas coming from systems biology, we show that for multiple macroeconomic models, including an agent-based model and several Dynamic Stochastic General Equilibrium (DSGE) models, there are only a few stiff parameter combinations that have strong effects, while the other sloppy directions are irrelevant. This suggest an algorithm that efficiently explores the space of parameters by primarily moving along the stiff directions. We apply our algorithm to a medium-sized agent-based model, and show that it recovers all possible dynamics of the unemployment rate. The application of this method to Agent-based Models may lead to a more thorough and robust understanding of their features, and provide enhanced parameter sensitivity analyses. Several promising paths for future research are discussed.","276":"We study the problem of estimating the parameters (i.e., infection rate and recovery rate) governing the spread of epidemics in networks. Such parameters are typically estimated by measuring various characteristics (such as the number of infected and recovered individuals) of the infected populations over time. However, these measurements also incur certain costs, depending on the population being tested and the times at which the tests are administered. We thus formulate the epidemic parameter estimation problem as an optimization problem, where the goal is to either minimize the total cost spent on collecting measurements, or to optimize the parameter estimates while remaining within a measurement budget. We show that these problems are NP-hard to solve in general, and then propose approximation algorithms with performance guarantees. We validate our algorithms using numerical examples.","277":"This paper explains the scalable methods used for extracting and analyzing the Covid-19 vaccine data. Using Big Data such as Hadoop and Hive, we collect and analyze the massive data set of the confirmed, the fatality, and the vaccination data set of Covid-19. The data size is about 3.2 Giga-Byte. We show that it is possible to store and process massive data with Big Data. The paper proceeds tempo-spatial analysis, and visual maps, charts, and pie charts visualize the result of the investigation. We illustrate that the more vaccinated, the fewer the confirmed cases.","278":"Personal Knowledge Graphs (PKGs) are introduced by the semantic web community as small-sized user-centric knowledge graphs (KGs). PKGs fill the gap of personalised representation of user data and interests on the top of big, well-established encyclopedic KGs, such as DBpedia. Inspired by the widely recent usage of PKGs in the medical domain to represent patient data, this PhD proposal aims to adopt a similar technique in the educational domain in e-learning platforms by deploying PKGs to represent users and learners. We propose a novel PKG development that relies on ontology and interlinks to Linked Open Data. Hence, adding the dimension of personalisation and explainability in users' featured data while respecting privacy. This research design is developed in two use cases: a collaborative search learning platform and an e-learning platform. Our preliminary results show that e-learning platforms can get benefited from our approach by providing personalised recommendations and more user and group-specific data.","279":"Mobile communications have been undergoing a generational change every ten years or so. However, the time difference between the so-called\"G's\"is also decreasing. While fifth-generation (5G) systems are becoming a commercial reality, there is already significant interest in systems beyond 5G - which we refer to as the sixth-generation (6G) of wireless systems. In contrast to the many published papers on the topic, we take a top-down approach to 6G. We present a holistic discussion of 6G systems beginning with the lifestyle and societal changes driving the need for next generation networks, to the technical requirements needed to enable 6G applications, through to the challenges, as well as possibilities for practically realizable system solutions across all layers of the Open Systems Interconnection stack. Since many of the 6G applications will need access to an order-of-magnitude more spectrum, utilization of frequencies between 100 GHz and 1 THz becomes of paramount importance. We comprehensively characterize the limitations that must be overcome to realize working systems in these bands; and provide a unique perspective on the physical, as well as higher layer challenges relating to the design of next generation core networks, new modulation and coding methods, novel multiple access techniques, antenna arrays, wave propagation, radio-frequency transceiver design, as well as real-time signal processing. We rigorously discuss the fundamental changes required in the core networks of the future, such as the redesign or significant reduction of the transport architecture that serves as a major source of latency. While evaluating the strengths and weaknesses of key technologies, we differentiate what may be practically achievable over the next decade, relative to what is possible in theory. For each discussed system aspect, we present concrete research challenges.","280":"Sweden has adopted far less restrictive social distancing policies than most countries following the COVID-19 pandemic. This paper uses data on all mobile phone users, from one major Swedish mobile phone network, to examine the impact of the Coronavirus outbreak under the Swedish mild recommendations and restrictions regime on individual mobility and if changes in geographical mobility vary over different socio-economic strata. Having access to data for January-March in both 2019 and 2020 enables the estimation of causal effects of the COVID-19 outbreak by adopting a Difference-in-Differences research design. The paper reaches four main conclusions: (i) The daytime population in residential areas increased significantly (64 percent average increase); (ii) The daytime presence in industrial and commercial areas decreased significantly (33 percent average decrease); (iii) The distance individuals move from their homes during a day was substantially reduced (38 percent decrease in the maximum distance moved and 36 percent increase in share of individuals who move less than one kilometer from home); (iv) Similar reductions in mobility were found for residents in areas with different socioeconomic and demographic characteristics. These results show that mild government policies can compel people to adopt social distancing behavior.","281":"Due to the surge of spatio-temporal data volume, the popularity of location-based services and applications, and the importance of extracted knowledge from spatio-temporal data to solve a wide range of real-world problems, a plethora of research and development work has been done in the area of spatial and spatio-temporal data analytics in the past decade. The main goal of existing works was to develop algorithms and technologies to capture, store, manage, analyze, and visualize spatial or spatio-temporal data. The researchers have contributed either by adding spatio-temporal support with existing systems, by developing a new system from scratch for processing spatio-temporal data, or by implementing algorithms for mining spatio-temporal data. The existing ecosystem of spatial and spatio-temporal data analytics can be categorized into three groups, (1) spatial databases (SQL and NoSQL), (2) big spatio-temporal data processing infrastructures, and (3) programming languages and software tools for processing spatio-temporal data. Since existing surveys mostly investigated big data infrastructures for processing spatial data, this survey has explored the whole ecosystem of spatial and spatio-temporal analytics along with an up-to-date review of big spatial data processing systems. This survey also portrays the importance and future of spatial and spatio-temporal data analytics.","282":"The present paper shows a proposal of the characteristics Cloud Risk Assessment Models should have and presents the review of the literature considering those characteristics in order to identify current gaps. This work shows a ranking of Cloud RA models and their degree of compliance with the theoretical reference Cloud Risk Assessment model. The review of literature shows that RA approaches leveraging CSA (Cloud Security Alliance) STAR Registry that have into account organizations security requirements present higher degree of compliance, but they still lack risk economic quantification. The myriad of conceptual models, methodologies and frameworks although based on current NIST SP 800:30, ISO 27001, ISO 27005, ISO 30001, ENISA standards could be enhanced by the use of techno-economic models like UTEM, created by the author, in order to conceive more simplified models for effective Risk Assessment and Mitigation closer to the theoretical reference model for Cloud Risk Assessment, available for all cloud models (IaaS, PaaS, SaaS) and easy to use for all stakeholders.","283":"The world has seen in 2020 an unprecedented global outbreak of SARS-CoV-2, a new strain of coronavirus, causing the COVID-19 pandemic, and radically changing our lives and work conditions. Many scientists are working tirelessly to find a treatment and a possible vaccine. Furthermore, governments, scientific institutions and companies are acting quickly to make resources available, including funds and the opening of large-volume data repositories, to accelerate innovation and discovery aimed at solving this pandemic. In this paper, we develop a novel automated theme-based visualisation method, combining advanced data modelling of large corpora, information mapping and trend analysis, to provide a top-down and bottom-up browsing and search interface for quick discovery of topics and research resources. We apply this method on two recently released publications datasets (Dimensions' COVID-19 dataset and the Allen Institute for AI's CORD-19). The results reveal intriguing information including increased efforts in topics such as social distancing; cross-domain initiatives (e.g. mental health and education); evolving research in medical topics; and the unfolding trajectory of the virus in different territories through publications. The results also demonstrate the need to quickly and automatically enable search and browsing of large corpora. We believe our methodology will improve future large volume visualisation and discovery systems but also hope our visualisation interfaces will currently aid scientists, researchers, and the general public to tackle the numerous issues in the fight against the COVID-19 pandemic.","284":"Predictions of short-term directional movement of the futures contract can be challenging as its pricing is often based on multiple complex dynamic conditions. This work presents a method for predicting the short-term directional movement of an underlying futures contract. We engineered a set of features from technical analysis, order flow, and order-book data. Then, Tabnet, a deep learning neural network, is trained using these features. We train our model on the Silver Futures Contract listed on Shanghai Futures Exchange and achieve an accuracy of 0.601 on predicting the directional change during the selected period.","285":"Recently, the performance of Pre-trained Language Models (PLMs) has been significantly improved by injecting knowledge facts to enhance their abilities of language understanding. For medical domains, the background knowledge sources are especially useful, due to the massive medical terms and their complicated relations are difficult to understand in text. In this work, we introduce SMedBERT, a medical PLM trained on large-scale medical corpora, incorporating deep structured semantic knowledge from neighbors of linked-entity.In SMedBERT, the mention-neighbor hybrid attention is proposed to learn heterogeneous-entity information, which infuses the semantic representations of entity types into the homogeneous neighboring entity structure. Apart from knowledge integration as external features, we propose to employ the neighbors of linked-entities in the knowledge graph as additional global contexts of text mentions, allowing them to communicate via shared neighbors, thus enrich their semantic representations. Experiments demonstrate that SMedBERT significantly outperforms strong baselines in various knowledge-intensive Chinese medical tasks. It also improves the performance of other tasks such as question answering, question matching and natural language inference.","286":"The ongoing outbreak of coronavirus disease (COVID-19) had burst out in Wuhan China, specifically in December 2019. COVID-19 has caused by a new virus that had not been identified in human previously. This was followed by a widespread and rapid spread of this epidemic throughout the world. Daily, the number of the confirmed cases are increasing rapidly, number of the suspect increases, based on the symptoms that accompany this disease, and unfortunately number of the deaths also increase. Therefore, with these increases in number of cases around the world, it becomes hard to manage all these cases information with different situations; if the patient either injured or suspect with which symptoms that appeared on the patient. Therefore, there is a critical need to construct a multi-dimensional system to store and analyze the generated large-scale data. In this paper, a Comprehensive Storing System for COVID-19 data using Apache Spark (CSS-COVID) is proposed, to handle and manage the problem caused by increasing the number of COVID-19 daily. CSS-COVID helps in decreasing the processing time for querying and storing COVID-19 daily data. CSS-COVID consists of three stages, namely, inserting and indexing, storing, and querying stage. In the inserting stage, data is divided into subsets and then index each subset separately. The storing stage uses set of storing-nodes to store data, while querying stage is responsible for handling the querying processes. Using Apache Spark in CSS-COVID leverages the performance of dealing with large-scale data of the coronavirus disease injured whom increase daily. A set of experiments are applied, using real COVID-19 Datasets, to prove the efficiency of CSS-COVID in indexing large-scale data.","287":"We present modeling of the COVID-19 epidemic in Illinois, USA, capturing the implementation of a Stay-at-Home order and scenarios for its eventual release. We use a non-Markovian age-of-infection model that is capable of handling long and variable time delays without changing its model topology. Bayesian estimation of model parameters is carried out using Markov Chain Monte Carlo (MCMC) methods. This framework allows us to treat all available input information, including both the previously published parameters of the epidemic and available local data, in a uniform manner. To accurately model deaths as well as demand on the healthcare system, we calibrate our predictions to total and in-hospital deaths as well as hospital and ICU bed occupancy by COVID-19 patients. We apply this model not only to the state as a whole but also its sub-regions in order to account for the wide disparities in population size and density. Without prior information on non-pharmaceutical interventions (NPIs), the model independently reproduces a mitigation trend closely matching mobility data reported by Google and Unacast. Forward predictions of the model provide robust estimates of the peak position and severity and also enable forecasting the regional-dependent results of releasing Stay-at-Home orders. The resulting highly constrained narrative of the epidemic is able to provide estimates of its unseen progression and inform scenarios for sustainable monitoring and control of the epidemic.","288":"We examine whether a company's corporate reputation gained from their CSR activities and a company leader's reputation, one that is unrelated to his or her business acumen, can impact economic action fairness appraisals. We provide experimental evidence that good corporate reputation causally buffers individuals' negative fairness judgment following the firm's decision to profiteer from an increase in the demand. Bad corporate reputation does not make the decision to profiteer as any less acceptable. However, there is evidence that individuals judge as more unfair an ill-reputed firm's decision to raise their product's price to protect against losses. Thus, our results highlight the importance of a good reputation in protecting a firm against severe negative judgments from making an economic decision that the public deems unfair.","289":"The recent Covid-19 epidemic has lead to comparisons of the countries suffering from it. These are based on the number of excess deaths attributed either directly or indirectly to the epidemic. Unfortunately the data on which such comparisons rely are often incomplete and unreliable. This article discusses problems of interpretation of data even when the data is largely accurate and delayed by at most two to three weeks. This applies to the Office of National Statistics in the UK, the Statistisches Bundesamt in Germany and the Belgian statistical office Statbel. The data in the article is taken from these three sources. The number of excess deaths is defined as the number of deaths minus the baseline, the definition of which varies from country to country. In the UK it is the average number of deaths over the last five years, in Germany it is over the last four years and in Belgium over the last 11 years. This means that in all cases the individual baselines depend strongly on the timing and intensity of adverse factors such as past influenza epidemics and heat waves. This makes cross-country comparisons difficult. A baseline defined as the number the number of deaths in the absence of adverse factors can be operationalized by taking say the 10\\% quantile of the number of deaths. This varies little over time and European countries within given age groups. It therefore enables more robust and accurate comparisons of different countries. The article criticizes the use of Z-scores which distort the comparison between countries. Finally the problem of describing past epidemics by their timing, that is start and finish and time of the maximum, and by their effect, the height of the maximum and the total number of deaths, is considered.","290":"The HGR is a quite challenging task as its performance is influenced by various aspects such as illumination variations, cluttered backgrounds, spontaneous capture, etc. The conventional CNN networks for HGR are following two stage pipeline to deal with the various challenges: complex signs, illumination variations, complex and cluttered backgrounds. The existing approaches needs expert expertise as well as auxiliary computation at stage 1 to remove the complexities from the input images. Therefore, in this paper, we proposes an novel end-to-end compact CNN framework: fine grained feature attentive network for hand gesture recognition (Fit-Hand) to solve the challenges as discussed above. The pipeline of the proposed architecture consists of two main units: FineFeat module and dilated convolutional (Conv) layer. The FineFeat module extracts fine grained feature maps by employing attention mechanism over multiscale receptive fields. The attention mechanism is introduced to capture effective features by enlarging the average behaviour of multi-scale responses. Moreover, dilated convolution provides global features of hand gestures through a larger receptive field. In addition, integrated layer is also utilized to combine the features of FineFeat module and dilated layer which enhances the discriminability of the network by capturing complementary context information of hand postures. The effectiveness of Fit- Hand is evaluated by using subject dependent (SD) and subject independent (SI) validation setup over seven benchmark datasets: MUGD-I, MUGD-II, MUGD-III, MUGD-IV, MUGD-V, Finger Spelling and OUHANDS, respectively. Furthermore, to investigate the deep insights of the proposed Fit-Hand framework, we performed ten ablation study.","291":"Regression testing is an essential activity to assure that software code changes do not adversely affect existing functionalities. With the wide adoption of Continuous Integration (CI) in software projects, which increases the frequency of running software builds, running all tests can be time-consuming and resource-intensive. To alleviate that problem, Test case Selection and Prioritization (TSP) techniques have been proposed to improve regression testing by selecting and prioritizing test cases in order to provide early feedback to developers. In recent years, researchers have relied on Machine Learning (ML) techniques to achieve effective TSP (ML-based TSP). Such techniques help combine information about test cases, from partial and imperfect sources, into accurate prediction models. This work conducts a systematic literature review focused on ML-based TSP techniques, aiming to perform an in-depth analysis of the state of the art, thus gaining insights regarding future avenues of research. To that end, we analyze 29 primary studies published from 2006 to 2020, which have been identified through a systematic and documented process. This paper addresses five research questions addressing variations in ML-based TSP techniques and feature sets for training and testing ML models, alternative metrics used for evaluating the techniques, the performance of techniques, and the reproducibility of the published studies. We summarize the results related to our research questions in a high-level summary that can be used as a taxonomy for classifying future TSP studies.","292":"U-net is an image segmentation technique developed primarily for medical image analysis that can precisely segment images using a scarce amount of training data. These traits provide U-net with a very high utility within the medical imaging community and have resulted in extensive adoption of U-net as the primary tool for segmentation tasks in medical imaging. The success of U-net is evident in its widespread use in all major image modalities from CT scans and MRI to X-rays and microscopy. Furthermore, while U-net is largely a segmentation tool, there have been instances of the use of U-net in other applications. As the potential of U-net is still increasing, in this review we look at the various developments that have been made in the U-net architecture and provide observations on recent trends. We examine the various innovations that have been made in deep learning and discuss how these tools facilitate U-net. Furthermore, we look at image modalities and application areas where U-net has been applied.","293":"As the coronavirus disease 2019 (COVID-19) becomes a global pandemic, policy makers must enact interventions to stop its spread. Data driven approaches might supply information to support the implementation of mitigation and suppression strategies. To facilitate research in this direction, we present a machine-readable dataset that aggregates relevant data from governmental, journalistic, and academic sources on the county level. In addition to county-level time-series data from the JHU CSSE COVID-19 Dashboard, our dataset contains more than 300 variables that summarize population estimates, demographics, ethnicity, housing, education, employment and in come, climate, transit scores, and healthcare system-related metrics. Furthermore, we present aggregated out-of-home activity information for various points of interest for each county, including grocery stores and hospitals, summarizing data from SafeGraph. By collecting these data, as well as providing tools to read them, we hope to aid researchers investigating how the disease spreads and which communities are best able to accommodate stay-at-home mitigation efforts. Our dataset and associated code are available at https:\/\/github.com\/JieYingWu\/COVID-19_US_County-level_Summaries.","294":"By successfully solving the problem of forecasting, the processes in the work of various companies are optimized and savings are achieved. In this process, the analysis of time series data is of particular importance. Since the creation of Facebook's Prophet, and Amazon's DeepAR+ and CNN-QR forecasting models, algorithms have attracted a great deal of attention. The paper presents the application and comparison of the above algorithms for sales forecasting in distribution companies. A detailed comparison of the performance of algorithms over real data with different lengths of sales history was made. The results show that Prophet gives better results for items with a longer history and frequent sales, while Amazon's algorithms show superiority for items without a long history and items that are rarely sold.","295":"In recent years, a vivid interest in hybrid development methods has been observed as practitioners combine various approaches to software creation to improve productivity, product quality, and adaptability of the process to react to change. Scientific papers on the subject proliferate, however evaluation of the effectiveness of hybrid methods in academic contexts has yet to follow. The work presented investigates if introducing a hybrid approach for student projects brings added value as compared to iterative and sequential development. A controlled experiment was carried out among Bachelor students of a French engineering school to assess the impacts of a given development method on the success of student computing undertakings. Its three dimensions were examined via a set of metrics: product quality, team productivity as well as human factors (teamwork quality&learning outcomes). Several patterns were observed, which can provide a starting point for educators and researchers wishing to tailor or design a software development process for academic needs.","296":"We adapt a SEIRD differential model with asymptomatic population and Covid deaths, which we call SEAIRD, to simulate the evolution of COVID-19, and add a control function affecting both the diffusion of the virus and GDP, featuring all direct and indirect containment policies; to model feasibility, the control is assumed to be a piece-wise linear function satisfying additional constraints. We describe the joint dynamics of infection and the economy and discuss the trade-off between production and fatalities. In particular, we carefully study the conditions for the existence of the optimal policy response and its uniqueness. Uniqueness crucially depends on the marginal rate of substitution between the statistical value of a human life and GDP; we show an example with a phase transition: above a certain threshold, there is a unique optimal containment policy; below the threshold, it is optimal to abstain from any containment; and at the threshold itself there are two optimal policies. We then explore and evaluate various profiles of various control policies dependent on a small number of parameters.","297":"Due to the COVID-19 pandemic, conducting Human-Robot Interaction (HRI) studies in person is not permissible due to social distancing practices to limit the spread of the virus. Therefore, a virtual reality (VR) simulation with a virtual robot may offer an alternative to real-life HRI studies. Like a real intelligent robot, a virtual robot can utilize the same advanced algorithms to behave autonomously. This paper introduces HAVEN (HRI-based Augmentation in a Virtual robot Environment using uNity), a VR simulation that enables users to interact with a virtual robot. The goal of this system design is to enable researchers to conduct HRI Augmented Reality studies using a virtual robot without being in a real environment. This framework also introduces two common HRI experiment designs: a hallway passing scenario and human-robot team object retrieval scenario. Both reflect HAVEN's potential as a tool for future AR-based HRI studies.","298":"Mechanically ventilated patients typically exhibit abnormal respiratory sounds. Squawks are short inspiratory adventitious sounds that may occur in patients with pneumonia, such as COVID-19 patients. In this work we devised a method for squawk detection in mechanically ventilated patients by developing algorithms for respiratory cycle estimation, squawk candidate identification, feature extraction, and clustering. The best classifier reached an F1 of 0.48 at the sound file level and an F1 of 0.66 at the recording session level. These preliminary results are promising, as they were obtained in noisy environments. This method will give health professionals a new feature to assess the potential deterioration of critically ill patients.","299":"We use the aggregate information from individual-to-firm and firm-to-firm in Garanti BBVA Bank transactions to mimic domestic private demand. Particularly, we replicate the quarterly national accounts aggregate consumption and investment (gross fixed capital formation) and its bigger components (Machinery and Equipment and Construction) in real time for the case of Turkey. In order to validate the usefulness of the information derived from these indicators we test the nowcasting ability of both indicators to nowcast the Turkish GDP using different nowcasting models. The results are successful and confirm the usefulness of Consumption and Investment Banking transactions for nowcasting purposes. The value of the Big data information is more relevant at the beginning of the nowcasting process, when the traditional hard data information is scarce. This makes this information specially relevant for those countries where statistical release lags are longer like the Emerging Markets.","300":"With the proliferation of new technologies such as Internet of Things (IOT) and Software-Defined Networking(SDN) in the recent years, the distributed denial of service (DDoS)attack vector has broadened and opened new opportunities for more sophisticated DDoS attacks on the targeted victims. The new attack vector includes unsecured and vulnerable IoT devices connected to the internet, denial of service vulnerabilities like southbound channel saturation in the SDN architecture. Given the high-volume and pervasive nature of these attacks, it is beneficial for stakeholders to collaborate in detecting and mitigating the denial of service attacks in a timely manner. The blockchain technology is considered to improve the security aspects owing to the decentralized design, secured distributed storage and privacy. A thorough exploration and classification of blockchain techniques used for DDoS attack mitigation is not explored in the prior art. This paper reviews and categorizes the existed state-of-the-art DDoS mitigation solutions based on blockchain technology. The DDoS mitigation techniques are classified based on the solution deployment location i.e. network based, near attacker location, near victim location and hybrid solutions in the network architecture with emphasis on the IoT and SDN architectures. Additionally, based on our study, the research challenges and future directions to implement the blockchain based DDoS mitigation solutions are discussed. We believe that this paper could serve as a starting point and reference resource for future researchers working on denial of service attacks detection and mitigation using blockchain technology.","301":"The efficacy of an intervention can be assessed by randomizing patients to different diagnostic tests instead of directly to an intervention and control. This principle is applied by allocating individuals to intervention if the test result is positive (or on one side of a threshold) but allocating individuals to a control if the result is negative (or on the other side of the threshold). This can also be done with different dichotomizing thresholds for one test. The frequencies of the outcome in those with each of the four resulting observations are then used to calculate the risk ratio (RR) for the marginal probabilities by solving simultaneous equations. This assumes that the RR due to intervention compared to control is the same in both test groups created by randomization. The calculations are illustrated by using data from a randomized controlled trial (RCT) that assessed the efficacy of an angiotensin receptor blocker (ARB) in lowering the risk of diabetic nephropathy in patients conditional on urinary albumin excretion rates (AERs). The calculations are also illustrated with simulated data for assessing the effectiveness of test, trace and isolation to reduce transmission of the SARS-Cov-2 virus by randomizing to RT-PCR or LFD tests. This approach allows the probabilities of outcomes, their RRs and odds ratios (OR) conditional on the results of covariates to be determined, also suggesting a way forward for natural as opposed to active randomization.","302":"During the COVID-19 pandemic, conflicting opinions on physical distancing swept across social media, affecting both human behavior and the spread of COVID-19. Inspired by such phenomena, we construct a two-layer multiplex network for the coupled spread of a disease and conflicting opinions. We model each process as a contagion. On one layer, we consider the concurrent evolution of two opinions -- pro-physical-distancing and anti-physical-distancing -- that compete with each other and have mutual immunity to each other. The disease evolves on the other layer, and individuals are less likely (respectively, more likely) to become infected when they adopt the pro-physical-distancing (respectively, anti-physical-distancing) opinion. We develop approximations of mean-field type by generalizing monolayer pair approximations to multilayer networks; these approximations agree well with Monte Carlo simulations for a broad range of parameters and several network structures. Through numerical simulations, we illustrate the influence of opinion dynamics on the spread of the disease from complex interactions both between the two conflicting opinions and between the opinions and the disease. We find that lengthening the duration that individuals hold an opinion may help suppress disease transmission, and we demonstrate that increasing the cross-layer correlations or intra-layer correlations of node degrees may lead to fewer individuals becoming infected with the disease.","303":"Solar Orbiter strives to unveil how the Sun controls and shapes the heliosphere and fills it with energetic particle radiation. To this end, its Energetic Particle Detector (EPD) has now been in operation, providing excellent data, for just over a year. EPD measures suprathermal and energetic particles in the energy range from a few keV up to (near-) relativistic energies (few MeV for electrons and about 500 MeV\/nuc for ions). We present an overview of the initial results from the first year of operations and we provide a first assessment of issues and limitations. During this first year of operations of the Solar Orbiter mission, EPD has recorded several particle events at distances between 0.5 and 1 au from the Sun. We present dynamic and time-averaged energy spectra for ions that were measured with a combination of all four EPD sensors, namely: the SupraThermal Electron and Proton sensor (STEP), the Electron Proton Telescope (EPT), the Suprathermal Ion Spectrograph (SIS), and the High-Energy Telescope (HET) as well as the associated energy spectra for electrons measured with STEP and EPT. We illustrate the capabilities of the EPD suite using the 10-11 December 2020 solar particle event. This event showed an enrichment of heavy ions as well as $^3$He, for which we also present dynamic spectra measured with SIS. The high anisotropy of electrons at the onset of the event and its temporal evolution is also shown using data from these sensors. We discuss the ongoing in-flight calibration and a few open instrumental issues using data from the 21 July and the 10-11 December 2020 events and give guidelines and examples for the usage of the EPD data. We explain how spacecraft operations may affect EPD data and we present a list of such time periods in the appendix. A list of the most significant particle enhancements as observed by EPT during this first year is also provided.","304":"In this paper, we address the problem of face recognition with masks. Given the global health crisis caused by COVID-19, mouth and nose-covering masks have become an essential everyday-clothing-accessory. This sanitary measure has put the state-of-the-art face recognition models on the ropes since they have not been designed to work with masked faces. In addition, the need has arisen for applications capable of detecting whether the subjects are wearing masks to control the spread of the virus. To overcome these problems a full training pipeline is presented based on the ArcFace work, with several modifications for the backbone and the loss function. From the original face-recognition dataset, a masked version is generated using data augmentation, and both datasets are combined during the training process. The selected network, based on ResNet-50, is modified to also output the probability of mask usage without adding any computational cost. Furthermore, the ArcFace loss is combined with the mask-usage classification loss, resulting in a new function named Multi-Task ArcFace (MTArcFace). Experimental results show that the proposed approach highly boosts the original model accuracy when dealing with masked faces, while preserving almost the same accuracy on the original non-masked datasets. Furthermore, it achieves an average accuracy of 99.78% in mask-usage classification.","305":"Event detection on social media has attracted a number of researches, given the recent availability of large volumes of social media discussions. Previous works on social media event detection either assume a specific type of event, or assume certain behavior of observed variables. In this paper, we propose a general method for event detection on social media that makes few assumptions. The main assumption we make is that when an event occurs, affected semantic aspects will behave differently from its usual behavior. We generalize the representation of time units based on word embeddings of social media text, and propose an algorithm to detect events in time series in a general sense. In the experimental evaluation, we use a novel setting to test if our method and baseline methods can exhaustively catch all real-world news in the test period. The evaluation results show that when the event is quite unusual with regard to the base social media discussion, it can be captured more effectively with our method. Our method can be easily implemented and can be treated as a starting point for more specific applications.","306":"The R package knnwtsim provides functions to implement k nearest neighbors (KNN) forecasting using a similarity metric tailored to the forecasting problem of predicting future observations of a response series where recent observations, seasonal or periodic patterns, and the values of one or more exogenous predictors all have predictive value in forecasting new response points. This paper will introduce the similarity measure of interest, and the functions in knnwtsim used to calculate, tune, and ultimately utilize it in KNN forecasting. This package may be of particular value in forecasting problems where the functional relationships between response and predictors are non-constant or piece-wise and thus can violate the assumptions of popular alternatives. In addition both real world and simulated time series datasets used in the development and testing of this approach have been made available with the package.","307":"Current online moderation follows a one-size-fits-all approach, where each intervention is applied in the same way to all users. This naive approach is challenged by established socio-behavioral theories and by recent empirical results that showed the limited effectiveness of such interventions. We propose a paradigm-shift in online moderation by moving towards a personalized and user-centered approach. Our multidisciplinary vision combines state-of-the-art theories and practices in diverse fields such as computer science, sociology and psychology, to design personalized moderation interventions (PMIs). In outlining the path leading to the next-generation of moderation interventions, we also discuss the most prominent challenges introduced by such a disruptive change.","308":"The explosion of disinformation related to the COVID-19 pandemic has overloaded fact-checkers and media worldwide. To help tackle this, we developed computational methods to support COVID-19 disinformation debunking and social impacts research. This paper presents: 1) the currently largest available manually annotated COVID-19 disinformation category dataset; and 2) a classification-aware neural topic model (CANTM) that combines classification and topic modelling under a variational autoencoder framework. We demonstrate that CANTM efficiently improves classification performance with low resources, and is scalable. In addition, the classification-aware topics help researchers and end-users to better understand the classification results.","309":"We consider the problem of deciding how best to target and prioritize existing vaccines that may offer protection against new variants of an infectious disease. Sequential experiments are a promising approach; however, challenges due to delayed feedback and the overall ebb and flow of disease prevalence make available method inapplicable for this task. We present a method, partial likelihood Thompson sampling, that can handle these challenges. Our method involves running Thompson sampling with belief updates determined by partial likelihood each time we observe an event. To test our approach, we ran a semi-synthetic experiment based on 200 days of COVID-19 infection data in the US.","310":"Internet memes have become powerful means to transmit political, psychological, and socio-cultural ideas. Although memes are typically humorous, recent days have witnessed an escalation of harmful memes used for trolling, cyberbullying, and abuse. Detecting such memes is challenging as they can be highly satirical and cryptic. Moreover, while previous work has focused on specific aspects of memes such as hate speech and propaganda, there has been little work on harm in general. Here, we aim to bridge this gap. We focus on two tasks: (i)detecting harmful memes, and (ii)identifying the social entities they target. We further extend a recently released HarMeme dataset, which covered COVID-19, with additional memes and a new topic: US politics. To solve these tasks, we propose MOMENTA (MultimOdal framework for detecting harmful MemEs aNd Their tArgets), a novel multimodal deep neural network that uses global and local perspectives to detect harmful memes. MOMENTA systematically analyzes the local and the global perspective of the input meme (in both modalities) and relates it to the background context. MOMENTA is interpretable and generalizable, and our experiments show that it outperforms several strong rivaling approaches.","311":"Since December 2019, A novel coronavirus (2019-nCoV) has been breaking out in China, which can cause respiratory diseases and severe pneumonia. Epidemic models relying on the incidence rate of new cases for forecasting epidemic outbreaks have received increasing attention. However, many prior works in this area mostly focus on the application of the traditional SIR model and disregard the transmission characteristics of 2019-nCoV, exceptionally the infectious of undiagnosed cases. Here, we propose a SUIR model based on the classical SIR model object to supervise the effective prediction, prevention, and control of infectious diseases. SUIR model adds a unique $U$ (Undiagnosed) state of the epidemic and divides the population into four states: S (Susceptible), U (Undiagnosed), I (Infectious and Uninfectious), and R (Recovered). This approach enables us to predict the incidence of 2019-nCoV effectively and the clear advantage of the model accuracy more reliable than the traditional SIR model.","312":"The article describes the approaches for forming different predictive features of tweet data sets and using them in the predictive analysis for decision-making support. The graph theory as well as frequent itemsets and association rules theory is used for forming and retrieving different features from these datasests. The use of these approaches makes it possible to reveal a semantic structure in tweets related to a specified entity. It is shown that quantitative characteristics of semantic frequent itemsets can be used in predictive regression models with specified target variables.","313":"We present a word-sense induction method based on pre-trained masked language models (MLMs), which can cheaply scale to large vocabularies and large corpora. The result is a corpus which is sense-tagged according to a corpus-derived sense inventory and where each sense is associated with indicative words. Evaluation on English Wikipedia that was sense-tagged using our method shows that both the induced senses, and the per-instance sense assignment, are of high quality even compared to WSD methods, such as Babelfy. Furthermore, by training a static word embeddings algorithm on the sense-tagged corpus, we obtain high-quality static senseful embeddings. These outperform existing senseful embeddings methods on the WiC dataset and on a new outlier detection dataset we developed. The data driven nature of the algorithm allows to induce corpora-specific senses, which may not appear in standard sense inventories, as we demonstrate using a case study on the scientific domain.","314":"We present an assessment of the greenhouse gases emissions of the Institute for Research in Astrophysics and Planetology (IRAP), located in Toulouse (France). It was performed following the established\"Bilan Carbone\"methodology, over a large scope compared to similar previous studies, including in particular the contribution from the purchase of goods and services as well as IRAP's use of external research infrastructures, such as ground-based observatories and space-borne facilities. The carbon footprint of the institute for the reference year 2019 is 7400 +\/- 900 tCO2e. If we exclude the contribution from external research infrastructures to focus on a restricted perimeter over which the institute has some operational control, IRAP's emissions in 2019 amounted to 3300 +\/- 400 tCO2e. Over the restricted perimeter, the contribution from purchasing goods and services is dominant, about 40% of the total, slightly exceeding the contribution from professional travel including hotel stays, which accounts for 38%. Local infrastructures make a smaller contribution to IRAP's carbon footprint, about 25% over the restricted perimeter. We note that this repartition may be specific to IRAP, since the energy used to produce the electricity and heating has a relatively low carbon footprint. Over the full perimeter, the large share from the use of ground-based observatories and space-borne facilities and the fact that the majority of IRAP purchases are related to instrument development indicate that research infrastructures represent the most significant challenge for reducing the carbon footprint of research at our institute. With ~260 staff members employed, our results imply that performing research in astronomy and astrophysics at IRAP according to the standards of 2019 produces average GHG emissions of 28 tCO2e\/yr per person involved in that activity (Abridged).","315":"We analyse the mechanisms ruling galactic disc heating through the dynamics of space velocities $U$, $V$ and $W$, extracted from the Geneva-Copenhagen catalogue. To do this, we use a model based on non-extensive statistical mechanics, where we derive the probability distribution functions that quantify the non-Gaussian effects. Furthermore, we find that the deviation $q-1$ at a given stellar age follows non-random behaviour. As a result, the $q$-index behaviour indicates that the vertical component $W$, perpendicular to the Galactic plane, does not ``heat up'' at random, which is in disagreement with previous works that attributed the evolution of $W$ to randomness. Finally, our results bring a new perspective to this matter and open the way for studying Galactic kinematic components through the eyes of more robust statistical models that consider non-Gaussian effects.","316":"When deployed in the wild, machine learning models are usually confronted with data and requirements that constantly vary, either because of changes in the generating distribution or because external constraints change the environment where the model operates. To survive in such an ecosystem, machine learning models need to adapt to new conditions by evolving over time. The idea of model adaptability has been studied from different perspectives. In this paper, we propose a solution based on reusing the knowledge acquired by the already deployed machine learning models and leveraging it to train future generations. This is the idea behind differential replication of machine learning models.","317":"The US astronomy\/astrophysics community comes together to create a decadal report that summarizes grant funding priorities, observatory&instrumental priorities as well as community accomplishments and community goals such as increasing the number of women and the number of people from underrepresented groups. In the 2010 US National Academies Decadal Survey of Astronomy (National Research Council, 2010), it was suggested that having to move so frequently which is a career necessity may be unattractive to people wanting to start a family, especially impacting women. Whether in Europe or elsewhere, as postdocs, astrophysicists will relocate every two to three years, until they secure a permanent position or leave research altogether. Astrophysicists do perceive working abroad as important and positive for their careers (Parenti, 2002); however, it was found that the men at equal rank had not had to spend as much time abroad to further their careers (Fohlmeister&Helling, 2012). By implication, women need to work abroad longer or have more positions abroad to achieve the same rank as men. Astrophysicists living in the United Kingdom prefer to work in their country of origin, but many did not do so because of worse working conditions or difficultly finding a job for their spouse (Fohlmeister&Helling, 2014). In sum, mobility and moving is necessary for a career in astrophysics, and even more necessary for women, but astrophysicists prefer not to move as frequently as needed to maintain a research career. To gather more data on these issues and to broaden the discourse beyond male\/female to include the gender diverse as well as to include other forms of diversity, I designed the ASTROMOVES project which is funded through a Marie Curie Individual Fellowship. Though slowed down by COVID-19, several interviews have been conducted and some preliminary results will be presented.","318":"In the era of precision medicine, time-to-event outcomes such as time to death or progression are routinely collected, along with high-throughput covariates. These high-dimensional data defy classical survival regression models, which are either infeasible to fit or likely to incur low predictability due to over-fitting. To overcome this, recent emphasis has been placed on developing novel approaches for feature selection and survival prognostication. We will review various cutting-edge methods that handle survival outcome data with high-dimensional predictors, highlighting recent innovations in machine learning approaches for survival prediction. We will cover the statistical intuitions and principles behind these methods and conclude with extensions to more complex settings, where competing events are observed. We exemplify these methods with applications to the Boston Lung Cancer Survival Cohort study, one of the largest cancer epidemiology cohorts investigating the complex mechanisms of lung cancer.","319":"Do black-box neural network models learn clinically relevant features for fracture diagnosis? The answer not only establishes reliability quenches scientific curiosity but also leads to explainable and verbose findings that can assist the radiologists in the final and increase trust. This work identifies the concepts networks use for vertebral fracture diagnosis in CT images. This is achieved by associating concepts to neurons highly correlated with a specific diagnosis in the dataset. The concepts are either associated with neurons by radiologists pre-hoc or are visualized during a specific prediction and left for the user's interpretation. We evaluate which concepts lead to correct diagnosis and which concepts lead to false positives. The proposed frameworks and analysis pave the way for reliable and explainable vertebral fracture diagnosis.","320":"The need to forecast COVID-19 related variables continues to be pressing as the epidemic unfolds. Different efforts have been made, with compartmental models in epidemiology and statistical models such as AutoRegressive Integrated Moving Average (ARIMA), Exponential Smoothing (ETS) or computing intelligence models. These efforts have proved useful in some instances by allowing decision makers to distinguish different scenarios during the emergency, but their accuracy has been disappointing, forecasts ignore uncertainties and less attention is given to local areas. In this study, we propose a simple Multiple Linear Regression model, optimised to use call data to forecast the number of daily confirmed cases. Moreover, we produce a probabilistic forecast that allows decision makers to better deal with risk. Our proposed approach outperforms ARIMA, ETS and a regression model without call data, evaluated by three point forecast error metrics, one prediction interval and two probabilistic forecast accuracy measures. The simplicity, interpretability and reliability of the model, obtained in a careful forecasting exercise, is a meaningful contribution to decision makers at local level who acutely need to organise resources in already strained health services. We hope that this model would serve as a building block of other forecasting efforts that on the one hand would help front-line personal and decision makers at local level, and on the other would facilitate the communication with other modelling efforts being made at the national level to improve the way we tackle this pandemic and other similar future challenges.","321":"For parents of young children and adolescents, the digital age has introduced many new challenges, including excessive screen time, inappropriate online content, cyber predators, and cyberbullying. To address these challenges, many parents rely on numerous parental control solutions on different platforms, including parental control network devices (e.g., WiFi routers) and software applications on mobile devices and laptops. While these parental control solutions may help digital parenting, they may also introduce serious security and privacy risks to children and parents, due to their elevated privileges and having access to a significant amount of privacy-sensitive data. In this paper, we present an experimental framework for systematically evaluating security and privacy issues in parental control software and hardware solutions. Using the developed framework, we provide the first comprehensive study of parental control tools on multiple platforms including network devices, Windows applications, Chrome extensions and Android apps. Our analysis uncovers pervasive security and privacy issues that can lead to leakage of private information, and\/or allow an adversary to fully control the parental control solution, and thereby may directly aid cyberbullying and cyber predators.","322":"In this paper, a workflow for designing a bot using Robotic Process Automation (RPA), associated with Artificial Intelligence (AI) that is used for information extraction, classification, etc., is proposed. The bot is equipped with many features that make email handling a stress-free job. It automatically login into the mailbox through secured channels, distinguishes between the useful and not useful emails, classifies the emails into different labels, downloads the attached files, creates different directories, and stores the downloaded files into relevant directories. It moves the not useful emails into the trash. Further, the bot can also be trained to rename the attached files with the names of the sender\/applicant in case of a job application for the sake of convenience. The bot is designed and tested using the UiPath tool to improve the performance of the system. The paper also discusses the further possible functionalities that can be added on to the bot.","323":"This document supports a proposed APS statement that encourages academic, research, and other institutions to add the participation in informal science education activities to the criteria they use for hiring and career advancement decisions. There is a prevalent attitude that the time spent by a researcher on these activities takes time away from research efforts that are more valued by their peers and their institution. To change this mindset, we enumerate the many benefits of informal science education activities to the public, to researchers, to their institutions, and to the field of physics. We also suggest aspects of these activities that may be considered by institutions in evaluating informal educational efforts for recruitment and career advancement decisions.","324":"The COVID-19 pandemic has influenced virtually all aspects of our lives. Across the world, countries have applied various mitigation strategies, based on social, political, and technological instruments. We postulate that multi-agent systems can provide a common platform to study (and balance) their essential properties. We also show how to obtain a comprehensive list of the properties by\"distilling\"them from media snippets. Finally, we present a preliminary take on their formal specification, using ideas from multi-agent logics.","325":"The TopDown Algorithm (TDA) first produces differentially private counts at the nation and then produces counts at lower geolevels (e.g.: state, county, etc.) subject to the constraint that all query answers in lower geolevels are consistent with those at previously estimated geolevels. This paper describes the three sets of definitions of these geolevels, or the geographic spines, that are implemented within TDA. These include the standard Census geographic spine and two other spines that improve accuracy in geographic areas that are far from the standard Census spine, such as cities, towns, and\/or AIAN areas. The third such spine, which is called the optimized spine, also modifies the privacy-loss budget allocated to the entities within the geolevels, or the geounits, to ensure the privacy-loss budget is used efficiently within TDA.","326":"The spreading pattern of COVID-19 differ a lot across the US states under different quarantine measures and reopening policies. We proposed to cluster the US states into distinct communities based on the daily new confirmed case counts via a nonnegative matrix factorization (NMF) followed by a k-means clustering procedure on the coefficients of the NMF basis. A cross-validation method was employed to select the rank of the NMF. Applying the method to the entire study period from March 22 to July 25, we clustered the 49 continental states (including District of Columbia) into 7 groups, two of which contained a single state. To investigate the dynamics of the clustering results over time, the same method was successively applied to the time periods with increment of one week, starting from the period of March 22 to March 28. The results suggested a change point in the clustering in the week starting on May 30, which might be explained by a combined impact of both quarantine measures and reopening policies.","327":"This paper presents an Improved Bayesian Optimization (IBO) algorithm to solve complex high-dimensional epidemic models' optimal control solution. Evaluating the total objective function value for disease control models with hundreds of thousands of control time periods is a high computational cost. In this paper, we improve the conventional Bayesian Optimization (BO) approach from two parts. The existing BO methods optimize the minimizer step for once time during each acquisition function update process. To find a better solution for each acquisition function update, we do more local minimization steps to tune the algorithm. When the model is high dimensions, and the objective function is complicated, only some update iterations of the acquisition function may not find the global optimal solution. The IBO algorithm adds a series of Adam-based steps at the final stage of the algorithm to increase the solution's accuracy. Comparative simulation experiments using different kernel functions and acquisition functions have shown that the Improved Bayesian Optimization algorithm is effective and suitable for handing large-scale and complex epidemic models under study. The IBO algorithm is then compared with four other global optimization algorithms on three well-known synthetic test functions. The effectiveness and robustness of the IBO algorithm are also demonstrated through some simulation experiments to compare with the Particle Swarm Optimization algorithm and Random Search algorithm. With its reliable convergence behaviors and straightforward implementation, the IBO algorithm has a great potential to solve other complex optimal control problems with high dimensionality.","328":"Many of the tasks that a service robot can perform at home involve navigation skills. In a real world scenario, the navigation system should consider individuals beyond just objects, theses days it is necessary to offer particular and dynamic representation in the scenario in order to enhance the HRI experience. In this paper, we use the proxemic theory to do this representation. The proxemic zones are not static. The culture or the context influences them and, if we have this influence into account, we can increase humans' comfort. Moreover, there are collaborative tasks in which these zones take different shapes to allow the task's best performance. This research develops a layer, the social layer, to represent and distribute the proxemics zones' information in a standard way, through a cost map and using it to perform a social navigate task. We have evaluated these components in a simulated scenario, performing different collaborative and human-robot interaction tasks and reducing the personal area invasion in a 32\\%. The material developed during this research can be found in a public repository, as well as instructions to facilitate the reproducibility of the results.","329":"A pandemic caused by a new coronavirus (COVID-19) has spread worldwide, inducing an epidemic still active in Argentina. In this chapter, we present a case study using an SEIR (Susceptible-Exposed-Infected-Recovered) diffusion model of fractional order in time to analyze the evolution of the epidemic in Buenos Aires and neighboring areas (Regi\\'on Metropolitana de Buenos Aires, (RMBA)) comprising about 15 million inhabitants. In the SEIR model, individuals are divided into four classes, namely, susceptible (S), exposed (E), infected (I) and recovered (R). The SEIR model of fractional order allows for the incorporation of memory, with hereditary properties of the system, being a generalization of the classic SEIR first-order system, where such effects are ignored. Furthermore, the fractional model provides one additional parameter to obtain a better fit of the data. The parameters of the model are calibrated by using as data the number of casualties officially reported. Since infinite solutions honour the data, we show a set of cases with different values of the lockdown parameters, fatality rate, and incubation and infectious periods. The different reproduction ratios R0 and infection fatality rates (IFR) so obtained indicate the results may differ from recent reported values, constituting possible alternative solutions. A comparison with results obtained with the classic SEIR model is also included. The analysis allows us to study how isolation and social distancing measures affect the time evolution of the epidemic.","330":"The process of revitalizing cities in the United States suffers from balky and unresponsive processes---de jure egalitarian but de facto controlled and mediated by city officials and powerful interests, not residents. We argue that, instead, our goal should be to put city planning in the hands of the people, and to that end, give ordinary residents pattern-based planning tools to help them redesign (and repair) their urban surrounds. Through this, residents can explore many disparate ideas, try them, and, if successful, replicate them, enabling bottom-up city planning through direct action. We describe a prototype for such a tool that leverages classic patterns to enable city planning by residents, using case studies from Los Angeles as guides for both the problem and potential solution.","331":"This paper describes a general approach for stochastic modeling of assets returns and liability cash-flows of a typical pensions insurer. On the asset side, we model the investment returns on equities and various classes of fixed-income instruments including short- and long-maturity fixed-rate bonds as well as index-linked and corporate bonds. On the liability side, the risks are driven by future mortality developments as well as price and wage inflation. All the risk factors are modeled as a multivariate stochastic process that captures the dynamics and the dependencies across different risk factors. The model is easy to interpret and to calibrate to both historical data and to forecasts or expert views concerning the future. The simple structure of the model allows for efficient computations. The construction of a million scenarios takes only a few minutes on a personal computer. The approach is illustrated with an asset-liability analysis of a defined benefit pension fund.","332":"The state of the art of epidemic modelling in terrestrial ecosystems is the compartmental SIR model and its extensions from the now classical work of Kermack-Mackendrick. In contrast, epidemic modelling of marine ecosystems is a bit behind, and compartmental models have been introduced only recently. One of the reasons is that many epidemic processes in terrestrial ecosystems can be described through a contact process, while modelling marine epidemics is more subtle in many cases. Here we present a model describing disease outbreaks caused by parasites in bivalve populations. The SIRP model is a multicompartmental model with four compartments, three of which describe the different states of the host, susceptible (i.e. healthy), S, infected, I, and removed (dead), R, and one compartment for the parasite in the marine medium, P, written as a 4-dimensional dynamical system. Even if this is the simplest model one can write to describe this system, it is still too complicated for both direct analytical manipulation and direct comparison with experimental observations, as it depends on four parameters to be fitted. We show that it is possible to simplify the model, including a reduction to the standard SIR model if the parameters fulfil certain conditions. The model is validated with available data for the recent Mass Mortality Event of the noble pen shell Pinna nobilis, a disease caused by the parasite Haplosporidium pinnae, showing that the reduced SIR model is able to fit the data. So, we show that a model in which the species that suffers the epidemics (host) cannot move, and contagion occurs through parasites, can be reduced to the standard SIR model that represents epidemic transmission between mobile hosts. The fit indicates that the assumptions made to simplify the model are reasonable in practice, although it leads to an indeterminacy in three of the original parameters.","333":"The massive availability of cameras results in a wide variability of imaging conditions, producing large intra-class variations and a significant performance drop if heterogeneous images are compared for person recognition. However, as biometrics is deployed, it is common to replace damaged or obsolete hardware, or to exchange information between heterogeneous applications. Variations in spectral bands can also occur. For example, surveillance face images (typically acquired in the visible spectrum, VIS) may need to be compared against a legacy iris database (typically acquired in near-infrared, NIR). Here, we propose a multialgorithmic approach to cope with periocular images from different sensors. With face masks in the front line against COVID-19, periocular recognition is regaining popularity since it is the only face region that remains visible. We integrate different comparators with a fusion scheme based on linear logistic regression, in which scores are represented by log-likelihood ratios. This allows easy interpretation of scores and the use of Bayes thresholds for optimal decision-making since scores from different comparators are in the same probabilistic range. We evaluate our approach in the context of the Cross-Eyed Competition, whose aim was to compare recognition approaches when NIR and VIS periocular images are matched. Our approach achieves EER=0.2% and FRR of just 0.47% at FAR=0.01%, representing the best overall approach of the competition. Experiments are also reported with a database of VIS images from different smartphones. We also discuss the impact of template size and computation times, with the most computationally heavy comparator playing an important role in the results. Lastly, the proposed method is shown to outperform other popular fusion approaches, such as the average of scores, SVMs or Random Forest.","334":"Risk models in medical statistics and healthcare machine learning are increasingly used to guide clinical or other interventions. Should a model be updated after a guided intervention, it may lead to its own failure at making accurate predictions. The use of a `holdout set' -- a subset of the population that does not receive interventions guided by the model -- has been proposed to prevent this. Since patients in the holdout set do not benefit from risk predictions, the chosen size must trade off maximising model performance whilst minimising the number of held out patients. By defining a general loss function, we prove the existence and uniqueness of an optimal holdout set size, and introduce parametric and semi-parametric algorithms for its estimation. We demonstrate their use on a recent risk score for pre-eclampsia. Based on these results, we argue that a holdout set is a safe, viable and easily implemented solution to the model update problem.","335":"In this work, we extended a stochastic model for football leagues based on the team's potential [R. da Silva et al. Comput. Phys. Commun. \\textbf{184} 661--670 (2013)] for making predictions instead of only performing a successful characterization of the statistics on the punctuation of the real leagues. Our adaptation considers the advantage of playing at home when considering the potential of the home and away teams. The algorithm predicts the tournament's outcome by using the market value or\/and the ongoing team's performance as initial conditions in the context of Monte Carlo simulations. We present and compare our results to the worldwide known SPI predictions performed by the\"FiveThirtyEight\"project. The results show that the algorithm can deliver good predictions even with a few ingredients and in more complicated seasons like the 2020 editions where the matches were played without fans in the stadiums.","336":"Loneliness has been associated with negative outcomes for physical and mental health. Understanding how people express and cope with various forms of loneliness is critical for early screening and targeted interventions to reduce loneliness, particularly among vulnerable groups such as young adults. To examine how different forms of loneliness and coping strategies manifest in loneliness self-disclosure, we built a dataset, FIG-Loneliness (FIne-Grained Loneliness) by using Reddit posts in two young adult-focused forums and two loneliness related forums consisting of a diverse age group. We provided annotations by trained human annotators for binary and fine-grained loneliness classifications of the posts. Trained on FIG-Loneliness, two BERT-based models were used to understand loneliness forms and authors' coping strategies in these forums. Our binary loneliness classification achieved an accuracy above 97%, and fine-grained loneliness category classification reached an average accuracy of 77% across all labeled categories. With FIG-Loneliness and model predictions, we found that loneliness expressions in the young adults related forums were distinct from other forums. Those in young adult-focused forums were more likely to express concerns pertaining to peer relationship, and were potentially more sensitive to geographical isolation impacted by the COVID-19 pandemic lockdown. Also, we showed that different forms of loneliness have differential use in coping strategies.","337":"Protein-ligand interactions (PLIs) are fundamental to biochemical research and their identification is crucial for estimating biophysical and biochemical properties for rational therapeutic design. Currently, experimental characterization of these properties is the most accurate method, however, this is very time-consuming and labor-intensive. A number of computational methods have been developed in this context but most of the existing PLI prediction heavily depends on 2D protein sequence data. Here, we present a novel parallel graph neural network (GNN) to integrate knowledge representation and reasoning for PLI prediction to perform deep learning guided by expert knowledge and informed by 3D structural data. We develop two distinct GNN architectures, GNNF is the base implementation that employs distinct featurization to enhance domain-awareness, while GNNP is a novel implementation that can predict with no prior knowledge of the intermolecular interactions. The comprehensive evaluation demonstrated that GNN can successfully capture the binary interactions between ligand and proteins 3D structure with 0.979 test accuracy for GNNF and 0.958 for GNNP for predicting activity of a protein-ligand complex. These models are further adapted for regression tasks to predict experimental binding affinities and pIC50 is crucial for drugs potency and efficacy. We achieve a Pearson correlation coefficient of 0.66 and 0.65 on experimental affinity and 0.50 and 0.51 on pIC50 with GNNF and GNNP, respectively, outperforming similar 2D sequence-based models. Our method can serve as an interpretable and explainable artificial intelligence (AI) tool for predicted activity, potency, and biophysical properties of lead candidates. To this end, we show the utility of GNNP on SARS-Cov-2 protein targets by screening a large compound library and comparing our prediction with the experimentally measured data.","338":"We present a collection of networks that describe the travel patterns between municipalities in Mexico between 2020 and 2021. Using anonymized mobile device geo-location data we constructed directed, weighted networks representing the (normalized) volume of travels between municipalities. We analysed changes in global (graph total weight sum), local (centrality measures), and mesoscale (community structure) network features. We observe that changes in these features are associated with factors such as Covid-19 restrictions and population size. In general, events in early 2020 (when initial Covid-19 restrictions were implemented) induced more intense changes in network features, whereas later events had a less notable impact in network features. We believe these networks will be useful for researchers and decision makers in the areas of transportation, infrastructure planning, epidemic control and network science at large.","339":"Facebook Disaster Maps (FBDM) is the first platform providing analysis-ready population change products derived from crowdsourced data targeting disaster relief practices. We evaluate the representativeness of FBDM data using the Mann-Kendall test and emerging hot and cold spots in an anomaly analysis to reveal the trend, magnitude, and agglommeration of population displacement during the Mendocino Complex and Woolsey fires in California, USA. Our results show that the distribution of FBDM pre-crisis users fits well with the total population from different sources. Due to usage habits, the elder population is underrepresented in FBDM data. During the two mega-fires in California, FBDM data effectively captured the temporal change of population arising from the placing and lifting of evacuation orders. Coupled with monotonic trends, the fall and rise of cold and hot spots of population revealed the areas with the greatest population drop and potential places to house the displaced residents. A comparison between the Mendocino Complex and Woolsey fires indicates that a densely populated region can be evacuated faster than a scarcely populated one, possibly due to the better access to transportation. In sparsely populated fire-prone areas, resources should be prioritized to move people to shelters as the displaced residents do not have many alternative options, while their counterparts in densely populated areas can utilize their social connections to seek temporary stay at nearby locations during an evacuation. Integrated with an assessment on underrepresented communities, FBDM data and the derivatives can provide much needed information of near real-time population displacement for crisis response and disaster relief. As applications and data generation mature, FBDM will harness crowdsourced data and aid first responder decision-making.","340":"Nowcasting and forecasting of epidemic spreading rely on incidence series of reported cases to derive the fundamental epidemiological parameters for a given pathogen. Two relevant drawbacks for predictions are the unknown fractions of undocumented cases and levels of nonpharmacological interventions, which span highly heterogeneously across different places and times. We describe a simple data-driven approach using a compartmental model including asymptomatic and presymptomatic contagions that allows to estimate both the level of undocumented infections and the value of effective reproductive number R t from time series of reported cases, deaths, and epidemiological parameters. The method was applied to epidemic series for COVID-19 across different municipalities in Brazil allowing to estimate the heterogeneity level of under-reporting across different places. The reproductive number derived within the current framework is little sensitive to both diagnosis and infection rates during the asymptomatic states. The methods described here can be extended to more general cases if data is available and adapted to other epidemiological approaches and surveillance data.","341":"The application of network analysis has found great success in a wide variety of disciplines; however, the popularity of these approaches has revealed the difficulty in handling networks whose complexity scales rapidly. One of the main interests in network analysis is the online detection of anomalous behaviour. To overcome the curse of dimensionality, we introduce a network surveillance method bringing together network modelling and statistical process control. Our approach is to apply multivariate control charts based on exponential smoothing and cumulative sums in order to monitor networks determined by temporal exponential random graph models (TERGM). This allows us to account for temporal dependence, while simultaneously reducing the number of parameters to be monitored. The performance of the proposed charts is evaluated by calculating the average run length for both simulated and real data. To prove the appropriateness of the TERGM to describe network data, some measures of goodness of fit are inspected. We demonstrate the effectiveness of the proposed approach by an empirical application, monitoring daily flights in the United States to detect anomalous patterns.","342":"We present a search for dark photon dark matter that could couple to gravitational-wave interferometers using data from Advanced LIGO and Virgo's third observing run. To perform this analysis, we use two methods, one based on cross-correlation of the strain channels in the two nearly aligned LIGO detectors, and one that looks for excess power in the strain channels of the LIGO and Virgo detectors. The excess power method optimizes the Fourier Transform coherence time as a function of frequency, to account for the expected signal width due to Doppler modulations. We do not find any evidence of dark photon dark matter with a mass between $m_{\\rm A} \\sim 10^{-14}-10^{-11}$ eV\/$c^2$, which corresponds to frequencies between 10-2000 Hz, and therefore provide upper limits on the square of the minimum coupling of dark photons to baryons, i.e. $U(1)_{\\rm B}$ dark matter. For the cross-correlation method, the best median constraint on the squared coupling is $\\sim1.31\\times10^{-47}$ at $m_{\\rm A}\\sim4.2\\times10^{-13}$ eV\/$c^2$; for the other analysis, the best constraint is $\\sim 2.4\\times 10^{-47}$ at $m_{\\rm A}\\sim 5.7\\times 10^{-13}$ eV\/$c^2$. These limits improve upon those obtained in direct dark matter detection experiments by a factor of $\\sim100$ for $m_{\\rm A}\\sim [2-4]\\times 10^{-13}$ eV\/$c^2$, and are, in absolute terms, the most stringent constraint so far in a large mass range $m_A\\sim$ $2\\times 10^{-13}-8\\times 10^{-12}$ eV\/$c^2$.","343":"This study presents a new risk-averse multi-stage stochastic epidemics-ventilator-logistics compartmental model to address the resource allocation challenges of mitigating COVID-19. This epidemiological logistics model involves the uncertainty of untested asymptomatic infections and incorporates short-term human migration. Disease transmission is also forecasted through a new formulation of transmission rates that evolve over space and time with respect to various non-pharmaceutical interventions, such as wearing masks, social distancing, and lockdown. The proposed multi-stage stochastic model overviews different scenarios on the number of asymptomatic individuals while optimizing the distribution of resources, such as ventilators, to minimize the total expected number of newly infected and deceased people. The Conditional Value at Risk (CVaR) is also incorporated into the multi-stage mean-risk model to allow for a trade-off between the weighted expected loss due to the outbreak and the expected risks associated with experiencing disastrous pandemic scenarios. We apply our multi-stage mean-risk epidemics-ventilator-logistics model to the case of controlling the COVID-19 in highly-impacted counties of New York and New Jersey. We calibrate, validate, and test our model using actual infection, population, and migration data. The results indicate that short-term migration influences the transmission of the disease significantly. The optimal number of ventilators allocated to each region depends on various factors, including the number of initial infections, disease transmission rates, initial ICU capacity, the population of a geographical location, and the availability of ventilator supply. Our data-driven modeling framework can be adapted to study the disease transmission dynamics and logistics of other similar epidemics and pandemics.","344":"With the ubiquitous reach of social media, influencers are increasingly central to articulation of political agendas on a range of topics. We curate a sample of tweets from the 200 most followed sportspersons in India and the United States respectively since 2019, map their connections with politicians, and visualize their engagements with key topics online. We find significant differences between the ways in which Indian and US sportspersons engage with politics online-while leading Indian sportspersons tend to align closely with the ruling party and engage minimally in dissent, American sportspersons engage with a range of political issues and are willing to publicly criticize politicians or policy. Our findings suggest that the ownership and governmental control of sports impact public stances on issues that professional sportspersons are willing to engage in online. It might also be inferred, depending upon the government of the day, that the costs of speaking up against the state and the government in power have different socio-economic costs in the US and India.","345":"We propose a Bayesian hierarchical model to simultaneously estimate mean based changepoints in spatially correlated functional time series. Unlike previous methods that assume a shared changepoint at all spatial locations or ignore spatial correlation, our method treats changepoints as a spatial process. This allows our model to respect spatial heterogeneity and exploit spatial correlations to improve estimation. Our method is derived from the ubiquitous cumulative sum (CUSUM) statistic that dominates changepoint detection in functional time series. However, instead of directly searching for the maximum of the CUSUM based processes, we build spatially correlated two-piece linear models with appropriate variance structure to locate all changepoints at once. The proposed linear model approach increases the robustness of our method to variability in the CUSUM process, which, combined with our spatial correlation model, improves changepoint estimation near the edges. We demonstrate through extensive simulation studies that our method outperforms existing functional changepoint estimators in terms of both estimation accuracy and uncertainty quantification, under either weak and strong spatial correlation, and weak and strong change signals. Finally, we demonstrate our method using a temperature data set and a coronavirus disease 2019 (COVID-19) study.","346":"The COVID-19 epidemic has swept the world for over a year. However, a large number of infectious asymptomatic COVID-19 cases (\\textit{ACC}s) are still making the breaking up of the transmission chains very difficult. Efforts by epidemiological researchers in many countries have thrown light on the clinical features of ACCs, but there is still a lack of practical approaches to detect ACCs so as to help contain the pandemic. To address the issue of ACCs, this paper presents a neural network model called Spatio-Temporal Episodic Memory for COVID-19 (\\textit{STEM-COVID}) to identify ACCs from contact tracing data. Based on the fusion Adaptive Resonance Theory (\\textit{ART}), the model encodes a collective spatio-temporal episodic memory of individuals and incorporates an effective mechanism of parallel searches for ACCs. Specifically, the episodic traces of the identified positive cases are used to map out the episodic traces of suspected ACCs using a weighted evidence pooling method. To evaluate the efficacy of STEM-COVID, a realistic agent based simulation model for COVID-19 spreading is implemented based on the recent epidemiological findings on ACCs. The experiments based on rigorous simulation scenarios, manifesting the current situation of COVID-19 spread, show that the STEM-COVID model with weighted evidence pooling has a higher level of accuracy and efficiency for identifying ACCs when compared with several baselines. Moreover, the model displays strong robustness against noisy data and different ACC proportions, which partially reflects the effect of breakthrough infections after vaccination on the virus transmission.","347":"Most depression assessment tools are based on self-report questionnaires, such as the Patient Health Questionnaire (PHQ-9). These psychometric instruments can be easily adapted to an online setting by means of electronic forms. However, this approach lacks the interacting and engaging features of modern digital environments. With the aim of making depression screening more available, attractive and effective, we developed Perla, a conversational agent able to perform an interview based on the PHQ-9. We also conducted a validation study in which we compared the results obtained by the traditional self-report questionnaire with Perla's automated interview. Analyzing the results from this study we draw two significant conclusions: firstly, Perla is much preferred by Internet users, achieving more than 2.5 times more reach than a traditional form-based questionnaire; secondly, her psychometric properties (Cronbach's alpha of 0.81, sensitivity of 96% and specificity of 90%) are excellent and comparable to the traditional well-established depression screening questionnaires.","348":"In time series data analysis, detecting change points on a real-time basis (online) is of great interest in many areas, such as finance, environmental monitoring, and medicine. One promising means to achieve this is the Bayesian online change point detection (BOCPD) algorithm, which has been successfully adopted in particular cases in which the time series of interest has a fixed baseline. However, we have found that the algorithm struggles when the baseline irreversibly shifts from its initial state. This is because with the original BOCPD algorithm, the sensitivity with which a change point can be detected is degraded if the data points are fluctuating at locations relatively far from the original baseline. In this paper, we not only extend the original BOCPD algorithm to be applicable to a time series whose baseline is constantly shifting toward unknown values but also visualize why the proposed extension works. To demonstrate the efficacy of the proposed algorithm compared to the original one, we examine these algorithms on two real-world data sets and six synthetic data sets.","349":"In cone-beam X-ray transmission imaging, due to the divergence of X-rays, imaged structures with different depths have different magnification factors on an X-ray detector, which results in perspective deformation. Perspective deformation causes difficulty in direct, accurate geometric assessments of anatomical structures. In this work, to reduce perspective deformation in X-ray images acquired from regular cone-beam computed tomography (CBCT) systems, we investigate on learning perspective deformation, i.e., converting perspective projections into orthogonal projections. Directly converting a single perspective projection image into an orthogonal projection image is extremely challenging due to the lack of depth information. Therefore, we propose to utilize one additional perspective projection, a complementary (180-degree) or orthogonal (90-degree) view, to provide a certain degree of depth information. Furthermore, learning perspective deformation in different spatial domains is investigated. Our proposed method is evaluated on numerical spherical bead phantoms as well as patients' chest and head X-ray data. The experiments on numerical bead phantom data demonstrate that learning perspective deformation in polar coordinates has significant advantages over learning in Cartesian coordinates, as root-mean-square error (RMSE) decreases from 5.31 to 1.40, while learning in log-polar coordinates has no further considerable improvement (RMSE = 1.85). In addition, using a complementary view (RMSE = 1.40) is better than an orthogonal view (RMSE = 3.87). The experiments on patients' chest and head data demonstrate that learning perspective deformation using dual complementary views is also applicable in anatomical X-ray data, allowing accurate cardiothoracic ratio measurements in chest X-ray images and cephalometric analysis in synthetic cephalograms from cone-beam X-ray projections.","350":"Human beings learn and accumulate hierarchical knowledge over their lifetime. This knowledge is associated with previous concepts for consolidation and hierarchical construction. However, current incremental learning methods lack the ability to build a concept hierarchy by associating new concepts to old ones. A more realistic setting tackling this problem is referred to as Incremental Implicitly-Refined Classification (IIRC), which simulates the recognition process from coarse-grained categories to fine-grained categories. To overcome forgetting in this benchmark, we propose Hierarchy-Consistency Verification (HCV) as an enhancement to existing continual learning methods. Our method incrementally discovers the hierarchical relations between classes. We then show how this knowledge can be exploited during both training and inference. Experiments on three setups of varying difficulty demonstrate that our HCV module improves performance of existing continual learning methods under this IIRC setting by a large margin. Code is available in https:\/\/github.com\/wangkai930418\/HCV_IIRC.","351":"Learning modality-fused representations and processing unaligned multimodal sequences are meaningful and challenging in multimodal emotion recognition. Existing approaches use directional pairwise attention or a message hub to fuse language, visual, and audio modalities. However, those approaches introduce information redundancy when fusing features and are inefficient without considering the complementarity of modalities. In this paper, we propose an efficient neural network to learn modality-fused representations with CB-Transformer (LMR-CBT) for multimodal emotion recognition from unaligned multimodal sequences. Specifically, we first perform feature extraction for the three modalities respectively to obtain the local structure of the sequences. Then, we design a novel transformer with cross-modal blocks (CB-Transformer) that enables complementary learning of different modalities, mainly divided into local temporal learning,cross-modal feature fusion and global self-attention representations. In addition, we splice the fused features with the original features to classify the emotions of the sequences. Finally, we conduct word-aligned and unaligned experiments on three challenging datasets, IEMOCAP, CMU-MOSI, and CMU-MOSEI. The experimental results show the superiority and efficiency of our proposed method in both settings. Compared with the mainstream methods, our approach reaches the state-of-the-art with a minimum number of parameters.","352":"A model of the spread of viruses in selected city and in a network of cities is considered, taking into account the delay caused by the long incubation period of the virus. The effect of delay effects is shown in comparison with pandemics without such delay. A temporary asymmetry of the spread of infection has been identified, which means that the time for a pandemic to develop significantly exceeds the time for its completion. Model calculations of the spread of viruses in a network of interconnected large and small cities were carried out, and dynamics features were revealed in comparison with the spread of viruses in a single city, including the possibility of reinfection of megalopolis.","353":"The purpose of this study is to investigate the effects of the COVID-19 pandemic on economic policy uncertainty in the US and the UK. The impact of the increase in COVID-19 cases and deaths in the country, and the increase in the number of cases and deaths outside the country may vary. To examine this, the study employs bootstrap ARDL cointegration approach from March 8, 2020 to May 24, 2020. According to the bootstrap ARDL results, a long-run equilibrium relationship is confirmed for five out of the 10 models. The long-term coefficients obtained from the ARDL models suggest that an increase in COVID-19 cases and deaths outside of the UK and the US has a significant effect on economic policy uncertainty. The US is more affected by the increase in the number of COVID-19 cases. The UK, on the other hand, is more negatively affected by the increase in the number of COVID-19 deaths outside the country than the increase in the number of cases. Moreover, another important finding from the study demonstrates that COVID-19 is a factor of great uncertainty for both countries in the short-term.","354":"With the recent world-wide COVID-19 pandemic, using face masks have become an important part of our lives. People are encouraged to cover their faces when in public area to avoid the spread of infection. The use of these face masks has raised a serious question on the accuracy of the facial recognition system used for tracking school\/office attendance and to unlock phones. Many organizations use facial recognition as a means of authentication and have already developed the necessary datasets in-house to be able to deploy such a system. Unfortunately, masked faces make it difficult to be detected and recognized, thereby threatening to make the in-house datasets invalid and making such facial recognition systems inoperable. This paper addresses a methodology to use the current facial datasets by augmenting it with tools that enable masked faces to be recognized with low false-positive rates and high overall accuracy, without requiring the user dataset to be recreated by taking new pictures for authentication. We present an open-source tool, MaskTheFace to mask faces effectively creating a large dataset of masked faces. The dataset generated with this tool is then used towards training an effective facial recognition system with target accuracy for masked faces. We report an increase of 38% in the true positive rate for the Facenet system. We also test the accuracy of re-trained system on a custom real-world dataset MFR2 and report similar accuracy.","355":"Convolutional Neural Networks (CNNs) intrinsically requires large-scale data whereas Chest X-Ray (CXR) images tend to be data\/annotation-scarce, leading to over-fitting. Therefore, based on our development experience and related work, this paper thoroughly introduces tricks to improve generalization in the CXR diagnosis: how to (i) leverage additional data, (ii) augment\/distillate data, (iii) regularize training, and (iv) conduct efficient segmentation. As a development example based on such optimization techniques, we also feature LPIXEL's CNN-based CXR solution, EIRL Chest Nodule, which improved radiologists\/non-radiologists' nodule detection sensitivity by 0.100\/0.131, respectively, while maintaining specificity.","356":"This is a labor of the Learning Community cohort that was convened by MAIEI in Winter 2021 to work through and discuss important research issues in the field of AI ethics from a multidisciplinary lens. The community came together supported by facilitators from the MAIEI staff to vigorously debate and explore the nuances of issues like bias, privacy, disinformation, accountability, and more especially examining them from the perspective of industry, civil society, academia, and government. The outcome of these discussions is reflected in the report that you are reading now - an exploration of a variety of issues with deep-dive, critical commentary on what has been done, what worked and what didn't, and what remains to be done so that we can meaningfully move forward in addressing the societal challenges posed by the deployment of AI systems. The chapters titled\"Design and Techno-isolationism\",\"Facebook and the Digital Divide: Perspectives from Myanmar, Mexico, and India\",\"Future of Work\", and\"Media&Communications&Ethical Foresight\"will hopefully provide with you novel lenses to explore this domain beyond the usual tropes that are covered in the domain of AI ethics.","357":"Combining microfluidic devices with nuclear magnetic resonance (NMR) has the potential of unlocking their vast sample handling and processing operation space for use with the powerful analytics provided by NMR. One particularly challenging class of integrated functional elements from the perspective of NMR are conductive structures. Metallic electrodes could be used for electrochemical sample interaction for example, yet they can cause severe NMR spectral degradation. In this study, a combination of simulation and experimental validation was used to identify an electrode geometry that, in terms of NMR spectral parameters, performs as well as for the case when no electrodes are present. By placing the metal tracks in the side-walls of a microfluidic channel, we found that NMR RF excitation performance was actually enhanced, without compromising $B_0$ homogeneity. Monitoring in situ deposition of chitosan in the microfluidic platform is presented as a proof-of-concept demonstration of NMR characterisation of an electrochemical process.","358":"In this paper, we concern with the problem of how to automatically extract the steps that compose real-life hand activities. This is a key competence towards processing, monitoring and providing video guidance in Mixed Reality systems. We use egocentric vision to observe hand-object interactions in real-world tasks and automatically decompose a video into its constituent steps. Our approach combines hand-object interaction (HOI) detection, object similarity measurement and a finite state machine (FSM) representation to automatically edit videos into steps. We use a combination of Convolutional Neural Networks (CNNs) and the FSM to discover, edit cuts and merge segments while observing real hand activities. We evaluate quantitatively and qualitatively our algorithm on two datasets: the GTEA\\cite{li2015delving}, and a new dataset we introduce for Chinese Tea making. Results show our method is able to segment hand-object interaction videos into key step segments with high levels of precision.","359":"Information sharing on social media must be accompanied by attentive behavior so that in a distorted digital environment, users are not rushed and distracted in deciding to share information. The spread of misinformation, especially those related to the COVID-19, can divide and create negative effects of falsehood in society. Individuals can also cause feelings of fear, health anxiety, and confusion in the treatment COVID-19. Although much research has focused on understanding human judgment from a psychological underline, few have addressed the essential issue in the screening phase of what technology can interfere amidst users' attention in sharing information. This research aims to intervene in the user's attention with a visual selective attention approach. This study uses a quantitative method through studies 1 and 2 with pre-and post-intervention experiments. In study 1, we intervened in user decisions and attention by stimulating ten information and misinformation using the Visual Selective Attention System (VSAS) tool. In Study 2, we identified associations of user tendencies in evaluating information using the Implicit Association Test (IAT). The significant results showed that the user's attention and decision behavior improved after using the VSAS. The IAT results show a change in the association of user exposure, where after the intervention using VSAS, users tend not to share misinformation about COVID-19. The results are expected to be the basis for developing social media applications to combat the negative impact of the infodemic COVID-19 misinformation.","360":"The IIT Kanpur Consulting Group is one of the pioneering research groups in India which focuses on the applications of Machine Learning and Strategy Consulting for social good. The group has been working since 2018 to help social organizations, nonprofits, and government entities in India leverage better insights from their data, with a special emphasis on the healthcare, environmental, and agriculture sectors. The group has worked on critical social problems which India is facing including Polio recurrence, COVID-19, air pollution and agricultural crop damage. This position paper summarises the focus areas and relevant projects which the group has worked on since its establishment, and also highlights the group's plans for using machine learning to address social problems during the COVID-19 crisis.","361":"While attention is a predictor for digital asset prices, and jumps in Bitcoin prices are well-known, we know little about its alternatives. Studying high frequency crypto data gives us the unique possibility to confirm that cross market digital asset returns are driven by high frequency jumps clustered around black swan events, resembling volatility and trading volume seasonalities. Regressions show that intra-day jumps significantly influence end of day returns in size and direction. This provides fundamental research for crypto option pricing models. However, we need better econometric methods for capturing the specific market microstructure of cryptos. All calculations are reproducible via the quantlet.com technology.","362":"Electrical detection of messenger ribonucleic acid (mRNA) is a promising approach to enhancing transcriptomics and disease diagnostics because of its sensitivity, rapidity, and modularity. Reported here is a fast SARS-CoV-2 mRNA biosensor (<1 minute) with a limit of detection of 1 aM, and dynamic range of 4 orders of magnitude and a linear sensitivity of 22 mV per molar decade. These figures of merit were obtained on photoresistlessly patterned monolayer graphene field-effect transistors (FETs) derived from commercial four-inch graphene on 90 nm of silicon dioxide on p-type silicon. Then, to facilitate mRNA hybridization, graphene sensing mesa were coated with an ultrathin sub-percolation threshold gold film for bonding 3'-thiolated single-stranded deoxyribonucleic acid (ssDNA) probes complementary to SARS-CoV-2 nucleocapsid phosphoprotein (N) gene. Sub-percolated gold was used to minimize the distance between the graphene material and surface hybridization events. The liquid-transfer characteristics of the graphene FETs repeatedly shows correlation between the Dirac voltage and the copy number of polynucleotide. Ultrathin percolated gold films on graphene FETs facilitate two-dimensional electron gas (2DEG) mRNA biosensors for transcriptomic profiling.","363":"The performance of face recognition systems can be negatively impacted in the presence of masks and other types of facial coverings that have become prevalent due to the COVID-19 pandemic. In such cases, the periocular region of the human face becomes an important biometric cue. In this article, we present a detailed review of periocular biometrics. We first examine the various face and periocular techniques specially designed to recognize humans wearing a face mask. Then, we review different aspects of periocular biometrics: (a) the anatomical cues present in the periocular region useful for recognition, (b) the various feature extraction and matching techniques developed, (c) recognition across different spectra, (d) fusion with other biometric modalities (face or iris), (e) recognition on mobile devices, (f) its usefulness in other applications, (g) periocular datasets, and (h) competitions organized for evaluating the efficacy of this biometric modality. Finally, we discuss various challenges and future directions in the field of periocular biometrics.","364":"In the past few years, the use of new technologies and digital, educational material has been increasing. Owing to the situation brought about in Spain by the SARS-CoV-2 (COVID-19) pandemic, schools have closed and families were asked to confine at home since March 2020. During this period there has been a need of resources to be made available to the families and teachers so that they would be able to continue their teaching practice. Consequently, this study analyzes the importance of online methodologies and usage tendency of an educational resource example: The Smile and Learn platform. Thereby, the study presents the different models implemented to support education and its impact in the use of the platform. The analyzed outcomes and their effect on education are discussed.","365":"Covid-19 pandemic generated many problems and show other hidden issues in countries in South America. Every government analyzed his own context and decided which health policies would be used. Peru is a country in the middle of South America region, the first reported case was on March 6. Besides, a lockdown was established in ground borders, sea and air. Peruvian government analyzed the context and proposed many policies around health, economy, employment, transport. But, these action were not enough for the existence of previous lack of infrastructure in hospitals, as result of past governments. By the other hand, a variety of politic parties in the Parliament and their search for own interests, was evidenced during this pandemic period. Considering previous condition of lack of success in health, economic policies, the discussion about possible impeachment started. Therefore, this work has the main aim of finding evidence about what users were talking about and what was the impact on Peruvian population using Twitter.","366":"We propose UWash, an intelligent solution upon smartwatches, to assess handwashing for the purpose of raising users' awareness and cultivating habits in high-quality handwashing. UWash can identify the onset\/offset of handwashing, measure the duration of each gesture, and score each gesture as well as the entire procedure in accordance with the WHO guidelines. Technically, we address the task of handwashing assessment as the semantic segmentation problem in computer vision, and propose a lightweight UNet-like network, only 496KBits, to achieve it effectively. Experiments over 51 subjects show that UWash achieves the accuracy of 92.27\\% on sample-wise handwashing gesture recognition, $<$0.5 \\textit{seconds} error in onset\/offset detection, and $<$5 out of 100 \\textit{points} error in scoring in the user-dependent setting, while remains promising in the cross-user evaluation and in the cross-user-cross-location evaluation.","367":"During its history, the ultimate goal of economics has been to develop similar frameworks for modeling economic behavior as invented in physics. This has not been successful, however, and current state of the process is the neoclassical framework that bases on static optimization. By using a static framework, however, we cannot model and forecast the time paths of economic quantities because for a growing firm or a firm going into bankruptcy, a positive profit maximizing flow of production does not exist. Due to these problems, we present a dynamic theory for the production of a profit-seeking firm where the adjustment may be stable or unstable. This is important, currently, because we should be able to forecast the possible future bankruptcies of firms due to the Covid-19 pandemic. By using the model, we can solve the time moment of bankruptcy of a firm as a function of several parameters. The proposed model is mathematically identical with Newtonian model of a particle moving in a resisting medium, and so the model explains the reasons that stop the motion too. The frameworks for modeling dynamic events in physics are thus applicable in economics, and we give reasons why physics is more important for the development of economics than pure mathematics. (JEL D21, O12) Keywords: Limitations of neoclassical framework, Dynamics of production, Economic force, Connections between economics and physics.","368":"Inspired by studies on the overwhelming presence of experience-sharing in human-human conversations, Emora, the social chatbot developed by Emory University, aims to bring such experience-focused interaction to the current field of conversational AI. The traditional approach of information-sharing topic handlers is balanced with a focus on opinion-oriented exchanges that Emora delivers, and new conversational abilities are developed that support dialogues that consist of a collaborative understanding and learning process of the partner's life experiences. We present a curated dialogue system that leverages highly expressive natural language templates, powerful intent classification, and ontology resources to provide an engaging and interesting conversational experience to every user.","369":"Human population growth has driven rising demand for food that has, in turn, imposed huge impacts on the environment. In an effort to reconcile our need to produce more sustenance while also protecting the ecosystems of the world, farming is becoming more reliant on smart tools and communication technologies. Developing a smart farming framework allows farmers to make more efficient use of inputs, thus protecting water quality and biodiversity habitat. Internet of Things (IoT), which has revolutionized every sphere of the economy, is being applied to agriculture by connecting on-farm devices and providing real-time monitoring of everything from environmental conditions to market signals through to animal health data. However, utilizing IoT means farming networks are now vulnerable to malicious activities, mostly when wireless communications are highly employed. With that in mind, this research aims to review different utilized communication technologies in smart farming. Moreover, possible cyber attacks are investigated to discover the vulnerabilities of communication technologies considering the most frequent cyber-attacks that have been happened.","370":"The burden of depression and anxiety in the world is rising. Identification of individuals at increased risk of developing these conditions would help to target them for prevention and ultimately reduce the healthcare burden. We developed a 10-year predictive algorithm for depression and anxiety using the full cohort of over 400,000 UK Biobank (UKB) participants without pre-existing depression or anxiety using digitally obtainable information. From the initial 204 variables selected from UKB, processed into>520 features, iterative backward elimination using Cox proportional hazards model was performed to select predictors which account for the majority of its predictive capability. Baseline and reduced models were then trained for depression and anxiety using both Cox and DeepSurv, a deep neural network approach to survival analysis. The baseline Cox model achieved concordance of 0.813 and 0.778 on the validation dataset for depression and anxiety, respectively. For the DeepSurv model, respective concordance indices were 0.805 and 0.774. After feature selection, the depression model contained 43 predictors and the concordance index was 0.801 for both Cox and DeepSurv. The reduced anxiety model, with 27 predictors, achieved concordance of 0.770 in both models. The final models showed good discrimination and calibration in the test datasets.We developed predictive risk scores with high discrimination for depression and anxiety using the UKB cohort, incorporating predictors which are easily obtainable via smartphone. If deployed in a digital solution, it would allow individuals to track their risk, as well as provide some pointers to how to decrease it through lifestyle changes.","371":"Governments and researchers around the world are implementing digital contact tracing solutions to stem the spread of infectious disease, namely COVID-19. Many of these solutions threaten individual rights and privacy. Our goal is to break past the false dichotomy of effective versus privacy-preserving contact tracing. We offer an alternative approach to assess and communicate users' risk of exposure to an infectious disease while preserving individual privacy. Our proposal uses recent GPS location histories, which are transformed and encrypted, and a private set intersection protocol to interface with a semi-trusted authority. There have been other recent proposals for privacy-preserving contact tracing, based on Bluetooth and decentralization, that could further eliminate the need for trust in authority. However, solutions with Bluetooth are currently limited to certain devices and contexts while decentralization adds complexity. The goal of this work is two-fold: we aim to propose a location-based system that is more privacy-preserving than what is currently being adopted by governments around the world, and that is also practical to implement with the immediacy needed to stem a viral outbreak.","372":"Female researchers may have experienced more difficulties than their male counterparts since the COVID-19 outbreak because of gendered housework and childcare. Using Microsoft Academic Graph data from 2016 to 2020, this study examined how the proportion of female authors in academic journals on a global scale changed in 2020 (net of recent yearly trends). We observed a decrease in research productivity for female researchers in 2020, mostly as first authors, followed by last author position. Female researchers were not necessarily excluded from but were marginalised in research. We also identified various factors that amplified the gender gap by dividing the authors' backgrounds into individual, organisational and national characteristics. Female researchers were more vulnerable when they were in their mid-career, affiliated to the least influential organisations, and more importantly from less gender-equal countries with higher mortality and restricted mobility as a result of COVID-19.","373":"Pre-trained language models (PLMs) have been the de facto paradigm for most natural language processing (NLP) tasks. This also benefits biomedical domain: researchers from informatics, medicine, and computer science (CS) communities propose various PLMs trained on biomedical datasets, e.g., biomedical text, electronic health records, protein, and DNA sequences for various biomedical tasks. However, the cross-discipline characteristics of biomedical PLMs hinder their spreading among communities; some existing works are isolated from each other without comprehensive comparison and discussions. It expects a survey that not only systematically reviews recent advances of biomedical PLMs and their applications but also standardizes terminology and benchmarks. In this paper, we summarize the recent progress of pre-trained language models in the biomedical domain and their applications in biomedical downstream tasks. Particularly, we discuss the motivations and propose a taxonomy of existing biomedical PLMs. Their applications in biomedical downstream tasks are exhaustively discussed. At last, we illustrate various limitations and future trends, which we hope can provide inspiration for the future research of the research community.","374":"We propose VoIPLoc, a novel location fingerprinting technique and apply it to the VoIP call provenance problem. It exploits echo-location information embedded within VoIP audio to support fine-grained location inference. We found consistent statistical features induced by the echo-reflection characteristics of the location into recorded speech. These features are discernible within traces received at the VoIP destination, enabling location inference. We evaluated VoIPLoc by developing a dataset of audio traces received through VoIP channels over the Tor network. We show that recording locations can be fingerprinted and detected remotely with a low false-positive rate, even when a majority of the audio samples are unlabelled. Finally, we note that the technique is fully passive and thus undetectable, unlike prior art. VoIPLoc is robust to the impact of environmental noise and background sounds, as well as the impact of compressive codecs and network jitter. The technique is also highly scalable and offers several degrees of freedom terms of the fingerprintable space.","375":"Modern Bayesian approaches and workflows emphasize in how simulation is important in the context of model developing. Simulation can help researchers understand how the model behaves in a controlled setting and can be used to stress the model in different ways before it is exposed to any real data. This improved understanding could be beneficial in epidemiological models, specially when dealing with COVID-19. Unfortunately, few researchers perform any simulations. We present a simulation algorithm that implements a simple agent-based model for disease transmission that works with a standard compartment epidemiological model for COVID-19. Our algorithm can be applied in different parameterizations to reflect several plausible epidemic scenarios. Additionally, we also model how social media information in the form of daily symptom mentions can be incorporate into COVID-19 epidemiological models. We test our social media COVID-19 model with two experiments. The first using simulated data from our agent-based simulation algorithm and the second with real data using a machine learning tweet classifier to identify tweets that mention symptoms from noise. Our results shows how a COVID-19 model can be (1) used to incorporate social media data and (2) assessed and evaluated with simulated and real data.","376":"Throughout the Covid-19 pandemic, a significant amount of effort had been put into developing techniques that predict the number of infections under various assumptions about the public policy and non-pharmaceutical interventions. While both the available data and the sophistication of the AI models and available computing power exceed what was available in previous years, the overall success of prediction approaches was very limited. In this paper, we start from prediction algorithms proposed for XPrize Pandemic Response Challenge and consider several directions that might allow their improvement. Then, we investigate their performance over medium-term predictions extending over several months. We find that augmenting the algorithms with additional information about the culture of the modeled region, incorporating traditional compartmental models and up-to-date deep learning architectures can improve the performance for short term predictions, the accuracy of medium-term predictions is still very low and a significant amount of future research is needed to make such models a reliable component of a public policy toolbox.","377":"Parking demand forecasting and behaviour analysis have received increasing attention in recent years because of their critical role in mitigating traffic congestion and understanding travel behaviours. However, previous studies usually only consider temporal dependence but ignore the spatial correlations among parking lots for parking prediction. This is mainly due to the lack of direct physical connections or observable interactions between them. Thus, how to quantify the spatial correlation remains a significant challenge. To bridge the gap, in this study, we propose a spatial-aware parking prediction framework, which includes two steps, i.e. spatial connection graph construction and spatio-temporal forecasting. A case study in Ningbo, China is conducted using parking data of over one million records before and during COVID-19. The results show that the approach is superior on parking occupancy forecasting than baseline methods, especially for the cases with high temporal irregularity such as during COVID-19. Our work has revealed the impact of the pandemic on parking behaviour and also accentuated the importance of modelling spatial dependence in parking behaviour forecasting, which can benefit future studies on epidemiology and human travel behaviours.","378":"The COVID-19 pandemic has by-and-large prevented in-person meetings since March 2020. While the increasing deployment of effective vaccines around the world is a very positive development, the timeline and pathway to\"normality\"is uncertain and the\"new normal\"we will settle into is anyone's guess. Particle physics, like many other scientific fields, has more than a year of experience in holding virtual meetings, workshops, and conferences. A great deal of experimentation and innovation to explore how to execute these meetings effectively has occurred. Therefore, it is an appropriate time to take stock of what we as a community learned from running virtual meetings and discuss possible strategies for the future. Continuing to develop effective strategies for meetings with a virtual component is likely to be important for reducing the carbon footprint of our research activities, while also enabling greater diversity and inclusion for participation. This report summarizes a virtual two-day workshop on Virtual Meetings held May 5-6, 2021 which brought together experts from both inside and outside of high-energy physics to share their experiences and practices with organizing and executing virtual workshops, and to develop possible strategies for future meetings as we begin to emerge from the COVID-19 pandemic. This report outlines some of the practices and tools that have worked well which we hope will serve as a valuable resource for future virtual meeting organizers in all scientific fields.","379":"This work sought to find out the effectiveness of Anambra Broadcasting Service (ABS) Radio news on teaching and learning. The study focused mainly on listeners of ABS radio news broadcast in Awka, the capital of Anambra State, Nigeria. Its objectives were to find out; if Awka based students are exposed to ABS radio; to discover the ABS radio program students favorite; the need gratification that drives students to listen to ABS radio news; the contributions of radio news to students teaching and learning; and effectiveness of ABS radio news on teaching and learning in Awka. The population of Awka students is 198,868. This is also the population of the study. But a sample size of 400 was chosen and administered with questionnaires. The study was hinged on the uses and gratification theory. It adopted a survey research design. The data gathered was analyzed using simple percentages and frequency of tables. The study revealed that news is very effective in teaching and learning. It was concluded that news is the best instructional media to be employed in teaching and learning. Among other things, it was recommended that teachers and students should listen to and make judicious use of news for academic purposes.","380":"The role of software in society has changed drastically since the start of the 21st century. Software can now partially or fully facilitate anything from diagnosis to treatment of a disease, regardless of whether it is psychological or pathological, with the consequence of software being comparable to any other type of medical equipment, and this makes discovering when software must comply with such rules vital to both manufacturers and regulators. In lieu of the Medical Device Regulation we expand on the idea of intention, and identify the criteria software must fulfil to be considered medical devices within EU-law.","381":"This workshop is the fourth issue of a series of workshops on automatic extraction of socio-political events from news, organized by the Emerging Market Welfare Project, with the support of the Joint Research Centre of the European Commission and with contributions from many other prominent scholars in this field. The purpose of this series of workshops is to foster research and development of reliable, valid, robust, and practical solutions for automatically detecting descriptions of socio-political events, such as protests, riots, wars and armed conflicts, in text streams. This year workshop contributors make use of the state-of-the-art NLP technologies, such as Deep Learning, Word Embeddings and Transformers and cover a wide range of topics from text classification to news bias detection. Around 40 teams have registered and 15 teams contributed to three tasks that are i) multilingual protest news detection, ii) fine-grained classification of socio-political events, and iii) discovering Black Lives Matter protest events. The workshop also highlights two keynote and four invited talks about various aspects of creating event data sets and multi- and cross-lingual machine learning in few- and zero-shot settings.","382":"Mental health conditions remain underdiagnosed even in countries with common access to advanced medical care. The ability to accurately and efficiently predict mood from easily collectible data has several important implications for the early detection, intervention, and treatment of mental health disorders. One promising data source to help monitor human behavior is daily smartphone usage. However, care must be taken to summarize behaviors without identifying the user through personal (e.g., personally identifiable information) or protected (e.g., race, gender) attributes. In this paper, we study behavioral markers of daily mood using a recent dataset of mobile behaviors from adolescent populations at high risk of suicidal behaviors. Using computational models, we find that language and multimodal representations of mobile typed text (spanning typed characters, words, keystroke timings, and app usage) are predictive of daily mood. However, we find that models trained to predict mood often also capture private user identities in their intermediate representations. To tackle this problem, we evaluate approaches that obfuscate user identity while remaining predictive. By combining multimodal representations with privacy-preserving learning, we are able to push forward the performance-privacy frontier.","383":"Transformers have dominated the field of natural language processing, and recently impacted the computer vision area. In the field of medical image analysis, Transformers have also been successfully applied to full-stack clinical applications, including image synthesis\/reconstruction, registration, segmentation, detection, and diagnosis. Our paper presents both a position paper and a primer, promoting awareness and application of Transformers in the field of medical image analysis. Specifically, we first overview the core concepts of the attention mechanism built into Transformers and other basic components. Second, we give a new taxonomy of various Transformer architectures tailored for medical image applications and discuss their limitations. Within this review, we investigate key challenges revolving around the use of Transformers in different learning paradigms, improving the model efficiency, and their coupling with other techniques. We hope this review can give a comprehensive picture of Transformers to the readers in the field of medical image analysis.","384":"The new coronavirus known as COVID-19 is rapidly spreading since December 2019. Without any vaccination or medicine, the means of controlling it are limited to quarantine and social distancing. Here we study the spatio-temporal propagation of the COVID-19 virus in China and compare it to other global locations. Our results suggest that the disease propagation is highly related to population migration from Hubei resembling a L\\'{e}vy flight which is characteristic of human mobility. Our results also suggest that the disease spread in a city in China is characterized by two-stages process. At early times, at order of few days, the infection rate in the city is almost constant probably due to the lack of means to detect infected individuals before infection signs are observed and at later times it decays approximately exponentially due to quarantines. These two stages can explain the significant differences between the propagation in China and in other world-wide locations. While most Chinese cities control the disease which resulted in the decaying stage, in other world-wide countries the situation is still becoming worse probably due to less control.","385":"The unprecedented global crisis brought about by the COVID-19 pandemic has sparked numerous efforts to create predictive models for the detection and prognostication of SARS-CoV-2 infections with the goal of helping health systems allocate resources. Machine learning models, in particular, hold promise for their ability to leverage patient clinical information and medical images for prediction. However, most of the published COVID-19 prediction models thus far have little clinical utility due to methodological flaws and lack of appropriate validation. In this paper, we describe our methodology to develop and validate multi-modal models for COVID-19 mortality prediction using multi-center patient data. The models for COVID-19 mortality prediction were developed using retrospective data from Madrid, Spain (N=2547) and were externally validated in patient cohorts from a community hospital in New Jersey, USA (N=242) and an academic center in Seoul, Republic of Korea (N=336). The models we developed performed differently across various clinical settings, underscoring the need for a guided strategy when employing machine learning for clinical decision-making. We demonstrated that using features from both the structured electronic health records and chest X-ray imaging data resulted in better 30-day-mortality prediction performance across all three datasets (areas under the receiver operating characteristic curves: 0.85 (95% confidence interval: 0.83-0.87), 0.76 (0.70-0.82), and 0.95 (0.92-0.98)). We discuss the rationale for the decisions made at every step in developing the models and have made our code available to the research community. We employed the best machine learning practices for clinical model development. Our goal is to create a toolkit that would assist investigators and organizations in building multi-modal models for prediction, classification and\/or optimization.","386":"Named after Clio, the Greek goddess of history, cliophysics is a daughter (and in a sense an extension) of econophysics. Like econophysics it relies on the methodology of experimental physics. Its purpose is to conduct a scientific analysis of historical events. Such events can be of sociological, political or economic nature. In this last case cliophysics would coincide with econophysics. The main difference between cliophysics and econophysics is that the description of historical events may be qualitative as well as quantitative. For the handling of qualitative accounts cliophysics has developed an approach based on the identification of patterns. To detect a pattern the main challenge is to break the\"noise barrier\". The very existence of patterns is what makes cliophysics possible and ensures its success. Briefly stated, once a pattern is detected, it allows predictions to be made. As the capacity to make successful predictions is the hallmark of any science, it becomes easy to decide whether or not the claim made in the title of the paper is indeed fulfilled. A number of examples of clusters of similar events will be given which should convince readers that historical events can be simplified almost at will very much as in physics. One should not forget that physical effects are also subject to the environment. For instance, if tried at the equator, the experiment of the Foucault pendulum will fail. In the last part of the paper, we describe cliophysical investigations conducted over the past decades; they make us confident that cliophysics can be a valuable tool for decision makers.","387":"We develop the basic tools for classifying edge-to-edge tilings of the sphere by congruent quadrilaterals. Then we classify the case with 3 distinct edge lengths: such tilings are a sequence of two-parameter families of 2-layer earth map tilings with 2n (n>=3) tiles, a one-parameter family of quadrilateral subdivisions of the octahedron with 24 tiles together with a flip modification in one special case, and a sequence of 3-layer earth map tilings with 8n (n>=2) tiles together with two flip modifications when n is odd. We also describe the moduli and calculate the geometric data.","388":"Closed-form, analytical approximations for electrostatic properties of molecules are of unique value, as these can provide computational speed, versatility, and physical insight. Here, we derive a simple, closed-form formula for the apparent surface charge (ASC), as well as for the electric field, generated by a molecular charge distribution in aqueous solution. The approximation, with no fitted parameters, is tested against numerical solutions of the Poisson equation, where it yields a significant speed-up. For neutral small molecules, the hydration free energies estimated from the closed-form ASC formula are within 0.8 kcal\/mol RMSD from the numerical Poisson reference; the electric field at the surface is in quantitative agreement with the reference. Performance of the approximation is also tested on larger structures, including a protein, a DNA fragment, and a viral receptor-target complex. For all structures tested, a near quantitative agreement with the numerical Poisson reference is achieved, except in regions of high negative curvature, where the new approximation is still qualitatively correct. A unique efficiency feature of the proposed\"source-based\"closed-form approximation is that the ASC and electric field can be estimated individually at any point or surface patch, without the need to obtain the full global solution. An open source software implementation of the method is available: http:\/\/people.cs.vt.edu\/~onufriev\/CODES\/aasc.zip","389":"The probability prediction of multivariate time series is a notoriously challenging but practical task. On the one hand, the challenge is how to effectively capture the cross-series correlations between interacting time series, to achieve accurate distribution modeling. On the other hand, we should consider how to capture the contextual information within time series more accurately to model multivariate temporal dynamics of time series. In this work, we proposed a novel non-autoregressive deep learning model, called Multi-scale Attention Normalizing Flow(MANF), where we integrate multi-scale attention and relative position information and the multivariate data distribution is represented by the conditioned normalizing flow. Additionally, compared with autoregressive modeling methods, our model avoids the influence of cumulative error and does not increase the time complexity. Extensive experiments demonstrate that our model achieves state-of-the-art performance on many popular multivariate datasets.","390":"Health-policy planning requires evidence on the burden that epidemics place on healthcare systems. Multiple, often dependent, datasets provide a noisy and fragmented signal from the unobserved epidemic process including transmission and severity dynamics. This paper explores important challenges to the use of state-space models for epidemic inference when multiple dependent datasets are analysed. We propose a new semi-stochastic model that exploits deterministic approximations for large-scale transmission dynamics while retaining stochasticity in the occurrence and reporting of relatively rare severe events. This model is suitable for many real-time situations including large seasonal epidemics and pandemics. Within this context, we develop algorithms to provide exact parameter inference and test them via simulation. Finally, we apply our joint model and the proposed algorithm to several surveillance data on the 2017-18 influenza epidemic in England to reconstruct transmission dynamics and estimate the daily new influenza infections as well as severity indicators as the case-hospitalisation risk and the hospital-intensive care risk.","391":"Motion degradation is a central problem in Magnetic Resonance Imaging (MRI). This work addresses the problem of how to obtain higher quality, super-resolved motion-free, reconstructions from highly undersampled MRI data. In this work, we present for the first time a variational multi-task framework that allows joining three relevant tasks in MRI: reconstruction, registration and super-resolution. Our framework takes a set of multiple undersampled MR acquisitions corrupted by motion into a novel multi-task optimisation model, which is composed of an $L^2$ fidelity term that allows sharing representation between tasks, super-resolution foundations and hyperelastic deformations to model biological tissue behaviors. We demonstrate that this combination yields to significant improvements over sequential models and other bi-task methods. Our results exhibit fine details and compensate for motion producing sharp and highly textured images compared to state of the art methods.","392":"Injecting external domain-specific knowledge (e.g., UMLS) into pretrained language models (LMs) advances their capability to handle specialised in-domain tasks such as biomedical entity linking (BEL). However, such abundant expert knowledge is available only for a handful of languages (e.g., English). In this work, by proposing a novel cross-lingual biomedical entity linking task (XL-BEL) and establishing a new XL-BEL benchmark spanning 10 typologically diverse languages, we first investigate the ability of standard knowledge-agnostic as well as knowledge-enhanced monolingual and multilingual LMs beyond the standard monolingual English BEL task. The scores indicate large gaps to English performance. We then address the challenge of transferring domain-specific knowledge in resource-rich languages to resource-poor ones. To this end, we propose and evaluate a series of cross-lingual transfer methods for the XL-BEL task, and demonstrate that general-domain bitext helps propagate the available English knowledge to languages with little to no in-domain data. Remarkably, we show that our proposed domain-specific transfer methods yield consistent gains across all target languages, sometimes up to 20 Precision@1 points, without any in-domain knowledge in the target language, and without any in-domain parallel data.","393":"The COVID-19 pandemic has led many governments to implement lockdowns. While lockdowns may help to contain the spread of the virus, it is possible that substantial damage to population well-being will result. This study relies on Google Trends data and tests whether the lockdowns implemented in Europe and America led to changes in well-being related topic search terms. Using different methods to evaluate the causal effects of lockdown, we find a substantial increase in the search intensity for boredom in Europe and the US. We also found a significant increase in searches for loneliness, worry and sadness, while searches for stress, suicide and divorce on the contrary fell. Our results suggest that people's mental health may have been severely affected by the lockdown.","394":"With the invention of the COVID-19 vaccine, shipping and distributing are crucial in controlling the pandemic. In this paper, we build a mean-field variational problem in a spatial domain, which controls the propagation of pandemic by the optimal transportation strategy of vaccine distribution. Here we integrate the vaccine distribution into the mean-field SIR model designed in our previous paper arXiv:2006.01249. Numerical examples demonstrate that the proposed model provides practical strategies in vaccine distribution on a spatial domain.","395":"Several R-packages to deal with spatiotemporal count areal data are currently available. Each package is based on different models and\/or statistical methodologies. Unfortunately, results generated by these models are rarely comparable due to differences in notation and methods which ultimately hinder the robustness and effectiveness of such models. These models have become crucial nowadays, because their ability to analyze COVID-19 related data. This prompted us to revisit three spatiotemporal count models and monitor their performance under the same conditions (on a same data set and under a consensus notation). The three different models assessed are representative of the main two extended methodologies used to analyse spatiotemporal count data i.e., the classical approach (based on Penalised Likelihood meanwhile and based to Estimating Equations) and the Bayesian point of view. As a case study, we have used these packages to analyze COVID-19 data corresponding to 24 health regions within the Valencian Community, Spain. In particular, daily COVID-19 positive individuals are used to predict daily hospitalisations. Data corresponds to the period from 28 of June to 13 of December 2020.","396":"The non-pharmaceutical interventions (NPIs), aimed at reducing the diffusion of the COVID-19 pandemic, has dramatically influenced our behaviour in everyday life. In this work, we study how individuals adapted their daily movements and person-to-person contact patterns over time in response to the COVID-19 pandemic and the NPIs. We leverage longitudinal GPS mobility data of hundreds of thousands of anonymous individuals in four US states and empirically show the dramatic disruption in people's life. We find that local interventions did not just impact the number of visits to different venues but also how people experience them. Individuals spend less time in venues, preferring simpler and more predictable routines and reducing person-to-person contact activities. Moreover, we show that the stringency of interventions alone does explain the number and duration of visits to venues: individual patterns of visits seem to be influenced by the local severity of the pandemic and a risk adaptation factor, which increases the people's mobility regardless of the stringency of interventions.","397":"We developed a noncontact measurement system for monitoring the respiration of multiple people using millimeter-wave array radar. To separate the radar echoes of multiple people, conventional techniques cluster the radar echoes in the time, frequency, or spatial domain. Focusing on the measurement of the respiratory signals of multiple people, we propose a method called respiratory-space clustering, in which individual differences in the respiratory rate are effectively exploited to accurately resolve the echoes from human bodies. The proposed respiratory-space clustering can separate echoes, even when people are located close to each other. In addition, the proposed method can be applied when the number of targets is unknown and can accurately estimate the number and positions of people. We perform multiple experiments involving five or seven participants to verify the performance of the proposed method, and quantitatively evaluate the estimation accuracy for the number of people and the respiratory intervals. The experimental results show that the average root-mean-square error in estimating the respiratory interval is 196 ms using the proposed method. The use of the proposed method, rather the conventional method, improves the accuracy of the estimation of the number of people by 85.0%, which indicates the effectiveness of the proposed method for the measurement of the respiration of multiple people.","398":"Following the pandemic outbreak, several works have proposed to diagnose COVID-19 with deep learning in computed tomography (CT); reporting performance on-par with experts. However, models trained\/tested on the same in-distribution data may rely on the inherent data biases for successful prediction, failing to generalize on out-of-distribution samples or CT with different scanning protocols. Early attempts have partly addressed bias-mitigation and generalization through augmentation or re-sampling, but are still limited by collection costs and the difficulty of quantifying bias in medical images. In this work, we propose Mixing-AdaSIN; a bias mitigation method that uses a generative model to generate de-biased images by mixing texture information between different labeled CT scans with semantically similar features. Here, we use Adaptive Structural Instance Normalization (AdaSIN) to enhance de-biasing generation quality and guarantee structural consistency. Following, a classifier trained with the generated images learns to correctly predict the label without bias and generalizes better. To demonstrate the efficacy of our method, we construct a biased COVID-19 vs. bacterial pneumonia dataset based on CT protocols and compare with existing state-of-the-art de-biasing methods. Our experiments show that classifiers trained with de-biased generated images report improved in-distribution performance and generalization on an external COVID-19 dataset.","399":"The SARS-CoV-2 pandemic has created a global race for a cure. One approach focuses on designing a novel variant of the human angiotensin-converting enzyme 2 (ACE2) that binds more tightly to the SARS-CoV-2 spike protein and diverts it from human cells. Here we formulate a novel protein design framework as a reinforcement learning problem. We generate new designs efficiently through the combination of a fast, biologically-grounded reward function and sequential action-space formulation. The use of Policy Gradients reduces the compute budget needed to reach consistent, high-quality designs by at least an order of magnitude compared to standard methods. Complexes designed by this method have been validated by molecular dynamics simulations, confirming their increased stability. This suggests that combining leading protein design methods with modern deep reinforcement learning is a viable path for discovering a Covid-19 cure and may accelerate design of peptide-based therapeutics for other diseases.","400":"The Covid-19 pandemic has caused a spur in the medical research literature. With new research advances in understanding the virus, there is a need for robust text mining tools which can process, extract and present answers from the literature in a concise and consumable way. With a DialoGPT based multi-turn conversation generation module, and BM-25 \\&neural embeddings based ensemble information retrieval module, in this paper we present a conversational system, which can retrieve and answer coronavirus-related queries from the rich medical literature, and present it in a conversational setting with the user. We further perform experiments to compare neural embedding-based document retrieval and the traditional BM25 retrieval algorithm and report the results.","401":"The COVID-19 pandemic has highlighted the urgency for developing more efficient molecular discovery pathways. As exhaustive exploration of the vast chemical space is infeasible, discovering novel inhibitor molecules for emerging drug-target proteins is challenging, particularly for targets with unknown structure or ligands. We demonstrate the broad utility of a single deep generative framework toward discovering novel drug-like inhibitor molecules against two distinct SARS-CoV-2 targets -- the main protease (Mpro) and the receptor binding domain (RBD) of the spike protein. To perform target-aware design, the framework employs a target sequence-conditioned sampling of novel molecules from a generative model. Micromolar-level in vitro inhibition was observed for two candidates (out of four synthesized) for each target. The most potent spike RBD inhibitor also emerged as a rare non-covalent antiviral with broad-spectrum activity against several SARS-CoV-2 variants in live virus neutralization assays. These results show that a broadly deployable machine intelligence framework can accelerate hit discovery across different emerging drug-targets.","402":"Conversation agents, commonly referred to as chatbots, are increasingly deployed in many domains to allow people to have a natural interaction while trying to solve a specific problem. Given their widespread use, it is important to provide their users with methods and tools to increase users awareness of various properties of the chatbots, including non-functional properties that users may consider important in order to trust a specific chatbot. For example, users may want to use chatbots that are not biased, that do not use abusive language, that do not leak information to other users, and that respond in a style which is appropriate for the user's cognitive level. In this paper, we address the setting where a chatbot cannot be modified, its training data cannot be accessed, and yet a neutral party wants to assess and communicate its trustworthiness to a user, tailored to the user's priorities over the various trust issues. Such a rating can help users choose among alternative chatbots, developers test their systems, business leaders price their offering, and regulators set policies. We envision a personalized rating methodology for chatbots that relies on separate rating modules for each issue, and users' detected priority orderings among the relevant trust issues, to generate an aggregate personalized rating for the trustworthiness of a chatbot. The method is independent of the specific trust issues and is parametric to the aggregation procedure, thereby allowing for seamless generalization. We illustrate its general use, integrate it with a live chatbot, and evaluate it on four dialog datasets and representative user profiles, validated with user surveys.","403":"This paper focuses on and analyzes realistic SIR models that take stochasticity into account. The proposed systems are applicable to most incidence rates that are used in the literature including the bilinear incidence rate, the Beddington-DeAngelis incidence rate, and a Holling type II functional response. Given that many diseases can lead to asymptomatic infections, we look at a system of stochastic differential equations that also includes a class of hidden state individuals, for which the infection status is unknown. We assume that the direct observation of the percentage of hidden state individuals that are infected, $\\alpha(t)$, is not given and only a noise-corrupted observation process is available. Using the nonlinear filtering techniques in conjunction with an invasion type analysis (or analysis using Lyapunov exponents from the dynamical system point of view), this paper proves that the long-term behavior of the disease is governed by a threshold $\\lambda\\in \\mathbb{R}$ that depends on the model parameters. It turns out that if $\\lambda<0$ the number $I(t)$ of infected individuals converges to zero exponentially fast, or the extinction happens. In contrast, if $\\lambda>0$, the infection is endemic and the system is permanent. We showcase our results by applying them in specific illuminating examples. Numerical simulations are also given to illustrate our results.","404":"Social distancing and lockdown are the two main non-pharmaceutical interventions being used by the UK government to contain and control the COVID-19 epidemic; these are being applied uniformly across the entire country, even though the results of the Imperial College report by Ferguson et al show that the impact of the infection increases sharply with age. This paper develops a variant of the workhorse SIR model for epidemics, where the population is classified into a number of age groups. This allows us to understand the effects of age-dependent controls on the epidemic, and explore possible exit strategies.","405":"On garment intelligence influenced by artificial neural networks and neuromorphic computing is emerging as a research direction in the e-textile sector. In particular, bio inspired Spiking Neural Networks mimicking the workings of the brain show promise in recent ICT research applications. Taking such technological advancements and new research directions driving forward the next generation of e-textiles and smart materials, we present a wearable micro Brain capable of event driven artificial spiking neural network computation in a fabric based environment. We demonstrate a wearable Brain SNN prototype with multi-layer computation, enabling scalability and flexibility in terms of modifications for hidden layers to be augmented to the network. The wearable micro Brain provides a low size, weight and power artificial on-garment intelligent wearable solution with embedded functionality enabling offline adaptive learning through the provision of interchangeable resistor synaptic weightings. The prototype has been evaluated for fault tolerance, where we have determine the robustness of the circuit when certain parts are damaged. Validations were also conducted for movements to determine if the circuit can still perform accurate computation.","406":"The ability to estimate the current affective statuses of web users has considerable potential for the realization of user-centric services in the society. However, in real-world web services, it is difficult to determine the type of data to be used for such estimation, as well as collecting the ground truths of such affective statuses. We propose a novel method of such estimation based on the combined use of user web search queries and mobile sensor data. The system was deployed in our product server stack, and a large-scale data analysis with more than 11,000,000 users was conducted. Interestingly, our proposed\"Nation-wide Mood Score,\"which bundles the mood values of users across the country, (1) shows the daily and weekly rhythm of people's moods, (2) explains the ups and downs of people's moods in the COVID-19 pandemic, which is inversely synchronized to the number of new COVID-19 cases, and (3) detects the linkage with big news, which may affect many user's mood states simultaneously, even in a fine-grained time resolution, such as the order of hours.","407":"In recent years, individuals, business organizations or the country have paid more and more attention to their data privacy. At the same time, with the rise of federated learning, federated learning is involved in more and more fields. However, there is no good evaluation standard for each agent participating in federated learning. This paper proposes an online evaluation method for federated learning and compares it with the results obtained by Shapley Value in game theory. The method proposed in this paper is more sensitive to data quality and quantity.","408":"Humans ability to transfer knowledge through teaching is one of the essential aspects for human intelligence. A human teacher can track the knowledge of students to customize the teaching on students needs. With the rise of online education platforms, there is a similar need for machines to track the knowledge of students and tailor their learning experience. This is known as the Knowledge Tracing (KT) problem in the literature. Effectively solving the KT problem would unlock the potential of computer-aided education applications such as intelligent tutoring systems, curriculum learning, and learning materials' recommendation. Moreover, from a more general viewpoint, a student may represent any kind of intelligent agents including both human and artificial agents. Thus, the potential of KT can be extended to any machine teaching application scenarios which seek for customizing the learning experience for a student agent (i.e., a machine learning model). In this paper, we provide a comprehensive and systematic review for the KT literature. We cover a broad range of methods starting from the early attempts to the recent state-of-the-art methods using deep learning, while highlighting the theoretical aspects of models and the characteristics of benchmark datasets. Besides these, we shed light on key modelling differences between closely related methods and summarize them in an easy-to-understand format. Finally, we discuss current research gaps in the KT literature and possible future research and application directions.","409":"Fake news on social media has become a hot topic of research as it negatively impacts the discourse of real news in the public. Specifically, the ongoing COVID-19 pandemic has seen a rise of inaccurate and misleading information due to the surrounding controversies and unknown details at the beginning of the pandemic. The FakeNews task at MediaEval 2020 tackles this problem by creating a challenge to automatically detect tweets containing misinformation based on text and structure from Twitter follower network. In this paper, we present a simple approach that uses BERT embeddings and a shallow neural network for classifying tweets using only text, and discuss our findings and limitations of the approach in text-based misinformation detection.","410":"Data curation is the process of making a dataset fit-for-use and archiveable. It is critical to data-intensive science because it makes complex data pipelines possible, makes studies reproducible, and makes data (re)usable. Yet the complexities of the hands-on, technical and intellectual work of data curation is frequently overlooked or downplayed. Obscuring the work of data curation not only renders the labor and contributions of the data curators invisible; it also makes it harder to tease out the impact curators' work has on the later usability, reliability, and reproducibility of data. To better understand the specific work of data curation -- and thereby, explore ways of showing curators' impact -- we conducted a close examination of data curation at a large social science data repository, the Inter-university Consortium of Political and Social Research (ICPSR). We asked, What does curatorial work entail at ICPSR, and what work is more or less visible to different stakeholders and in different contexts? And, how is that curatorial work coordinated across the organization? We triangulate accounts of data curation from interviews and records of curation in Jira tickets to develop a rich and detailed account of curatorial work. We find that curators describe a number of craft practices needed to perform their work, which defies the rote sequence of events implied by many lifecycle or workflow models. Further, we show how best practices and craft practices are deeply intertwined.","411":"Early identification of college dropouts can provide tremendous value for improving student success and institutional effectiveness, and predictive analytics are increasingly used for this purpose. However, ethical concerns have emerged about whether including protected attributes in the prediction models discriminates against underrepresented student groups and exacerbates existing inequities. We examine this issue in the context of a large U.S. research university with both residential and fully online degree-seeking students. Based on comprehensive institutional records for this entire student population across multiple years, we build machine learning models to predict student dropout after one academic year of study, and compare the overall performance and fairness of model predictions with or without four protected attributes (gender, URM, first-generation student, and high financial need). We find that including protected attributes does not impact the overall prediction performance and it only marginally improves algorithmic fairness of predictions. While these findings suggest that including protected attributes is preferred, our analysis also offers guidance on how to evaluate the impact in a local context, where institutional stakeholders seek to leverage predictive analytics to support student success.","412":"We present the Latvian Twitter Eater Corpus - a set of tweets in the narrow domain related to food, drinks, eating and drinking. The corpus has been collected over time-span of over 8 years and includes over 2 million tweets entailed with additional useful data. We also separate two sub-corpora of question and answer tweets and sentiment annotated tweets. We analyse contents of the corpus and demonstrate use-cases for the sub-corpora by training domain-specific question-answering and sentiment-analysis models using data from the corpus.","413":"Data is the most powerful decision-making tool at our disposal. However, despite the exponentially growing volumes of data generated in the world, putting it to effective use still presents many challenges. Relevant data seems to be never there when it is needed - it remains siloed, hard to find, hard to access, outdated, and of bad quality. As a result, governments, institutions, and businesses remain largely impaired in their ability to make data-driven decisions. At the same time, data science is undergoing a reproducibility crisis. The results of the vast majority of studies cannot be replicated by other researchers, and provenance often cannot be established, even for data used in medical studies that affect lives of millions. We are losing our ability to collaborate at a time when significant improvements to data are badly needed. We believe that the fundamental reason lies in the modern data management processes being entirely at odds with the basic principles of collaboration and trust. Our field needs a fundamental shift of approach in how data is viewed, how it is shared and transformed. We must transition away from treating data as static, from exchanging it as anemic binary blobs, and instead focus on making multi-party data management more sustainable: such as reproducibility, verifiability, provenance, autonomy, and low latency. In this paper, we present the Open Data Fabric, a new decentralized data exchange and transformation protocol designed from the ground up to simplify data management and enable collaboration around data on a similar scale as currently seen in open-source software.","414":"The COVID-19 pandemic, caused by the severe acute respiratory syndrome coronavirus 2, emerged into a world being rapidly transformed by artificial intelligence (AI) based on big data, computational power and neural networks. The gaze of these networks has in recent years turned increasingly towards applications in healthcare. It was perhaps inevitable that COVID-19, a global disease propagating health and economic devastation, should capture the attention and resources of the world's computer scientists in academia and industry. The potential for AI to support the response to the pandemic has been proposed across a wide range of clinical and societal challenges, including disease forecasting, surveillance and antiviral drug discovery. This is likely to continue as the impact of the pandemic unfolds on the world's people, industries and economy but a surprising observation on the current pandemic has been the limited impact AI has had to date in the management of COVID-19. This correspondence focuses on exploring potential reasons behind the lack of successful adoption of AI models developed for COVID-19 diagnosis and prognosis, in front-line healthcare services. We highlight the moving clinical needs that models have had to address at different stages of the epidemic, and explain the importance of translating models to reflect local healthcare environments. We argue that both basic and applied research are essential to accelerate the potential of AI models, and this is particularly so during a rapidly evolving pandemic. This perspective on the response to COVID-19, may provide a glimpse into how the global scientific community should react to combat future disease outbreaks more effectively.","415":"Automatic processing of 3D Point Cloud data for object detection, tracking and segmentation is the latest trending research in the field of AI and Data Science, which is specifically aimed at solving different challenges of autonomous driving cars and getting real time performance. However, the amount of data that is being produced in the form of 3D point cloud (with LiDAR) is very huge, due to which the researchers are now on the way inventing new data compression algorithms to handle huge volumes of data thus generated. However, compression on one hand has an advantage in overcoming space requirements, but on the other hand, its processing gets expensive due to the decompression, which indents additional computing resources. Therefore, it would be novel to think of developing algorithms that can operate\/analyse directly with the compressed data without involving the stages of decompression and recompression (required as many times, the compressed data needs to be operated or analyzed). This research field is termed as Compressed Domain Processing. In this paper, we will quickly review few of the recent state-of-the-art developments in the area of LiDAR generated 3D point cloud data compression, and highlight the future challenges of compressed domain processing of 3D point cloud data.","416":"We present a modified diffusive epidemic process that has a finite threshold on scale-free graphs. The diffusive epidemic process describes the epidemic spreading in a non-sedentary population, and it is a reaction-diffusion process. In the diffusion stage, the individuals can jump between connected nodes, according to different diffusive rates for the infected and susceptible individuals. In the reaction stage, the contagion can happen if there is an infected individual sharing the same node, and infected individuals can spontaneously recover. Our main modification is to turn the number of individuals' interactions independent on the population size by using Gillespie algorithm with a reaction time $t_\\mathrm{max}$, exponentially distributed with mean inversely proportional to the node concentration. Our simulation results of the modified model on Barabasi-Albert networks are compatible with a continuous phase transition with a finite threshold from an absorbing phase to an active phase when increasing the concentration. The transition obeys the mean-field critical exponents of the order parameter, its fluctuations and the spatial correlation length, whose values are $\\beta=1$, $\\gamma'=0$ and $\\nu_\\perp=1\/2$, respectively. In addition, the system presents logarithmic corrections with pseudo-exponents $\\widehat{\\beta}=\\widehat{\\gamma}'=-3\/2$ on the order parameter and its fluctuations, respectively. The most evident implication of our simulation results is if the individuals avoid social interactions in order to not spread a disease, this leads the system to have a finite threshold in scale-free graphs, allowing for epidemic control.","417":"DBLP is the largest open-access repository of scientific articles on computer science and provides metadata associated with publications, authors, and venues. We retrieved more than 6 million publications from DBLP and extracted pertinent metadata (e.g., abstracts, author affiliations, citations) from the publication texts to create the DBLP Discovery Dataset (D3). D3 can be used to identify trends in research activity, productivity, focus, bias, accessibility, and impact of computer science research. We present an initial analysis focused on the volume of computer science research (e.g., number of papers, authors, research activity), trends in topics of interest, and citation patterns. Our findings show that computer science is a growing research field (approx. 15% annually), with an active and collaborative researcher community. While papers in recent years present more bibliographical entries in comparison to previous decades, the average number of citations has been declining. Investigating papers' abstracts reveals that recent topic trends are clearly reflected in D3. Finally, we list further applications of D3 and pose supplemental research questions. The D3 dataset, our findings, and source code are publicly available for research purposes.","418":"In recent years, programming has witnessed a shift towards using standard libraries as a black box. However, there has not been a synchronous development of tools that can help demonstrate the working of such libraries in general programs, which poses an impediment to improved learning outcomes and makes debugging exasperating. We introduce Eye, an interactive pedagogical tool that visualizes a program's execution as it runs. It demonstrates properties and usage of data structures in a general environment, thereby helping in learning, logical debugging, and code comprehension. Eye provides a comprehensive overview at each stage during run time including the execution stack and the state of data structures. The modular implementation allows for extension to other languages and modification of the graphics as desired. Eye opens up a gateway for CS2 students to more easily understand myriads of programs that are available on online programming websites, lowering the barrier towards self-learning of coding. It expands the scope of visualizing data structures from standard algorithms to general cases, benefiting both teachers as well as programmers who face issues in debugging. Line by line interpreting allows Eye to describe the execution and not only the current state. We also conduct experiments to evaluate the efficacy of Eye for debugging and comprehending a new piece of code. Our findings show that it becomes faster and less frustrating to debug certain problems using this tool, and also makes understanding new code a much more pleasant experience.","419":"The Reverse Transcription Polymerase Chain Reaction (RTPCR) test is the silver bullet diagnostic test to discern COVID infection. Rapid antigen detection is a screening test to identify COVID positive patients in little as 15 minutes, but has a lower sensitivity than the PCR tests. Besides having multiple standardized test kits, many people are getting infected&either recovering or dying even before the test due to the shortage and cost of kits, lack of indispensable specialists and labs, time-consuming result compared to bulk population especially in developing and underdeveloped countries. Intrigued by the parametric deviations in immunological&hematological profile of a COVID patient, this research work leveraged the concept of COVID-19 detection by proposing a risk-free and highly accurate Stacked Ensemble Machine Learning model to identify a COVID patient from communally available-widespread-cheap routine blood tests which gives a promising accuracy, precision, recall&F1-score of 100%. Analysis from R-curve also shows the preciseness of the risk-free model to be implemented. The proposed method has the potential for large scale ubiquitous low-cost screening application. This can add an extra layer of protection in keeping the number of infected cases to a minimum and control the pandemic by identifying asymptomatic or pre-symptomatic people early.","420":"Conspiracy theories, and suspicion in general, define us as human beings. Our suspicion and tendency to create conspiracy theories have always been with the human race, powered by our evolutionary drive to survive. Although this evolutionary drive to survive is helpful, it can often become extreme and lead to\"apophenia.\"Apophenia refers to the notion of connecting previously unconnected ideas and theories. Unlike learning, apophenia refers to a cognitive, paranoid disorder due to the unreality of the connections they make. Social networks allow people to connect in many ways. Besides communicating with a distant family member and sharing funny memes with friends, people also use social networks to share their paranoid, unrealistic ideas that may cause panic, harm democracies, and gather other unsuspecting followers. In this work, we focus on characterizing the QAnon movement on Voat.co. QAnon is a conspiracy theory that supports the idea that powerful politicians, aristocrats, and celebrities are closely engaged in a pedophile ring. At the same time, many governments are controlled by the\"puppet masters\"where the democratically elected officials serve as a fake showroom of democracy. Voat, a 5-year-old news aggregator, captured the interest of many journalists because of the often hateful content its users' post. Hence, we collect data from seventeen QAnon related subverses to characterize the narrative around QAnon, detect the most prominent topics of discussion, and showcase how the different topics and terms used in QAnon related subverses are interconnected.","421":"In this work, we classify all the effective $U(1)$ symmetries and their associated Noether charges in the Standard Model (SM) and its minimal supersymmetric extension (MSSM) from the highest scale after inflation down to the weak scale. We then demonstrate that the discovery of the violation of baryon minus lepton number ($B-L$) which pinpoints to its violation in primordial Universe at any cosmic temperature above $30$ TeV will open up a new window of baryogenesis in these effective charges above this scale. While the fast SM baryon number violation in the early Universe could be the first piece to solve the puzzle of cosmic baryon asymmetry, $(B-L)$ violation could be the second important piece. In the background of expanding Universe, there is ample opportunity for out-of-equilibrium processes to generate an asymmetry in the numerous effective charges in the SM or the MSSM, making a baryon asymmetric Universe almost unavoidable. Finally we provide examples in the SM and the MSSM where baryogenesis can proceed through out-of-equilibrium dynamics without explicitly breaking baryon nor lepton number.","422":"The spread of an infectious disease depends on intrinsic properties of the disease as well as the connectivity and actions of the population. This study investigates the dynamics of an SIR type model which accounts for human tendency to avoid infection while also maintaining preexisting, interpersonal relationships. Specifically, we use a network model in which individuals probabilistically deactivate connections to infected individuals and later reconnect to the same individuals upon recovery. To analyze this network model, a mean field approximation consisting of a system of fourteen ordinary differential equations for the number of nodes and edges is developed. This system of equations is closed using a moment closure approximation for the number of triple links. By analyzing the differential equations, it is shown that, in addition to force of infection and recovery rate, the probability of deactivating edges and the average node degree of the underlying network determine if an epidemic occurs.","423":"The increasing progress in artificial intelligence and respective machine learning technology has fostered the proliferation of chatbots to the point where today they are being embedded into various human-technology interaction tasks. In enterprise contexts, the use of chatbots seeks to reduce labor costs and consequently increase productivity. For simple, repetitive customer service tasks such already proves beneficial, yet more complex collaborative knowledge work seems to require a better understanding of how the technology may best be integrated. Particularly, the additional mental burden which accompanies the use of these natural language based artificial assistants, often remains overlooked. To this end, cognitive load theory implies that unnecessary use of technology can induce additional extrinsic load and thus may have a contrary effect on users' productivity. The research presented in this paper thus reports on a study assessing cognitive load and productivity implications of human chatbot interaction in a realistic enterprise setting. A\/B testing software-only vs. software + chatbot interaction, and the NASA TLX were used to evaluate and compare the cognitive load of two user groups. Results show that chatbot users experienced less cognitive load and were more productive than software-only users. Furthermore, they show lower frustration levels and better overall performance (i.e, task quality) despite their slightly longer average task completion time.","424":"In this paper we introduce a contact process in an evolving random environment (CPERE) on a connected and transitive graph with bounded degree, where we assume that this environment is described through an ergodic spin systems with finite range. We show that under a certain growth condition the phase transition of survival is independent of the initial configuration of the process. We study the invariant laws of the CPERE and show that under aforementioned growth condition the phase transition for survival coincides with the phase transition of non-triviality of the upper invariant law. Furthermore, we prove continuity properties for the survival probability and derive equivalent conditions for complete convergence, in an analogous way as for the classical contact process. We then focus on the special case, where the evolving random environment is described through a dynamical percolation. We show that the contact process on a dynamical percolation on the $d$-dimensional integer dies out a.s. at criticality and complete convergence holds for all parameter choices. In the end we derive some comparison results between a dynamical percolation and ergodic spin systems with finite range such that we get bounds on the survival probability of a contact process in an evolving random environment and we determine in this case that complete convergence holds in a certain parameter regime.","425":"News outlets are developing formats dedicated to social platforms that capture audience attention, such as Instagram stories, Facebook Instant articles, and YouTube videos. In some cases, these formats are created in collaboration with the tech companies themselves. At the same time, the use of data-driven storytelling is becoming increasingly integrated into the ever-complex business models of news outlets, generating more impact and visibility. Previous studies have focused on studying these two effects separately. To address this gap in the literature, this paper identifies and analyzes the use of data journalism on the Instagram content of AJ Labs, the team dedicated to producing data-driven and interactive stories for the Al Jazeera news network. Drawing upon a mixed-method approach, this study examines the use and characteristics of data stories on social media platforms. Results suggest that there is reliance on producing visual content that covers topics such as politics and violence. In general, AJ Labs relies on the use of infographics and produces its own unique data. To conclude, this paper suggests potential ways to improve the use of Instagram to tell data stories.","426":"When the trained physician interprets medical images, they understand the clinical importance of visual features. By applying cognitive attention, they apply greater focus onto clinically relevant regions while disregarding unnecessary features. The use of computer vision to automate the classification of medical images is widely studied. However, the standard convolutional neural network (CNN) does not necessarily employ subconscious feature relevancy evaluation techniques similar to the trained medical specialist and evaluates features more generally. Self-attention mechanisms enable CNNs to focus more on semantically important regions or aggregated relevant context with long-range dependencies. By using attention, medical image analysis systems can potentially become more robust by focusing on more important clinical feature regions. In this paper, we provide a comprehensive comparison of various state-of-the-art self-attention mechanisms across multiple medical image analysis tasks. Through both quantitative and qualitative evaluations along with a clinical user-centric survey study, we aim to provide a deeper understanding of the effects of self-attention in medical computer vision tasks.","427":"Machine learning has seen a vast increase of interest in recent years, along with an abundance of learning resources. While conventional lectures provide students with important information and knowledge, we also believe that additional project-based learning components can motivate students to engage in topics more deeply. In addition to incorporating project-based learning in our courses, we aim to develop project-based learning components aligned with real-world tasks, including experimental design and execution, report writing, oral presentation, and peer-reviewing. This paper describes the organization of our project-based machine learning courses with a particular emphasis on the class project components and shares our resources with instructors who would like to include similar elements in their courses.","428":"Periodic disinfection of workspaces can reduce SARS-CoV-2 transmission. In many buildings periodic disinfection is performed manually; this has several disadvantages: it is expensive, limited in the number of times it can be done over a day, and poses an increased risk to the workers performing the task. To solve these problems, we developed an automated decontamination system that uses ultraviolet C (UVC) radiation for disinfection, coupled with occupancy detection for its safe operation. UVC irradiation is a well-established technology for the deactivation of a wide range of pathogens. Our proposed system can deactivate pathogens both on surfaces and in the air. The coupling with occupancy detection ensures that occupants are never directly exposed to UVC lights and their potential harmful effects. To help the wider community, we have shared our complete work as an open-source repository, to be used under GPL v3.","429":"As the latest video coding standard, versatile video coding (VVC) has shown its ability in retaining pixel quality. To excavate more compression potential for video conference scenarios under ultra-low bitrate, this paper proposes a bitrate adjustable hybrid compression scheme for face video. This hybrid scheme combines the pixel-level precise recovery capability of traditional coding with the generation capability of deep learning based on abridged information, where Pixel wise Bi-Prediction, Low-Bitrate-FOM and Lossless Keypoint Encoder collaborate to achieve PSNR up to 36.23 dB at a low bitrate of 1.47 KB\/s. Without introducing any additional bitrate, our method has a clear advantage over VVC under a completely fair comparative experiment, which proves the effectiveness of our proposed scheme. Moreover, our scheme can adapt to any existing encoder \/ configuration to deal with different encoding requirements, and the bitrate can be dynamically adjusted according to the network condition.","430":"We consider two large datasets consisting of all games played among top-tier European soccer clubs in the last 60 years, and among professional American basketball teams in the past 70 years. We leverage game data to build networks of pairwise interactions between the head coaches of the teams, and measure their career performance in terms of network centrality metrics. We identify Arsene Wenger, Sir Alex Ferguson, Jupp Heynckes, Carlo Ancelotti, and Jose Mourinho as the top 5 European soccer coaches of all time. In American basketball, the first 5 positions of the all-time ranking are occupied by Red Auerbach, Gregg Popovich, Phil Jackson, Don Nelson, and Lenny Wilkens. We further establish rankings by decade and season. We develop a simple methodology to monitor performance throughout a coach's career, and to dynamically compare the performance of two or more coaches at a given time. The manuscript is accompanied by the website coachscore.luddy.indiana.edu where complete results of our analysis are accessible to the interested readers.","431":"Financial time series are both autocorrelated and nonstationary, presenting modelling challenges that violate the independent and identically distributed random variables assumption of most regression and classification models. The prediction with expert advice framework makes no assumptions on the data-generating mechanism yet generates predictions that work well for all sequences, with performance nearly as good as the best expert with hindsight. We conduct research using S&P 250 daily sampled data, extending the academic research into cross-sectional momentum trading strategies. We introduce a novel ranking algorithm from the prediction with expert advice framework, the naive Bayes asset ranker, to select subsets of assets to hold in either long-only or long\/short portfolios. Our algorithm generates the best total returns and risk-adjusted returns, net of transaction costs, outperforming the long-only holding of the S&P 250 with hindsight. Furthermore, our ranking algorithm outperforms a proxy for the regress-then-rank cross-sectional momentum trader, a sequentially fitted curds and whey multivariate regression procedure.","432":"A conventional study of fluid simulation involves different stages including conception, simulation, visualization, and analysis tasks. It is, therefore, necessary to switch between different software and interactive contexts which implies costly data manipulation and increases the time needed for decision making. Our interactive simulation approach was designed to shorten this loop, allowing users to visualize and steer a simulation in progress without waiting for the end of the simulation. The methodology allows the users to control, start, pause, or stop a simulation in progress, to change global physical parameters, to interact with its 3D environment by editing boundary conditions such as walls or obstacles. This approach is made possible by using a methodology such as the Lattice Boltzmann Method (LBM) to achieve interactive time while remaining physically relevant. In this work, we present our platform dedicated to interactive fluid simulation based on LBM. The contribution of our interactive simulation approach to decision making will be evaluated in a study based on a simple but realistic use case.","433":"COVID-19 is affecting every social sector significantly, including human mobility and subsequently road traffic safety. In this study, we analyze the impact of the pandemic on traffic accidents using two cities, namely Los Angeles and New York City in the U.S., as examples. Specifically, we have analyzed traffic accidents associated with various demographic groups, how traffic accidents are distributed in time and space, and the severity level of traffic accidents that both involve and do not involve other transportation modes (e.g., pedestrians and motorists). We have made the following observations: 1) the pandemic has disproportionately affected certain age groups, races, and genders; 2) the\"hotspots\"of traffic accidents have been shifted in both time and space compared to time periods that are prior to the pandemic, demonstrating irregularity; and 3) the number of non-fatal accident cases has decreased but the number of severe and fatal cases of traffic accidents remains the same under the pandemic.","434":"Drones are effective for reducing human activity and interactions by performing tasks such as exploring and inspecting new environments, monitoring resources and delivering packages. Drones need a controller to maintain stability and to reach their goal. The most well-known drone controllers are proportional-integral-derivative (PID) and proportional-derivative (PD) controllers. However, the controller parameters need to be tuned and optimized. In this paper, we introduce the use of two evolutionary algorithms, biogeography-based optimization~(BBO) and particle swarm optimization (PSO), for multi-objective optimization (MOO) to tune the parameters of the PD controller of a drone. The combination of MOO, BBO, and PSO results in various methods for optimization: vector evaluated BBO and PSO, denoted as VEBBO and VEPSO; and non-dominated sorting BBO and PSO, denoted as NSBBO and NSPSO. The multi-objective cost function is based on tracking errors for the four states of the system. Two criteria for evaluating the Pareto fronts of the optimization methods, normalized hypervolume and relative coverage, are used to compare performance. Results show that NSBBO generally performs better than the other methods.","435":"Background: There is uncertainty about the role of different age groups in propagating the SARS-CoV-2 epidemics in different countries. Methods: We used the Koch Institute data on COVID-19 cases in Germany. To minimize the effect of changes in healthcare seeking behavior and testing practices, we included the following 5-year age groups in the analyses: 10-14y through 45-49y. For each age group g, we considered the proportion PL(g) of individuals in age group g among all detected cases aged 10-49y during weeks 13-14, 2020 (later period), as well as corresponding proportion PE(g) for weeks 10-11, 2020 (early period), and the relative risk RR(g)=PL(g)\/PE(g). For each pair of age groups g1,g2, a higher value of RR(g1) compared to RR(g2) is interpreted as the relative increase in the population incidence of SARS-Cov-2 for g1 compared to g2 for the later vs. early period. Results: The relative risk was highest for individuals aged 20-24y (RR=1.4(95% CI (1.27,1.55))), followed by individuals aged 15-19y (RR=1.14(0.99,1.32)), aged 30-34y (RR= 1.07(0.99,1.16)), aged 25-29y (RR= 1.06(0.98,1.15)), aged 35-39y (RR=0.95(0.87,1.03)), aged 40-44y (RR=0.9(0.83,0.98)), aged 45-49y (RR=0.83(0.77,0.89)) and aged 10-14y (RR=0.78(0.64,0.95)). Conclusions: The observed relative increase with time in the prevalence of individuals aged 15-34y (particularly those aged 20-24y) among COVID-19 cases is unlikely to be explained by increases in the likelihood of seeking medical care\/being tested for individuals in those age groups compared to individuals aged 35-49y or 10-14y, suggesting an actual increase in the prevalence of individuals aged 15-34y among SARS-CoV-2 infections in the German population. That increase likely reflects elevated mixing among individuals aged 15-34y (particularly those aged 20-24y) compared to other age groups, possibly due to lesser adherence to social distancing practices.","436":"The ability to rewire ties in communication networks is vital for large-scale human cooperation and the spread of new ideas. Especially important for knowledge dissemination is the ability to form new weak ties -- ties which act as bridges between distant parts of the social system and enable the flow of novel information. Here we show that lack of researcher co-location during the COVID-19 lockdown caused the loss of more than 4800 weak ties over 18 months in the email network of a large North American university -- the MIT campus. Furthermore, we find that the re-introduction of partial co-location through a hybrid work mode starting in September 2021 led to a partial regeneration of weak ties, especially between researchers who work in close proximity. We quantify the effect of co-location in renewing ties -- a process that we have termed nexogenesis -- through a novel model based on physical proximity, which is able to reproduce all empirical observations. Results highlight that employees who are not co-located are less likely to form ties, weakening the spread of information in the workplace. Such findings could contribute to a better understanding of the spatio-temporal dynamics of human communication networks -- and help organizations that are moving towards the implementation of hybrid work policies to evaluate the minimum amount of in-person interaction necessary for a healthy work life.","437":"The explosive growth and popularity of Social Media has revolutionised the way we communicate and collaborate. Unfortunately, this same ease of accessing and sharing information has led to an explosion of misinformation and propaganda. Given that stance detection can significantly aid in veracity prediction, this work focuses on boosting automated stance detection, a task on which pre-trained models have been extremely successful on, as on several other tasks. This work shows that the task of stance detection can benefit from feature based information, especially on certain under performing classes, however, integrating such features into pre-trained models using ensembling is challenging. We propose a novel architecture for integrating features with pre-trained models that address these challenges and test our method on the RumourEval 2019 dataset. This method achieves state-of-the-art results with an F1-score of 63.94 on the test set.","438":"Privacy regulations and the physical distribution of heterogeneous data are often primary concerns for the development of deep learning models in a medical context. This paper evaluates the feasibility of differentially private federated learning for chest X-ray classification as a defense against data privacy attacks. To the best of our knowledge, we are the first to directly compare the impact of differentially private training on two different neural network architectures, DenseNet121 and ResNet50. Extending the federated learning environments previously analyzed in terms of privacy, we simulated a heterogeneous and imbalanced federated setting by distributing images from the public CheXpert and Mendeley chest X-ray datasets unevenly among 36 clients. Both non-private baseline models achieved an area under the receiver operating characteristic curve (AUC) of $0.94$ on the binary classification task of detecting the presence of a medical finding. We demonstrate that both model architectures are vulnerable to privacy violation by applying image reconstruction attacks to local model updates from individual clients. The attack was particularly successful during later training stages. To mitigate the risk of privacy breach, we integrated R\\'enyi differential privacy with a Gaussian noise mechanism into local model training. We evaluate model performance and attack vulnerability for privacy budgets $\\epsilon \\in$ {1, 3, 6, 10}. The DenseNet121 achieved the best utility-privacy trade-off with an AUC of $0.94$ for $\\epsilon$ = 6. Model performance deteriorated slightly for individual clients compared to the non-private baseline. The ResNet50 only reached an AUC of $0.76$ in the same privacy setting. Its performance was inferior to that of the DenseNet121 for all considered privacy constraints, suggesting that the DenseNet121 architecture is more robust to differentially private training.","439":"Crucial for healthcare and biomedical applications, respiration monitoring often employs wearable sensors in practice, causing inconvenience due to their direct contact with human bodies. Therefore, researchers have been constantly searching for contact-free alternatives. Nonetheless, existing contact-free designs mostly require human subjects to remain static, largely confining their adoptions in everyday environments where body movements are inevitable. Fortunately, radio-frequency (RF) enabled contact-free sensing, though suffering motion interference inseparable by conventional filtering, may offer a potential to distill respiratory waveform with the help of deep learning. To realize this potential, we introduce MoRe-Fi to conduct fine-grained respiration monitoring under body movements. MoRe-Fi leverages an IR-UWB radar to achieve contact-free sensing, and it fully exploits the complex radar signal for data augmentation. The core of MoRe-Fi is a novel variational encoder-decoder network; it aims to single out the respiratory waveforms that are modulated by body movements in a non-linear manner. Our experiments with 12 subjects and 66-hour data demonstrate that MoRe-Fi accurately recovers respiratory waveform despite the interference caused by body movements. We also discuss potential applications of MoRe-Fi for pulmonary disease diagnoses.","440":"A manuscript identified bat sarbecoviruses with high sequence homology to SARS-CoV-2 found in caves in Laos that can directly infect human cells via the human ACE2 receptor (Coronaviruses with a SARS-CoV-2-like receptor binding domain allowing ACE2-mediated entry into human cells isolated from bats of Indochinese peninsula, Temmam S., et al.). Here, I examine the genomic sequence of one of these viruses, BANAL-236, and show it has 5-UTR and 3-UTR secondary structures that are non-canonical and, in fact, have never been seen in an infective coronavirus. Specifically, the 5-UTR has a 177 nt copy-back extension which forms an extended, highly stable duplex RNA structure. Because of this copy-back, the four obligate Stem Loops (SL) -1, -2, -3, and -4 cis-acting elements found in all currently known replicating coronaviruses are buried in the extended duplex. The 3-UTR has a similar fold-back duplex of 144 nts and is missing the obligate poly-A tail. Taken together, these findings demonstrate BANAL-236 is missing eight obligate UTR cis-acting elements; each one of which has previously been lethal to replication when modified individually. Neither duplex copyback has ever been observed in an infective sarbecovirus, although some of the features have been seen in defective interfering particles, which can be found in co-infections with non-defective, replicating viruses. They are also a common error seen during synthetic genome assembly in a laboratory. BANAL-236 must have evolved an entirely unique mechanism for replication, RNA translation, and RNA packaging never seen in a coronavirus and because it is a bat sarbecovirus closely related to SARS-CoV-2, it is imperative that we understand its unique mode of infectivity by a collaborative, international research effort.","441":"We study portfolio optimization of four major cryptocurrencies. Our time series model is a generalized autoregressive conditional heteroscedasticity (GARCH) model with multivariate normal tempered stable (MNTS) distributed residuals used to capture the non-Gaussian cryptocurrency return dynamics. Based on the time series model, we optimize the portfolio in terms of Foster-Hart risk. Those sophisticated techniques are not yet documented in the context of cryptocurrency. Statistical tests suggest that the MNTS distributed GARCH model fits better with cryptocurrency returns than the competing GARCH-type models. We find that Foster-Hart optimization yields a more profitable portfolio with better risk-return balance than the prevailing approach.","442":"Training and refreshing a web-scale Question Answering (QA) system for a multi-lingual commercial search engine often requires a huge amount of training examples. One principled idea is to mine implicit relevance feedback from user behavior recorded in search engine logs. All previous works on mining implicit relevance feedback target at relevance of web documents rather than passages. Due to several unique characteristics of QA tasks, the existing user behavior models for web documents cannot be applied to infer passage relevance. In this paper, we make the first study to explore the correlation between user behavior and passage relevance, and propose a novel approach for mining training data for Web QA. We conduct extensive experiments on four test datasets and the results show our approach significantly improves the accuracy of passage ranking without extra human labeled data. In practice, this work has proved effective to substantially reduce the human labeling cost for the QA service in a global commercial search engine, especially for languages with low resources. Our techniques have been deployed in multi-language services.","443":"We propose a new microscopy simulation system that can depict atomistic models in a micrograph visual style, similar to results of physical electron microscopy imaging. This system is scalable, able to represent simulation of electron microscopy of tens of viral particles and synthesizes the image faster than previous methods. On top of that, the simulator is differentiable, both its deterministic as well as stochastic stages that form signal and noise representations in the micrograph. This notable property has the capability for solving inverse problems by means of optimization and thus allows for generation of microscopy simulations using the parameter settings estimated from real data. We demonstrate this learning capability through two applications: (1) estimating the parameters of the modulation transfer function defining the detector properties of the simulated and real micrographs, and (2) denoising the real data based on parameters trained from the simulated examples. While current simulators do not support any parameter estimation due to their forward design, we show that the results obtained using estimated parameters are very similar to the results of real micrographs. Additionally, we evaluate the denoising capabilities of our approach and show that the results showed an improvement over state-of-the-art methods. Denoised micrographs exhibit less noise in the tilt-series tomography reconstructions, ultimately reducing the visual dominance of noise in direct volume rendering of microscopy tomograms.","444":"The attention mechanism has demonstrated superior performance for inference over nodes in graph neural networks (GNNs), however, they result in a high computational burden during both training and inference. We propose FastGAT, a method to make attention based GNNs lightweight by using spectral sparsification to generate an optimal pruning of the input graph. This results in a per-epoch time that is almost linear in the number of graph nodes as opposed to quadratic. Further, we provide a re-formulation of a specific attention based GNN, Graph Attention Network (GAT) that interprets it as a graph convolution method using the random walk normalized graph Laplacian. Using this framework, we theoretically prove that spectral sparsification preserves the features computed by the GAT model, thereby justifying our FastGAT algorithm. We experimentally evaluate FastGAT on several large real world graph datasets for node classification tasks, FastGAT can dramatically reduce (up to 10x) the computational time and memory requirements, allowing the usage of attention based GNNs on large graphs.","445":"In our article, we construct an age-structured model for COVID-19 with vaccination and analyze it from multiple perspectives. We derive the unique disease-free equilibrium point and the basic reproduction number $ \\mathscr{R}_0 $, then we show that the disease-free equilibrium is locally asymptotically stable when $ \\mathscr{R}_0<1 $, while is unstable when $ \\mathscr{R}_0>1 $. We also investigate the properties of our model in endemic equilibrium. We work out endemic equilibrium points and reveal the stability. We turn to sensitivity analysis to explore how parameters influence $ \\mathscr{R}_0 $. Sensitivity analysis helps us develop more targeted strategies to control epidemics. Finally, this model is used to discuss the cases in Shijiazhuang, Hebei Province at the beginning of 2021.","446":"Automated and accurate segmentation of the infected regions in computed tomography (CT) images is critical for the prediction of the pathological stage and treatment response of COVID-19. Several deep convolutional neural networks (DCNNs) have been designed for this task, whose performance, however, tends to be suppressed by their limited local receptive fields and insufficient global reasoning ability. In this paper, we propose a pixel-wise sparse graph reasoning (PSGR) module and insert it into a segmentation network to enhance the modeling of long-range dependencies for COVID-19 infected region segmentation in CT images. In the PSGR module, a graph is first constructed by projecting each pixel on a node based on the features produced by the segmentation backbone, and then converted into a sparsely-connected graph by keeping only K strongest connections to each uncertain pixel. The long-range information reasoning is performed on the sparsely-connected graph to generate enhanced features. The advantages of this module are two-fold: (1) the pixel-wise mapping strategy not only avoids imprecise pixel-to-node projections but also preserves the inherent information of each pixel for global reasoning; and (2) the sparsely-connected graph construction results in effective information retrieval and reduction of the noise propagation. The proposed solution has been evaluated against four widely-used segmentation models on three public datasets. The results show that the segmentation model equipped with our PSGR module can effectively segment COVID-19 infected regions in CT images, outperforming all other competing models.","447":"Autonomous mobile manipulation robots that can collect trolleys are widely used to liberate human resources and fight epidemics. Most prior robotic trolley collection solutions only detect trolleys with 2D poses or are merely based on specific marks and lack the formal design of planning algorithms. In this paper, we present a novel mobile manipulation system with applications in luggage trolley collection. The proposed system integrates a compact hardware design and a progressive perception and planning framework, enabling the system to efficiently and robustly collect trolleys in dynamic and complex environments. For the perception, we first develop a 3D trolley detection method that combines object detection and keypoint estimation. Then, a docking process in a short distance is achieved with an accurate point cloud plane detection method and a novel manipulator design. On the planning side, we formulate the robot's motion planning under a nonlinear model predictive control framework with control barrier functions to improve obstacle avoidance capabilities while maintaining the target in the sensors' field of view at close distances. We demonstrate our design and framework by deploying the system on actual trolley collection tasks, and their effectiveness and robustness are experimentally validated.","448":"These lectures present some basic ideas and techniques in the spectral analysis of lattice Schrodinger operators with disordered potentials. In contrast to the classical Anderson tight binding model, the randomness is also allowed to possess only finitely many degrees of freedom. This refers to dynamically defined potentials, i.e., those given by evaluating a function along an orbit of some ergodic transformation (or of several commuting such transformations on higher-dimensional lattices). Classical localization theorems by Frohlich--Spencer for large disorders are presented, both for random potentials in all dimensions, as well as even quasi-periodic ones on the line. After providing the needed background on subharmonic functions, we then discuss the Bourgain-Goldstein theorem on localization for quasiperiodic Schrodinger cocycles assuming positive Lyapunov exponents.","449":"We investigate the motivation and means through which individuals expand their skill-set by analyzing a survey of applicants from the Facebook Jobs product. Individuals who report being influenced by their networks or local economy are over 29% more likely to have a postsecondary degree, but peer effects still exist among those who do not acknowledge such influences. Users with postsecondary degrees are more likely to upskill in general, by continuing coursework or applying to higher-skill jobs, though the latter is more common among users across all education backgrounds. These findings indicate that policies aimed at connecting individuals with different educational backgrounds can encourage upskilling. Policies that encourage users to enroll in coursework may not be as effective among individuals with a high school degree or less. Instead, connecting such individuals to opportunities that value skills acquired outside of a formal education, and allow for on-the-job training, may be more effective.","450":"Even though laboratory and epidemiological studies have demonstrated the effects of ambient temperature on the transmission and survival of coronaviruses, not much has been done on the effects of weather on the spread of COVID-19. This study investigates the effects of temperature, humidity, precipitation, wind speed and the specific government policy intervention of partial lockdown on the new cases of COVID-19 infection in Ghana. Daily data on confirmed cases of COVID-19 from March 13, 2020 to April 21, 2020 were obtained from the official website of Our World in Data (OWID) dedicated to COVID-19 while satellite climate data for the same period was obtained from the official website of NASA's Prediction of Worldwide Energy Resources (POWER) project. Considering the nature of the data and the objectives of the study, a time series generalized linear model which allows for regressing on past observations of the response variable and covariates was used for model fitting. The results indicate significant effects of maximum temperature, relative humidity and precipitation in predicting new cases of the disease. Also, results of the intervention analysis indicate that the null hypothesis of no significant effect of the specific policy intervention of partial lockdown should be rejected (p-value=0.0164) at a 5\\% level of significance. These findings provide useful insights for policymakers and the public.","451":"Balance (Counterfeit coin) puzzles have been part of recreational mathematics for a few decades. A particular type of Counterfeit coin puzzle is known in the literature as the\"Beam balance puzzle\". An abstract solution to it is provided by Iwama et.al as a modification of the Bernstein-Vazirani algorithm, making use of quantum parallelism and entanglement. Moreover, during this pandemic, group testing has proved to be an efficient algorithm to save time and cost of testing specimens for the presence of infection. In this article, we propose a\"Binary Spring Balance\"(BSB) puzzle, to facilitate a unified picture of the counterfeit coin problem and the testing for infection problem, as both aim to reduce the number of queries. We then showcase two solutions to the BSB problem, one using bits and other using classical-qubits ('cebits\") for querying. Both solutions are demonstrated using circuits. In this pursuit, we develop a modified optical implementation of Bernstein-Vazirani algorithm using only polarizers (no need of beam splitters), which has surprisingly not yet been proposed earlier. Under the pretext of this demonstration we question why we have not yet developed testing mechanisms inspired by Bernstein-Vazirani algorithm for the pandemic, as they solve the problem in single query, they have no issues related to prevalence of infection in the population, nor are they plagued by the issue of dilution of samples due to pooling. The modified implementation of Bernstein-Vazirani algorithm using polarizers can also be a cost-effective demonstration in an undergraduate lab.","452":"Objective: Computed Tomography (CT) has an important role to detect lung lesion related to Covide 19. The purpose of this work is to obtain diagnostic findings of Ultra-Low Dose (ULD) chest CT image and compare with routine dose chest CT. Material and Methods: Patients, suspected of Covid 19 infection, were scanned successively with routine dose, and ULD, with 98% or 94% dose reduction, protocols. Axial images of routine and ULD chest CT were evaluated objectively by two expert radiologists and quantitatively by Signal to Noise Ratio (SNR) and pixel by pixel noise measurement. Results: It was observed that the ULD and routine dose chest CT images could detect Covid 19 related lung lesions in patients with PCR positive test. Also, SNR and pixel noise values were comparable in these protocols. Conclusion: ULD chest CT with 98% dose reduction can be used in non-pandemic situation as a substitute for chest radiograph for screening and follow up. Routine chest CT protocol can be replaced by ULD, with 94% dose reduction, to detect patients suspected with Covid 19 at an early stage and for its follow up.","453":"This note continues study of exchangeability martingales, i.e., processes that are martingales under any exchangeable distribution for the observations. Such processes can be used for detecting violations of the IID assumption, which is commonly made in machine learning. Violations of the IID assumption are sometimes referred to as dataset shift, and dataset shift is sometimes subdivided into concept shift, covariate shift, etc. Our primary interest is in concept shift, but we will also discuss exchangeability martingales that decompose perfectly into two components one of which detects concept shift and the other detects what we call label shift. Our methods will be based on techniques of conformal prediction.","454":"Since SARS-CoV-2 started spreading in Europe in early 2020, there has been a strong call for technical solutions to combat or contain the pandemic, with contact tracing apps at the heart of the debates. The EU's General Daten Protection Regulation (GDPR) requires controllers to carry out a data protection impact assessment (DPIA) where their data processing is likely to result in a high risk to the rights and freedoms (Art. 35 GDPR). A DPIA is a structured risk analysis that identifies and evaluates possible consequences of data processing relevant to fundamental rights and describes the measures envisaged to address these risks or expresses the inability to do so. Based on the Standard Data Protection Model (SDM), we present a scientific DPIA which thoroughly examines three published contact tracing app designs that are considered to be the most\"privacy-friendly\": PEPP-PT, DP-3T and a concept summarized by Chaos Computer Club member Linus Neumann, all of which process personal health data. The DPIA starts with an analysis of the processing context and some expected use cases. Then, the processing activities are described by defining a realistic processing purpose. This is followed by the legal assessment and threshold analysis. Finally, we analyse the weak points, the risks and determine appropriate protective measures. We show that even decentralized implementations involve numerous serious weaknesses and risks. Legally, consent is unfit as legal ground hence data must be processed based on a law. We also found that measures to realize the rights of data subjects and affected people are not sufficient. Last but not least, we show that anonymization must be understood as a continuous process, which aims at separating the personal reference and is based on a mix of legal, organizational and technical measures. All currently available proposals lack such an explicit separation process.","455":"In this paper we study a relation between two positive geometries: the momentum amplituhedron, relevant for tree-level scattering amplitudes in $\\mathcal{N} = 4$ super Yang-Mills theory, and the kinematic associahedron, encoding tree-level amplitudes in bi-adjoint scalar $\\phi^3$ theory. We study the implications of restricting the latter to four spacetime dimensions and give a direct link between its canonical form and the canonical form for the momentum amplituhedron. After removing the little group scaling dependence of the gauge theory, we find that we can compare the resulting reduced form with the pull-back of the associahedron form. In particular, the associahedron form is the sum over all helicity sectors of the reduced momentum amplituhedron forms. This relation highlights the common singularity structure of the respective amplitudes; in particular the factorization channels, corresponding to vanishing planar Mandelstam variables, are the same. Additionally, we also find a relation between these canonical forms directly on the kinematic space of the scalar theory when reduced to four spacetime dimensions by Gram determinant constraints. As a by-product of our work we provide a detailed analysis of the kinematic spaces relevant for the four-dimensional gauge and scalar theories, and provide direct links between them.","456":"Federated learning (FL) allows multiple medical institutions to collaboratively learn a global model without centralizing all clients data. It is difficult, if possible at all, for such a global model to commonly achieve optimal performance for each individual client, due to the heterogeneity of medical data from various scanners and patient demographics. This problem becomes even more significant when deploying the global model to unseen clients outside the FL with new distributions not presented during federated training. To optimize the prediction accuracy of each individual client for critical medical tasks, we propose a novel unified framework for both Inside and Outside model Personalization in FL (IOP-FL). Our inside personalization is achieved by a lightweight gradient-based approach that exploits the local adapted model for each client, by accumulating both the global gradients for common knowledge and local gradients for client-specific optimization. Moreover, and importantly, the obtained local personalized models and the global model can form a diverse and informative routing space to personalize a new model for outside FL clients. Hence, we design a new test-time routing scheme inspired by the consistency loss with a shape constraint to dynamically incorporate the models, given the distribution information conveyed by the test data. Our extensive experimental results on two medical image segmentation tasks present significant improvements over SOTA methods on both inside and outside personalization, demonstrating the great potential of our IOP-FL scheme for clinical practice. Code will be released at https:\/\/github.com\/med-air\/IOP-FL.","457":"Bikes are among the healthiest, greenest, and most affordable means of transportation for a better future city, but mobility patterns of riders with different income were rarely studied due to limitations on collecting data. Newly emergent dockless bike-sharing platforms that record detailed information regarding each trip provide us a unique opportunity. Attribute to its better usage flexibility and accessibility, dockless bike-sharing platforms are booming over the past a few years worldwide and reviving the riding fashion in cities. In this work, by exploiting massive riding records in two megacities from a dockless bike-sharing platform, we reveal that individual mobility patterns, including radius of gyration and average travel distance, are similar among users with different income, which indicates that human beings all follow similar physical rules. However, collective mobility patterns, including average range and diversity of visitation, and commuting directions, all exhibit different behaviors and spatial patterns across income categories. Hotspot locations that attract more cycling activities are quite different over groups, and locations where users reside are of a low user ratio for both higher and lower income groups. Lower income groups are inclined to visit less flourishing locations, and commute towards the direction to the city center in both cities, and of a smaller mobility diversity in Beijing but a larger diversity in Shanghai. In addition, differences on mobility patterns among socioeconomic categories are more evident in Beijing than in Shanghai. Our findings would be helpful on designing better promotion strategies for dockless bike-sharing platforms and towards the transition to a more sustainable green transportation.","458":"The international financial system is currently not yet prepared to face a foreseeable crisis mainly motivated by the dichotomy between the real economy and the virtual economy. Skepticism is widespread even when it comes to investments in sustainable economy. The concentration of capital in a few persons is one of the greatest risks for the possible reiteration of economic crises. The benevolent sentences of the courts to some of the fraudsters do not contribute to dispelling the ghost of fraud nor to the disappearance of tax havens. From the diachronic perspective, it is observed that economic crises are increasingly frequent and incidents always in the financial field; which forces us to rethink an economic model on an international scale in which there is a greater weight of the economic policy of governments over the power of multinational companies in the context of globalization. In the context of Corporate Social Responsibility, Corporate Governance is listed as one of the fundamental levers to curb large business fraud, but its efficiency seems insufficient due to the lack of international regulations and the ignorance of hidden forces in what has been known as fiscal and financial engineering. The application of liberal policies in an unorthodox way is causing real social gaps in the distribution of income and is undermining the current capitalist system. The need to implement corporate governments is recommended as one of the essential formulas for sustaining the international economic system.","459":"A major challenge in training deep learning models is the lack of high quality and complete datasets. In the paper, we present a masking approach for training deep learning models from a publicly available but incomplete dataset. For example, city of Hamburg, Germany maintains a list of trees along the roads, but this dataset does not contain any information about trees in private homes and parks. To train a deep learning model on such a dataset, we mask the street trees and aerial images with the road network. Road network used for creating the mask is downloaded from OpenStreetMap, and it marks the area where the training data is available. The mask is passed to the model as one of the inputs and it also coats the output. Our model learns to successfully predict trees only in the masked region with 78.4% accuracy.","460":"As sensitivities improve and more detectors are added to the global network of gravitational wave observatories, calibration accuracy and precision are becoming increasingly important. Photon calibrators, relying on power-modulated auxiliary laser beams reflecting from suspended interferometer optics, enable continuous calibration by generating displacement fiducials proportional to the modulated laser power. Developments in the propagation of laser power calibration via transfer standards to on-line power sensors monitoring the modulated laser power have enabled generation of length fiducials with improved accuracy. Estimated uncertainties are almost a factor of two smaller than the lowest values previously reported. This is partly due to improvements in methodology that have increased confidence in the results reported. Referencing the laser power calibration standards for each observatory to a single transfer standard enables reducing relative calibration errors between elements of the detector network. Efforts within the national metrology institute community to realize improved laser power sensor calibration accuracy are ongoing.","461":"A recent observational result finds that the quenching of satellites in groups at $z=0.08$ has an angular dependence relative to the semi-major axis of the central galaxy. This observation is described as `anisotropic quenching' or `angular conformity'. In this paper I study the variation in the colour of a mass limited sample of satellite galaxies relative to their angle from the major axis of the Brightest Cluster Galaxy in the CLASH clusters up to $z\\sim0.5$, 4 Gyr further in lookback time. The same result is found: galaxies close to the major axis are more quenched than those along the minor axis. I also find that the star-forming galaxies tend to avoid a region +\/-45 degrees from the major axis. This quenching signal is thought to be driven by AGN outflows along the minor axis, reducing the density of the intergalactic medium and thus the strength of ram pressure. Here I will discuss potential alternative mechanisms. Finally, I note that the advent of the Legacy Survey of Space and Time (LSST) and Euclid surveys will allow for a more detailed study of this phenomenon and its evolution.","462":"In contrast to the common assumption in epidemic models that the rate of infection between individuals is constant, in reality, an individual's viral load determines their infectiousness. We compare the average and individual reproductive numbers and epidemic dynamics for a model incorporating time-dependent infectiousness and a standard SIR model for both fully-mixed and category-mixed populations. We find that the reproductive number only depends on the total infectious exposure and the largest eigenvalue of the mixing matrix and that these two effects are independent of each other. When we compare our time-dependent mean-field model to the SIR model with equivalent rates, the epidemic peak is advanced and modifying the infection rate function has a strong effect on the time dynamics of the epidemic. We also observe behavior akin to a traveling wave as individuals transition through infectious states.","463":"Several disastrous security attacks can be attributed to delays in patching software vulnerabilities. While researchers and practitioners have paid significant attention to automate vulnerabilities identification and patch development activities of software security patch management, there has been relatively little effort dedicated to gain an in-depth understanding of the socio-technical aspects, e.g., coordination of interdependent activities of the patching process and patching decisions, that may cause delays in applying security patches. We report on a Grounded Theory study of the role of coordination in security patch management. The reported theory consists of four inter-related dimensions, i.e., causes, breakdowns, constraints, and mechanisms. The theory explains the causes that define the need for coordination among interdependent software and hardware components and multiple stakeholders' decisions, the constraints that can negatively impact coordination, the breakdowns in coordination, and the potential corrective measures. This study provides potentially useful insights for researchers and practitioners who can carefully consider the needs of and devise suitable solutions for supporting the coordination of interdependencies involved in security patch management.","464":"In the spirit of Trimble's\"Astrophysics in XXXX' series, I very briefly and subjectively review developments in SETI in 2020. My primary focus is 75 papers and books published or made public in 2020, which I sort into six broad categories: results from actual searches, new search methods and instrumentation, target and frequency seleciton, the development of technosignatures, theory of ETIs, and social aspects of SETI.","465":"This paper presents the results of the wet lab information extraction task at WNUT 2020. This task consisted of two sub tasks: (1) a Named Entity Recognition (NER) task with 13 participants and (2) a Relation Extraction (RE) task with 2 participants. We outline the task, data annotation process, corpus statistics, and provide a high-level overview of the participating systems for each sub task.","466":"Deep Bayesian neural networks (BNNs) are a powerful tool, though computationally demanding, to perform parameter estimation while jointly estimating uncertainty around predictions. BNNs are typically implemented using arbitrary normal-distributed prior distributions on the model parameters. Here, we explore the effects of different prior distributions on classification tasks in BNNs and evaluate the evidence supporting the predictions based on posterior probabilities approximated by Markov Chain Monte Carlo sampling and by computing Bayes factors. We show that the choice of priors has a substantial impact on the ability of the model to confidently assign data to the correct class (true positive rates). Prior choice also affects significantly the ability of a BNN to identify out-of-distribution instances as unknown (false positive rates). When comparing our results against neural networks (NN) with Monte Carlo dropout we found that BNNs generally outperform NNs. Finally, in our tests we did not find a single best choice as prior distribution. Instead, each dataset yielded the best results under a different prior, indicating that testing alternative options can improve the performance of BNNs.","467":"Information and communication technologies (ICTs) have increasingly become vital for people on the move including the nearly 80 million displaced due to conflict, violence, and human right violations globally. However, existing research on ICTs and migrants, which almost entirely focused on migrants' ICT use 'en route' or within developed economies principally in the perspectives of researchers from these regions, is very fragmented posing a difficulty in understanding the key objects of research. Moreover, ICTs are often celebrated as liberating and exploitable at migrants' rational discretion even though they are 'double-edged swords' with significant risks, burdens, pressures and inequality challenges particularly for vulnerable migrants including those forcefully displaced and trafficked. Towards addressing these limitations and illuminating future directions, this paper, first, scrutinises the existing research vis-a-vis ICTs' liberating and authoritarian role particularly for vulnerable migrants whereby explicating key issues in the research domain. Second, it identifies key gaps and opportunities for future research. Using a tailored methodology, broad literature relating to ICTs and migration\/development published in the period 1990-2020 was surveyed resulting in 157 selected publications which were critically appraised vis-a-vis the key themes, major technologies dealt with, and methodologies and theories\/concepts adopted. Furthermore, key insights, trends, gaps, and future research opportunities pertaining to both the existing and missing objects of research in ICTs and migration\/development are spotlighted.","468":"In the last decade, Single-Board Computers (SBCs) have been employed more frequently in engineering and computer science both to technical and educational levels. Several factors such as the versatility, the low-cost, and the possibility to enhance the learning process through technology have contributed to the educators and students usually employ these devices. However, the implications, possibilities, and constraints of these devices in engineering and Computer Science (CS) education have not been explored in detail. In this systematic literature review, we explore how the SBCs are employed in engineering and computer science and what educational results are derived from their usage in the period 2010-2020 at tertiary education. For that, 154 studies were selected out of n=605 collected from the academic databases Ei Compendex, ERIC, and Inspec. The analysis was carried-out in two phases, identifying, e.g., areas of application, learning outcomes, and students and researchers' perceptions. The results mainly indicate the following aspects: (1) The areas of laboratories and e-learning, computing education, robotics, Internet of Things (IoT), and persons with disabilities gather the studies in the review. (2) Researchers highlight the importance of the SBCs to transform the curricula in engineering and CS for the students to learn complex topics through experimentation in hands-on activities. (3) The typical cognitive learning outcomes reported by the authors are the improvement of the students' grades and the technical skills regarding the topics in the courses. Concerning the affective learning outcomes, the increase of interest, motivation, and engagement are commonly reported by the authors.","469":"Since the introduction of electronic cigarettes to the United States market in 2007, vaping prevalence has surged in both adult and adolescent populations. E-cigarettes are advertised as a safer alternative to traditional cigarettes and as a method of smoking cessation, but the U.S. government and health professionals are concerned that e-cigarettes attract young non-smokers. Here, we develop and analyze a dynamical systems model of competition between traditional and electronic cigarettes for users. With this model, we predict the change in smoking prevalence due to the introduction of vaping, and we determine the conditions under which e-cigarettes present a net public health benefit or harm to society.","470":"Workers from a variety of industries rapidly shifted to remote work at the onset of the COVID-19 pandemic. While existing work has examined the impact of this shift on office workers, little work has examined how shifting from in-person to online work affected workers in the informal labor sector. We examine the impact of shifting from in-person to online-only work on a particularly marginalized group of workers: sex workers. Through 34 qualitative interviews with sex workers from seven countries in the Global North, we examine how a shift to online-only sex work impacted: (1) working conditions, (2) risks and protective behaviors, and (3) labor rewards. We find that online work offers benefits to sex workers' financial and physical well-being. However, online-only work introduces new and greater digital and mental health risks as a result of the need to be publicly visible on more platforms and to share more explicit content. From our findings we propose design and platform governance suggestions for digital sex workers and for informal workers more broadly, particularly those who create and sell digital content.","471":"We present a targeted search for continuous gravitational waves (GWs) from 236 pulsars using data from the third observing run of LIGO and Virgo (O3) combined with data from the second observing run (O2). Searches were for emission from the $l=m=2$ mass quadrupole mode with a frequency at only twice the pulsar rotation frequency (single harmonic) and the $l=2, m=1,2$ modes with a frequency of both once and twice the rotation frequency (dual harmonic). No evidence of GWs was found so we present 95\\% credible upper limits on the strain amplitudes $h_0$ for the single harmonic search along with limits on the pulsars' mass quadrupole moments $Q_{22}$ and ellipticities $\\varepsilon$. Of the pulsars studied, 23 have strain amplitudes that are lower than the limits calculated from their electromagnetically measured spin-down rates. These pulsars include the millisecond pulsars J0437\\textminus4715 and J0711\\textminus6830 which have spin-down ratios of 0.87 and 0.57 respectively. For nine pulsars, their spin-down limits have been surpassed for the first time. For the Crab and Vela pulsars our limits are factors of $\\sim 100$ and $\\sim 20$ more constraining than their spin-down limits, respectively. For the dual harmonic searches, new limits are placed on the strain amplitudes $C_{21}$ and $C_{22}$. For 23 pulsars we also present limits on the emission amplitude assuming dipole radiation as predicted by Brans-Dicke theory.","472":"In this paper we study some mathematical models describing evolution of population density and spread of epidemics in population systems in which spatial movement of individuals depends only on the departure and arrival locations and does not have apparent connection with the population density. We call such models as population migration models and migration epidemics models, respectively. We first apply the theories of positive operators and positive semigroups to make systematic investigation to asymptotic behavior of solutions of the population migration models as time goes to infinity, and next use such results to study asymptotic behavior of solutions of the migration epidemics models as time goes to infinity. Some interesting properties of solutions of these models are obtained.","473":"The growth of online Digital\/social media has allowed a variety of ideas and opinions to coexist. Social Media has appealed users due to the ease of fast dissemination of information at low cost and easy access. However, due to the growth in affordance of Digital platforms, users have become prone to consume disinformation, misinformation, propaganda, and conspiracy theories. In this paper, we wish to explore the links between the personality traits given by the Big Five Inventory and their susceptibility to disinformation. More speciDically, this study is attributed to capture the short- term as well as the long-term effects of disinformation and its effects on the Dive personality traits. Further, we expect to observe that different personalities traits have different shifts in opinion and different increase or decrease of uncertainty on an issue after consuming the disinformation. Based on the Dindings of this study, we would like to propose a personalized narrative-based change in behavior for different personality traits.","474":"This paper analyzes the influence of partisan content from national cable TV news on local reporting in U.S. newspapers. We provide a new machine-learning-based measure of cable news slant, trained on a corpus of 40K transcribed TV episodes from Fox News Channel (FNC), CNN, and MSNBC (2005-2008). Applying the method to a corpus of 24M local newspaper articles, we find that in response to an exogenous increase in local viewership of FNC relative to CNN\/MSNBC, local newspaper articles become more similar to FNC transcripts (and vice versa). Consistent with newspapers responding to changes in reader preferences, we see a shift in the framing of local news coverage rather than just direct borrowing of cable news content. Further, cable news slant polarizes local news content: right-leaning newspapers tend to adopt right-wing FNC language, while left-leaning newspapers tend to become more left-wing. Media slant is contagious.","475":"Since the onset of the COVID-19's global spread we have been following the debate around contact tracing apps -- the tech-enabled response to the pandemic. As corporations, academics, governments, and civil society discuss the right way to implement these apps, we noticed recurring implicit assumptions. The proposed solutions are designed for a world where Internet access and smartphone ownership are a given, people are willing and able to install these apps, and those who receive notifications about potential exposure to the virus have access to testing and can isolate safely. In this work we challenge these assumptions. We not only show that there are not enough smartphones worldwide to reach required adoption thresholds but also highlight a broad lack of internet access, which affects certain groups more: the elderly, those with lower incomes, and those with limited ability to socially distance. Unfortunately, these are also the groups that are at the highest risks from COVID-19. We also report that the contact tracing apps that are already deployed on an opt-in basis show disappointing adoption levels. We warn about the potential consequences of over-extending the existing state and corporate surveillance powers. Finally, we describe a multitude of scenarios where contact tracing apps will not help regardless of access or policy. In this work we call for a comprehensive and equitable policy response that prioritizes the needs of the most vulnerable, protects human rights, and considers long term impact instead of focusing on technology-first fixes.","476":"Pedestrians are often encountered walking in the company of some social relations, rather than alone. The social groups thus formed, in variable proportions depending on the context, are not randomly organised but exhibit distinct features, such as the well-known tendency of 3-member groups to be arranged in a V-shape. The existence of group structures is thus likely to impact the collective dynamics of the crowd, possibly in a critical way when emergency situations are considered. After turning a blind eye to these group aspects for years, endeavours to model groups in crowd simulation software have thrived in the past decades. This fairly short review opens on a description of their empirical characteristics and their impact on the global flow. Then, it aims to offer a pedagogical discussion of the main strategies to model such groups, within different types of models, in order to provide guidance for prospective modellers.","477":"The integration of electric vehicles (EVs) with the energy grid has become an important area of research due to the increasing EV penetration in today's transportation systems. Under appropriate management of EV charging and discharging, the grid can currently satisfy the energy requirements of a considerable number of EVs. Furthermore, EVs can help enhance the reliability and stability of the energy grid through ancillary services such as energy storage. This paper proposes the EV routing problem with time windows under time-variant electricity prices (EVRPTW-TP) which optimizes the routing of an EV fleet that are delivering products to customers, jointly with the scheduling of the charging and discharging of the EVs from\/to the grid. The proposed model is a multiperiod vehicle routing problem where EVs can stop at charging stations to either recharge their batteries or inject stored energy to the grid. Given the energy costs that vary based on time-of-use, the charging and discharging schedules of the EVs are optimized to benefit from the capability of storing energy by shifting energy demands from peak hours to off-peak hours when the energy price is lower. The vehicles can recover the energy costs and potentially realize profits by injecting energy back to the grid at high price periods. EVRPTW-TP is formulated as an optimization problem. A Lagrangian relaxation approach and a hybrid variable neighborhood search\/tabu search heuristic are proposed to obtain high quality lower bounds and feasible solutions, respectively. Numerical experiments on instances from the literature are provided. The proposed heuristic is also evaluated on a case study of an EV fleet providing grocery delivery at the region of Kitchener-Waterloo in Ontario, Canada. Insights on the impacts of energy pricing, service time slots, range reduction in winter as well as fleet size are presented.","478":"We propose a multi-layer network model for the spread of COVID-19 that accounts for interactions within the family, between schoolmates, and casual contacts in the population. We utilize the proposed model-calibrated on epidemiological and demographic data-to investigate current questions concerning the implementation of non-pharmaceutical interventions (NPIs) during the vaccination campaign. Specifically, we consider scenarios in which the most fragile population has already received the vaccine, and we focus our analysis on the role of schools as drivers of the contagions and on the implementation of targeted intervention policies oriented to children and their families. We perform our analysis by means of a campaign of Monte Carlo simulations. Our findings suggest that, in a phase with NPIs enacted but in-person education, children play a key role in the spreading of COVID-19. Interestingly, we show that children's testing might be an important tool to flatten the epidemic curve, in particular when combined with enacting temporary online education for classes in which infected students are detected. Finally, we test a vaccination strategy that prioritizes the members of large families and we demonstrate its good performance. We believe that our modeling framework and our findings could be of help for public health authorities for planning their current and future interventions, as well as to increase preparedness for future epidemic outbreaks.","479":"Small businesses (0-19 employees) are becoming attractive targets for cyber-criminals, but struggle to implement cyber-security measures that large businesses routinely deploy. There is an urgent need for effective and suitable cyber-security solutions for small businesses as they employ a significant proportion of the workforce. In this paper, we consider the small business cyber-security challenges not currently addressed by research or products, contextualised via an Australian lens. We also highlight some unique characteristics of small businesses conducive to cyber-security actions. Small business cyber-security discussions to date have been narrow in focus and lack re-usability beyond specific circumstances. Our study uses global evidence from industry, government and research communities across multiple disciplines. We explore the technical and non-technical factors negatively impacting a small business' ability to safeguard itself, such as resource constraints, organisational process maturity, and legal structures. Our research shows that some small business characteristics, such as agility, large cohort size, and piecemeal IT architecture, could allow for increased cyber-security. We conclude that there is a gap in current research in small business cyber-security. In addition, legal and policy work are needed to help small businesses become cyber-resilient.","480":"We consider the development of hyperbolic transport models for the propagation in space of an epidemic phenomenon described by a classical compartmental dynamics. The model is based on a kinetic description at discrete velocities of the spatial movement and interactions of a population of susceptible, infected and recovered individuals. Thanks to this, the unphysical feature of instantaneous diffusive effects, which is typical of parabolic models, is removed. In particular, we formally show how such reaction-diffusion models are recovered in an appropriate diffusive limit. The kinetic transport model is therefore considered within a spatial network, characterizing different places such as villages, cities, countries, etc. The transmission conditions in the nodes are analyzed and defined. Finally, the model is solved numerically on the network through a finite-volume IMEX method able to maintain the consistency with the diffusive limit without restrictions due to the scaling parameters. Several numerical tests for simple epidemic network structures are reported and confirm the ability of the model to correctly describe the spread of an epidemic.","481":"The SARS-CoV-2 infectious outbreak has rapidly spread across the globe and precipitated varying policies to effectuate physical distancing to ameliorate its impact. In this study, we propose a new hybrid machine learning model, SIRNet, for forecasting the spread of the COVID-19 pandemic that couples with the epidemiological models. We use categorized spatiotemporally explicit cellphone mobility data as surrogate markers for physical distancing, along with population weighted density and other local data points. We demonstrate at varying geographical granularity that the spectrum of physical distancing options currently being discussed among policy leaders have epidemiologically significant differences in consequences, ranging from viral extinction to near complete population prevalence. The current mobility inflection points vary across geographical regions. Experimental results from SIRNet establish preliminary bounds on such localized mobility that asymptotically induce containment. The model can support in studying non-pharmacological interventions and approaches that minimize societal collateral damage and control mechanisms for an extended period of time.","482":"Social media platforms have been exploited to disseminate misinformation in recent years. The widespread online misinformation has been shown to affect users' beliefs and is connected to social impact such as polarization. In this work, we focus on misinformation's impact on specific user behavior and aim to understand whether general Twitter users changed their behavior after being exposed to misinformation. We compare the before and after behavior of exposed users to determine whether the frequency of the tweets they posted, or the sentiment of their tweets underwent any significant change. Our results indicate that users overall exhibited statistically significant changes in behavior across some of these metrics. Through language distance analysis, we show that exposed users were already different from baseline users before the exposure. We also study the characteristics of two specific user groups, multi-exposure and extreme change groups, which were potentially highly impacted. Finally, we study if the changes in the behavior of the users after exposure to misinformation tweets vary based on the number of their followers or the number of followers of the tweet authors, and find that their behavioral changes are all similar.","483":"Ocean current, fluid mechanics, and many other spatio-temporal physical dynamical systems are essential components of the universe. One key characteristic of such systems is that certain physics laws -- represented as ordinary\/partial differential equations (ODEs\/PDEs) -- largely dominate the whole process, irrespective of time or location. Physics-informed learning has recently emerged to learn physics for accurate prediction, but they often lack a mechanism to leverage localized spatial and temporal correlation or rely on hard-coded physics parameters. In this paper, we advocate a physics-coupled neural network model to learn parameters governing the physics of the system, and further couple the learned physics to assist the learning of recurring dynamics. A spatio-temporal physics-coupled neural network (ST-PCNN) model is proposed to achieve three goals: (1) learning the underlying physics parameters, (2) transition of local information between spatio-temporal regions, and (3) forecasting future values for the dynamical system. The physics-coupled learning ensures that the proposed model can be tremendously improved by using learned physics parameters, and can achieve good long-range forecasting (e.g., more than 30-steps). Experiments, using simulated and field-collected ocean current data, validate that ST-PCNN outperforms existing physics-informed models.","484":"We measure the effect of different public health regulations to the spread of COVID-19, based on a SEIRA model -- a SEIR model including asymptomatic transmissions. The cumulative confirmed cases and death show nonlinear positive relationship with the value of asymptomatic rate. Based on this model, we analyze the inhibit effects to COVID-19 of three types of public health policies, i.e. isolation of laboratory confirmed cases, general personal protection and quarantine (lock-down). The simulations conclude that the isolation display limited effects to the asymptomatic viral carriers. The general personal protection and quarantine perform similar effects when the their percentages of participants are same. When the total proportion of asymptomatic, mild symptomatic and neglected patients is 40%, only depends on isolation policy may lead to an additional 75% infections, compared with general personal protection or quarantine with an efficiency 80%. At end, we provide seven recommendations of public health intervention before and during an aerial transmitted epidemic (COVID-19).","485":"Recently we see a rising number of methods in the field of eXplainable Artificial Intelligence. To our surprise, their development is driven by model developers rather than a study of needs for human end users. The analysis of needs, if done, takes the form of an A\/B test rather than a study of open questions. To answer the question\"What would a human operator like to ask the ML model?\"we propose a conversational system explaining decisions of the predictive model. In this experiment, we developed a chatbot called dr_ant to talk about machine learning model trained to predict survival odds on Titanic. People can talk with dr_ant about different aspects of the model to understand the rationale behind its predictions. Having collected a corpus of 1000+ dialogues, we analyse the most common types of questions that users would like to ask. To our knowledge, it is the first study which uses a conversational system to collect the needs of human operators from the interactive and iterative dialogue explorations of a predictive model.","486":"Deep learning methodologies have been employed in several different fields, with an outstanding success in image recognition applications, such as material quality control, medical imaging, autonomous driving, etc. Deep learning models rely on the abundance of labelled observations to train a prospective model. These models are composed of millions of parameters to estimate, increasing the need of more training observations. Frequently it is expensive to gather labelled observations of data, making the usage of deep learning models not ideal, as the model might over-fit data. In a semi-supervised setting, unlabelled data is used to improve the levels of accuracy and generalization of a model with small labelled datasets. Nevertheless, in many situations different unlabelled data sources might be available. This raises the risk of a significant distribution mismatch between the labelled and unlabelled datasets. Such phenomena can cause a considerable performance hit to typical semi-supervised deep learning frameworks, which often assume that both labelled and unlabelled datasets are drawn from similar distributions. Therefore, in this paper we study the latest approaches for semi-supervised deep learning for image recognition. Emphasis is made in semi-supervised deep learning models designed to deal with a distribution mismatch between the labelled and unlabelled datasets. We address open challenges with the aim to encourage the community to tackle them, and overcome the high data demand of traditional deep learning pipelines under real-world usage settings.","487":"The rise of streaming libraries such as Akka Stream, Reactive Extensions, and LINQ popularized the declarative functional style of data processing. The stream paradigm offers concise syntax to write down processing pipelines to consume the vast amounts of real-time data available today. These libraries offer the programmer a domain specific language (DSL) embedded in the host language to describe data streams. These libraries however, all suffer from extensibility issues. The semantics of a stream is hard-coded into the DSL language and cannot be changed by the user of the library. We introduce an approach to modify the semantics of a streaming library by means of meta-programming at both run-time and compile-time, and showcase its generality. We show that the expressiveness of the meta-facilities is strong enough to enable push and pull semantics, error handling, parallelism, and operator fusion. We evaluate our work by implementing the identified shortcomings in terms of a novel stream meta-architecture and show that its design and architecture adhere to the design principles of a meta-level architecture. The state of the art offers plenty of choice to programmers regarding reactive stream processing libraries. Expressing reactive systems is otherwise difficult to do in general purpose languages. Extensibility and fine-tuning should be possible in these libraries to ensure a broad variety of applications can be expressed within this single DSL.","488":"The rise in computational capability has increased reliance on simulations to inform aircraft design. However aircraft airworthiness testing for flight certification remains rooted in real-world experiments performed after manufacturing an aircraft prototype. Leveraging multi-fidelity modeling and uncertainty quantification, we present a framework creating a stochastic representation of the aircraft, uses it to simulate flight certification maneuvers, and determines the likelihood of successfully meeting the certification requirement. We focus on uncertainties associated with Computational Fluid Dynamics simulations solving the Reynolds-Averaged Navier-Stokes equations. The simulation predictions and associated uncertainties are combined with data from other analysis tools to create stochastic aerodynamics and controls databases. The databases describe the aircraft's behavior across its flight envelope and provide probability distributions for its predictions. Databases are generated for two aircraft configurations, the National Aeronautics and Space Administration (NASA) Common Research Model and the Generic T-tail Transport aircraft. Samples from the databases, representing different aircraft behavior, are created. Each sample is run through a flight simulation representing a real-world airworthiness test performed by the Federal Aviation Administration (FAA). These tests are agglomerated to create distributions of the performance metrics, quantifying the probability that the aircraft succeeds in performing the certification maneuver. Simulating flight certification testing before building a full-size aircraft prototype mitigates the enormous costs of expensive redesigns late in the aircraft design process. The calculation of the failure rates provides design suggestions to ensure the aircraft can meet the certification requirement with a prescribed success rate.","489":"Companies have considered adoption of various high-level artificial intelligence (AI) principles for responsible AI, but there is less clarity on how to implement these principles as organizational practices. This paper reviews the principles-to-practices gap. We outline five explanations for this gap ranging from a disciplinary divide to an overabundance of tools. In turn, we argue that an impact assessment framework which is broad, operationalizable, flexible, iterative, guided, and participatory is a promising approach to close the principles-to-practices gap. Finally, to help practitioners with applying these recommendations, we review a case study of AI's use in forest ecosystem restoration, demonstrating how an impact assessment framework can translate into effective and responsible AI practices.","490":"Graph Neural Networks (GNNs) have recently emerged as a robust framework for graph-structured data. They have been applied to many problems such as knowledge graph analysis, social networks recommendation, and even Covid19 detection and vaccine developments. However, unlike other deep neural networks such as Feed Forward Neural Networks (FFNNs), few analyses such as verification and property inferences exist, potentially due to dynamic behaviors of GNNs, which can take arbitrary graphs as input, whereas FFNNs which only take fixed size numerical vectors as inputs. This paper proposes an approach to analyze GNNs by converting them into FFNNs and reusing existing FFNNs analyses. We discuss various designs to ensure the scalability and accuracy of the conversions. We illustrate our method on a study case of node classification. We believe that our approach opens new research directions for understanding and analyzing GNNs.","491":"Viruses causing flu or milder coronavirus colds are often referred to as\"seasonal viruses\"as they tend to subside in warmer months. In other words, meteorological conditions tend to impact the activity of viruses, and this information can be exploited for the operational management of hospitals. In this study, we use three years of daily data from one of the biggest hospitals in Switzerland and focus on modelling the extremes of hospital visits from patients showing flu-like symptoms and the number of positive cases of flu. We propose employing a discrete Generalized Pareto distribution for the number of positive and negative cases, and a Generalized Pareto distribution for the odds of positive cases. Our modelling framework allows for the parameters of these distributions to be linked to covariate effects, and for outlying observations to be dealt with via a robust estimation approach. Because meteorological conditions may vary over time, we use meteorological and not calendar variations to explain hospital charge extremes, and our empirical findings highlight their significance. We propose a measure of hospital congestion and a related tool to estimate the resulting CaRe (Charge-at-Risk-estimation) under different meteorological conditions. The relevant numerical computations can be easily carried out using the freely available GJRM R package. The introduced approach could be applied to several types of seasonal disease data such as those derived from the new virus SARS-CoV-2 and its COVID-19 disease which is at the moment wreaking havoc worldwide. The empirical effectiveness of the proposed method is assessed through a simulation study.","492":"Let $p(n)$ denote the smallest prime divisor of the integer $n$. Define the function $g(k)$ to be the smallest integer $>k+1$ such that $p(\\binom{g(k)}{k})>k$. So we have $g(2)=6$ and $g(3)=g(4)=7$. In this paper we present the following new results on the Erd\\H{o}s-Selfridge function $g(k)$: We present a new algorithm to compute the value of $g(k)$, and use it to both verify previous work and compute new values of $g(k)$, with our current limit being $$ g(323)= 1\\ 69829\\ 77104\\ 46041\\ 21145\\ 63251\\ 22499. $$ We define a new function $\\hat{g}(k)$, and under the assumption of our Uniform Distribution Heuristic we show that $$ \\log g(k) = \\log \\hat{g}(k) + O(\\log k) $$ with high\"probability\". We also provide computational evidence to support our claim that $\\hat{g}(k)$ estimates $g(k)$ reasonably well in practice. There are several open conjectures on the behavior of $g(k)$ which we are able to prove for $\\hat{g}(k)$, namely that $$ 0.525\\ldots +o(1) \\quad \\le \\quad \\frac{\\log \\hat{g}(k)}{k\/\\log k} \\quad \\le \\quad 1+o(1), $$ and that $$ \\limsup_{k\\rightarrow\\infty} \\frac{\\hat{g}(k+1)}{\\hat{g}(k)}=\\infty.$$ Let $G(x,k)$ count the number of integers $n\\le x$ such that $p(\\binom{n}{k})>k$. Unconditionally, we prove that for large $x$, $G(x,k)$ is asymptotic to $x\/\\hat{g}(k)$. And finally, we show that the running time of our new algorithm is at most $g(k) \\exp[ -c (k\\log\\log k) \/(\\log k)^2 (1+o(1))]$ for a constant $c>0$.","493":"Modern generative models achieve excellent quality in a variety of tasks including image or text generation and chemical molecule modeling. However, existing methods often lack the essential ability to generate examples with requested properties, such as the age of the person in the photo or the weight of the generated molecule. Incorporating such additional conditioning factors would require rebuilding the entire architecture and optimizing the parameters from scratch. Moreover, it is difficult to disentangle selected attributes so that to perform edits of only one attribute while leaving the others unchanged. To overcome these limitations we propose PluGeN (Plugin Generative Network), a simple yet effective generative technique that can be used as a plugin to pre-trained generative models. The idea behind our approach is to transform the entangled latent representation using a flow-based module into a multi-dimensional space where the values of each attribute are modeled as an independent one-dimensional distribution. In consequence, PluGeN can generate new samples with desired attributes as well as manipulate labeled attributes of existing examples. Due to the disentangling of the latent representation, we are even able to generate samples with rare or unseen combinations of attributes in the dataset, such as a young person with gray hair, men with make-up, or women with beards. We combined PluGeN with GAN and VAE models and applied it to conditional generation and manipulation of images and chemical molecule modeling. Experiments demonstrate that PluGeN preserves the quality of backbone models while adding the ability to control the values of labeled attributes.","494":"Structural global parameter identifiability indicates whether one can determine a parameter's value from given inputs and outputs in the absence of noise. If a given model has parameters for which there may be infinitely many values, such parameters are called non-identifiable. We present a procedure for accelerating a global identifiability query by eliminating algebraically independent non-identifiable parameters. Our proposed approach significantly improves performance across different computer algebra frameworks.","495":"We study the SIR epidemiological model, with a variable contagion rate, applied to the evolution of COVID19 in Cuba. It is highlighted that an increase in the predictive character depends on understanding the dynamics for the temporal evolution of the rate of contagion $\\beta^*$. A semi-empirical model for this dynamics is formulated, where reaching $\\beta^*\\approx0$ due to isolation is achieved after the mean duration of the disease $\\tau=1\/\\gamma$, in which the number of infected in the confined families has decreased. It is considered that $\\beta^*(t)$ should have an abrupt decrease on the day of initiation of confinement and decrease until canceling at the end of the interval $\\tau$. The analysis describes appropriately the infection curve for Germany. The model is applied to predict an infection curve for Cuba, which estimates a maximum number of infected as less than 2000 in the middle of May, depending on the rigor of the isolation. This is suggested by the ratio between the daily detected cases and the total. We consider the ratio between the observed and real infected cases (k) less than unity. The low value of k decreases the maximum obtained when $\\beta^*-\\gamma>0$. The observed evolution is independent of k in the linear region. The value of $\\beta^*$ is also studied by time intervals, adjusting to the data of Cuba, Germany and South Korea. We compare the extrapolation of the evolution of Cuba with the contagion rate until 16.04.20 with that obtained by a strict quarantine at the end of April. This model with variable $\\beta^*$ correctly describes the observed infected evolution curves. We emphasize that the desired maximum of the SIR infected curve is not the maximum standard with constant $\\beta^*$, but one achieved due to quarantine when $\\tilde R_0=\\beta^*\/\\gamma<1$. For the countries controlling the epidemic the maxima are in the region in which SIR equations are linear.","496":"Urban analytics combines spatial analysis, statistics, computer science, and urban planning to understand and shape city futures. While it promises better policymaking insights, concerns exist around its epistemological scope and impacts on privacy, ethics, and social control. This chapter reflects on the history and trajectory of urban analytics as a scholarly and professional discipline. In particular, it considers the direction in which this field is going and whether it improves our collective and individual welfare. It first introduces early theories, models, and deductive methods from which the field originated before shifting toward induction. It then explores urban network analytics that enrich traditional representations of spatial interaction and structure. Next it discusses urban applications of spatiotemporal big data and machine learning. Finally, it argues that privacy and ethical concerns are too often ignored as ubiquitous monitoring and analytics can empower social repression. It concludes with a call for a more critical urban analytics that recognizes its epistemological limits, emphasizes human dignity, and learns from and supports marginalized communities.","497":"Highest density regions refer to level sets containing points of relatively high density. Their estimation from a random sample, generated from the underlying density, allows to determine the clusters of the corresponding distribution. This task can be accomplished considering different nonparametric perspectives. From a practical point of view, reconstructing highest density regions can be interpreted as a way of determining hot-spots, a crucial task for understanding COVID-19 space-time evolution. In this work, we compare the behavior of classical plug-in methods and a recently proposed hybrid algorithm for highest density regions estimation through an extensive simulation study. Both methodologies are applied to analyze a real data set about COVID-19 cases in the United States.","498":"Uber's business is highly real-time in nature. PBs of data is continuously being collected from the end users such as Uber drivers, riders, restaurants, eaters and so on everyday. There is a lot of valuable information to be processed and many decisions must be made in seconds for a variety of use cases such as customer incentives, fraud detection, machine learning model prediction. In addition, there is an increasing need to expose this ability to different user categories, including engineers, data scientists, executives and operations personnel which adds to the complexity. In this paper, we present the overall architecture of the real-time data infrastructure and identify three scaling challenges that we need to continuously address for each component in the architecture. At Uber, we heavily rely on open source technologies for the key areas of the infrastructure. On top of those open-source software, we add significant improvements and customizations to make the open-source solutions fit in Uber's environment and bridge the gaps to meet Uber's unique scale and requirements. We then highlight several important use cases and show their real-time solutions and tradeoffs. Finally, we reflect on the lessons we learned as we built, operated and scaled these systems.","499":"In this paper we propose a theoretical model including a susceptible-infected-recovered-dead (SIRD) model of epidemic in a dynamic macroeconomic general equilibrium framework with agents' mobility. The latter affect both their income (and consumption) and their probability of infecting and of being infected. Strategic complementarities among individual mobility choices drive the evolution of aggregate economic activity, while infection externalities caused by individual mobility affect disease diffusion. Rational expectations of forward looking agents on the dynamics of aggregate mobility and epidemic determine individual mobility decisions. The model allows to evaluate alternative scenarios of mobility restrictions, especially policies dependent on the state of epidemic. We prove the existence of an equilibrium and provide a recursive construction method for finding equilibrium(a), which also guides our numerical investigations. We calibrate the model by using Italian experience on COVID-19 epidemic in the period February 2020 - May 2021. We discuss how our economic SIRD (ESIRD) model produces a substantially different dynamics of economy and epidemic with respect to a SIRD model with constant agents' mobility. Finally, by numerical explorations we illustrate how the model can be used to design an efficient policy of state-of-epidemic-dependent mobility restrictions, which mitigates the epidemic peaks stressing health system, and allows for trading-off the economic losses due to reduced mobility with the lower death rate due to the lower spread of epidemic.","500":"In this paper, we perform Monte Carlo calculations to study the critical behavior of the spread of infectious diseases through a novel approach to the SIR epidemiological model. A stochastic lattice gas version of the model was applied on hybrid lattices which, in turn, are generated from typical square lattices when inserting a connection probability $p$ that a given lattice site has both first- and second-nearest neighbor interactions. By combining percolation theory and finite-size scaling analysis, we estimate both the critical threshold and leading critical exponent ratios of the non-absorbing SIR model in different cases of hybrid lattices. An analysis of the average size of the percolating cluster and the size distribution of non-percolating clusters of recovered individuals was carried out to determine the universality class of the model.","501":"We study multi-patch epidemic models where individuals may migrate from one patch to another in either of the susceptible, exposed\/latent, infectious and recovered states. We assume that infections occur both locally with a rate that depends on the patch as well as\"from distance\"from all the other patches. The exposed and infectious periods have general distributions, and are not affected by the possible migrations of the individuals. The migration processes in either of the three states are assumed to be Markovian, and independent of the exposed and infectious periods. We establish a functional law of large number (FLLN) and a function central limit theorem (FCLT) for the susceptible, exposed\/latent, infectious and recovered processes. In the FLLN, the limit is determined by a set of Volterra integral equations. In the special case of deterministic exposed and infectious periods, the limit becomes a system of ODEs with delays. In the FCLT, the limit is given by a set of stochastic Volterra integral equations driven by a sum of independent Brownian motions and continuous Gaussian processes with an explicit covariance structure.","502":"We report a statistical analysis of some highly infected countries by the novel coronavirus (COVID-19). The cumulative infected data were fitted with various growth models (e.g. Logistic equation, Weibull equation and Hill equation) and obtained the power index of top ten highly infected countries. The newly infected data were fitted with Gaussian distribution with the peak at ~40 days for the countries whose infection curves are seem to be saturated. The similarity in growth kinetics of infected people of different countries provides first-hand guidelines to take proper precautions to minimize human damage.","503":"Agent-based models play an important role in simulating complex emergent phenomena and supporting critical decisions. In this context, a software fault may result in poorly informed decisions that lead to disastrous consequences. The ability to rigorously test these models is therefore essential. In this systematic literature review, we answer five research questions related to the key aspects of test case generation in agent-based models: What are the information artifacts used to generate tests? How are these tests generated? How is a verdict assigned to a generated test? How is the adequacy of a generated test suite measured? What level of abstraction of an agent-based model is targeted by a generated test? Our results show that whilst the majority of techniques are effective for testing functional requirements at the agent and integration levels of abstraction, there are comparatively few techniques capable of testing society-level behaviour. Additionally, we identify a need for more thorough evaluation using realistic case studies that feature challenging properties associated with a typical agent-based model.","504":"We develop a Bayesian non-parametric quantile panel regression model. Within each quantile, the response function is a convex combination of a linear model and a non-linear function, which we approximate using Bayesian Additive Regression Trees (BART). Cross-sectional information at the pth quantile is captured through a conditionally heteroscedastic latent factor. The non-parametric feature of our model enhances flexibility, while the panel feature, by exploiting cross-country information, increases the number of observations in the tails. We develop Bayesian Markov chain Monte Carlo (MCMC) methods for estimation and forecasting with our quantile factor BART model (QF-BART), and apply them to study growth at risk dynamics in a panel of 11 advanced economies.","505":"The spread of misinformation and\"fake news\"continues to be a major focus of public concern. A great deal of research has examined who falls for misinformation and why, and what can be done to make people more discerning consumers of news. Comparatively little work, however, has considered misinformation producers, and how their strategies interact with the psychology of news consumers. Here we use game-theoretic models to study the strategic interaction between news publishers and news readers. We show that publishers who seek to spread misinformation can generate high engagement with falsehoods by using strategies that mix true and false stories over time, in such a way that they serve more false stories to more loyal readers. These coercive strategies cause false stories to receive higher reader engagement than true stories - even when readers strictly prefer truth over falsehood. In contrast, publishers who seek to promote engagement with accurate information will use strategies that generate more engagement with true stories than with false stories. We confirm these predictions empirically by examining 1,000 headlines from 20 mainstream and 20 fake news sites, comparing Facebook engagement data with 20,000 perceived accuracy ratings collected in a survey experiment. We show that engagement is negatively correlated with perceived accuracy among misinformation sites, but positively correlated with perceived accuracy among mainstream sites. We then use our model to analyze the conditions under which news sites seeking engagement will produce false stories. We show that if a publisher incorrectly assumes that readers prefer falsehoods, their resulting publication strategy can itself manufacture greater engagement with false news - leading to a self-reinforcing cycle of false news promotion.","506":"From global pandemics to geopolitical turmoil, leaders in logistics, product allocation, procurement and operations are facing increasing difficulty with safeguarding their organizations against supply chain vulnerabilities. It is recommended to opt for forecasting against trending based benchmark because auditing a future forecast puts more focus on seasonality. The forecasting models provide with end-to-end, real time oversight of the entire supply chain, while utilizing predictive analytics and artificial intelligence to identify potential disruptions before they occur. By combining internal and external data points, coming up with an AI-enabled modelling engine can greatly reduce risk by helping retail companies proactively respond to supply and demand variability. This research paper puts focus on creating an ingenious way to tackle the impact of COVID19 on Supply chain, product allocation, trending and seasonality. Key words: Supply chain, covid-19, forecasting, coronavirus, manufacturing, seasonality, trending, retail.","507":"With the surge of pretrained language models, a new pathway has been opened to incorporate Persian text contextual information. Meanwhile, as many other countries, including Iran, are fighting against COVID-19, a plethora of COVID-19 related articles has been published in Iranian Healthcare magazines to better inform the public of the situation. However, finding answers in this sheer volume of information is an extremely difficult task. In this paper, we collected a large dataset of these articles, leveraged different BERT variations as well as other keyword models such as BM25 and TF-IDF, and created a search engine to sift through these documents and rank them, given a user's query. Our final search engine consists of a ranker and a re-ranker, which adapts itself to the query. We fine-tune our models using Semantic Textual Similarity and evaluate them with standard task metrics. Our final method outperforms the rest by a considerable margin.","508":"Developing a robust algorithm to diagnose and quantify the severity of COVID-19 using Chest X-ray (CXR) requires a large number of well-curated COVID-19 datasets, which is difficult to collect under the global COVID-19 pandemic. On the other hand, CXR data with other findings are abundant. This situation is ideally suited for the Vision Transformer (ViT) architecture, where a lot of unlabeled data can be used through structural modeling by the self-attention mechanism. However, the use of existing ViT is not optimal, since feature embedding through direct patch flattening or ResNet backbone in the standard ViT is not intended for CXR. To address this problem, here we propose a novel Vision Transformer that utilizes low-level CXR feature corpus obtained from a backbone network that extracts common CXR findings. Specifically, the backbone network is first trained with large public datasets to detect common abnormal findings such as consolidation, opacity, edema, etc. Then, the embedded features from the backbone network are used as corpora for a Transformer model for the diagnosis and the severity quantification of COVID-19. We evaluate our model on various external test datasets from totally different institutions to evaluate the generalization capability. The experimental results confirm that our model can achieve the state-of-the-art performance in both diagnosis and severity quantification tasks with superior generalization capability, which are sine qua non of widespread deployment.","509":"This paper introduces the third DIHARD challenge, the third in a series of speaker diarization challenges intended to improve the robustness of diarization systems to variation in recording equipment, noise conditions, and conversational domain. The challenge comprises two tracks evaluating diarization performance when starting from a reference speech segmentation (track 1) and diarization from raw audio scratch (track 2). We describe the task, metrics, datasets, and evaluation protocol.","510":"Due to the abrupt arise of pandemic worldwide, the video conferencing platforms are becoming ubiquitously available and being embedded into either various digital devices or the collaborative daily work. Even though the service provider has designed many security functions to protect individual's privacy, such as virtual background (VB), it still remains to be explored that how the instability of VB leaks users' privacy or impacts their mentality and behaviours. In order to understand and locate implications for the contextual of the end-users' privacy awareness and its mental model, we will conduct survey and interviews for users as the first stage research. We will raise conceptual challenges in terms of the designing safety and stable VB, as well as provide design suggestions.","511":"As the coronavirus disease 2019 (COVID-19) has shown profound effects on public health and the economy worldwide, it becomes crucial to assess the impact on the virus transmission and develop effective strategies to address the challenge. A new statistical model derived from the SIR epidemic model with functional parameters is proposed to understand the impact of weather and government interventions on the virus spread in the presence of asymptomatic infections among eight metropolitan areas in the United States. The model uses Bayesian inference with Gaussian process priors to study the functional parameters nonparametrically, and sensitivity analysis is adopted to investigate the main and interaction effects of these factors. This analysis reveals several important results including the potential interaction effects between weather and government interventions, which shed new light on the effective strategies for policymakers to mitigate the COVID-19 outbreak.","512":"PEACH Tree is an easy-to-use, online tool for displaying multiple sequence alignments and phylogenetic trees side-by-side. PEACH Tree is powerful for rapidly tracing evolutionary and transmission histories by filtering invariant sites out of the display, and allowing samples to readily be filtered out of the display. These features, coupled with the ability to display epidemiological metadata, make the tool suitable for infectious disease epidemiology. PEACH Tree further enables much needed communication between the fields of genomics and infectious disease epidemiology, as exemplified by the COVID-19 pandemic.","513":"The creation, dissemination, and consumption of disinformation and fabricated content on social media is a growing concern, especially with the ease of access to such sources, and the lack of awareness of the existence of such false information. In this paper, we present an overview of the techniques explored to date for the combating of disinformation with various forms. We introduce different forms of disinformation, discuss factors related to the spread of disinformation, elaborate on the inherent challenges in detecting disinformation, and show some approaches to mitigating disinformation via education, research, and collaboration. Looking ahead, we present some promising future research directions on disinformation.","514":"Lung cancer is the leading cause of cancer death and morbidity worldwide. Many studies have shown machine learning models to be effective at detecting lung nodules from chest X-ray images. However, these techniques have yet to be embraced by the medical community due to several practical, ethical, and regulatory constraints stemming from the black-box nature of deep learning models. Additionally, most lung nodules visible on chest X-ray are benign; therefore, the narrow task of computer vision-based lung nodule detection cannot be equated to automated lung cancer detection. Addressing both concerns, this study introduces a novel hybrid deep learning and decision tree-based computer vision model which presents lung cancer malignancy predictions as interpretable decision trees. The deep learning component of this process is trained using a large publicly available dataset on pathological biomarkers associated with lung cancer. These models are then used to inference biomarker scores for chest X-ray images from two, independent data sets for which malignancy metadata is available. We mine multi-variate predictive models by fitting shallow decision trees to the malignancy stratified datasets and interrogate a range of metrics to determine the best model. Our best decision tree model achieves sensitivity and specificity of 86.7% and 80.0% respectively with a positive predictive value of 92.9%. Decision trees mined using this method may be considered as a starting point for refinement into clinically useful multi-variate lung cancer malignancy models for implementation as a workflow augmentation tool to improve the efficiency of human radiologists.","515":"Contact tracing is a very powerful method to implement and enforce social distancing to avoid spreading of infectious diseases. The traditional approach of contact tracing is time consuming, manpower intensive, dangerous and prone to error due to fatigue or lack of skill. Due to this there is an emergence of mobile based applications for contact tracing. These applications primarily utilize a combination of GPS based absolute location and Bluetooth based relative location remitted from user's smartphone to infer various insights. These applications have eased the task of contact tracing; however, they also have severe implication on user's privacy, for example, mass surveillance, personal information leakage and additionally revealing the behavioral patterns of the user. This impact on user's privacy leads to trust deficit in these applications, and hence defeats their purpose. In this work we discuss the various scenarios which a contact tracing application should be able to handle. We highlight the privacy handling of some of the prominent contact tracing applications. Additionally, we describe the various threat actors who can disrupt its working, or misuse end user's data, or hamper its mass adoption. Finally, we present privacy guidelines for contact tracing applications from different stakeholder's perspective. To best of our knowledge, this is the first generic work which provides privacy guidelines for contact tracing applications.","516":"We study a class of individual-based, fixed-population size epidemic models under general assumptions, e.g., heterogeneous contact rates encapsulating changes in behavior and\/or enforcement of control measures. We show that the large-population dynamics are deterministic and relate to the Kermack-McKendrick PDE. Our assumptions are minimalistic in the sense that the only important requirement is that the basic reproduction number of the epidemic $R_0$ be finite, and allow us to tackle both Markovian and non-Markovian dynamics. The novelty of our approach is to study the\"infection graph\"of the population. We show local convergence of this random graph to a Poisson (Galton-Watson) marked tree, recovering Markovian backward-in-time dynamics in the limit as we trace back the transmission chain leading to a focal infection. This effectively models the process of contact tracing in a large population. It is expressed in terms of the Doob $h$-transform of a certain renewal process encoding the time of infection along the chain. Our results provide a mathematical formulation relating a fundamental epidemiological quantity, the generation time distribution, to the successive time of infections along this transmission chain.","517":"Since early 2020, the world has been dealing with a raging pandemic outbreak: COVID-19. A year later, vaccines have become accessible, but in limited quantities, so that governments needed to devise a strategy to decide which part of the population to prioritize when assigning the available doses, and how to manage the interval between doses for multi-dose vaccines. In this paper, we present an optimization framework to address the dynamic double-dose vaccine allocation problem whereby the available vaccine doses must be administered to different age-groups to minimize specific societal objectives. In particular, we first identify an age-dependent Susceptible-Exposed-Infected-Recovered (SEIR) epidemic model including an extension capturing partially and fully vaccinated people, whereby we account for age-dependent immunity and infectiousness levels together with disease severity. Second, we leverage our model to frame the dynamic age-dependent vaccine allocation problem for different societal objectives, such as the minimization of infections or fatalities, and solve it with nonlinear programming techniques. Finally, we carry out a numerical case study with real-world data from The Netherlands. Our results show how different societal objectives can significantly alter the optimal vaccine allocation strategy. For instance, we find that minimizing the overall number of infections results in delaying second doses, whilst to minimize fatalities it is important to fully vaccinate the elderly first.","518":"In this study, we explored how the coronavirus disease (COVID-19) affected the demand for insurance and vaccines in Japan from mid-March to mid-April 2020. Through independent internet surveys, respondents were asked hypothetical questions concerning the demand for insurance and vaccines for protection against COVID-19. Using the collected short-panel data, after controlling for individual characteristics using the fixed effects model, the key findings, within the context of the pandemic, were as follows: (1) Contrary to extant studies, the demand for insurance by females was smaller than that by their male counterparts; (2) The gap in demand for insurance between genders increased as the pandemic prevailed; (3) The demand for a vaccine by females was higher than that for males; and (4) As COVID-19 spread throughout Japan, demand for insurance decreased, whereas the demand for a vaccine increased.","519":"In statistics, researchers use Regression models for data analysis and prediction in many productive sectors (industry, business, academy, etc.). Regression models are mathematical functions representing an approximation of dependent variable $Y$ from n independent variables $X_i \\in X$. The literature presents many regression methods divided into single and multiple regressions. There are several procedures to generate regression models and sets of commercial and academic tools that implement these procedures. This work presents one open-source program called Regressor that makes models from a specific variation of polynomial regression. These models relate the independent variables to generate an approximation of the original output dependent data. In many tests, Regressor was able to build models five times more accurate than commercial tools.","520":"The pandemic of coronavirus disease 2019 (COVID-19) caused by syndrome-coronavirus-2 (SARS-CoV-2) has been found rapid and large-scale diagnosis to spread across the communities. The risk of infectious airborne aerosol transmission has been an area of increasing concern. In this regard, a selective optic detection of SARS-CoV-2 aerosol is highly desirable, which can actively collect the surrounding air samples and provide real-time feedbacks to users. In this work, we developed an integrated Air Quality Monitoring System (iAQMS) that enables real-time SARS-CoV-2 aerosol detection with high sensitivity. The detection device, inspired by localized surface plasmon resonance (LSPR), was designed to measure the intensity changes of light going through our biochips in which gold nanoparticles (AuNP) combined with antibodies were exposed to airflow containing virus aerosols. We found that the variation of the measured photocurrent after 350 seconds of exposure to virus aerosols is a function of virus particle concentrations. By incorporating the air collection device and the optic detection device into a handheld sized device that can communicate with a smart-phone through Bluetooth serial, we demonstrate real-time and sensitive detection of SARS-CoV-2 virus aerosols with a concentration dynamic range of 0.00001 - 0.1 pfu\/uL. Because of its sensitivity and reliability, our device can also be used for detection of other virus aerosols by simply switching the antibodies combined to the AuNP in the biochips.","521":"Nb3Sn is a promising next-generation material for superconducting radiofrequency cavities, with significant potential for both large scale and compact accelerator applications. However, so far, Nb3Sn cavities have been limited to cw accelerating fields<18 MV\/m. In this paper, new results are presented with significantly higher fields, as high as 24 MV\/m in single cell cavities. Results are also presented from the first ever Nb3Sn-coated 1.3 GHz 9-cell cavity, a full-scale demonstration on the cavity type used in production for the European XFEL and LCLS-II. Results are presented together with heat dissipation curves to emphasize the potential for industrial accelerator applications using cryocooler-based cooling systems. The cavities studied have an atypical shiny visual appearance, and microscopy studies of witness samples reveal significantly reduced surface roughness and smaller film thickness compared to typical Nb3Sn films for superconducting cavities. Possible mechanisms for increased maximum field are discussed as well as implications for physics of RF superconductivity in the low coherence length regime. Outlook for continued development is presented.","522":"In this paper, we introduce the Age of Incorrect Information (AoII) as an enabler for semantics-empowered communication, a newly advocated communication paradigm centered around data's role and its usefulness to the communication's goal. First, we shed light on how the traditional communication paradigm, with its role-blind approach to data, is vulnerable to performance bottlenecks. Next, we highlight the shortcomings of several proposed performance measures destined to deal with the traditional communication paradigm's limitations, namely the Age of Information (AoI) and the error-based metrics. We also show how the AoII addresses these shortcomings and captures more meaningfully the purpose of data. Afterward, we consider the problem of minimizing the average AoII in a transmitter-receiver pair scenario where packets are sent over an unreliable channel subject to a transmission rate constraint. We prove that the optimal transmission strategy is a randomized threshold policy, and we propose a low complexity algorithm that finds both the optimal threshold and the randomization parameter. Furthermore, we provide a theoretical comparison between the AoII framework and the standard error-based metrics counterpart. Interestingly, we show that the AoII-optimal policy is also error-optimal for the adopted information source model. At the same time, the converse is not necessarily true. Finally, we implement our proposed policy in various real-life applications, such as video streaming, and we showcase its performance advantages compared to both the error-optimal and the AoI-optimal policies.","523":"Scientists, governments, and companies increasingly publish datasets on the Web. Google's Dataset Search extracts dataset metadata -- expressed using schema.org and similar vocabularies -- from Web pages in order to make datasets discoverable. Since we started the work on Dataset Search in 2016, the number of datasets described in schema.org has grown from about 500K to almost 30M. Thus, this corpus has become a valuable snapshot of data on the Web. To the best of our knowledge, this corpus is the largest and most diverse of its kind. We analyze this corpus and discuss where the datasets originate from, what topics they cover, which form they take, and what people searching for datasets are interested in. Based on this analysis, we identify gaps and possible future work to help make data more discoverable.","524":"We consider the $SEIRS$ epidemiology model with such features of the COVID-19 outbreak as: abundance of unidentified infected individuals, limited time of immunity and a possibility of vaccination. Within a compartmental realization of this model, we found the disease-free and the endemic stationary states. They exist in their respective restricted regions of the le via linear stability analysis. The expression for the basic reproductive number is obtained as well. The positions and heights of a first peak for the fractions of infected individuals are obtained numerically and are fitted to simple algebraic forms, that depend on model rates. Computer simulations of a lattice-based realization for this model was performed by means of the cellular automaton algorithm. These allowed to study the effect of the quarantine measures explicitly, via changing the neighbourhood size. The attempt is made to match both quarantine and vaccination measures aimed on balanced solution for effective suppression of the pandemic.","525":"3CL-Pro (or M-Pro) is the SARS-CoV-2 main protease, responsible for the cleavage of the large polyprotein 1ab transcript and the liberation of eleven proteins, responsible for viral growth and replication. It acts exclusively as a homodimer, while monomers are inactive. Due to its pivotal role, 3CL-Pro has been one of the most studied SARS-CoV-2 proteins and the subject of a number of therapeutic interventions, targeting its catalytic domain. A number of potential drug candidates have been reported, including some natural products. Here, we have investigated in silico, through fully flexible binding and extensive molecular dynamics simulations, the natural product space for the identification of potential candidates of 3CL-Pro dimerization inhibitors. We report that fortunellin (acacetin 7-O-neohesperidoside), a natural flavonoid O-glycoside, isolated from the fruits of Citrus japonica var. margarita (kumquat), is a potent inhibitor of 3CL-Pro dimerization. A search of the ZINC natural products database identified another 16 related molecules, which possess interesting pharmacological properties. We propose that fortunellin and its analogs might be the basis of novel pharmaceuticals against SARS-CoV-2 induced COVID-19 disease.","526":"The lack of scientific openness is identified as one of the key challenges of computational reproducibility. In addition to Open Data, Free and Open-source Software (FOSS) and Open Hardware (OH) can address this challenge by introducing open policies, standards, and recommendations. However, while both FOSS and OH are free to use, study, modify, and redistribute, there are significant differences in sharing and reusing these artifacts. FOSS is increasingly supported with software repositories, but support for OH is lacking, potentially due to the complexity of its digital format and licensing. This paper proposes leveraging FAIR principles to make OH findable, accessible, interoperable, and reusable. We define what FAIR means for OH, how it differs from FOSS, and present examples of unique demands. Also, we evaluate dissemination platforms currently used for OH and provide recommendations.","527":"The weaponization of digital communications and social media to conduct disinformation campaigns at immense scale, speed, and reach presents new challenges to identify and counter hostile influence operations (IO). This paper presents an end-to-end framework to automate detection of disinformation narratives, networks, and influential actors. The framework integrates natural language processing, machine learning, graph analytics, and a novel network causal inference approach to quantify the impact of individual actors in spreading IO narratives. We demonstrate its capability on real-world hostile IO campaigns with Twitter datasets collected during the 2017 French presidential elections, and known IO accounts disclosed by Twitter. Our system detects IO accounts with 96% precision, 79% recall, and 96% area-under-the-PR-curve, maps out salient network communities, and discovers high-impact accounts that escape the lens of traditional impact statistics based on activity counts and network centrality. Results are corroborated with independent sources of known IO accounts from U.S. Congressional reports, investigative journalism, and IO datasets provided by Twitter.","528":"We present a new CUSUM procedure for sequentially detecting change-point in the self and mutual exciting processes, a.k.a. Hawkes networks using discrete events data. Hawkes networks have become a popular model for statistics and machine learning due to their capability in modeling irregularly observed data where the timing between events carries a lot of information. The problem of detecting abrupt changes in Hawkes networks arises from various applications, including neuronal imaging, sensor network, and social network monitoring. Despite this, there has not been a computationally and memory-efficient online algorithm for detecting such changes from sequential data. We present an efficient online recursive implementation of the CUSUM statistic for Hawkes processes, both decentralized and memory-efficient, and establish the theoretical properties of this new CUSUM procedure. We then show that the proposed CUSUM method achieves better performance than existing methods, including the Shewhart procedure based on count data, the generalized likelihood ratio (GLR) in the existing literature, and the standard score statistic. We demonstrate this via a simulated example and an application to population code change-detection in neuronal networks.","529":"The COVID-19 pandemic has completely disrupted the operation of our societies. Its elusive transmission process, characterized by an unusually long incubation period, as well as a high contagion capacity, has forced many countries to take quarantine and social isolation measures that conspire against the performance of national economies. This situation confronts decision makers in different countries with the alternative of reopening the economies, thus facing the unpredictable cost of a rebound of the infection. This work tries to offer an initial theoretical framework to handle this alternative.","530":"Multiple small- to middle-scale cities, mostly located in northern China, became epidemic hotspots during the second wave of the spread of COVID-19 in early 2021. Despite qualitative discussions of potential social-economic causes, it remains unclear how this pattern could be accounted for from a quantitative approach. Through the development of an urban epidemic hazard index (EpiRank), we came up with a mathematical explanation for this phenomenon. The index is constructed from epidemic simulations on a multi-layer transportation network model on top of local SEIR transmission dynamics, which characterizes intra- and inter-city compartment population flow with a detailed mathematical description. Essentially, we argue that these highlighted cities possess greater epidemic hazards due to the combined effect of large regional population and small inter-city transportation. The proposed index, dynamic and applicable to different epidemic settings, could be a useful indicator for the risk assessment and response planning of urban epidemic hazards in China; the model framework is modularized and can be adapted for other nations without much difficulty.","531":"We present a robust data-driven machine learning analysis of the COVID-19 pandemic from its early infection dynamics, specifically infection counts over time. The goal is to extract actionable public health insights. These insights include the infectious force, the rate of a mild infection becoming serious, estimates for asymtomatic infections and predictions of new infections over time. We focus on USA data starting from the first confirmed infection on January 20 2020. Our methods reveal significant asymptomatic (hidden) infection, a lag of about 10 days, and we quantitatively confirm that the infectious force is strong with about a 0.14% transition from mild to serious infection. Our methods are efficient, robust and general, being agnostic to the specific virus and applicable to different populations or cohorts.","532":"During the last years our society was often exposed to the coherent information waves of high amplitudes. These are waves of huge social energy. Often they are of the destructive character, a kind of information tsunami. But, they can carry as well positive improvements in the human society, as waves of decision making matching rational recommendations of societal institutes. The main distinguishing features of these waves are their high amplitude, coherence (homogeneous character of social actions generated by them), and short time needed for their generation and relaxation. Such waves can be treated as large scale exhibition of the Bandwagon effect. We show that this socio-psychic phenomenon can be modeled on the basis of the recently developed {\\it social laser theory}. This theory can be used to model {\\it stimulated amplification of coherent social actions}.\"Actions\"are treated very generally, from mass protests to votes and other collective decisions, as, e.g., acceptance (often unconscious) of some societal recommendations. In this paper, we concentrate on theory of laser resonators, physical vs. social. For the latter, we analyze in very detail functioning of the internet based Echo-Chambers. Their main purpose is increasing of the power of the quantum information field as well as its coherence. Of course, the Bandwagon effect is well known and well studied in social psychology. However, the social laser theory gives the possibility to model it by using the general formalism of quantum field theory. The paper contains minimum of mathematics and it can be readable by researchers working in psychology, cognitive, social, and political sciences; it might also be interesting for experts in information theory and artificial intelligence.","533":"The COVID-19 pandemic is one of the most challenging healthcare crises during the 21st century. As the virus continues to spread on a global scale, the majority of efforts have been on the development of vaccines and the mass immunization of the public. While the daily case numbers were following a decreasing trend, the emergent of new virus mutations and variants still pose a significant threat. As economies start recovering and societies start opening up with people going back into office buildings, schools, and malls, we still need to have the ability to detect and minimize the spread of COVID-19. Individuals with COVID-19 may show multiple symptoms such as cough, fever, and shortness of breath. Many of the existing detection techniques focus on symptoms having the same equal importance. However, it has been shown that some symptoms are more prevalent than others. In this paper, we present a multimodal method to predict COVID-19 by incorporating existing deep learning classifiers using convolutional neural networks and our novel probability-based weighting function that considers the prevalence of each symptom. The experiments were performed on an existing dataset with respect to the three considered modes of coughs, fever, and shortness of breath. The results show considerable improvements in the detection of COVID-19 using our weighting function when compared to an equal weighting function.","534":"The task of machine reading comprehension (MRC) is a useful benchmark to evaluate the natural language understanding of machines. It has gained popularity in the natural language processing (NLP) field mainly due to the large number of datasets released for many languages. However, the research in MRC has been understudied in several domains, including religious texts. The goal of the Qur'an QA 2022 shared task is to fill this gap by producing state-of-the-art question answering and reading comprehension research on Qur'an. This paper describes the DTW entry to the Quran QA 2022 shared task. Our methodology uses transfer learning to take advantage of available Arabic MRC data. We further improve the results using various ensemble learning strategies. Our approach provided a partial Reciprocal Rank (pRR) score of 0.49 on the test set, proving its strong performance on the task.","535":"Autism spectrum disorder is a developmental disorder characterized by significant social, communication, and behavioral challenges. Individuals diagnosed with autism, intellectual, and developmental disabilities (AUIDD) typically require long-term care and targeted treatment and teaching. Effective treatment of AUIDD relies on efficient and careful behavioral observations done by trained applied behavioral analysts (ABAs). However, this process overburdens ABAs by requiring the clinicians to collect and analyze data, identify the problem behaviors, conduct pattern analysis to categorize and predict categorical outcomes, hypothesize responsiveness to treatments, and detect the effects of treatment plans. Successful integration of digital technologies into clinical decision-making pipelines and the advancements in automated decision-making using Artificial Intelligence (AI) algorithms highlights the importance of augmenting teaching and treatments using novel algorithms and high-fidelity sensors. In this article, we present an AI-Augmented Learning and Applied Behavior Analytics (AI-ABA) platform to provide personalized treatment and learning plans to AUIDD individuals. By defining systematic experiments along with automated data collection and analysis, AI-ABA can promote self-regulative behavior using reinforcement-based augmented or virtual reality and other mobile platforms. Thus, AI-ABA could assist clinicians to focus on making precise data-driven decisions and increase the quality of individualized interventions for individuals with AUIDD.","536":"Genome assembly is a fundamental problem in Bioinformatics, requiring to reconstruct a source genome from an assembly graph built from a set of reads (short strings sequenced from the genome). A notion of genome assembly solution is that of an arc-covering walk of the graph. Since assembly graphs admit many solutions, the goal is to find what is definitely present in all solutions, or what is safe. Most practical assemblers are based on heuristics having at their core unitigs, namely paths whose internal nodes have unit in-degree and out-degree, and which are clearly safe. The long-standing open problem of finding all the safe parts of the solutions was recently solved [RECOMB 2016] yielding a 60% increase in contig length. This safe and complete genome assembly algorithm was followed by other works improving the time bounds, as well as extending the results for different notions of assembly solution. But it remained open whether one can be complete also for models of genome assembly of practical applicability. In this paper we present a universal framework for obtaining safe and complete algorithms which unify the previous results, while also allowing for easy generalisations to assembly problems including many practical aspects. This is based on a novel graph structure, called the hydrostructure of a walk, which highlights the reachability properties of the graph from the perspective of the walk. The hydrostructure allows for simple characterisations of the existing safe walks, and of their new practical versions. Almost all of our characterisations are directly adaptable to optimal verification algorithms, and simple enumeration algorithms. Most of these algorithms are also improved to optimality using an incremental computation procedure and a previous optimal algorithm of a specific model.","537":"In this paper, we propose a continuous-time stochastic intensity model, namely, two-phase dynamic contagion process(2P-DCP), for modelling the epidemic contagion of COVID-19 and investigating the lockdown effect based on the dynamic contagion model introduced by Dassios and Zhao (2011). It allows randomness to the infectivity of individuals rather than a constant reproduction number as assumed by standard models. Key epidemiological quantities, such as the distribution of final epidemic size and expected epidemic duration, are derived and estimated based on real data for various regions and countries. The associated time lag of the effect of intervention in each country or region is estimated. Our results are consistent with the incubation time of COVID-19 found by recent medical study. We demonstrate that our model could potentially be a valuable tool in the modeling of COVID-19. More importantly, the proposed model of 2P-DCP could also be used as an important tool in epidemiological modelling as this type of contagion models with very simple structures is adequate to describe the evolution of regional epidemic and worldwide pandemic.","538":"In a wide range of applications, the estimate of droplet evaporation time is based on the classical d2-law, which, assuming a fast mixing and fixed environmental properties, states that the droplet surface decreases linearly with time at a determined rate. However, in many cases the predicted evaporation rate is overestimated. In this Letter, we propose a revision of the d2-law capable to accurately determine droplet evaporation rate in dilute conditions by a proper estimate of the asymptotic droplet properties. Besides a discussion of the main assumptions, we tested the proposed model against data from direct numerical simulations finding an excellent agreement for predicted droplet evaporation time in dilute turbulent jet-sprays.","539":"The problem of adaptive sampling for estimating probability mass functions (pmf) uniformly well is considered. Performance of the sampling strategy is measured in terms of the worst-case mean squared error. A Bayesian variant of the existing upper confidence bound (UCB) based approaches is proposed. It is shown analytically that the performance of this Bayesian variant is no worse than the existing approaches. The posterior distribution on the pmfs in the Bayesian setting allows for a tighter computation of upper confidence bounds which leads to significant performance gains in practice. Using this approach, adaptive sampling protocols are proposed for estimating SARS-CoV-2 seroprevalence in various groups such as location and ethnicity. The effectiveness of this strategy is discussed using data obtained from a seroprevalence survey in Los Angeles county.","540":"This manuscript is focused on features' definition for the outcome prediction of matches of NBA basketball championship. It is shown how models based on one a single feature (Elo rating or the relative victory frequency) have a quality of fit better than models using box-score predictors (e.g. the Four Factors). Features have been ex ante calculated for a dataset containing data of 16 NBA regular seasons, paying particular attention to home court factor. Models have been produced via Deep Learning, using cross validation.","541":"The digital twin has emerged as a technology to predict the undesirables, and ensure desired performance of complex systems. Although digital twins have got attention in the manufacturing research spectrum, yet their industrial application has been limited. Virtual simulations are considered an integral part of a digital twin, but a challenge is the lack of structured approaches to creating simulation models that can be extended as a digital twin. At the same time, the virtual models need to be accurate and flexible enough to be updated along the life cycle of the factory as desired in a digital twin. This paper presents a framework for virtual modeling of factories that can be extended as a digital twin. The case of a manufacturing company is presented to model and simulate a manufacturing system in a structured yet flexible way.","542":"In response to the shortage of ventilators caused by the COVID-19 pandemic, many organizations have designed low-cost emergency ventilators. Many of these devices are pressure-cycled pneumatic ventilators, which are easy to produce but often do not include the sensing or alarm features found on commercial ventilators. This work reports a low-cost, easy-to-produce electronic sensor and alarm system for pressure-cycled ventilators that estimates clinically useful metrics such as pressure and respiratory rate and sounds an alarm when the ventilator malfunctions. A low-complexity signal processing algorithm uses a pair of nonlinear recursive envelope trackers to monitor the signal from an electronic pressure sensor connected to the patient airway. The algorithm, inspired by those used in hearing aids, requires little memory and performs only a few calculations on each sample so that it can run on nearly any microcontroller.","543":"With the continued spread of coronavirus, the task of forecasting distinctive COVID-19 growth curves in different cities, which remain inadequately explained by standard epidemiological models, is critical for medical supply and treatment. Predictions must take into account non-pharmaceutical interventions to slow the spread of coronavirus, including stay-at-home orders, social distancing, quarantine and compulsory mask-wearing, leading to reductions in intra-city mobility and viral transmission. Moreover, recent work associating coronavirus with human mobility and detailed movement data suggest the need to consider urban mobility in disease forecasts. Here we show that by incorporating intra-city mobility and policy adoption into a novel metapopulation SEIR model, we can accurately predict complex COVID-19 growth patterns in U.S. cities ($R^2$ = 0.990). Estimated mobility change due to policy interventions is consistent with empirical observation from Apple Mobility Trends Reports (Pearson's R = 0.872), suggesting the utility of model-based predictions where data are limited. Our model also reproduces urban\"superspreading\", where a few neighborhoods account for most secondary infections across urban space, arising from uneven neighborhood populations and heightened intra-city churn in popular neighborhoods. Therefore, our model can facilitate location-aware mobility reduction policy that more effectively mitigates disease transmission at similar social cost. Finally, we demonstrate our model can serve as a fine-grained analytic and simulation framework that informs the design of rational non-pharmaceutical interventions policies.","544":"In the current COVID-19 pandemic, manual contact tracing has been proven very helpful to reach close contacts of infected users and slow down virus spreading. To improve its scalability, a number of automated contact tracing (ACT) solutions have proposed and some of them have been deployed. Despite the dedicated efforts, security and privacy issues of these solutions are still open and under intensive debate. In this paper, we examine the ACT concept from a broader perspective, by focusing on not only security and privacy issues but also functional issues such as interface, usability and coverage. We first elaborate on these issues and particularly point out the inevitable privacy leakages in existing BLE-based ACT solutions. Then, we propose a venue-based ACT concept, which only monitors users' contacting history in virus-spreading-prone venues and is able to incorporate different location tracking technologies such as BLE and WIFI. Finally, we instantiate the venue-based ACT concept and show that our instantiation can mitigate most of the issues we have identified in our analysis.","545":"The impact that information diffusion has on epidemic spreading has recently attracted much attention. As a disease begins to spread in the population, information about the disease is transmitted to others, which in turn has an effect on the spread of disease. In this paper, using empirical results of the propagation of H7N9 and information about the disease, we clearly show that the spreading dynamics of the two-types of processes influence each other. We build a mathematical model in which both types of spreading dynamics are described using the SIS process in order to illustrate the influence of information diffusion on epidemic spreading. Both the simulation results and the pairwise analysis reveal that information diffusion can increase the threshold of an epidemic outbreak, decrease the final fraction of infected individuals and significantly decrease the rate at which the epidemic propagates. Additionally, we find that the multi-outbreak phenomena of epidemic spreading, along with the impact of information diffusion, is consistent with the empirical results. These findings highlight the requirement to maintain social awareness of diseases even when the epidemics seem to be under control in order to prevent a subsequent outbreak. These results may shed light on the in-depth understanding of the interplay between the dynamics of epidemic spreading and information diffusion.","546":"The first BV bands photometric observations and the low-resolution spectrum of the shortest period Am type eclipsing binary TYC 6408-989-1 have been obtained. The stellar atmospheric parameters of the primary star were obtained through the spectral fitting as follows: $T_{eff}=6990\\pm117 K$, $\\log g=4.25\\pm0.26 cm\/s^2$, $[Fe\/H]=-0.45\\pm0.03 dex$. The original spectra obtained by European Southern Observatory (ESO) were processed with IRAF package by us. Based on the ESO blue-violet spectra, TYC 6408-989-1 was concluded as a marginal Am (Am:) star with a spectral type of kA3hF1mA5 IV-V identified through the MKCLASS program. The observed light curves were analyzed through the Wilson-Devinney code. The final photometric solutions show that TYC 6408-989-1 is a marginal contact binary with a low mass ratio (q=0.27). The temperature of the secondary component derived through the light curve analysis is significantly higher than main sequence stars. In addition, TYC 6408-989-1 is a poor thermal contact binary. The temperature differences between the two components is about 1800K. TYC 6408-989-1 should be located in the oscillation stage predicted by the thermal relaxation oscillations theory (TRO) and will evolve into the shallow contact stage eventually. The very short period (less than one day), marginal Am peculiarity and quit large rotational velocity ($v\\sin i \\simeq 160 km s^{-1}$) make TYC 6408-989-1 became a challenge to the cut-off of rotation velocities and periods of Am stars. We have collected the well known eclipsing Am binaries with absolute parameters from the literature.","547":"We investigate the spatial structure and evolution of star formation and the interstellar medium (ISM) in interacting galaxies. We use an extensive suite of parsec-scale galaxy merger simulations (stellar mass ratio = 2.5:1), which employs the\"Feedback In Realistic Environments-\"model (fire-2). This framework resolves star formation, feedback processes, and the multi-phase structure of the ISM. We focus on the galaxy-pair stages of interaction. We find that close encounters substantially augment cool (HI) and cold-dense (H2) gas budgets, elevating the formation of new stars as a result. This enhancement is centrally-concentrated for the secondary galaxy, and more radially extended for the primary. This behaviour is weakly dependent on orbital geometry. We also find that galaxies with elevated global star formation rate (SFR) experience intense nuclear SFR enhancement, driven by high levels of either star formation efficiency (SFE) or available cold-dense gas fuel. Galaxies with suppressed global SFR also contain a nuclear cold-dense gas reservoir, but low SFE levels diminish SFR in the central region. Concretely, in the majority of cases, SFR-enhancement in the central kiloparsec is fuel-driven (55% for the secondary, 71% for the primary) -- whilst central SFR-suppression is efficiency-driven (91% for the secondary, 97% for the primary). Our numerical predictions underscore the need of substantially larger, and\/or merger-dedicated, spatially-resolved galaxy surveys -- capable of examining vast and diverse samples of interacting systems -- coupled with multi-wavelength campaigns aimed to capture their internal ISM structure.","548":"This work presents the analysis of the impact of restrictions on mobility in Italy, with a focus on the period from 6 November 2020 to 31 January 2021, when a three-tier system based on different levels of risk was adopted and applied at regional level to contrast the second wave of COVID-19. The impact is first evaluated on mobility using Mobile Network Operator anonymised and aggregate data shared in the framework of a Business-to-Government initiative with the European Commission. Mobility data, alongside additional information about electricity consuption, are then used to assess the impacts on an economic level of the three-tier system in different areas of the country.","549":"Archetypal analysis is an unsupervised learning method for exploratory data analysis. One major challenge that limits the applicability of archetypal analysis in practice is the inherent computational complexity of the existing algorithms. In this paper, we provide a novel approximation approach to partially address this issue. Utilizing probabilistic ideas from high-dimensional geometry, we introduce two preprocessing techniques to reduce the dimension and representation cardinality of the data, respectively. We prove that provided the data is approximately embedded in a low-dimensional linear subspace and the convex hull of the corresponding representations is well approximated by a polytope with a few vertices, our method can effectively reduce the scaling of archetypal analysis. Moreover, the solution of the reduced problem is near-optimal in terms of prediction errors. Our approach can be combined with other acceleration techniques to further mitigate the intrinsic complexity of archetypal analysis. We demonstrate the usefulness of our results by applying our method to summarize several moderately large-scale datasets.","550":"A simplified method to compute $R_t$, the Effective Reproduction Number, is presented. The method relates the value of Rt to the estimation of the doubling time performed with a local exponential fit. The condition $R_t = 1$ corresponds to a growth rate equal to zero or equivalently an infinite doubling time. Different assumptions on the probability distribution of the generation time are considered. A simple analytical solution is presented in case the generation time follows a gamma distribution.","551":"This contribution argues that Reddit, as a massive, categorized, open-access dataset, is a useful data source, for\"almost any topic\". Hence, it can be used in data science, e.g. for knowledge exploration. This statement is backed-up with presented analysis, based on 180 manually annotated papers, related to Reddit itself, and data acquired from popular databases of scientific papers. Finally, an open source tool is introduced, which provides an easy access to Reddit resources, and an exploratory data analysis of how Reddit covers selected topics. These functions can be used as a prelude analysis to a broader exploration of Reddit's applicability.","552":"COVID-19 is a global health crisis that has had unprecedented, widespread impact on households across the United States and has been declared a global pandemic on March 11, 2020 by World Health Organization (WHO) [1]. According to Centers for Disease Control and Prevention (CDC) [2], the spread of COVID-19 occurs through person-to-person transmission i.e. close contact with infected people through contaminated surfaces and respiratory fluids carrying infectious virus. This paper presents a data-driven physics-based approach to analyze and predict the rapid growth and spread dynamics of the pandemic. Temporal and Spatial conservation laws are used to model the evolution of the COVID-19 pandemic. We integrate quadratic programming and neural networks to learn the parameters and estimate the pandemic growth. The proposed prediction model is validated through finite time estimation of the pandemic growth using the total number of cases, deaths and recoveries in the United States recorded from March 12, 2020 until October 1, 2021 [3].","553":"This paper describes the use of neural networks to enhance simulations for subsequent training of anomaly-detection systems. Simulations can provide edge conditions for anomaly detection which may be sparse or non-existent in real-world data. Simulations suffer, however, by producing data that is\"too clean\"resulting in anomaly detection systems that cannot transition from simulated data to actual conditions. Our approach enhances simulations using neural networks trained on real-world data to create outputs that are more realistic and variable than traditional simulations.","554":"Z. Schuss (1937-2018) was an applied mathematician, with several contributions in asymptotic, stochastic processes, PDEs, modeling and signal processing. He is well known for his original approach to the activation escape problem, based on WKB and boundary layer analysis, summarized in six books published by Springer. The text summarizes his intellectual approach in science, views and personal path across the XX century, WWII and his personal contribution to academia.","555":"In February 2020, the European Commission (EC) published a white paper entitled, On Artificial Intelligence - A European approach to excellence and trust. This paper outlines the EC's policy options for the promotion and adoption of artificial intelligence (AI) in the European Union. The Montreal AI Ethics Institute (MAIEI) reviewed this paper and published a response addressing the EC's plans to build an\"ecosystem of excellence\"and an\"ecosystem of trust,\"as well as the safety and liability implications of AI, the internet of things (IoT), and robotics. MAIEI provides 15 recommendations in relation to the sections outlined above, including: 1) focus efforts on the research and innovation community, member states, and the private sector; 2) create alignment between trading partners' policies and EU policies; 3) analyze the gaps in the ecosystem between theoretical frameworks and approaches to building trustworthy AI; 4) focus on coordination and policy alignment; 5) focus on mechanisms that promote private and secure sharing of data; 6) create a network of AI research excellence centres to strengthen the research and innovation community; 7) promote knowledge transfer and develop AI expertise through Digital Innovation Hubs; 8) add nuance to the discussion regarding the opacity of AI systems; 9) create a process for individuals to appeal an AI system's decision or output; 10) implement new rules and strengthen existing regulations; 11) ban the use of facial recognition technology; 12) hold all AI systems to similar standards and compulsory requirements; 13) ensure biometric identification systems fulfill the purpose for which they are implemented; 14) implement a voluntary labelling system for systems that are not considered high-risk; 15) appoint individuals to the oversight process who understand AI systems well and are able to communicate potential risks.","556":"COVID-19 has hit hard on the global community, and organizations are working diligently to cope with the new norm of\"work from home\". However, the volume of remote work is unprecedented and creates opportunities for cyber attackers to penetrate home computers. Attackers have been leveraging websites with COVID-19 related names, dubbed COVID-19 themed malicious websites. These websites mostly contain false information, fake forms, fraudulent payments, scams, or malicious payloads to steal sensitive information or infect victims' computers. In this paper, we present a data-driven study on characterizing and detecting COVID-19 themed malicious websites. Our characterization study shows that attackers are agile and are deceptively crafty in designing geolocation targeted websites, often leveraging popular domain registrars and top-level domains. Our detection study shows that the Random Forest classifier can detect COVID-19 themed malicious websites based on the lexical and WHOIS features defined in this paper, achieving a 98% accuracy and 2.7% false-positive rate.","557":"We report results from a survey of lab instructors on how they adapted their courses in the transition to emergency remote teaching due to the COVID-19 pandemic. The purpose of this report is to share the experiences of instructors in order to prepare for future remote teaching of labs. We include summaries of responses to help illustrate the types of lab activities that were done, learning goals for the remote labs, motivations for instructors' choices, challenges instructors faced, and ways in which instructors and students communicated. This is a first step in a larger project as part of an NSF RAPID grant to understand what happened during the switch to remote labs and how it impacted teaching methods and student learning.","558":"Medical professionals evaluating alternative treatment plans for a patient often encounter time varying confounders, or covariates that affect both the future treatment assignment and the patient outcome. The recently proposed Counterfactual Recurrent Network (CRN) accounts for time varying confounders by using adversarial training to balance recurrent historical representations of patient data. However, this work assumes that all time varying covariates are confounding and thus attempts to balance the full state representation. Given that the actual subset of covariates that may in fact be confounding is in general unknown, recent work on counterfactual evaluation in the static, non-temporal setting has suggested that disentangling the covariate representation into separate factors, where each either influence treatment selection, patient outcome or both can help isolate selection bias and restrict balancing efforts to factors that influence outcome, allowing the remaining factors which predict treatment without needlessly being balanced.","559":"With the rapid development of COVID-19, people are asked to maintain\"social distance\"and\"stay at home\". In this scenario, more and more social interactions move online, especially on social media like Twitter and Weibo. People post tweets to share information, express opinions and seek help during the pandemic, and these tweets on social media are valuable for studies against COVID-19, such as early warning and outbreaks detection. Therefore, in this paper, we release a large-scale COVID-19 social media dataset from Weibo called Weibo-COV, covering more than 30 million tweets from 1 November 2019 to 30 April 2020. Moreover, the field information of the dataset is very rich, including basic tweets information, interactive information, location information and retweet network. We hope this dataset can promote studies of COVID-19 from multiple perspectives and enable better and faster researches to suppress the spread of this disease.","560":"COVID-19 represented a major shock to global health systems, not the least to resource-challenged regions in the Global South. We report on a case of digital, information system resilience in the response to data needs from the COVID-19 pandemic in two countries in the Global South. In contrast to dominant perspectives where digital resilience enables bounce back or maintenance of a status quo, we identify five bounce forward resilience preconditions (i) distributed training, (ii) local expertise (iii) local autonomy and ownership (iv) local infrastructure and (v) platform design infrastructure. These preconditions enable an elevated degree of resilience that in the face of an external shock such as COVID-19 can deliver a bounce forward or strengthening of the information system beyond its pre-shock state.","561":"The recent outbreak of COVID-19 has led to urgent needs for reliable diagnosis and management of SARS-CoV-2 infection. As a complimentary tool, chest CT has been shown to be able to reveal visual patterns characteristic for COVID-19, which has definite value at several stages during the disease course. To facilitate CT analysis, recent efforts have focused on computer-aided characterization and diagnosis, which has shown promising results. However, domain shift of data across clinical data centers poses a serious challenge when deploying learning-based models. In this work, we attempt to find a solution for this challenge via federated and semi-supervised learning. A multi-national database consisting of 1704 scans from three countries is adopted to study the performance gap, when training a model with one dataset and applying it to another. Expert radiologists manually delineated 945 scans for COVID-19 findings. In handling the variability in both the data and annotations, a novel federated semi-supervised learning technique is proposed to fully utilize all available data (with or without annotations). Federated learning avoids the need for sensitive data-sharing, which makes it favorable for institutions and nations with strict regulatory policy on data privacy. Moreover, semi-supervision potentially reduces the annotation burden under a distributed setting. The proposed framework is shown to be effective compared to fully supervised scenarios with conventional data sharing instead of model weight sharing.","562":"The epidemic of COVID-19 has caused an unpredictable and devastated disaster to the public health in different territories around the world. Common phenotypes include fever, cough, shortness of breath, and chills. With more cases investigated, other clinical phenotypes are gradually recognized, for example, loss of smell, and loss of tastes. Compared with discharged or cured patients, severe or died patients often have one or more comorbidities, such as hypertension, diabetes, and cardiovascular disease. In this study, we systematically collected and analyzed COVID-19-related clinical phenotypes from 70 articles. The commonly occurring 17 phenotypes were classified into different groups based on the Human Phenotype Ontology (HPO). Based on the HP classification, we systematically analyze three nervous phenotypes (loss of smell, loss of taste, and headache) and four abdominal phenotypes (nausea, vomiting, abdominal pain, and diarrhea) identified in patients, and found that patients from Europe and USA turned to have higher nervous phenotypes and abdominal phenotypes than patients from Asia. A total of 23 comorbidities were found to commonly exist among COVID-19 patients. Patients with these comorbidities such as diabetes and kidney failure had worse outcomes compared with those without these comorbidities.","563":"We consider an epidemiological SIR model and a positive threshold $M$. Using a parametric expression for the solution curve of the SIR model and the Lambert W function, we establish necessary and sufficient conditions on the basic reproduction number $\\mathcal{R}_0$ to ensure that the infected population does not exceed the threshold $M$. We also propose and analyze different measures to quantify a possible threshold exceedance.","564":"The presence of oscillations in aggregated COVID-19 data not only raises questions about the data's accuracy, it hinders understanding of the pandemic. A spectral analysis is presented, and the oscillations in the data are replicated using sinusoidal resynthesis. The precise behavior of the seven-day moving average is also discussed, specifically, the cause of its jaggedness and the phase error it introduces. In comparison, other filtering techniques and Fourier processing produce superior smoothing and have zero phase error. Both of these are presented, and they are extended to isolate several frequency ranges. This extracts some of the same short-term variability that is resynthesized, and it shows that fluctuations with periods between 8 and 21 days are present in U.S. mortality data.","565":"Inferring the source of a diffusion in a large network of agents is a difficult but feasible task, if a few agents act as sensors revealing the time at which they got hit by the diffusion. A main limitation of current source detection algorithms is that they assume full knowledge of the contact network, which is rarely the case, especially for epidemics, where the source is called patient zero. Inspired by recent contact tracing algorithms, we propose a new framework, which we call Source Detection via Contact Tracing Framework (SDCTF). In the SDCTF, the source detection task starts at the time of the first hospitalization, and initially we have no knowledge about the contact network other than the identity of the first hospitalized agent. We may then explore the network by contact queries, and obtain symptom onset times by test queries in an adaptive way. We also assume that some of the agents may be asymptomatic, and therefore cannot reveal their symptom onset time. Our goal is to find patient zero with as few contact and test queries as possible. We propose two local search algorithms for the SDCTF: the LS algorithm is more data-efficient, but can fail to find the true source if many asymptomatic agents are present, whereas the LS+ algorithm is more robust to asymptomatic agents. By simulations we show that both LS and LS+ outperform state of the art adaptive and non-adaptive source detection algorithms adapted to the SDCTF, even though these baseline algorithms have full access to the contact network. Extending the theory of random exponential trees, we analytically approximate the probability of success of the LS\/ LS+ algorithms, and we show that our analytic results match the simulations. Finally, we benchmark our algorithms on the Data-driven COVID-19 Simulator developed by Lorch et al., which is the first time source detection algorithms are tested on such a complex dataset.","566":"Coronavirus disease 2019 (COVID-19) is an ongoing global pandemic in over 200 countries and territories, which has resulted in a great public health concern across the international community. Analysis of X-ray imaging data can play a critical role in timely and accurate screening and fighting against COVID-19. Supervised deep learning has been successfully applied to recognize COVID-19 pathology from X-ray imaging datasets. However, it requires a substantial amount of annotated X-ray images to train models, which is often not applicable to data analysis for emerging events such as COVID-19 outbreak, especially in the early stage of the outbreak. To address this challenge, this paper proposes a two-path semi-supervised deep learning model, ssResNet, based on Residual Neural Network (ResNet) for COVID-19 image classification, where two paths refer to a supervised path and an unsupervised path, respectively. Moreover, we design a weighted supervised loss that assigns higher weight for the minority classes in the training process to resolve the data imbalance. Experimental results on a large-scale of X-ray image dataset COVIDx demonstrate that the proposed model can achieve promising performance even when trained on very few labeled training images.","567":"Developing mechanisms that flexibly adapt dialog systems to unseen tasks and domains is a major challenge in dialog research. Neural models implicitly memorize task-specific dialog policies from the training data. We posit that this implicit memorization has precluded zero-shot transfer learning. To this end, we leverage the schema-guided paradigm, wherein the task-specific dialog policy is explicitly provided to the model. We introduce the Schema Attention Model (SAM) and improved schema representations for the STAR corpus. SAM obtains significant improvement in zero-shot settings, with a +22 F1 score improvement over prior work. These results validate the feasibility of zero-shot generalizability in dialog. Ablation experiments are also presented to demonstrate the efficacy of SAM.","568":"Distress propagation occurs in connected networks, its rate and extent being dependent on network topology. To study this, we choose economic production networks as a paradigm. An economic network can be examined at many levels: linkages among individual agents (microscopic), among firms\/sectors (mesoscopic) or among countries (macroscopic). New emergent dynamical properties appear at every level, so the granularity matters. For viral epidemics, even an individual node may act as an epicenter of distress and potentially affect the entire network. Economic networks, however, are known to be immune at the micro-levels and more prone to failure in the meso\/macro-levels. We propose a dynamical interaction model to characterize the mechanism of distress propagation, across different modules of a network, initiated at different epicenters. Vulnerable modules often lead to large degrees of destabilization. We demonstrate our methodology using a unique empirical data-set of input-output linkages across 0.14 million firms in one administrative state of India, a developing economy. The network has multiple hub-and-spoke structures that exhibits moderate disassortativity, which varies with the level of coarse-graining. The novelty lies in characterizing the production network at different levels of granularity or modularity, and finding `too-big-to-fail' modules supersede `too-central-to-fail' modules in distress propagation.","569":"Current definition of high-entropy alloys (HEAs) is commonly based on the configurational entropy. But this definition depends only on the chemical composition of a HEA and therefore cannot distinguish the information content that is encoded in various local atomic arrangements and measurable by the Shannon entropy in information theory. Here, inspired by the finding that two-dimensional (2D) Sudoku matrices exhibit higher average Shannon entropy than 2D random matrix counterparts, we propose high-Shannon-entropy alloys (HSEA), whose structures are based on 3D Sudoku matrices. Despite the constraints on the atomic arrangements to form a 3D Sudoku 4 x 4 x 4 matrix, we find that, a prototypical HSEA, NbMoTaW has a lower energy than the same HEA with a random structure due to the smaller lattice distortion. We compute the electronic structures and mechanical properties and find that the HSEA exhibits enhanced average Fermi velocity and ductility over the random NbMoTaW HEA. Finally, we evaluate the formation energies of single vacancies and a vacancy cluster in the HSEA, whose configurations mimic Sudoku puzzles with one and several missing numbers, respectively. We find that a range of large energies are required to generate such vacancies, which depend on the location and species of missing atoms. Our work shows that introducing the Shannon entropy to HEAs offers a useful metric to reveal more atomic details of HEAs and that HSEAs represent an uncharted domain in the complicated energy landscape of HEAs which may be endowed with improved properties.","570":"During the COVID-19 pandemic, a massive number of attempts on the predictions of the number of cases and the other future trends of this pandemic have been made. However, they fail to predict, in a reliable way, the medium and long term evolution of fundamental features of COVID-19 outbreak within acceptable accuracy. This paper gives an explanation for the failure of machine learning models in this particular forecasting problem. The paper shows that simple linear regression models provide high prediction accuracy values reliably but only for a 2-weeks period and that relatively complex machine learning models, which have the potential of learning long term predictions with low errors, cannot achieve to obtain good predictions with possessing a high generalization ability. It is suggested in the paper that the lack of a sufficient number of samples is the source of low prediction performance of the forecasting models. The reliability of the forecasting results about the active cases is measured in terms of the cross-validation prediction errors, which are used as expectations for the generalization errors of the forecasters. To exploit the information, which is of most relevant with the active cases, we perform feature selection over a variety of variables. We apply different feature selection methods, namely the Pairwise Correlation, Recursive Feature Selection, and feature selection by using the Lasso regression and compare them to each other and also with the models not employing any feature selection. Furthermore, we compare Linear Regression, Multi-Layer Perceptron, and Long-Short Term Memory models each of which is used for prediction active cases together with the mentioned feature selection methods. Our results show that the accurate forecasting of the active cases with high generalization ability is possible up to 3 days only because of the small sample size of COVID-19 data.","571":"Computational Colour Constancy (CCC) consists of estimating the colour of one or more illuminants in a scene and using them to remove unwanted chromatic distortions. Much research has focused on illuminant estimation for CCC on single images, with few attempts of leveraging the temporal information intrinsic in sequences of correlated images (e.g., the frames in a video), a task known as Temporal Colour Constancy (TCC). The state-of-the-art for TCC is TCCNet, a deep-learning architecture that uses a ConvLSTM for aggregating the encodings produced by CNN submodules for each image in a sequence. We extend this architecture with different models obtained by (i) substituting the TCCNet submodules with C4, the state-of-the-art method for CCC targeting images; (ii) adding a cascading strategy to perform an iterative improvement of the estimate of the illuminant. We tested our models on the recently released TCC benchmark and achieved results that surpass the state-of-the-art. Analyzing the impact of the number of frames involved in illuminant estimation on performance, we show that it is possible to reduce inference time by training the models on few selected frames from the sequences while retaining comparable accuracy.","572":"During the COVID-19 pandemic, the slope of the epidemic curve in Mexico City has been quite unstable. We have predicted that in the case that a fraction of the population above a certain threshold returns to the public space, the negative tendency of the epidemic curve will revert. Such predictions were based on modeling the reactivation of economic activity after lockdown by means of an epidemiological model on a contact network of Mexico City derived from mobile device co-localization. We evaluated the epidemic dynamics considering the tally of active and recovered cases documented in the mexican government's open database. Scenarios were modeled in which different percentages of the population are reintegrated to the public space by scanning values ranging from 5% up to 50%. Null models were built by using data from the Jornada Nacional de Sana Distancia (the Mexican model of elective lockdown) in which there was a mobility reduction of 75% and no mandatory mobility restrictions. We found that a new peak of cases in the epidemic curve was very likely for scenarios in which more than 5% of the population rejoined the public space; The return of more than 50% of the population synchronously will unleash a peak of a magnitude similar to the one that was predicted with no mitigation strategies. By evaluating the tendencies of the epidemic dynamics, the number of new cases registered, new cases hospitalized, and new deaths, we consider that under this scenario, reactivation following only elective measures may not be optimal. Given the need to reactivate economic activities, we suggest to consider alternative measures that allow to diminish the contacts among people returning to the public space. We evaluated that by\"encapsulating\"reactivated workers may allow a reactivation of a larger fraction of the population without compromising the desired tendency in the epidemic curve.","573":"Artificial neural networks (ANNs) have been the catalyst to numerous advances in a variety of fields and disciplines in recent years. Their impact on economics, however, has been comparatively muted. One type of ANN, the long short-term memory network (LSTM), is particularly wellsuited to deal with economic time-series. Here, the architecture's performance and characteristics are evaluated in comparison with the dynamic factor model (DFM), currently a popular choice in the field of economic nowcasting. LSTMs are found to produce superior results to DFMs in the nowcasting of three separate variables; global merchandise export values and volumes, and global services exports. Further advantages include their ability to handle large numbers of input features in a variety of time frequencies. A disadvantage is the inability to ascribe contributions of input features to model outputs, common to all ANNs. In order to facilitate continued applied research of the methodology by avoiding the need for any knowledge of deep-learning libraries, an accompanying Python library was developed using PyTorch, https:\/\/pypi.org\/project\/nowcast-lstm\/.","574":"Faced with the 2020 SARS-CoV2 epidemic, public health officials have been seeking models that could be used to predict not only the number of new cases but also the levels of hospitalisation, critical care and deaths. In this paper we present a stochastic compartmental model capable of real-time monitoring and forecasting of the pandemic incorporating multiple streams of real-world data, reported cases, testing intensity, deaths, hospitalisations and critical care occupancy. Model parameters are estimated via a Bayesian particle filtering technique. The model successfully tracks the key variables (reported cases, critical care and deaths) throughout the two waves (March-June and September-November 2020) of the COVID-19 outbreak in Scotland. The model hospitalisation predictions in Summer 2020 are consistently lower than the recorded data, but consistent with the change to the reporting criteria by the Health Protection Scotland on 15th September. Most parameter estimates were constant over the two waves, but the infection rate and consequently the reproductive number decrease in the later stages of the first wave and increase again from July 2020. The death rates are initially high but decrease over Summer 2020 before rising again in November. The model can also be used to provide short-term predictions. We show that the 2-week predictability is very good for the period from March to June 2020, even at early stages of the pandemic. The model has been slower to pick up the increase in the case numbers in September 2020 but forecasting improves again in the later stages of the epidemic.","575":"Dietary studies showed that dietary-related problem such as obesity is associated with other chronic diseases like hypertension, irregular blood sugar levels, and increased risk of heart attacks. The primary cause of these problems is poor lifestyle choices and unhealthy dietary habits, which are manageable using interactive mHealth apps. However, traditional dietary monitoring systems using manual food logging suffer from imprecision, underreporting, time consumption, and low adherence. Recent dietary monitoring systems tackle these challenges by automatic assessment of dietary intake through machine learning methods. This survey discusses the most performing methodologies that have been developed so far for automatic food recognition and volume estimation. First, we will present the rationale of visual-based methods for food recognition. The core of the paper is the presentation, discussion and evaluation of these methods on popular food image databases. Following that, we discussed the mobile applications that are implementing these methods. The survey ends with a discussion of research gaps and open issues in this area.","576":"The metric dimension (MD) of a graph is a combinatorial notion capturing the minimum number of landmark nodes needed to distinguish every pair of nodes in the graph based on graph distance. We study how much the MD can increase if we add a single edge to the graph. The extra edge can either be selected adversarially, in which case we are interested in the largest possible value that the MD can take, or uniformly at random, in which case we are interested in the distribution of the MD. The adversarial setting has already been studied by [Eroh et. al., 2015] for general graphs, who found an example where the MD doubles on adding a single edge. By constructing a different example, we show that this increase can be as large as exponential. However, we believe that such a large increase can occur only in specially constructed graphs, and that in most interesting graph families, the MD at most doubles on adding a single edge. We prove this for $d$-dimensional grid graphs, by showing that $2d$ appropriately chosen corners and the endpoints of the extra edge can distinguish every pair of nodes, no matter where the edge is added. For the special case of $d=2$, we show that it suffices to choose the four corners as landmarks. Finally, when the extra edge is sampled uniformly at random, we conjecture that the MD of 2-dimensional grids converges in probability to $3+\\mathrm{Ber}(8\/27)$, and we give an almost complete proof.","577":"Human-robot handover is a fundamental yet challenging task in human-robot interaction and collaboration. Recently, remarkable progressions have been made in human-to-robot handovers of unknown objects by using learning-based grasp generators. However, how to responsively generate smooth motions to take an object from a human is still an open question. Specifically, planning motions that take human comfort into account is not a part of the human-robot handover process in most prior works. In this paper, we propose to generate smooth motions via an efficient model-predictive control (MPC) framework that integrates perception and complex domain-specific constraints into the optimization problem. We introduce a learning-based grasp reachability model to select candidate grasps which maximize the robot's manipulability, giving it more freedom to satisfy these constraints. Finally, we integrate a neural net force\/torque classifier that detects contact events from noisy data. We conducted human-to-robot handover experiments on a diverse set of objects with several users (N=4) and performed a systematic evaluation of each module. The study shows that the users preferred our MPC approach over the baseline system by a large margin. More results and videos are available at https:\/\/sites.google.com\/nvidia.com\/mpc-for-handover.","578":"Reinforcement learning (RL) is acquiring a key role in the space of adaptive interventions (AIs), attracting a substantial interest within methodological and theoretical literature and becoming increasingly popular within health sciences. Despite potential benefits, its application in real life is still limited due to several operational and statistical challenges--in addition to ethical and cost issues among others--that remain open in part due to poor communication and synergy between methodological and applied scientists. In this work, we aim to bridge the different domains that contribute to and may benefit from RL, under a unique framework that intersects the areas of RL, causal inference, and AIs, among others. We provide the first unified instructive survey on RL methods for building AIs, encompassing both dynamic treatment regimes (DTRs) and just-in-time adaptive interventions in mobile health (mHealth). We outline similarities and differences between the two areas, and discuss their implications for using RL. We combine our relevant methodological knowledge with motivating studies in both DTRs and mHealth to illustrate the tremendous collaboration opportunities between statistical, RL, and healthcare researchers in the space of AIs.","579":"Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. We see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and\/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.","580":"The adoption of electric vehicles (EVs) have proven to be a crucial factor to decreasing the emission of greenhouse gases (GHG) into the atmosphere. However, there are various hurdles that impede people from purchasing EVs. For example, long charging time, short driving range, cost and insufficient charging infrastructures available, etc. This article reports the public perception of EV-adoption using statistical analyses and proposes some recommendations for improving EV-adoption in Qatar. User perspectives on EV-adoption barriers in Qatar were investigated based on survey questionnaires. The survey questionnaires were based on similar studies done in other regions of the world. The study attempted to look at different perspectives of the adoption of EV, when asked to a person who is aware of EVs or a person who may or may not be aware of EVs. Cumulative survey responses from the two groups were compared and analyzed using a two sample t-test statistical analysis. Detailed analyses showed that among various major hindrances raising of public awareness of such greener modes of transportation, the availability of charging options in more places and policy incentives towards EVs would play a major role in EV-adoption. The authors provide recommendations that along with government incentives could help make a gradual shift to a greater number of EVs convenient for people of Qatar. The proposed systematic approach for such a study and analysis may help in streamlining research on policies, infrastructures and technologies for efficient penetration of EVs in Qatar.","581":"The energy trade is an important pillar of each country's development, making up for the imbalance in the production and consumption of fossil fuels. Geopolitical risks affect the energy trade of various countries to a certain extent, but the causes of geopolitical risks are complex, and energy trade also involves many aspects, so the impact of geopolitics on energy trade is also complex. Based on the monthly data from 2000 to 2020 of 17 emerging economies, this paper employs the fixed-effect model and the regression-discontinuity (RD) model to verify the negative impact of geopolitics on energy trade first and then analyze the mechanism and heterogeneity of the impact. The following conclusions are drawn: First, geopolitics has a significant negative impact on the import and export of the energy trade, and the inhibition on the export is greater than that on the import. Second, the impact mechanism of geopolitics on the energy trade is reflected in the lagging effect and mediating effect on the imports and exports; that is, the negative impact of geopolitics on energy trade continued to be significant 10 months later. Coal and crude oil prices, as mediating variables, decreased to reduce the imports and exports, whereas natural gas prices showed an increase. Third, the impact of geopolitics on energy trade is heterogeneous in terms of national attribute characteristics and geo-event types.","582":"We document the evolution of labour market power by employers on the US and Peruvian labour markets during the 2010s. Making use of a structural estimation model of labour market dynamics, we estimate differences in market power that workers face depending on their sector of activity, their age, sex, location and educational level. In particular, we show that differences in cross-sectional market power are significant and higher than variations over the ten-year time span of our data. In contrast to findings of labour market power in developed countries such as the US, we document significant market power of employers in Peru vis-\\`a-vis the tertiary educated workforce, regardless of age and sector. In contrast, for the primary educated workforce, market power seems to be high in (private) services and manufacturing. For secondary educated workers, only the mining sector stands out as moderately more monopsonistic than the rest of the labour market. We also show that at least for the 2010s, labour market power declined in Peru. We contrast these findings with similar estimates obtained for the United States where we are able to show that increases in labour market power are particularly acute in certain sectors, such as agriculture and entertainment and recreational services, as well as in specific geographic areas where these sectors are dominant. Moreover, we show that for the US, the labour market power has gradually increased over the past ten years, in line with the general rise in inequality. Importantly, we show that the pervasive gender pay gap cannot be linked to differential market power as men face higher labour market power than women. We also discuss possible reasons for these findings, including for the differences of labour market power across skill levels. Especially, we discuss the reasons for polarization of market power that are specific to sectors and locations.","583":"We present a novel method for interactive construction and rendering of extremely large molecular scenes, capable of representing multiple biological cells at atomistic detail. Our method is tailored for scenes, which are procedurally constructed, based on a given set of building rules. Rendering of large scenes normally requires the entire scene available in-core, or alternatively, it requires out-of-core management to load data into the memory hierarchy as a part of the rendering loop. Instead of out-of-core memory management, we propose to procedurally generate the scene on-demand on the fly. The key idea is a positional- and view-dependent procedural scene-construction strategy, where only a fraction of the atomistic scene around the camera is available in the GPU memory at any given time. The atomistic detail is populated into a uniform-space partitioning using a grid that covers the entire scene. Most of the grid cells are not filled with geometry, only those are populated that are potentially seen by the camera. The atomistic detail is populated in a compute shader and its representation is connected with acceleration data structures for hardware ray-tracing of modern GPUs. Objects which are far away, where atomistic detail is not perceivable from a given viewpoint, are represented by a triangle mesh mapped with a seamless texture, generated from the rendering of geometry from atomistic detail. The algorithm consists of two pipelines, the construction computes pipeline and the rendering pipeline, which work together to render molecular scenes at an atomistic resolution far beyond the limit of the GPU memory containing trillions of atoms. We demonstrate our technique on multiple models of SARS-CoV-2 and the red blood cell.","584":"We consider the control of the COVID-19 pandemic through a standard SIR compartmental model. This control is induced by the aggregation of individuals' decisions to limit their social interactions: when the epidemic is ongoing, an individual can diminish his\/her contact rate in order to avoid getting infected, but this effort comes at a social cost. If each individual lowers his\/her contact rate, the epidemic vanishes faster, but the effort cost may be high. A Mean Field Nash equilibrium at the population level is formed, resulting in a lower effective transmission rate of the virus. We prove theoretically that equilibrium exists and compute it numerically. However, this equilibrium selects a sub-optimal solution in comparison to the societal optimum (a centralized decision respected fully by all individuals), meaning that the cost of anarchy is strictly positive. We provide numerical examples and a sensitivity analysis, as well as an extension to a SEIR compartmental model to account for the relatively long latent phase of the COVID-19 disease. In all the scenarii considered, the divergence between the individual and societal strategies happens both before the peak of the epidemic, due to individuals' fears, and after, when a significant propagation is still underway.","585":"An increasing number of mental health services are offered through mobile systems, a paradigm called mHealth. Although there is an unprecedented growth in the adoption of mHealth systems, partly due to the COVID-19 pandemic, concerns about data privacy risks due to security breaches are also increasing. Whilst some studies have analyzed mHealth apps from different angles, including security, there is relatively little evidence for data privacy issues that may exist in mHealth apps used for mental health services, whose recipients can be particularly vulnerable. This paper reports an empirical study aimed at systematically identifying and understanding data privacy incorporated in mental health apps. We analyzed 27 top-ranked mental health apps from Google Play Store. Our methodology enabled us to perform an in-depth privacy analysis of the apps, covering static and dynamic analysis, data sharing behaviour, server-side tests, privacy impact assessment requests, and privacy policy evaluation. Furthermore, we mapped the findings to the LINDDUN threat taxonomy, describing how threats manifest on the studied apps. The findings reveal important data privacy issues such as unnecessary permissions, insecure cryptography implementations, and leaks of personal data and credentials in logs and web requests. There is also a high risk of user profiling as the apps' development do not provide foolproof mechanisms against linkability, detectability and identifiability. Data sharing among third parties and advertisers in the current apps' ecosystem aggravates this situation. Based on the empirical findings of this study, we provide recommendations to be considered by different stakeholders of mHealth apps in general and apps developers in particular. [...]","586":"The development of trajectory-based operations and the rolling network operations plan in European air traffic management network implies a move towards more collaborative, strategic flight planning. This opens up the possibility for inclusion of additional information in the collaborative decision-making process. With that in mind, we define the indicator for the economic risk of network elements (e.g., sectors or airports) as the expected costs that the elements impose on airspace users due to Air Traffic Flow Management (ATFM) regulations. The definition of the indicator is based on the analysis of historical ATFM regulations data, that provides an indication of the risk of accruing delay. This risk of delay is translated into a monetary risk for the airspace users, creating the new metric of the economic risk of a given airspace element. We then use some machine learning techniques to find the parameters leading to this economic risk. The metric is accompanied by an indication of the accuracy of the delay cost prediction model. Lastly, the economic risk is transformed into a qualitative economic severity classification. The economic risks and consequently economic severity can be estimated for different temporal horizons and time periods providing an indicator which can be used by Air Navigation Service Providers to identify areas which might need the implementation of strategic measures (e.g., resectorisation or capacity provision change), and by Airspace Users to consider operation of routes which use specific airspace regions.","587":"This study aims to understand the South African political context by analysing the sentiments shared on Twitter during the local government elections. An emphasis on the analysis was placed on understanding the discussions led around four predominant political parties ANC, DA, EFF and ActionSA. A semi-supervised approach by means of a graph-based technique to label the vast accessible Twitter data for the classification of tweets into negative and positive sentiment was used. The tweets expressing negative sentiment were further analysed through latent topic extraction to uncover hidden topics of concern associated with each of the political parties. Our findings demonstrated that the general sentiment across South African Twitter users is negative towards all four predominant parties with the worst negative sentiment among users projected towards the current ruling party, ANC, relating to concerns cantered around corruption, incompetence and loadshedding.","588":"Biomedical knowledge graphs permit an integrative computational approach to reasoning about biological systems. The nature of biological data leads to a graph structure that differs from those typically encountered in benchmarking datasets. To understand the implications this may have on the performance of reasoning algorithms, we conduct an empirical study based on the real-world task of drug repurposing. We formulate this task as a link prediction problem where both compounds and diseases correspond to entities in a knowledge graph. To overcome apparent weaknesses of existing algorithms, we propose a new method, PoLo, that combines policy-guided walks based on reinforcement learning with logical rules. These rules are integrated into the algorithm by using a novel reward function. We apply our method to Hetionet, which integrates biomedical information from 29 prominent bioinformatics databases. Our experiments show that our approach outperforms several state-of-the-art methods for link prediction while providing interpretability.","589":"I estimate the Susceptible-Infected-Recovered (SIR) epidemic model for Coronavirus Disease 2019 (COVID-19). The transmission rate is heterogeneous across countries and far exceeds the recovery rate, which enables a fast spread. In the benchmark model, 28% of the population may be simultaneously infected at the peak, potentially overwhelming the healthcare system. The peak reduces to 6.2% under the optimal mitigation policy that controls the timing and intensity of social distancing. A stylized asset pricing model suggests that the stock price temporarily decreases by 50% in the benchmark case but shows a W-shaped, moderate but longer bear market under the optimal policy.","590":"Western tonality provides a hierarchy of stability among melodic scale-degrees, from the maximally stable tonic to unstable chromatic notes. Tonal stability has been linked to emotion, yet systematic investigations of the associations between the hierarchy of melodic scale-degrees and perceived emotional valence are lacking. Here, we examined such associations in three experiments, in musicians and in non-musicians. We used an explicit task, in which participants matched probe tones following key-establishing sequences to facial expressions ranging from sad to happy, and an implicit speeded task, a variant of the Implicit Association Test. Stabler scale-degrees were associated with more positive valence in all experiments, for both musicians and nonmusicians. This notwithstanding, results significantly differed from those of a comparable goodness-of-fit task, suggesting that perceived tonal valence is not reducible to tonal fit. Comparisons of the explicit and implicit measures suggest that associations of tonality and emotional valence may rely on two distinct mechanisms, one mediated by conceptual musical knowledge and conscious decisional processes, the other - largely by non-conceptual and involuntary processes. The joint experimental paradigms introduced here may help mapping additional connotative meanings, both emotional and cross-modal, embedded in tonal structure, thus suggesting how extra-musical meanings are conveyed to listeners through musical syntax.","591":"As the COVID-19 pandemic significantly affects every aspect of human life, there is an urgent need for high-quality datasets for further COVID-19 research. We, therefore, introduce Sound-Dr dataset that not only provides quality coughing, mouth breathing, nose breathing sounds, but also valuable related metadata for detecting relevant-respiratory illness. We propose a proof-of-concept system that is effective for the detection of abnormalities in the respiratory sounds of patients. Our system has a promising processing time and good accuracy for real-time trials on mobile devices. The proposed dataset and system will serve as effective tools to assist physicians in diagnosing respiratory disorders.","592":"Rapid and accurate simulation of cerebral aneurysm flow modifications by flow diverters (FDs) can help improving patient-specific intervention and predicting treatment outcome. However, with explicit FD devices being placed in patient-specific aneurysm model, the computational domain must be resolved around the thin stent wires, leading to high computational cost in computational fluid dynamics (CFD). Classic homogeneous porous medium (PM) methods cannot accurately predict the post-stenting aneurysmal flow field due to the inhomogeneous FD wire distributions on anatomic arteries. We propose a novel approach that models the FD flow modification as a thin inhomogeneous porous medium (iPM). It improves over classic PM approaches in that, first, FD is treated as a screen, which is more accurate than the classic Darcy-Forchheimer relation based on 3D PM. second, the pressure drop is calculated using local FD geometric parameters across an inhomogeneous PM, which is more realistic. To test its accuracy and speed, we applied the iPM technique to simulate the post stenting flow field in three patient-specific aneurysms and compared the results against CFD simulations with explicit FD devices. The iPM CFD ran 500% faster than the explicit CFD while achieving 94%-99% accuracy. Thus iPM is a promising clinical bedside modeling tool to assist endovascular interventions with FD and stents.","593":"By the start of 2020, the novel coronavirus disease (COVID-19) has been declared a worldwide pandemic. Because of the severity of this infectious disease, several kinds of research have focused on combatting its ongoing spread. One potential solution to detect COVID-19 is by analyzing the chest X-ray images using Deep Learning (DL) models. In this context, Convolutional Neural Networks (CNNs) are presented as efficient techniques for early diagnosis. In this study, we propose a novel randomly initialized CNN architecture for the recognition of COVID-19. This network consists of a set of different-sized hidden layers created from scratch. The performance of this network is evaluated through two public datasets, which are the COVIDx and the enhanced COVID-19 datasets. Both of these datasets consist of 3 different classes of images: COVID19, pneumonia, and normal chest X-ray images. The proposed CNN model yields encouraging results with 94% and 99% of accuracy for COVIDx and enhanced COVID-19 dataset, respectively.","594":"Coronavirus Disease 2019 (COVID-19) has spread all over the world since it broke out massively in December 2019, which has caused a large loss to the whole world. Both the confirmed cases and death cases have reached a relatively frightening number. Syndrome coronaviruses 2 (SARS-CoV-2), the cause of COVID-19, can be transmitted by small respiratory droplets. To curb its spread at the source, wearing masks is a convenient and effective measure. In most cases, people use face masks in a high-frequent but short-time way. Aimed at solving the problem that we don't know which service stage of the mask belongs to, we propose a detection system based on the mobile phone. We first extract four features from the GLCMs of the face mask's micro-photos. Next, a three-result detection system is accomplished by using KNN algorithm. The results of validation experiments show that our system can reach a precision of 82.87% (standard deviation=8.5%) on the testing dataset. In future work, we plan to expand the detection objects to more mask types. This work demonstrates that the proposed mobile microscope system can be used as an assistant for face mask being used, which may play a positive role in fighting against COVID-19.","595":"We prove the nonintegrability of the susceptible-exposed-infected-removed (SEIR) epidemic model in the Bogoyavlenskij sense. This property of the SEIR model is different from the more fundamental susceptible-infected-removed (SIR) model, which is Bogoyavlenskij-nonintegrable. Our basic tool for the proof is an extension of the Morales-Ramis theory due to Ayoul and Zung. Moreover, we extend the system to a six-dimensional system to treat transcendental first integrals and commutative vector fields. We also use the fact that the incomplete gamma function $\\Gamma(\\alpha,x)$ is not elementary for $\\alpha\\not\\in\\mathbb{N}$, of which a proof is included.","596":"This paper models stochastic process of price time series of CSI 300 index in Chinese financial market, analyzes volatility characteristics of intraday high-frequency price data. In the new generalized Barndorff-Nielsen and Shephard model, the lag caused by asynchrony of market information is considered, and the problem of lack of long-term dependence is solved. To speed up the valuation process, several machine learning and deep learning algorithms are used to estimate parameter and evaluate forecast results. Tracking historical jumps of different magnitudes offers promising avenues for simulating dynamic price processes and predicting future jumps. Numerical results show that the deterministic component of stochastic volatility processes would always be captured over short and longer-term windows. Research finding could be suitable for influence investors and regulators interested in predicting market dynamics based on realized volatility.","597":"The Gravitational-wave Optical Transient Observer (GOTO) is a wide-field telescope project focused on detecting optical counterparts to gravitational-wave sources. GOTO uses arrays of 40 cm unit telescopes (UTs) on a shared robotic mount, which scales to provide large fields of view in a cost-effective manner. A complete GOTO mount uses 8 unit telescopes to give an overall field of view of 40 square degrees, and can reach a depth of 20th magnitude in three minutes. The GOTO-4 prototype was inaugurated with 4 unit telescopes in 2017 on La Palma, and was upgraded to a full 8-telescope array in 2020. A second 8-UT mount will be installed on La Palma in early 2021, and another GOTO node with two more mount systems is planned for a southern site in Australia. When complete, each mount will be networked to form a robotic, dual-hemisphere observatory, which will survey the entire visible sky every few nights and enable rapid follow-up detections of transient sources.","598":"COVID-19 infection caused by SARS-CoV-2 pathogen is a catastrophic pandemic outbreak all over the world with exponential increasing of confirmed cases and, unfortunately, deaths. In this work we propose an AI-powered pipeline, based on the deep-learning paradigm, for automated COVID-19 detection and lesion categorization from CT scans. We first propose a new segmentation module aimed at identifying automatically lung parenchyma and lobes. Next, we combined such segmentation network with classification networks for COVID-19 identification and lesion categorization. We compare the obtained classification results with those obtained by three expert radiologists on a dataset consisting of 162 CT scans. Results showed a sensitivity of 90\\% and a specificity of 93.5% for COVID-19 detection, outperforming those yielded by the expert radiologists, and an average lesion categorization accuracy of over 84%. Results also show that a significant role is played by prior lung and lobe segmentation that allowed us to enhance performance by over 20 percent points. The interpretation of the trained AI models, moreover, reveals that the most significant areas for supporting the decision on COVID-19 identification are consistent with the lesions clinically associated to the virus, i.e., crazy paving, consolidation and ground glass. This means that the artificial models are able to discriminate a positive patient from a negative one (both controls and patients with interstitial pneumonia tested negative to COVID) by evaluating the presence of those lesions into CT scans. Finally, the AI models are integrated into a user-friendly GUI to support AI explainability for radiologists, which is publicly available at http:\/\/perceivelab.com\/covid-ai.","599":"To understand the emergence of hashtag popularity in online social networking complex systems, we study the largest Chinese microblogging site Sina Weibo, which has a Hot Search List (HSL) showing in real time the ranking of the 50 most popular hashtags based on search activity. We investigate the prehistory of successful hashtags from 17 July 2020 to 17 September 2020 by mapping out the related interaction network preceding the selection to HSL. We have found that the circadian activity pattern has an impact on the time needed to get to the HSL. When analyzing this time we distinguish two extreme categories: a)\"Born in Rome\", which means hashtags are mostly first created by super-hubs or reach super-hubs at an early stage during their propagation and thus gain immediate wide attention from the broad public, and b)\"Sleeping Beauty\", meaning the hashtags gain little attention at the beginning and reach system-wide popularity after a considerable time lag. The evolution of the repost networks of successful hashtags before getting to the HSL show two types of growth patterns:\"smooth\"and\"stepwise\". The former is usually dominated by a super-hub and the latter results from consecutive waves of contributions of smaller hubs. The repost networks of unsuccessful hashtags exhibit a simple evolution pattern.","600":"The European Committee for Future Accelerators (ECFA) Early-Career Researchers (ECR) Panel was invited by the ECFA Detector R&D Roadmap conveners to collect feedback from the European ECR community. A working group within the ECFA ECR panel held a Townhall Meeting to get first input, and then designed and broadly circulated a detailed survey to gather feedback from the larger ECR community. A total of 473 responses to this survey were received, providing a useful overview of the experiences of ECRs in instrumentation training and related topics. This report summarises the feedback received, and is intended to serve as an input to the ECFA Detector R&D Roadmap process.","601":"As users increasingly introduce Internet-connected devices into their homes, having access to accurate and relevant cyber security information is a fundamental means of ensuring safe use. Given the paucity of information provided with many devices at the time of purchase, this paper engages in a critical study of the type of advice that home Internet of Things (IoT) or smart device users might be presented with on the Internet to inform their cyber security practices. We base our research on an analysis of 427 web pages from 234 organisations that present information on security threats and relevant cyber security advice. The results show that users searching online for information are subject to an enormous range of advice and news from various sources with differing levels of credibility and relevance. With no clear explanation of how a user may assess the threats as they are pertinent to them, it becomes difficult to understand which pieces of advice would be the most effective in their situation. Recommendations are made to improve the clarity, consistency and availability of guidance from recognised sources to improve user access and understanding.","602":"We study the maximum score statistic to detect and estimate local signals in the form of change-points in the level, slope, or other property of a sequence of observations, and to segment the sequence when there appear to be multiple changes. We find that when observations are serially dependent, the change-points can lead to upwardly biased estimates of autocorrelations, resulting in a sometimes serious loss of power. Examples involving temperature variations, the level of atmospheric greenhouse gases, suicide rates, incidence of COVID-19, and excess deaths during the pandemic illustrate the general theory.","603":"We are reporting on lessons learned from an e-contest for students held during the current pandemic. We compare the e-contest with the 10 previous editions of the same but face-to-face contest. While apparently the competition did not suffer because of being a virtual one, some disadvantages were noted. The main conclusions are: the basic interconnectivity means arise no serious technical issue, but the interconnectivity is more limited than the face-to-face one; online jury-competitors interactivity is poorer than face-to-face interactivity; human factors, higher uncertainties in the organization process, and less time to spend in the process for the local organizers are major limiting factors; concerns on the participation and evaluation fairness are higher; involuntary gender discrimination seems lower, but persists; there are serious concerns related to privacy, including differential privacy; some peculiarities of the presented topics and of the evaluation process emerged, but it is unclear if they are related to the online nature of the competition, to the extra stress on the participants during the pandemic, to other factors, or are random. While some conclusions may be intimately related to the analyzed case, some are general enough for being worth to other online competitions.","604":"Determining whether a configurable software system has a performance bug or it was misconfigured is often challenging. While there are numerous debugging techniques that can support developers in this task, there is limited empirical evidence of how useful the techniques are to address the actual needs that developers have when debugging the performance of configurable software systems; most techniques are often evaluated in terms of technical accuracy instead of their usability. In this paper, we take a human-centered approach to identify, design, implement, and evaluate a solution to support developers in the process of debugging the performance of configurable software systems. We first conduct an exploratory study with 19 developers to identify the information needs that developers have during this process. Subsequently, we design and implement a tailored tool, adapting techniques from prior work, to support those needs. Two user studies, with a total of 20 developers, validate and confirm that the information that we provide helps developers debug the performance of configurable software systems.","605":"In the face of a pandemic, urban protests, and an affordability crisis, is the desirability of dense urban settings at a turning point? Assessing cities' long term trends remains challenging. The first part of this chapter describes the short-run dynamics of the housing market in 2020. Evidence from prices and price-to-rent ratios suggests expectations of resilience. Zip-level evidence suggests a short-run trend towards suburbanization, and some impacts of urban protests on house prices. The second part of the chapter analyzes the long-run dynamics of urban growth between 1970 and 2010. It analyzes what, in such urban growth, is explained by short-run shocks as opposed to fundamentals such as education, industrial specialization, industrial diversification, urban segregation, and housing supply elasticity. This chapter's original results as well as a large established body of literature suggest that fundamentals are the key drivers of growth. The chapter illustrates this finding with two case studies: the New York City housing market after September 11, 2001; and the San Francisco Bay Area in the aftermath of the 1989 Loma Prieta earthquake. Both areas rebounded strongly after these shocks, suggesting the resilience of the urban metropolis.","606":"The rapid early spread of COVID-19 in the U.S. was experienced very differently by different socioeconomic groups and business industries. In this study, we study aggregate mobility patterns of New York City and Chicago to identify the relationship between the amount of interpersonal contact between people in urban neighborhoods and the disparity in the growth of positive cases among these groups. We introduce an aggregate Contact Exposure Index (CEI) to measure exposure due to this interpersonal contact and combine it with social distancing metrics to show its effect on positive case growth. With the help of structural equations modeling, we find that the effect of exposure on case growth was consistently positive and that it remained consistently higher in lower-income neighborhoods, suggesting a causal path of income on case growth via contact exposure. Using the CEI, schools and restaurants are identified as high-exposure industries, and the estimation suggests that implementing specific mobility restrictions on these point-of-interest categories are most effective. This analysis can be useful in providing insights for government officials targeting specific population groups and businesses to reduce infection spread as reopening efforts continue to expand across the nation.","607":"Riots originating during, or in the aftermath of, sports events can incur significant costs in damages, as well as large-scale panic and injuries. A mathematical description of sports riots is therefore sought to better understand their propagation and limit these physical and financial damages. In this work, we present an agent-based modelling framework that describes the qualitative features of populations engaging in riotous behaviour. Agents, pertaining to either a 'rioter' or a 'bystander' sub-population, move on an underlying lattice and can either be recruited or defect from their respective subpopulation. In particular, we allow these individual-level recruitment and defection processes to vary with local population density. This agent-based modelling framework provides the unifying link between multi-population stochastic models and density-dependent reaction processes. Furthermore, the continuum description of this ABM framework is shown to be a system of nonlinear reaction-diffusion equations and faithfully agrees with the average ABM behaviour from individual simulations. Finally, we determine the unique correspondence between the underlying individual-level recruitment and defection mechanisms with their population-level counterparts, providing a link between local-scale effects and macroscale rioting phenomena.","608":"Higher-order topological insulator (HOTI) represents a new phase of matter, the characterization of which goes beyond the conventional bulk-boundary correspondence and is attracting significant attention by the broad community. Using a square-root operation, it has been suggested that a square-root HOTI may emerge in a hybrid honeycomb-kagome lattice. Here, we report the first experimental realization of the square-root HOTI in topological LC circuits. We show theoretically and experimentally that the square-root HOTI inherits the feature of wave function from its parent, with corner states pinned to non-zero energies. The topological feature is fully characterized by the bulk polarization. To directly measure the finite-energy corner modes, we introduce extra grounded inductors to each node. Our results experimentally substantiate the emerging square-root HOTI and pave the way to realizing exotic topological phases that are challenging to observe in condensed matter physics.","609":"Monitoring the evolution of the Covid19 pandemic constitutes a critical step in sanitary policy design. Yet, the assessment of the pandemic intensity within the pandemic period remains a challenging task because of the limited quality of data made available by public health authorities (missing data, outliers and pseudoseasonalities, notably), that calls for cumbersome and ad-hoc preprocessing (denoising) prior to estimation. Recently, the estimation of the reproduction number, a measure of the pandemic intensity, was formulated as an inverse problem, combining data-model fidelity and space-time regularity constraints, solved by nonsmooth convex proximal minimizations. Though promising, that formulation lacks robustness against the limited quality of the Covid19 data and confidence assessment. The present work aims to address both limitations: First, it discusses solutions to produce a robust assessment of the pandemic intensity by accounting for the low quality of the data directly within the inverse problem formulation. Second, exploiting a Bayesian interpretation of the inverse problem formulation, it devises a Monte Carlo sampling strategy, tailored to a nonsmooth log-concave a posteriori distribution, to produce relevant credibility intervalbased estimates for the Covid19 reproduction number. Clinical relevance Applied to daily counts of new infections made publicly available by the Health Authorities for around 200 countries, the proposed procedures permit robust assessments of the time evolution of the Covid19 pandemic intensity, updated automatically and on a daily basis.","610":"In this paper, we firstly propose SQIARD and SIARD models to investigate the transmission of COVID-19 with quarantine, infected and asymptomatic infected, and discuss the relation between the respective basic reproduction number $R_0, R_Q$ and the stability of the equilibrium points of model. Secondly, after training the related data parameters, in our numerical simulations, we respectively conduct the forecast of the data of US, South Korea, Brazil, India, Russia and Italy, and the effect of prediction of the epidemic situation in each country. Furthermore, we apply US data to compare SQIARD with SIARD, and display the effects of predictions.","611":"In this paper, we investigate how the COVID-19 pandemics and more precisely the lockdown of a sector of the economy may have changed our habits and, there-fore, altered the demand of some goods even after the re-opening. In a two-sector infinite horizon economy, we show that the demand of the goods produced by the sector closed during the lockdown could shrink or expand with respect to their pre-pandemic level depending on the length of the lockdown and the relative strength of the satiation effect and the substitutability effect. We also provide conditions under which this sector could remain inactive even after the lockdown as well as an insight on the policy which should be adopted to avoid this outcome.","612":"In this demonstration, we present a privacy-preserving epidemic surveillance system. Recently, many countries that suffer from coronavirus crises attempt to access citizen's location data to eliminate the outbreak. However, it raises privacy concerns and may open the doors to more invasive forms of surveillance in the name of public health. It also brings a challenge for privacy protection techniques: how can we leverage people's mobile data to help combat the pandemic without scarifying our location privacy. We demonstrate that we can have the best of the two worlds by implementing policy-based location privacy for epidemic surveillance. Specifically, we formalize the privacy policy using graphs in light of differential privacy, called policy graph. Our system has three primary functions for epidemic surveillance: location monitoring, epidemic analysis, and contact tracing. We provide an interactive tool allowing the attendees to explore and examine the usability of our system: (1) the utility of location monitor and disease transmission model estimation, (2) the procedure of contact tracing in our systems, and (3) the privacy-utility trade-offs w.r.t. different policy graphs. The attendees can find that it is possible to have the full functionality of epidemic surveillance while preserving location privacy.","613":"Clinical evidence has shown that rib-suppressed chest X-rays (CXRs) can improve the reliability of pulmonary disease diagnosis. However, previous approaches on generating rib-suppressed CXR face challenges in preserving details and eliminating rib residues. We hereby propose a GAN-based disentanglement learning framework called Rib Suppression GAN, or RSGAN, to perform rib suppression by utilizing the anatomical knowledge embedded in unpaired computed tomography (CT) images. In this approach, we employ a residual map to characterize the intensity difference between CXR and the corresponding rib-suppressed result. To predict the residual map in CXR domain, we disentangle the image into structure- and contrast-specific features and transfer the rib structural priors from digitally reconstructed radiographs (DRRs) computed by CT. Furthermore, we employ additional adaptive loss to suppress rib residue and preserve more details. We conduct extensive experiments based on 1,673 CT volumes, and four benchmarking CXR datasets, totaling over 120K images, to demonstrate that (i) our proposed RSGAN achieves superior image quality compared to the state-of-the-art rib suppression methods; (ii) combining CXR with our rib-suppressed result leads to better performance in lung disease classification and tuberculosis area detection.","614":"India is one of the worst affected countries by the Covid-19 pandemic at present. We studied publicly available data of the Covid-19 patients in India and analyzed possible impacts of quarantine and social distancing within the stochastic framework of the SEQIR model to illustrate the controlling strategy of the pandemic. Our simulation results clearly show that proper quarantine and social distancing should be maintained from an early time just at the start of the pandemic and should be continued till its end to effectively control the pandemic. This calls for a more socially disciplined lifestyle in this perspective in future. The demographic stochasticity, which is quite visible in the system dynamics, has a critical role in regulating and controlling the pandemic.","615":"In contrast to other fields where conferences are typically for less polished or in-progress research, computing has long relied on referred conference papers as a venue for the final publication of completed research. While frequently a topic of informal discussion, debates about its efficacy, or library science research, the development of this phenomena has not been historically analyzed. This paper presents the first systematic investigation of the development of modern computing publications. It relies on semi-structured interviews with eight computing professors from diverse backgrounds to understand how researchers experienced changes in publication culture over time. Ultimately, the article concludes that the early presence of non-academic practitioners in research and a degree of\"path dependence\"or a tendency to continue on the established path rather than the most economically optimal one\"allowed conferences to gain and hold prominence as the field exploded in popularity during the 1980s.","616":"In March of this year, COVID-19 was declared a pandemic and it continues to threaten public health. This global health crisis imposes limitations on daily movements, which have deteriorated every sector in our society. Understanding public reactions to the virus and the non-pharmaceutical interventions should be of great help to fight COVID-19 in a strategic way. We aim to provide tangible evidence of the human mobility trends by comparing the day-by-day variations across the U.S. Large-scale public mobility at an aggregated level is observed by leveraging mobile device location data and the measures related to social distancing. Our study captures spatial and temporal heterogeneity as well as the sociodemographic variations regarding the pandemic propagation and the non-pharmaceutical interventions. All mobility metrics adapted capture decreased public movements after the national emergency declaration. The population staying home has increased in all states and becomes more stable after the stay-at-home order with a smaller range of fluctuation. There exists overall mobility heterogeneity between the income or population density groups. The public had been taking active responses, voluntarily staying home more, to the in-state confirmed cases while the stay-at-home orders stabilize the variations. The study suggests that the public mobility trends conform with the government message urging to stay home. We anticipate our data-driven analysis offers integrated perspectives and serves as evidence to raise public awareness and, consequently, reinforce the importance of social distancing while assisting policymakers.","617":"Full-field traffic state information (i.e., flow, speed, and density) is critical for the successful operation of Intelligent Transportation Systems (ITS) on freeways. However, incomplete traffic information tends to be directly collected from traffic detectors that are insufficiently installed in most areas, which is a major obstacle to the popularization of ITS. To tackle this issue, this paper introduces an innovative traffic state estimation (TSE) framework that hybrid regression machine learning techniques (e.g., artificial neural network (ANN), random forest (RF), and support vector machine (SVM)) with a traffic physics model (e.g., second-order macroscopic traffic flow model) using limited information from traffic sensors as inputs to construct accurate and full-field estimated traffic state for freeway systems. To examine the effectiveness of the proposed TSE framework, this paper conducted empirical studies on a real-world data set collected from a stretch of I-15 freeway in Salt Lake City, Utah. Experimental results show that the proposed method has been proved to estimate full-field traffic information accurately. Hence, the proposed method could provide accurate and full-field traffic information, thus providing the basis for the popularization of ITS.","618":"The Large-Sized Telescopes (LSTs) of Cherenkov Telescope Array (CTA) are designed for gamma-ray studies focusing on low energy threshold, high flux sensitivity, rapid telescope repositioning speed and a large field of view. Once the CTA array is complete, the LSTs will be dominating the CTA performance between 20 GeV and 150 GeV. During most of the CTA Observatory construction phase, however, the LSTs will be dominating the array performance until several TeVs. In this presentation we report on the status of the LST-1 telescope inaugurated in La Palma, Canary islands, Spain in 2018. We show the progress of the telescope commissioning, compare the expectations with the achieved performance, and give a glance of the first physics results.","619":"Randomness is an important factor in games, so much so that some games rely almost purely on it for its outcomes and increase players' engagement with them. However, randomness can affect the game experience depending on when it occurs in a game, altering the chances of planning for a player. In this paper, we refer to it as\"input-output randomness\". Input-output randomness is a cornerstone of collectable card games like Hearthstone, in which cards are drawn randomly (input randomness) and have random effects when played (output randomness). While the topic might have been commonly discussed by game designers and be present in many games, few empirical studies have been performed to evaluate the effects of these different kinds of randomness on the players' satisfaction. This research investigates the effects of input-output randomness on collectable card games across four input-output randomness conditions. We have developed our own collectable card game and experimented with the different kinds of randomness with the game. Our results suggest that input randomness can significantly impact game satisfaction negatively. Overall, our results present helpful considerations on how and when to apply randomness in game design when aiming for players' satisfaction.","620":"Medical robots can play an important role in mitigating the spread of infectious diseases and delivering quality care to patients during the COVID-19 pandemic. Methods and procedures involving medical robots in the continuum of care, ranging from disease prevention, screening, diagnosis, treatment, and homecare have been extensively deployed and also present incredible opportunities for future development. This paper provides an overview of the current state-of-the-art, highlighting the enabling technologies and unmet needs for prospective technological advances within the next 5-10 years. We also identify key research and knowledge barriers that need to be addressed in developing effective and flexible solutions to ensure preparedness for rapid and scalable deployment to combat infectious diseases.","621":"Today, data is being actively generated by a variety of devices, services, and applications. Such data is important not only for the information that it contains, but also for its relationships to other data and to interested users. Most existing Big Data systems focus on passively answering queries from users, rather than actively collecting data, processing it, and serving it to users. To satisfy both passive and active requests at scale, users need either to heavily customize an existing passive Big Data system or to glue multiple systems together. Either choice would require significant effort from users and incur additional overhead. In this paper, we present the BAD (Big Active Data) system, which is designed to preserve the merits of passive Big Data systems and introduce new features for actively serving Big Data to users at scale. We show the design and implementation of the BAD system, demonstrate how BAD facilitates providing both passive and active data services, investigate the BAD system's performance at scale, and illustrate the complexities that would result from instead providing BAD-like services with a\"glued\"system.","622":"This chapter provides new evidence on educational inequality and reviews the literature on the causes and consequences of unequal education. We document large achievement gaps between children from different socio-economic backgrounds, show how patterns of educational inequality vary across countries, time, and generations, and establish a link between educational inequality and social mobility. We interpret this evidence from the perspective of economic models of skill acquisition and investment in human capital. The models account for different channels underlying unequal education and highlight how endogenous responses in parents' and children's educational investments generate a close link between economic inequality and educational inequality. Given concerns over the extended school closures during the Covid-19 pandemic, we also summarize early evidence on the impact of the pandemic on children's education and on possible long-run repercussions for educational inequality.","623":"Research in NLP lacks geographic diversity, and the question of how NLP can be scaled to low-resourced languages has not yet been adequately solved.\"Low-resourced\"-ness is a complex problem going beyond data availability and reflects systemic problems in society. In this paper, we focus on the task of Machine Translation (MT), that plays a crucial role for information accessibility and communication worldwide. Despite immense improvements in MT over the past decade, MT is centered around a few high-resourced languages. As MT researchers cannot solve the problem of low-resourcedness alone, we propose participatory research as a means to involve all necessary agents required in the MT development process. We demonstrate the feasibility and scalability of participatory research with a case study on MT for African languages. Its implementation leads to a collection of novel translation datasets, MT benchmarks for over 30 languages, with human evaluations for a third of them, and enables participants without formal training to make a unique scientific contribution. Benchmarks, models, data, code, and evaluation results are released under https:\/\/github.com\/masakhane-io\/masakhane-mt.","624":"The COVID-19 disease spreads swiftly, and nearly three months after the first positive case was confirmed in China, Coronavirus started to spread all over the United States. Some states and counties reported high number of positive cases and deaths, while some reported lower COVID-19 related cases and mortality. In this paper, the factors that could affect the risk of COVID-19 infection and mortality were analyzed in county level. An innovative method by using K-means clustering and several classification models is utilized to determine the most critical factors. Results showed that mean temperature, percent of people below poverty, percent of adults with obesity, air pressure, population density, wind speed, longitude, and percent of uninsured people were the most significant attributes","625":"Recurring outbreaks of COVID-19 have posed enduring effects on global society, which calls for a predictor of pandemic waves using various data with early availability. Existing prediction models that forecast the first outbreak wave using mobility data may not be applicable to the multiwave prediction, because the evidence in the USA and Japan has shown that mobility patterns across different waves exhibit varying relationships with fluctuations in infection cases. Therefore, to predict the multiwave pandemic, we propose a Social Awareness-Based Graph Neural Network (SAB-GNN) that considers the decay of symptom-related web search frequency to capture the changes in public awareness across multiple waves. SAB-GNN combines GNN and LSTM to model the complex relationships among urban districts, inter-district mobility patterns, web search history, and future COVID-19 infections. We train our model to predict future pandemic outbreaks in the Tokyo area using its mobility and web search data from April 2020 to May 2021 across four pandemic waves collected by _ANONYMOUS_COMPANY_ under strict privacy protection rules. Results show our model outperforms other baselines including ST-GNN and MPNN+LSTM. Though our model is not computationally expensive (only 3 layers and 10 hidden neurons), the proposed model enables public agencies to anticipate and prepare for future pandemic outbreaks.","626":"E-learning in higher education is exponentially increased during the past decade due to its inevitable benefits in critical situations like natural disasters, and pandemic. The reliable, fair, and seamless execution of online exams in E-learning is highly significant. Particularly, online exams are conducted on E-learning platforms without the physical presence of students and instructors at the same place. This poses several issues like integrity and security during online exams. To address such issues, researchers frequently proposed different techniques and tools. However, a study summarizing and analyzing latest developments, particularly in the area of online examination, is hard to find in the literature. In this article, an SLR for online examination is performed to select and analyze 53 studies published during the last five years. Subsequently, five leading online exams features targeted in the selected studies are identified and underlying development approaches for the implementation of online exams solutions are explored. Furthermore, 16 important techniques and 11 datasets are presented. In addition, 21 online exams tools proposed in the selected studies are identified. Additionally, 25 leading existing tools used in the selected studies are also presented. Finally, the participation of countries in online exam research is investigated. Key factors for the global adoption of online exams are identified and investigated. This facilitates the selection of right online exam system for a particular country on the basis of existing E-learning infrastructure and overall cost. To conclude, the findings of this article provide a solid platform for the researchers and practitioners of the domain to select appropriate features along with underlying development approaches, tools and techniques for the implementation of a particular online exams solution as per given requirements.","627":"The technology development for point-of-care tests (POCTs) targeting respiratory diseases has witnessed a growing demand in the recent past. Investigating the presence of acoustic biomarkers in modalities such as cough, breathing and speech sounds, and using them for building POCTs can offer fast, contactless and inexpensive testing. In view of this, over the past year, we launched the ``Coswara'' project to collect cough, breathing and speech sound recordings via worldwide crowdsourcing. With this data, a call for development of diagnostic tools was announced in the Interspeech 2021 as a special session titled ``Diagnostics of COVID-19 using Acoustics (DiCOVA) Challenge''. The goal was to bring together researchers and practitioners interested in developing acoustics-based COVID-19 POCTs by enabling them to work on the same set of development and test datasets. As part of the challenge, datasets with breathing, cough, and speech sound samples from COVID-19 and non-COVID-19 individuals were released to the participants. The challenge consisted of two tracks. The Track-1 focused only on cough sounds, and participants competed in a leaderboard setting. In Track-2, breathing and speech samples were provided for the participants, without a competitive leaderboard. The challenge attracted 85 plus registrations with 29 final submissions for Track-1. This paper describes the challenge (datasets, tasks, baseline system), and presents a focused summary of the various systems submitted by the participating teams. An analysis of the results from the top four teams showed that a fusion of the scores from these teams yields an area-under-the-curve of 95.1% on the blind test data. By summarizing the lessons learned, we foresee the challenge overview in this paper to help accelerate technology for acoustic-based POCTs.","628":"E-health technologies have the potential to provide scalable and accessible interventions for youth mental health. As part of a developing an ecosystem of e-screening and e-therapy tools for New Zealand young people, a dialog agent, Headstrong, has been designed to promote resilience with methods grounded in cognitive behavioral therapy and positive psychology. This paper describes the architecture underlying the chatbot. The architecture supports a range of over 20 activities delivered in a 4-week program by relatable personas. The architecture provides a visual authoring interface to its content management system. In addition to supporting the original adolescent resilience chatbot, the architecture has been reused to create a 3-week 'stress-detox' intervention for undergraduates, and subsequently for a chatbot to support young people with the impacts of the COVID-19 pandemic, with all three systems having been used in field trials. The Headstrong architecture illustrates the feasibility of creating a domain-focused authoring environment in the context of e-therapy that supports non-technical expert input and rapid deployment.","629":"The effectiveness of vaccination highly depends on the choice of individuals to vaccinate, even if the same number of individuals are vaccinated. Vaccinating individuals with high centrality measures such as betweenness centrality (BC) and eigenvector centrality (EC) are effective in containing epidemics. However, in many real-world cases, each individual has distinct epidemic characteristics such as contagion, recovery, fatality rate, efficacy, and probability of severe reaction to a vaccine. Moreover, the relative effectiveness of vaccination strategies depends on the number of available vaccine shots. Centrality-based strategies cannot take the variability of epidemic characteristics or the availability of vaccines into account. Here, we propose a framework for vaccination strategy based on graph neural network ansatz (GNNA) and microscopic Markov chain approach (MMCA). In this framework, we can formulate an effective vaccination strategy that considers the properties of each node, and tailor the vaccination strategy according to the availability of vaccines. Our approach is highly scalable to large networks. We validate the method in many real-world networks for network dismantling, the susceptible-infected-susceptible (SIS) model with homogeneous and heterogeneous contagion\/recovery rates, and the susceptible-infected-recovered-dead (SIRD) model. We also extend our method to edge immunization strategy, which represents non-pharmaceutical containment measures such as travel regulations and social distancing.","630":"Predicting an accurate expected number of future COVID-19 cases is essential to properly evaluate the effectiveness of any treatment or preventive measure. This study aimed to identify the most appropriate mathematical model to prospectively predict the expected number of cases without any intervention. The total number of cases for the COVID-19 epidemic in 28 countries was analyzed and fitted to several simple rate models including the logistic, Gompertz, quadratic, simple square, and simple exponential growth models. The resulting model parameters were used to extrapolate predictions for more recent data. While the Gompertz growth models (mean R2 = 0.998) best fitted the current data, uncertainties in the eventual case limit made future predictions with logistic models prone to errors. Of the other models, the quadratic rate model (mean R2 = 0.992) fitted the current data best for 25 (89 %) countries as determined by R2 values. The simple square and quadratic models accurately predicted the number of future total cases 37 and 36 days in advance respectively, compared to only 15 days for the simple exponential model. The simple exponential model significantly overpredicted the total number of future cases while the quadratic and simple square models did not. These results demonstrated that accurate future predictions of the case load in a given country can be made significantly in advance without the need for complicated models of population behavior and generate a reliable assessment of the efficacy of current prescriptive measures against disease spread.","631":"Through lockdowns and other severe changes to daily life, almost everyone is affected by the COVID-19 pandemic. Scientists and medical doctors are - among others - mainly interested in researching, monitoring, and improving physical and mental health of the general population. Mobile health apps (mHealth), and apps conducting ecological momentary assessments (EMA) respectively, can help in this context. However, developing such mobile applications poses many challenges like costly software development efforts, strict privacy rules, compliance with ethical guidelines, local laws, and regulations. In this paper, we present TrackYourHealth (TYH), a highly configurable, generic, and modular mobile data collection and EMA platform, which enabled us to develop and release two mobile multi-platform applications related to COVID-19 in just a few weeks. We present TYH and highlight specific challenges researchers and developers of similar apps may also face, especially when developing apps related to the medical field.","632":"The light curve of the microlensing event KMT-2021-BLG-0912 exhibits a very short anomaly relative to a single-lens single-source form. We investigate the light curve for the purpose of identifying the origin of the anomaly. We model the light curve under various interpretations. From this, we find four solutions, in which three solutions are found under the assumption that the lens is composed of two masses (2L1S models), and the other solution is found under the assumption that the source is comprised of a binary-star system (1L2S model). The 1L2S model is ruled out based on the contradiction that the faint source companion is bigger than its primary, and one of the 2L1S solutions is excluded from the combination of the relatively worse fit, blending constraint, and lower overall probability, leaving two surviving solutions with the planet\/host mass ratios of $q\\sim 2.8\\times 10^{-5}$ and $\\sim 1.1\\times 10^{-5}$. A subtle central deviation supports the possibility of a tertiary lens component, either a binary companion to the host with a very large or small separation or a second planet lying near the Einstein ring, but it is difficult to claim a secure detection due to the marginal fit improvement, lack of consistency among different data sets, and difficulty in uniquely specifying the nature of the tertiary component. With the observables of the event, it is estimated that the masses of the planet and host are $\\sim (6.9~M_\\oplus, 0.75~M_\\odot)$ according to one solution and $\\sim (2.8~M_\\oplus, 0.80~M_\\odot)$ according to the other solution, indicating that the planet is a super Earth around a K-type star, regardless of the solution.","633":"Infectious diseases are caused by pathogenic microorganisms, such as bacteria, viruses, parasites or fungi, which can be spread, directly or indirectly, from one person to another. Infectious diseases pose a serious threat to human health, especially COVID-19 that has became a serious worldwide health concern since the end of 2019. Contact tracing is the process of identifying, assessing, and managing people who have been exposed to a disease to prevent its onward transmission. Contact tracing can help us better understand the transmission link of the virus, whereby better interrupting its transmission. Given the worldwide pandemic of COVID-19, contact tracing has become one of the most critical measures to effectively curb the spread of the virus. This paper presents a comprehensive survey on contact tracing, with a detailed coverage of the recent advancements the models, digital technologies, protocols and issues involved in contact tracing. The current challenges as well as future directions of contact tracing technologies are also presented.","634":"The 3D block matching (BM3D) method is among the state-of-art methods for denoising images corrupted with additive white Gaussian noise. With the help of a novel inter-frame connectivity strategy, we propose an extension of the BM3D method for the scenario where we have multiple images of the same scene. Our proposed extension outperforms all the existing trivial and non-trivial extensions of patch-based denoising methods for multi-frame images. We can achieve a quality difference of as high as 28% over the next best method without using any additional parameters. Our method can also be easily generalised to other similar existing patch-based methods.","635":"Customers' emotions play a vital role in the service industry. The better frontline personnel understand the customer, the better the service they can provide. As human emotions generate certain (unintentional) bodily reactions, such as increase in heart rate, sweating, dilation, blushing and paling, which are measurable, artificial intelligence (AI) technologies can interpret these signals. Great progress has been made in recent years to automatically detect basic emotions like joy, anger etc. Complex emotions, consisting of multiple interdependent basic emotions, are more difficult to identify. One complex emotion which is of great interest to the service industry is difficult to detect: whether a customer is telling the truth or just a story. This research presents an AI-method for capturing and sensing emotional data. With an accuracy of around 98 %, the best trained model was able to detect whether a participant of a debating challenge was arguing for or against her\/his conviction, using speech analysis. The data set was collected in an experimental setting with 40 participants. The findings are applicable to a wide range of service processes and specifically useful for all customer interactions that take place via telephone. The algorithm presented can be applied in any situation where it is helpful for the agent to know whether a customer is speaking to her\/his conviction. This could, for example, lead to a reduction in doubtful insurance claims, or untruthful statements in job interviews. This would not only reduce operational losses for service companies, but also encourage customers to be more truthful.","636":"Deep generative models are attracting great attention for molecular design with desired properties. Most existing models generate molecules by sequentially adding atoms. This often renders generated molecules with less correlation with target properties and low synthetic accessibility. Molecular fragments such as functional groups are more closely related to molecular properties and synthetic accessibility than atoms. Here, we propose a fragment-based molecular generative model which designs new molecules with target properties by sequentially adding molecular fragments to any given starting molecule. A key feature of our model is a high generalization ability in terms of property control and fragment types. The former becomes possible by learning the contribution of individual fragments to the target properties in an auto-regressive manner. For the latter, we used a deep neural network that predicts the bonding probability of two molecules from the embedding vectors of the two molecules as input. The high synthetic accessibility of the generated molecules is implicitly considered while preparing the fragment library with the BRICS decomposition method. We show that the model can generate molecules with the simultaneous control of multiple target properties at a high success rate. It also works equally well with unseen fragments even in the property range where the training data is rare, verifying the high generalization ability. As a practical application, we demonstrated that the model can generate potential inhibitors with high binding affinities against the 3CL protease of SARS-COV-2 in terms of docking score.","637":"We propose a new nonlinear point-coupling parameterized interaction, PC-L3, for the relativistic Hartree-Bogoliubov framework with a separable pairing force by fitting to observables of 65 selected spherical nuclei, including the binding energies and charge radii. Overall, from comparing with the experimental data, the implementation of PC-L3 in relativistic Hartree-Bogoliubov successfully yields the lowest root-mean-square deviation, 1.251 MeV, among currently and commonly used point-coupling interactions. Meanwhile, PC-L3 is capable of estimating the saturation properties of the symmetric nuclear matter and bulk properties of the finite nuclei and of appropriately predicting the isospin and mass dependence of binding energy, e.g., the isotopes of $Z$=20, 50, and 82, and isotones of $N$=20, 50, and 82. The comparison of the estimated binding energies for $\\sim$5500 neutron-rich nuclei based on PC-L3 and other point-coupling interactions is also presented.","638":"This paper studies a networked bivirus model, in which two competing viruses spread across a network of interconnected populations; each node represents a population with a large number of individuals. The viruses may spread through possibly different network structures, and an individual cannot be simultaneously infected with both viruses. Focusing on convergence and equilibria analysis, a number of new results are provided. First, we show that for networks with generic system parameters, there exist a finite number of equilibria. Exploiting monotone systems theory, we further prove that for bivirus networks with generic system parameters, then convergence to an equilibrium occurs for all initial conditions, except possibly for a set of measure zero. Given the network structure of one virus, a method is presented to construct an infinite family of network structures for the other virus that results in an infinite number of equilibria in which both viruses coexist. Necessary and sufficient conditions are derived for the local stability\/instability of boundary equilibria, in which one virus is present and the other is extinct. A sufficient condition for a boundary equilibrium to be almost globally stable is presented. Then, we show how to use monotone systems theory to generate conclusions on the ordering of stable and unstable equilibria, and in some instances identify the number of equilibria via rapid simulation testing. Last, we provide an analytical method for computing equilibria in networks with only two nodes, and show that it is possible for a bivirus network to have an unstable coexistence equilibrium and two locally stable boundary equilibria.","639":"Smartphone apps for exposure notification and contact tracing have been shown to be effective in controlling the COVID-19 pandemic. However, Bluetooth Low Energy tokens similar to those broadcast by existing apps can still be picked up far away from the transmitting device. In this paper, we present a new class of methods for detecting whether or not two Wi-Fi-enabled devices are in immediate physical proximity, i.e. 2 or fewer meters apart, as established by the U.S. Centers for Disease Control and Prevention (CDC). Our goal is to enhance the accuracy of smartphone-based exposure notification and contact tracing systems. We present a set of binary machine learning classifiers that take as input pairs of Wi-Fi RSSI fingerprints. We empirically verify that a single classifier cannot generalize well to a range of different environments with vastly different numbers of detectable Wi-Fi Access Points (APs). However, specialized classifiers, tailored to situations where the number of detectable APs falls within a certain range, are able to detect immediate physical proximity significantly more accurately. As such, we design three classifiers for situations with low, medium, and high numbers of detectable APs. These classifiers distinguish between pairs of RSSI fingerprints recorded 2 or fewer meters apart and pairs recorded further apart but still in Bluetooth range. We characterize their balanced accuracy for this task to be between 66.8% and 77.8%.","640":"In 2020, the SARS-CoV-2 virus causes a global pandemic of the new human coronavirus disease COVID-19. This pathogen primarily infects the respiratory system of the afflicted, usually resulting in pneumonia and in a severe case of acute respiratory distress syndrome. These disease developments result in the formation of different pathological structures in the lungs, similar to those observed in other viral pneumonias that can be detected by the use of chest X-rays. For this reason, the detection and analysis of the pulmonary regions, the main focus of affection of COVID-19, becomes a crucial part of both clinical and automatic diagnosis processes. Due to the overload of the health services, portable X-ray devices are widely used, representing an alternative to fixed devices to reduce the risk of cross-contamination. However, these devices entail different complications as the image quality that, together with the subjectivity of the clinician, make the diagnostic process more difficult. In this work, we developed a novel fully automatic methodology specially designed for the identification of these lung regions in X-ray images of low quality as those from portable devices. To do so, we took advantage of a large dataset from magnetic resonance imaging of a similar pathology and performed two stages of transfer learning to obtain a robust methodology with a low number of images from portable X-ray devices. This way, our methodology obtained a satisfactory accuracy of $0.9761 \\pm 0.0100$ for patients with COVID-19, $0.9801 \\pm 0.0104$ for normal patients and $0.9769 \\pm 0.0111$ for patients with pulmonary diseases with similar characteristics as COVID-19 (such as pneumonia) but not genuine COVID-19.","641":"The literature for count modeling provides useful tools to conduct causal inference when outcomes take non-negative integer values. Applied to the potential outcomes framework, we link the Bayesian causal inference literature to statistical models for count data. We discuss the general architectural considerations for constructing the predictive posterior of the missing potential outcomes. Special considerations for estimating average treatment effects are discussed, some generalizing certain relationships and some not yet encountered in the causal inference literature.","642":"COVID-19 has forced quarantine measures in several countries across the world. These measures have proven to be effective in significantly reducing the prevalence of the virus. To date, no effective treatment or vaccine is available. In the effort of preserving both public health as well as the economical and social textures, France and Italy governments have partially released lockdown measures. Here we extrapolate the long-term behavior of the epidemics in both countries using a Susceptible-Exposed-Infected-Recovered (SEIR) model where parameters are stochastically perturbed to handle the uncertainty in the estimates of COVID-19 prevalence. Our results suggest that uncertainties in both parameters and initial conditions rapidly propagate in the model and can result in different outcomes of the epidemics leading or not to a second wave of infections. Using actual knowledge, asymptotic estimates of COVID-19 prevalence can fluctuate of order of ten millions units in both countries.","643":"In perturbation theory, the spectral densities of two-point functions develop non-integrable threshold singularities at higher orders. In QCD, such singularities emerge when calculating the diagrams in terms of the pole quark mass, and they become stronger when one rearranges the perturbative expansion in terms of the running quark mass. In this letter we discuss the proper way to handle such singularities.","644":"We propose a novel pathology-sensitive deep learning model (PS-DeVCEM) for frame-level anomaly detection and multi-label classification of different colon diseases in video capsule endoscopy (VCE) data. Our proposed model is capable of coping with the key challenge of colon apparent heterogeneity caused by several types of diseases. Our model is driven by attention-based deep multiple instance learning and is trained end-to-end on weakly labeled data using video labels instead of detailed frame-by-frame annotation. The spatial and temporal features are obtained through ResNet50 and residual Long short-term memory (residual LSTM) blocks, respectively. Additionally, the learned temporal attention module provides the importance of each frame to the final label prediction. Moreover, we developed a self-supervision method to maximize the distance between classes of pathologies. We demonstrate through qualitative and quantitative experiments that our proposed weakly supervised learning model gives superior precision and F1-score reaching, 61.6% and 55.1%, as compared to three state-of-the-art video analysis methods respectively. We also show our model's ability to temporally localize frames with pathologies, without frame annotation information during training. Furthermore, we collected and annotated the first and largest VCE dataset with only video labels. The dataset contains 455 short video segments with 28,304 frames and 14 classes of colorectal diseases and artifacts. Dataset and code supporting this publication will be made available on our home page.","645":"Astronomy plays a major role in the scientific landscape of Namibia. Because of its excellent sky conditions, Namibia is home to ground-based observatories like the High Energy Spectroscopic System (H.E.S.S.), in operation since 2002. Located near the Gamsberg mountain, H.E.S.S. performs groundbreaking science by detecting very-high-energy gamma rays from astronomical objects. The fascinating stories behind many of them are featured regularly in the ``Source of the Month'', a blog-like format intended for the general public with more than 170 features to date. In addition to other online communication via social media, H.E.S.S. outreach activities have been covered locally, e.g. through `open days' and guided tours on the H.E.S.S. site itself. An overview of the H.E.S.S. outreach activities are presented in this contribution, along with discussions relating to the current landscape of astronomy outreach and education in Namibia. There has also been significant activity in the country in recent months, whereby astronomy is being used to further sustainable development via human capacity-building. Finally, as we take into account the future prospects of radio astronomy in the country, momentum for a wider range of astrophysics research is clearly building -- this presents a great opportunity for the astronomy community to come together to capitalise on this movement and support astronomy outreach, with the overarching aim to advance sustainable development in Namibia.","646":"Image generative models can learn the distributions of the training data and consequently generate examples by sampling from these distributions. However, when the training dataset is corrupted with outliers, generative models will likely produce examples that are also similar to the outliers. In fact, a small portion of outliers may induce state-of-the-art generative models, such as Vector Quantized-Variational AutoEncoder (VQ-VAE), to learn a significant mode from the outliers. To mitigate this problem, we propose a robust generative model based on VQ-VAE, which we name Robust VQ-VAE (RVQ-VAE). In order to achieve robustness, RVQ-VAE uses two separate codebooks for the inliers and outliers. To ensure the codebooks embed the correct components, we iteratively update the sets of inliers and outliers during each training epoch. To ensure that the encoded data points are matched to the correct codebooks, we quantize using a weighted Euclidean distance, whose weights are determined by directional variances of the codebooks. Both codebooks, together with the encoder and decoder, are trained jointly according to the reconstruction loss and the quantization loss. We experimentally demonstrate that RVQ-VAE is able to generate examples from inliers even if a large portion of the training data points are corrupted.","647":"Infectious disease outbreak has a significant impact on morbidity, mortality and can cause economic instability of many countries. As global trade is growing, goods and individuals are expected to travel across the border, an infected epidemic area carrier can pose a great danger to his hostile. If a disease outbreak is recognized promptly, then commercial products and travelers (traders\/visitors) will be effectively vaccinated, and therefore the disease stopped. Early detection of outbreaks plays an important role here, and beware of the rapid implementation of control measures by citizens, public health organizations, and government. Many indicators have valuable information, such as online news sources (RSS) and social media sources (Twitter, Facebook) that can be used, but are unstructured and bulky, to extract information about disease outbreaks. Few early warning outbreak systems exist with some limitation of linguistic (Urdu) and covering areas (Pakistan). In Pakistan, few channels are published the outbreak news in Urdu or English. The aim is to procure information from Pakistan's English and Urdu news channels and then investigate process, integrate, and visualize the disease epidemic. Urdu ontology is not existed before to match extracted diseases, so we also build that ontology of disease.","648":"We demonstrate novel instrumentation for spontaneous Raman spectroscopy in biofluids, enabling development of a portable, automated, reliable diagnostics technique requiring minimal operator expertise to quantify disease markers. Label-free Raman analysis of biofluids at physiologically-relevant sensitivities is achieved using a microfluidic-embedded liquid-core-waveguide augmented with a unique circulation approach: thermal damage and spectrum variance is minimized, eliminating conventional limits on integration time for excellent signal-to-noise ratio and temporal stability. Machine-learning then optimizes spectrum processing, yielding quantitative results independent of end-user proficiency. Sub-mM accuracy is achieved in solutions of both high and low turbidity, surpassing the sensitivity of previous techniques for analytes with a small scattering cross-section, such as glucose. We attain a new record for label-free glucose measurements in an artificial whole-blood, achieving an accuracy up to 0.14 mM, well-exceeding the 0.78 mM accuracy required for diabetic monitoring, establishing our technique's potential to significantly facilitate portable Raman for complex biofluid analysis.","649":"In the present study, we propose novel sequence-to-sequence pre-training objectives for low-resource machine translation (NMT): Japanese-specific sequence to sequence (JASS) for language pairs involving Japanese as the source or target language, and English-specific sequence to sequence (ENSS) for language pairs involving English. JASS focuses on masking and reordering Japanese linguistic units known as bunsetsu, whereas ENSS is proposed based on phrase structure masking and reordering tasks. Experiments on ASPEC Japanese--English&Japanese--Chinese, Wikipedia Japanese--Chinese, News English--Korean corpora demonstrate that JASS and ENSS outperform MASS and other existing language-agnostic pre-training methods by up to +2.9 BLEU points for the Japanese--English tasks, up to +7.0 BLEU points for the Japanese--Chinese tasks and up to +1.3 BLEU points for English--Korean tasks. Empirical analysis, which focuses on the relationship between individual parts in JASS and ENSS, reveals the complementary nature of the subtasks of JASS and ENSS. Adequacy evaluation using LASER, human evaluation, and case studies reveals that our proposed methods significantly outperform pre-training methods without injected linguistic knowledge and they have a larger positive impact on the adequacy as compared to the fluency. We release codes here: https:\/\/github.com\/Mao-KU\/JASS\/tree\/master\/linguistically-driven-pretraining.","650":"We continue a series of papers devoted to construction of semi-analytic solutions for barrier options. These options are written on underlying following some simple one-factor diffusion model, but all the parameters of the model as well as the barriers are time-dependent. We managed to show that these solutions are systematically more efficient for pricing and calibration than, eg., the corresponding finite-difference solvers. In this paper we extend this technique to pricing double barrier options and present two approaches to solving it: the General Integral transform method and the Heat Potential method. Our results confirm that for double barrier options these semi-analytic techniques are also more efficient than the traditional numerical methods used to solve this type of problems.","651":"Due to the wide-ranging travel restrictions and lockdowns applied to limit the diffusion of the SARS-CoV2 virus, the coronavirus disease of 2019 (COVID-19) pandemic has had an immediate and significant effect on human mobility at the global, national, and local levels. At the local level, bike-sharing played a significant role in urban transport during the pandemic since riders could travel outdoors with reduced infection risk. However, based on different data resources, this non-motorized mode of transportation was still negatively affected by the pandemic (i.e., relative reduction in ridership). This study has two objectives: 1) to investigate the impact of the COVID-19 pandemic on the numbers and duration of trips conducted through a bike-sharing system - the Capital Bikeshare in Washington, DC, USA; and 2) to explore whether land use and household income in the nation's capital influence the spatial variation of ridership during the pandemic. Towards realizing these objectives, this research looks at the relationship between bike sharing and COVID-19 transmission as a two-directional relationship rather than a one- directional causal relationship. Accordingly, this study models i) the impact of COVID-19 infection numbers and rates on the use of the Capital Bikeshare system and ii) the risk of COVID-19 transmission among individual bike-sharing users. In other words, we examine i) the cyclist's behavior as a function of the COVID-19 transmission evolution in an urban environment and ii) the possible relationship between the bike share usage and the COVID-19 transmission through adopting a probabilistic contagion model. The findings show the risk of using a bike-sharing system during the pandemic and whether bike sharing remains a healthier alternative mode of transportation in terms of infection risk.","652":"COVID-19 pandemic elucidated that knowledge systems will be instrumental in cases where accurate information needs to be communicated to a substantial group of people with different backgrounds and technological resources. However, several challenges and obstacles hold back the wide adoption of virtual assistants by public health departments and organizations. This paper presents the Instant Expert, an open-source semantic web framework to build and integrate voice-enabled smart assistants (i.e. chatbots) for any web platform regardless of the underlying domain and technology. The component allows non-technical domain experts to effortlessly incorporate an operational assistant with voice recognition capability into their websites. Instant Expert is capable of automatically parsing, processing, and modeling Frequently Asked Questions pages as an information resource as well as communicating with an external knowledge engine for ontology-powered inference and dynamic data utilization. The presented framework utilizes advanced web technologies to ensure reusability and reliability, and an inference engine for natural language understanding powered by deep learning and heuristic algorithms. A use case for creating an informatory assistant for COVID-19 based on the Centers for Disease Control and Prevention (CDC) data is presented to demonstrate the framework's usage and benefits.","653":"Early detection of COVID-19 is an ongoing area of research that can help with triage, monitoring and general health assessment of potential patients and may reduce operational strain on hospitals that cope with the coronavirus pandemic. Different machine learning techniques have been used in the literature to detect coronavirus using routine clinical data (blood tests, and vital signs). Data breaches and information leakage when using these models can bring reputational damage and cause legal issues for hospitals. In spite of this, protecting healthcare models against leakage of potentially sensitive information is an understudied research area. In this work, we examine two machine learning approaches, intended to predict a patient's COVID-19 status using routinely collected and readily available clinical data. We employ adversarial training to explore robust deep learning architectures that protect attributes related to demographic information about the patients. The two models we examine in this work are intended to preserve sensitive information against adversarial attacks and information leakage. In a series of experiments using datasets from the Oxford University Hospitals, Bedfordshire Hospitals NHS Foundation Trust, University Hospitals Birmingham NHS Foundation Trust, and Portsmouth Hospitals University NHS Trust we train and test two neural networks that predict PCR test results using information from basic laboratory blood tests, and vital signs performed on a patients' arrival to hospital. We assess the level of privacy each one of the models can provide and show the efficacy and robustness of our proposed architectures against a comparable baseline. One of our main contributions is that we specifically target the development of effective COVID-19 detection models with built-in mechanisms in order to selectively protect sensitive attributes against adversarial attacks.","654":"The classic classification scheme for Active Galactic Nuclei (AGNs) was recently challenged by the discovery of the so-called changing-state (changing-look) AGNs (CSAGNs). The physical mechanism behind this phenomenon is still a matter of open debate and the samples are too small and of serendipitous nature to provide robust answers. In order to tackle this problem, we need to design methods that are able to detect AGN right in the act of changing-state. Here we present an anomaly detection (AD) technique designed to identify AGN light curves with anomalous behaviors in massive datasets. The main aim of this technique is to identify CSAGN at different stages of the transition, but it can also be used for more general purposes, such as cleaning massive datasets for AGN variability analyses. We used light curves from the Zwicky Transient Facility data release 5 (ZTF DR5), containing a sample of 230,451 AGNs of different classes. The ZTF DR5 light curves were modeled with a Variational Recurrent Autoencoder (VRAE) architecture, that allowed us to obtain a set of attributes from the VRAE latent space that describes the general behaviour of our sample. These attributes were then used as features for an Isolation Forest (IF) algorithm, that is an anomaly detector for a\"one class\"kind of problem. We used the VRAE reconstruction errors and the IF anomaly score to select a sample of 8,809 anomalies. These anomalies are dominated by bogus candidates, but we were able to identify 75 promising CSAGN candidates.","655":"This paper presents a three-component work. The first component sets the overall theoretical context which lies in the argument that the increasing complexity of the world has made it more difficult for International Relations (IR) to succeed both in theory and practice. The era of information and the events of the 21st century have moved IR theory and practice away from real policy making (Walt, 2016) and have made it entrenched in opinions and political theories difficult to prove. At the same time, the rise of the\"Fourth Paradigm - Data Intensive Scientific Discovery\"(Hey et al., 2009) and the strengthening of data science offer an alternative:\"Computational International Relations\"(Unver, 2018). The use of traditional and contemporary data-centered tools can help to update the field of IR by making it more relevant to reality (Koutsoupias, Mikelis, 2020). The\"wedding\"between Data Science and IR is no panacea though. Changes are required both in perceptions and practices. Above all, for Data Science to enter IR, the relevant data must exist. This is where the second component comes into play. I mine the CIA World Factbook which provides cross-domain data covering all countries of the world. Then, I execute various data preprocessing tasks peaking in simple machine learning which imputes missing values providing with a more complete dataset. Lastly, the third component presents various projects making use of the produced dataset in order to illustrate the relevance of Data Science to IR through practical examples. Then, ideas regarding the future development of this project are discussed in order to optimize it and ensure continuity. Overall, I hope to contribute to the\"fourth paradigm\"discussion in IR by providing practical examples while providing at the same time the fuel for future research.","656":"Objective: Neural network de-identification studies have focused on individual datasets. These studies assume the availability of a sufficient amount of human-annotated data to train models that can generalize to corresponding test data. In real-world situations, however, researchers often have limited or no in-house training data. Existing systems and external data can help jump-start de-identification on in-house data; however, the most efficient way of utilizing existing systems and external data is unclear. This article investigates the transferability of a state-of-the-art neural clinical de-identification system, NeuroNER, across a variety of datasets, when it is modified architecturally for domain generalization and when it is trained strategically for domain transfer. Methods and Materials: We conducted a comparative study of the transferability of NeuroNER using four clinical note corpora with multiple note types from two institutions. We modified NeuroNER architecturally to integrate two types of domain generalization approaches. We evaluated each architecture using three training strategies. We measured: transferability from external sources; transferability across note types; the contribution of external source data when in-domain training data are available; and transferability across institutions. Results and Conclusions: Transferability from a single external source gave inconsistent results. Using additional external sources consistently yielded an F1-score of approximately 80%. Fine-tuning emerged as a dominant transfer strategy, with or without domain generalization. We also found that external sources were useful even in cases where in-domain training data were available. Transferability across institutions differed by note type and annotation label but resulted in improved performance.","657":"Pre-processing and Data Augmentation play an important role in Deep Convolutional Neural Networks (DCNN). Whereby several methods aim for standardization and augmentation of the dataset, we here propose a novel method aimed to feed DCNN with spherical space transformed input data that could better facilitate feature learning compared to standard Cartesian space images and volumes. In this work, the spherical coordinates transformation has been applied as a preprocessing method that, used in conjunction with normal MRI volumes, improves the accuracy of brain tumor segmentation and patient overall survival (OS) prediction on Brain Tumor Segmentation (BraTS) Challenge 2020 dataset. The LesionEncoder framework has been then applied to automatically extract features from DCNN models, achieving 0.586 accuracy of OS prediction on the validation data set, which is one of the best results according to BraTS 2020 leaderboard.","658":"Being low-level radiation exposure and less harmful to health, low-dose computed tomography (LDCT) has been widely adopted in the early screening of lung cancer and COVID-19. LDCT images inevitably suffer from the degradation problem caused by complex noises. It was reported that deep learning (DL)-based LDCT denoising methods using convolutional neural network (CNN) achieved impressive denoising performance. Although most existing DL-based methods (e.g., encoder-decoder framework) can implicitly utilize non-local and contextual information via downsampling operator and 3D CNN, the explicit multi-information (i.e., local, non-local, and contextual) integration may not be explored enough. To address this issue, we propose a novel graph convolutional network-based LDCT denoising model, namely GCN-MIF, to explicitly perform multi-information fusion for denoising purpose. Concretely, by constructing intra- and inter-slice graph, the graph convolutional network is introduced to leverage the non-local and contextual relationships among pixels. The traditional CNN is adopted for the extraction of local information. Finally, the proposed GCN-MIF model fuses all the extracted local, non-local, and contextual information. Extensive experiments show the effectiveness of our proposed GCN-MIF model by quantitative and visualized results. Furthermore, a double-blind reader study on a public clinical dataset is also performed to validate the usability of denoising results in terms of the structural fidelity, the noise suppression, and the overall score. Models and code are available at https:\/\/github.com\/tonyckc\/GCN-MIF_demo.","659":"The diurnal cycle CO$_2$ emissions from fossil fuel combustion and cement production reflect seasonality, weather conditions, working days, and more recently the impact of the COVID-19 pandemic. Here, for the first time we provide a daily CO$_2$ emission dataset for the whole year of 2020 calculated from inventory and near-real-time activity data (called Carbon Monitor project: https:\/\/carbonmonitor.org). It was previously suggested from preliminary estimates that did not cover the entire year of 2020 that the pandemics may have caused more than 8% annual decline of global CO$_2$ emissions. Here we show from detailed estimates of the full year data that the global reduction was only 5.4% (-1,901 MtCO$_2$, ). This decrease is 5 times larger than the annual emission drop at the peak of the 2008 Global Financial Crisis. However, global CO$_2$ emissions gradually recovered towards 2019 levels from late April with global partial re-opening. More importantly, global CO$_2$ emissions even increased slightly by +0.9% in December 2020 compared with 2019, indicating the trends of rebound of global emissions. Later waves of COVID-19 infections in late 2020 and corresponding lockdowns have caused further CO$_2$ emissions reductions particularly in western countries, but to a much smaller extent than the declines in the first wave. That even substantial world-wide lockdowns of activity led to a one-time decline in global CO$_2$ emissions of only 5.4% in one year highlights the significant challenges for climate change mitigation that we face in the post-COVID era. These declines are significant, but will be quickly overtaken with new emissions unless the COVID-19 crisis is utilized as a break-point with our fossil-fuel trajectory, notably through policies that make the COVID-19 recovery an opportunity to green national energy and development plans.","660":"Respiratory sound classification is an important tool for remote screening of respiratory-related diseases such as pneumonia, asthma, and COVID-19. To facilitate the interpretability of classification results, especially ones based on deep learning, many explanation methods have been proposed using prototypes. However, existing explanation techniques often assume that the data is non-biased and the prediction results can be explained by a set of prototypical examples. In this work, we develop a unified example-based explanation method for selecting both representative data (prototypes) and outliers (criticisms). In particular, we propose a novel application of adversarial attacks to generate an explanation spectrum of data instances via an iterative fast gradient sign method. Such unified explanation can avoid over-generalisation and bias by allowing human experts to assess the model mistakes case by case. We performed a wide range of quantitative and qualitative evaluations to show that our approach generates effective and understandable explanation and is robust with many deep learning models","661":"This article surveys the literature over the period 2003-2021 on heart-based biometric protocols. In particular, we focus on how the heart signal is transformed from a continuous wave to discrete values to be used afterwards in authentication protocols. We explain and classify the surveyed proposals according to three main parameters: i) the dataset they use for testing their results; ii) the delineation algorithms they use to extract the fiducial points, and; iii) the cryptographic tests they run (if any) to validate how random the extracted token is.","662":"We introduce Language-Informed Latent Actions (LILA), a framework for learning natural language interfaces in the context of human-robot collaboration. LILA falls under the shared autonomy paradigm: in addition to providing discrete language inputs, humans are given a low-dimensional controller $-$ e.g., a 2 degree-of-freedom (DoF) joystick that can move left\/right and up\/down $-$ for operating the robot. LILA learns to use language to modulate this controller, providing users with a language-informed control space: given an instruction like\"place the cereal bowl on the tray,\"LILA may learn a 2-DoF space where one dimension controls the distance from the robot's end-effector to the bowl, and the other dimension controls the robot's end-effector pose relative to the grasp point on the bowl. We evaluate LILA with real-world user studies, where users can provide a language instruction while operating a 7-DoF Franka Emika Panda Arm to complete a series of complex manipulation tasks. We show that LILA models are not only more sample efficient and performant than imitation learning and end-effector control baselines, but that they are also qualitatively preferred by users.","663":"We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. However, due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of corpora and evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the initial release for which we are organizing a shared task at our ACL 2021 Workshop and to which we invite the entire NLG community to participate.","664":"Neural Networks and related Deep Learning methods are currently at the leading edge of technologies used for classifying objects. However, they generally demand large amounts of time and data for model training; and their learned models can sometimes be difficult to interpret. In this paper, we re-introduce FastMapSVM, an interpretable Machine Learning framework for classifying complex objects. FastMapSVM combines the strengths of FastMap and Support-Vector Machines. FastMap is an efficient linear-time algorithm that maps complex objects to points in a Euclidean space, while preserving pairwise non-Euclidean distances between them. We demonstrate the efficiency and effectiveness of FastMapSVM in the context of classifying seismograms. We show that its performance, in terms of precision, recall, and accuracy, is comparable to that of other state-of-the-art methods. However, compared to other methods, FastMapSVM uses significantly smaller amounts of time and data for model training. It also provides a perspicuous visualization of the objects and the classification boundaries between them. We expect FastMapSVM to be viable for classification tasks in many other real-world domains.","665":"This report examines an unexpected but significant source of positive public health messaging during the COVID-19 pandemic -- K-pop fandoms. Leveraging more than 7 million tweets related to mask wearing and K-pop between March 2020 and March 2021, we analyzed the online spread of the hashtag \\#WearAMask amid anti-mask sentiments and public health misinformation. Analyses reveal the South Korean boyband BTS as the most significant driver of health discourse. Tweets from health agencies and prominent figures that mentioned K-pop generate 111 times more of online response compared to tweets that did not. These tweets also elicited a strong responses from South America, Southeast Asia, and rural States -- areas often neglected in Twitter-based messaging by mainstream social media campaigns. Our results suggest that public health institutions may leverage pre-existing audience markets to synergistically diffuse and target under-served communities both domestically and globally, especially during health crises such as COVID-19.","666":"We investigate what can be learned from translating numerical algorithms into neural networks. On the numerical side, we consider explicit, accelerated explicit, and implicit schemes for a general higher order nonlinear diffusion equation in 1D, as well as linear multigrid methods. On the neural network side, we identify corresponding concepts in terms of residual networks (ResNets), recurrent networks, and U-nets. These connections guarantee Euclidean stability of specific ResNets with a transposed convolution layer structure in each block. We present three numerical justifications for skip connections: as time discretisations in explicit schemes, as extrapolation mechanisms for accelerating those methods, and as recurrent connections in fixed point solvers for implicit schemes. Last but not least, we also motivate uncommon design choices such as nonmonotone activation functions. Our findings give a numerical perspective on the success of modern neural network architectures, and they provide design criteria for stable networks.","667":"Currently, many countries are facing the problems of aging population, serious imbalance of medical resources supply and demand, as well as uneven geographical distribution, resulting in a huge demand for remote e-health. Particularly, with invasions of COVID-19, the health of people and even social stability have been challenged unprecedentedly. To contribute to these urgent problems, this article proposes a general architecture of the remote e-health, where the city hospital provides the technical supports and services for remote hospitals. Meanwhile, 5G technologies supported telemedicine is introduced to satisfy the high-speed transmission of massive multimedia medical data, and further realize the sharing of medical resources. Moreover, to turn passivity into initiative to prevent COVID-19, a broad area epidemic prevention and control scheme is also investigated, especially for the remote areas. We discuss their principles and key features, and foresee the challenges, opportunities, and future research trends. Finally, a node value and content popularity based caching strategy is introduced to provide a preliminary solution of the massive data storage and low-latency transmission.","668":"Sonification, or encoding information in meaningful audio signatures, has several advantages in augmenting or replacing traditional visualization methods for human-in-the-loop decision-making. Standard sonification methods reported in the literature involve either (i) using only a subset of the variables, or (ii) first solving a learning task on the data and then mapping the output to an audio waveform, which is utilized by the end-user to make a decision. This paper presents a novel framework for sonifying high-dimensional data using a complex growth transform dynamical system model where both the learning (or, more generally, optimization) and the sonification processes are integrated together. Our algorithm takes as input the data and optimization parameters underlying the learning or prediction task and combines it with the psychoacoustic parameters defined by the user. As a result, the proposed framework outputs binaural audio signatures that not only encode some statistical properties of the high-dimensional data but also reveal the underlying complexity of the optimization\/learning process. Along with extensive experiments using synthetic datasets, we demonstrate the framework on sonifying Electro-encephalogram (EEG) data with the potential for detecting epileptic seizures in pediatric patients.","669":"Most research on novel techniques for 3D Medical Image Segmentation (MIS) is currently done using Deep Learning with GPU accelerators. The principal challenge of such technique is that a single input can easily cope computing resources, and require prohibitive amounts of time to be processed. Distribution of deep learning and scalability over computing devices is an actual need for progressing on such research field. Conventional distribution of neural networks consist in data parallelism, where data is scattered over resources (e.g., GPUs) to parallelize the training of the model. However, experiment parallelism is also an option, where different training processes are parallelized across resources. While the first option is much more common on 3D image segmentation, the second provides a pipeline design with less dependence among parallelized processes, allowing overhead reduction and more potential scalability. In this work we present a design for distributed deep learning training pipelines, focusing on multi-node and multi-GPU environments, where the two different distribution approaches are deployed and benchmarked. We take as proof of concept the 3D U-Net architecture, using the MSD Brain Tumor Segmentation dataset, a state-of-art problem in medical image segmentation with high computing and space requirements. Using the BSC MareNostrum supercomputer as benchmarking environment, we use TensorFlow and Ray as neural network training and experiment distribution platforms. We evaluate the experiment speed-up, showing the potential for scaling out on GPUs and nodes. Also comparing the different parallelism techniques, showing how experiment distribution leverages better such resources through scaling. Finally, we provide the implementation of the design open to the community, and the non-trivial steps and methodology for adapting and deploying a MIS case as the here presented.","670":"Temporal and causal relations play an important role in determining the dependencies between events. Classifying the temporal and causal relations between events has many applications, such as generating event timelines, event summarization, textual entailment and question answering. Temporal and causal relations are closely related and influence each other. So we propose a joint model that incorporates both temporal and causal features to perform causal relation classification. We use the syntactic structure of the text for identifying temporal and causal relations between two events from the text. We extract parts-of-speech tag sequence, dependency tag sequence and word sequence from the text. We propose an LSTM based model for temporal and causal relation classification that captures the interrelations between the three encoded features. Evaluation of our model on four popular datasets yields promising results for temporal and causal relation classification.","671":"In the Binary Networked Public Goods game, every player needs to decide if she participates in a public project whose utility is shared equally by the community. We study the problem of deciding if there exists a pure strategy Nash equilibrium (PSNE) in such games. The problem is already known to be NP-complete. We provide fine-grained analysis of this problem under the lens of parameterized complexity theory. We consider various natural graph parameters and show either W[1]-hardness or exhibit an FPT algorithm. We finally exhibit some special graph classes, for example path, cycle, bi-clique, complete graph, etc., which always have a PSNE if the utility function of the players are fully homogeneous.","672":"We propose a simple mathematical model to describe the evolution of violent crimes. For such purpose, we built a model based on ordinary differential equations that take into account the number of violent crimes and the number of legal and illegal guns. The dynamics is governed by probabilities, modeling for example the police action, the risk perception regarding crimes that leads to increase of ownership of legal guns, and so on. Our analytical and numerical results show that, in addition to the rise of criminality due to the presence of illegal guns, the increase of legal guns leads to a fast increase of violent crimes, suggesting that the access of firearms by civilians is not a good option regarding the control of crimes.","673":"Global warming leads the world to think of a different way of transportation: avoiding internal combustion engines and electrifying the transportation sector. With a high penetration of electric vehicle (EV) charging stations on an existing power distribution network, the impact may be consistent. The loads of the fast-charging stations would potentially result in increased peak load demand, reduced reserve margins, voltage instability, and reliability problems. The degrading performance of the power system due to the negative impact of the EV charging stations can even lead to penalties to be paid by the distribution system operator (DSO). This paper: i) investigates the impact of the \\ac{ev} charging station on the distribution network for what concerns voltage drop on MV feeders and loading of transformers in primary substations, and ii) proposes a mitigation mechanism. A realistic typical Italian grid has been used to assess the impact of EV charging stations and to validate the mitigation mechanism.","674":"Many studies have shown the benefits of introducing open-source projects into teaching Software Engineering (SE) courses. However, there are several limitations of existing studies that limit the wide adaptation of open-source projects in a classroom setting, including (1) the selected project is limited to one particular project, (2) most studies only investigated on its effect on teaching a specific SE concept, and (3) students may make mistakes in their contribution which leads to poor quality code. Meanwhile, software companies have successfully launched programs like Google Summer of Code (GSoC) and FindBugs\"fixit\"to contribute to open-source projects. Inspired by the success of these programs, we propose GitHub-OSS Fixit, a course project where students are taught to contribute to open-source Java projects by fixing bugs reported in GitHub. We described our course outline to teach students SE concepts by encouraging the usages of several automated program analysis tools. We also included the carefully designed instructions that we gave to students for participating in GitHub-OSS Fixit. As all lectures and labs are conducted online, we think that our course design could help in guiding future online SE courses. Overall, our survey results show that students think that GitHub-OSS Fixit could help them to improve many skills and apply the knowledge taught in class. In total, 154 students have submitted 214 pull requests to 24 different Java projects, in which 59 of them have been merged, and 82 have been closed by developers.","675":"This work proposes and analyzes the use of keystroke biometrics for content de-anonymization. Fake news have become a powerful tool to manipulate public opinion, especially during major events. In particular, the massive spread of fake news during the COVID-19 pandemic has forced governments and companies to fight against missinformation. In this context, the ability to link multiple accounts or profiles that spread such malicious content on the Internet while hiding in anonymity would enable proactive identification and blacklisting. Behavioral biometrics can be powerful tools in this fight. In this work, we have analyzed how the latest advances in keystroke biometric recognition can help to link behavioral typing patterns in experiments involving 100,000 users and more than 1 million typed sequences. Our proposed system is based on Recurrent Neural Networks adapted to the context of content de-anonymization. Assuming the challenge to link the typed content of a target user in a pool of candidate profiles, our results show that keystroke recognition can be used to reduce the list of candidate profiles by more than 90%. In addition, when keystroke is combined with auxiliary data (such as location), our system achieves a Rank-1 identification performance equal to 52.6% and 10.9% for a background candidate list composed of 1K and 100K profiles, respectively.","676":"The task of detecting whether a person wears a face mask from speech is useful in modelling speech in forensic investigations, communication between surgeons or people protecting themselves against infectious diseases such as COVID-19. In this paper, we propose a novel data augmentation approach for mask detection from speech. Our approach is based on (i) training Generative Adversarial Networks (GANs) with cycle-consistency loss to translate unpaired utterances between two classes (with mask and without mask), and on (ii) generating new training utterances using the cycle-consistent GANs, assigning opposite labels to each translated utterance. Original and translated utterances are converted into spectrograms which are provided as input to a set of ResNet neural networks with various depths. The networks are combined into an ensemble through a Support Vector Machines (SVM) classifier. With this system, we participated in the Mask Sub-Challenge (MSC) of the INTERSPEECH 2020 Computational Paralinguistics Challenge, surpassing the baseline proposed by the organizers by 2.8%. Our data augmentation technique provided a performance boost of 0.9% on the private test set. Furthermore, we show that our data augmentation approach yields better results than other baseline and state-of-the-art augmentation methods.","677":"Background. The paper concerns the SARS-CoV2 (COVID-19) pandemic that, starting from the end of February 2020, began spreading along the Italian peninsula, by first attacking small communities in north regions, and then extending to the center and south of Italy, including the two main islands. Objective. The creation of a forecast model that manages to alert the decision-making bodies and, in particular, the healthcare system, to hinder the emergence of any other pandemic outbreaks, or the arrival of subsequent pandemic waves. Methods. A new mathematical model to describe the pandemic is given. The model includes the class of undiagnosed infected people, and has a multi-region extension, to cope with the in-time and in-space heterogeneity of the epidemic. Results. We obtain a robust and reliable tool for the forecast of the total and active cases, which can be also used to simulate different scenarios. Conclusions. We are able to address a number of issues, such as assessing the adoption of the lockdown in Italy, started from 11 March 2020, and how to employ a rapid screening test campaign for containing the epidemic.","678":"The worldwide spread of COVID-19 has called for fast advancement of new modelling strategies to estimate its unprecedented spread. Here, we introduce a model based on the fundamental SIR equations with a stochastic disorder by a random exchange of infected populations between cities to study dynamics in an interacting network of epicentres in a model state. Although each stochastic exchange conserves populations pair-wise, the disorder drives the global system towards newer routes to dynamic equilibrium. Upon controlling the range of the exchange fraction, we show that it is possible to control the heterogeneity in the spread and the co-operativity among the interacting hotspots. Data of collective temporal evolution of the infected populations in federal states of Germany validate the qualitative features of the model.","679":"Simulation on directed graphs is an important method for understanding the dynamics in the systems where connectivity graphs contain cycles. Discrete Stochastic Heterogeneous Simulator (DiSH) is one of the simulation tools with wide application, which uses regulator values to calculate state updates of regulated elements. Here we present a new simulation approach DiSH-trend which also takes into account the trends in regulating elements. We demonstrate the features of trend-based regulation, as well as hybrid regulation, which is a combination of the trend- and level-based approaches. The modeling capabilities are demonstrated on a small toy model, showcasing different functionalities. Real-world capabilities are demonstrated on a larger network model of food insecurity in the Ethiopian region Oromia. Adding trend-based regulation to models results in increased modeling flexibility, and hybrid regulation improves qualitative dynamic behavior prediction. With appropriate data, DiSH-trend becomes a powerful tool for exploring intervention strategies.","680":"This research investigates how people engage with data visualizations when commenting on the social platform Reddit. There has been considerable research on collaborative sensemaking with visualizations and the personal relation of people with data. Yet, little is known about how public audiences without specific expertise and shared incentives openly express their thoughts, feelings, and insights in response to data visualizations. Motivated by the extensive social exchange around visualizations in online communities, this research examines characteristics and motivations of people's reactions to posts featuring visualizations. Following a Grounded Theory approach, we study 475 reactions from the \/r\/dataisbeautiful community, identify ten distinguishable reaction types, and consider their contribution to the discourse. A follow-up survey with 168 Reddit users clarified their intentions to react. Our results help understand the role of personal perspectives on data and inform future interfaces that integrate audience reactions into visualizations to foster a public discourse about data.","681":"Rapid and affordable methods of testing for COVID-19 infections are essential to reduce infection rates and prevent medical facilities from becoming overwhelmed. Current approaches of detecting COVID-19 require in-person testing with expensive kits that are not always easily accessible. This study demonstrates that crowdsourced cough audio samples acquired on smartphones from around the world can be used to develop a AI-based method that accurately predicts COVID-19 infection with an ROC-AUC of 77.1% (75.2%-78.3%). Furthermore, we show that our method is able to generalize to crowdsourced samples from Latin America and clinical samples from South Asia, without further training using the specific samples from those regions. As more crowdsourced data is collected, further development can be implemented using various respiratory audio samples to create a cough analysis-based machine learning (ML) solution for COVID-19 detection that can likely generalize globally to all demographic groups in both clinical and non-clinical settings.","682":"Early detection of person-to-person transmission of emerging infectious diseases such as avian influenza is crucial for containing pandemics. We developed a simple permutation test and its refined version for this purpose. A simulation study shows that the refined permutation test is as powerful as or outcompetes the conventional test built on asymptotic theory, especially when the sample size is small. In addition, our resampling methods can be applied to a broad range of problems where an asymptotic test is not available or fails. We also found that decent statistical power could be attained with just a small number of cases, if the disease is moderately transmissible between humans.","683":"Most mobile health apps employ data visualization to help people view their health and activity data, but these apps provide limited support for visual data exploration. Furthermore, despite its huge potential benefits, mobile visualization research in the personal data context is sparse. This work aims to empower people to easily navigate and compare their personal health data on smartphones by enabling flexible time manipulation with speech. We designed and developed Data@Hand, a mobile app that leverages the synergy of two complementary modalities: speech and touch. Through an exploratory study with 13 long-term Fitbit users, we examined how multimodal interaction helps participants explore their own health data. Participants successfully adopted multimodal interaction (i.e., speech and touch) for convenient and fluid data exploration. Based on the quantitative and qualitative findings, we discuss design implications and opportunities with multimodal interaction for better supporting visual data exploration on mobile devices.","684":"Motivated by the case fatality rate (CFR) of COVID-19, in this paper, we develop a fully parametric quantile regression model based on the generalized three-parameter beta (GB3) distribution. Beta regression models are primarily used to model rates and proportions. However, these models are usually specified in terms of a conditional mean. Therefore, they may be inadequate if the observed response variable follows an asymmetrical distribution, such as CFR data. In addition, beta regression models do not consider the effect of the covariates across the spectrum of the dependent variable, which is possible through the conditional quantile approach. In order to introduce the proposed GB3 regression model, we first reparameterize the GB3 distribution by inserting a quantile parameter and then we develop the new proposed quantile model. We also propose a simple interpretation of the predictor-response relationship in terms of percentage increases\/decreases of the quantile. A Monte Carlo study is carried out for evaluating the performance of the maximum likelihood estimates and the choice of the link functions. Finally, a real COVID-19 dataset from Chile is analyzed and discussed to illustrate the proposed approach.","685":"In this paper, we introduce two remote extended reality (XR) research methods that can overcome the limitations of lab-based controlled experiments, especially during the COVID-19 pandemic: (1) a predictive model-based task analysis and (2) a large-scale video-based remote evaluation. We used a box stacking task including three interaction modalities - two multimodal gaze-based interactions as well as a unimodal hand-based interaction which is defined as our baseline. For the first evaluation, a GOMS-based task analysis was performed by analyzing the tasks to understand human behaviors in XR and predict task execution times. For the second evaluation, an online survey was administered using a series of the first-person point of view videos where a user performs the corresponding task with three interaction modalities. A total of 118 participants were asked to compare the interaction modes based on their judgment. Two standard questionnaires were used to measure perceived workload and the usability of the modalities.","686":"The COVID-19 global pandemic spurred sudden and dramatic changes in the way universities and research programs operate, often with negative consequences that have disproportionately affected minorities, women, and people with low income. These consequences compound the difficulties these groups faced prior to the pandemic | in particular, the increased obstacles marginalized students encounter while pursuing STEM career paths. The National Astronomy Consortium (NAC) was founded to support underrepresented physics and astronomy students by providing research opportunities and long-term mentoring. In response to the pandemic and nation-wide shutdowns, the NAC shifted both its summer research program and its annual fall conference to a fully-remote format. Here, we discuss the changes the NAC made to achieve a successful virtual transition, and provide temporary and long-term recommendations for programs seeking to provide support to marginalized undergraduate and graduate researchers moving forward.","687":"Global concern regarding ultrafine particles (UFPs), which are particulate matter (PM) with a diameter of less than 100nm, is increasing. These particles-with more serious health effects than PM less than 2.5 micrometers (PM2.5)-are difficult to measure using the current methods because their characteristics are different from those of other air pollutants. Therefore, a new monitoring system is required to obtain accurate UFPs information, which will raise the financial burden of the government and people. In this study, we estimated the economic value of UFPs information by evaluating the willingness-to-pay (WTP) for the UFPs monitoring and reporting system. We used the contingent valuation method (CVM) and the one-and-one-half-bounded dichotomous choice (OOHBDC) spike model. We analyzed how the respondents' socio-economic variables, as well as their cognition level of PM, affected their WTP. Therefore, we collected WTP data of 1,040 Korean respondents through an online survey. The estimated mean WTP for building a UFPs monitoring and reporting system is KRW 6,958.55-7,222.55 (USD 6.22-6.45) per household per year. We found that people satisfied with the current air pollutant information, and generally possessing relatively greater knowledge of UFPs, have higher WTP for a UFPs monitoring and reporting system. The results can be used to establish new policies response to PM including UFPs.","688":"We present SHINeS, a space simulator which can be used to replicate the thermal environment in the immediate neighborhood of the Sun down to a heliocentric distance r~0.06 au. The system consists of three main parts: the solar simulator which was designed and constructed in-house, a vacuum chamber, and the probing and recording equipment needed to monitor the experimental procedures. Our motivation for building this experimental system was to study the effect of intense solar radiation on the surfaces of asteroids when their perihelion distances become smaller than the semi-major axis of the orbit of Mercury. Comparisons between observational data and recent orbit and size-frequency models of the population of near-Earth asteroids suggest that asteroids are super-catastrophically destroyed when they approach the Sun. Whereas the current models are agnostic about the disruption mechanism, SHINeS was developed to study the mechanism or mechanisms responsible. The system can, however, be used for other applications that need to study the effects of high solar radiation on other natural or artificial objects.","689":"Since December of 2019, novel coronavirus disease COVID-19 has spread around the world infecting millions of people and upending the global economy. One of the driving reasons behind its high rate of infection is due to the unreliability and lack of RT-PCR testing. At times the turnaround results span as long as a couple of days, only to yield a roughly 70% sensitivity rate. As an alternative, recent research has investigated the use of Computer Vision with Convolutional Neural Networks (CNNs) for the classification of COVID-19 from CT scans. Due to an inherent lack of available COVID-19 CT data, these research efforts have been forced to leverage the use of Transfer Learning. This commonly employed Deep Learning technique has shown to improve model performance on tasks with relatively small amounts of data, as long as the Source feature space somewhat resembles the Target feature space. Unfortunately, a lack of similarity is often encountered in the classification of medical images as publicly available Source datasets usually lack the visual features found in medical images. In this study, we propose the use of Multi-Source Transfer Learning (MSTL) to improve upon traditional Transfer Learning for the classification of COVID-19 from CT scans. With our multi-source fine-tuning approach, our models outperformed baseline models fine-tuned with ImageNet. We additionally, propose an unsupervised label creation process, which enhances the performance of our Deep Residual Networks. Our best performing model was able to achieve an accuracy of 0.893 and a Recall score of 0.897, outperforming its baseline Recall score by 9.3%.","690":"In this work, we create a web application to highlight the output of NLP models trained to parse and label discourse segments in law text. Our system is built primarily with journalists and legal interpreters in mind, and we focus on state-level law that uses U.S. Census population numbers to allocate resources and organize government. Our system exposes a corpus we collect of 6,000 state-level laws that pertain to the U.S. census, using 25 scrapers we built to crawl state law websites, which we release. We also build a novel, flexible annotation framework that can handle span-tagging and relation tagging on an arbitrary input text document and be embedded simply into any webpage. This framework allows journalists and researchers to add to our annotation database by correcting and tagging new data.","691":"This article presents the results of a study involving the translation of a short story by Kurt Vonnegut from English to Catalan and Dutch using three modalities: machine-translation (MT), post-editing (PE) and translation without aid (HT). Our aim is to explore creativity, understood to involve novelty and acceptability, from a quantitative perspective. The results show that HT has the highest creativity score, followed by PE, and lastly, MT, and this is unanimous from all reviewers. A neural MT system trained on literary data does not currently have the necessary capabilities for a creative translation; it renders literal solutions to translation problems. More importantly, using MT to post-edit raw output constrains the creativity of translators, resulting in a poorer translation often not fit for publication, according to experts.","692":"The coronavirus disease (COVID-19) has resulted in a pandemic crippling the a breadth of services critical to daily life. Segmentation of lung infections in computerized tomography (CT) slices could be be used to improve diagnosis and understanding of COVID-19 in patients. Deep learning systems lack interpretability because of their black box nature. Inspired by human communication of complex ideas through language, we propose a symbolic framework based on emergent languages for the segmentation of COVID-19 infections in CT scans of lungs. We model the cooperation between two artificial agents - a Sender and a Receiver. These agents synergistically cooperate using emergent symbolic language to solve the task of semantic segmentation. Our game theoretic approach is to model the cooperation between agents unlike Generative Adversarial Networks (GANs). The Sender retrieves information from one of the higher layers of the deep network and generates a symbolic sentence sampled from a categorical distribution of vocabularies. The Receiver ingests the stream of symbols and cogenerates the segmentation mask. A private emergent language is developed that forms the communication channel used to describe the task of segmentation of COVID infections. We augment existing state of the art semantic segmentation architectures with our symbolic generator to form symbolic segmentation models. Our symbolic segmentation framework achieves state of the art performance for segmentation of lung infections caused by COVID-19. Our results show direct interpretation of symbolic sentences to discriminate between normal and infected regions, infection morphology and image characteristics. We show state of the art results for segmentation of COVID-19 lung infections in CT.","693":"While billions of COVID-19 vaccines have been administered, too many people remain hesitant. Twitter, with its substantial reach and daily exposure, is an excellent resource for examining how people frame their vaccine hesitancy and to uncover vaccine hesitancy profiles. In this paper we expose our processing journey from identifying Vaccine Hesitancy Framings in a collection of 9,133,471 original tweets discussing the COVID-19 vaccines, establishing their ontological commitments, annotating the Moral Foundations they imply to the automatic recognition of the stance of the tweet authors toward any of the CoVaxFrames that we have identified. When we found that 805,336 Twitter users had a stance towards some CoVaxFrames in either the 9,133,471 original tweets or their 17,346,664 retweets, we were able to derive nine different Vaccine Hesitancy Profiles of these users and to interpret these profiles based on the ontological commitments of the frames they evoked in their tweets and on value of their stance towards the evoked frames.","694":"Rapidly scaling screening, testing and quarantine has shown to be an effective strategy to combat the COVID-19 pandemic. We consider the application of deep learning techniques to distinguish individuals with COVID from non-COVID by using data acquirable from a phone. Using cough and context (symptoms and meta-data) represent such a promising approach. Several independent works in this direction have shown promising results. However, none of them report performance across clinically relevant data splits. Specifically, the performance where the development and test sets are split in time (retrospective validation) and across sites (broad validation). Although there is meaningful generalization across these splits the performance significantly varies (up to 0.1 AUC score). In addition, we study the performance of symptomatic and asymptomatic individuals across these three splits. Finally, we show that our model focuses on meaningful features of the input, cough bouts for cough and relevant symptoms for context. The code and checkpoints are available at https:\/\/github.com\/WadhwaniAI\/cough-against-covid","695":"#StopAsianHate and #StopAAPIHate are two of the most commonly used hashtags that represent the current movement to end hate crimes against the Asian American and Pacific Islander community. We conduct a social media study of public opinion on the #StopAsianHate and #StopAAPIHate movement based on 46,058 Twitter users across 30 states in the United States ranging from March 18 to April 11, 2021. The movement attracts more participation from women, younger adults, Asian and Black communities. 51.56% of the Twitter users show direct support, 18.38% are news about anti-Asian hate crimes, while 5.43% show a negative attitude towards the movement. Public opinion varies across user characteristics. Furthermore, among the states with most racial bias motivated hate crimes, the negative attitude towards the #StopAsianHate and #StopAAPIHate movement is the weakest. To our best knowledge, this is the first large-scale social media-based study to understand public opinion on the #StopAsianHate and #StopAAPIHate movement. We hope our study can provide insights and promote research on anti-Asian hate crimes, and ultimately help address such a serious societal issue for the common benefits of all communities.","696":"The use of delivery services is an increasing trend worldwide, further enhanced by the COVID pandemic. In this context, drone delivery systems are of great interest as they may allow for faster and cheaper deliveries. This paper presents a navigation system that makes feasible the delivery of parcels with autonomous drones. The system generates a path between a start and a final point and controls the drone to follow this path based on its localization obtained through GPS, 9DoF IMU, and barometer. In the landing phase, information of poses estimated by a marker (ArUco) detection technique using a camera, ultra-wideband (UWB) devices, and the drone's software estimation are merged by utilizing an Extended Kalman Filter algorithm to improve the landing precision. A vector field-based method controls the drone to follow the desired path smoothly, reducing vibrations or harsh movements that could harm the transported parcel. Real experiments validate the delivery strategy and allow to evaluate the performance of the adopted techniques. Preliminary results state the viability of our proposal for autonomous drone delivery.","697":"Digitization means the use of digital technology to modify a business model to create new possibilities for sales to value creation. The main purpose of the study is to identify the effects of digitization in the federal parliament of Nepal. The performance of the parliament, secretariat, parliamentarians as well as employees working procedure should be fully digitalized based different prescribed different modules. Quantitative and quantitative method were followed. This research included a series of well-structured questionnaires, mainly directed at MPs and employees, and structured interviews with key persons of the organizations. The raw data were processed and analyzed by SPSS. Cochran's Q test, Chi-Square test and Friedman tests were applied to show the present status and need of digitization tools in Federal Parliament of Nepal. The system becomes automated, interlinked, based on the database and there should be a mutual understanding among committees on different issues. The overall activities of parliament, committees, and its secretariats records are managed systematically through tailored software. We would also intend to discuss the limits on the usage of different tools and recommendations for parliamentary application. The holistic framework is a digital framework for FPN. This will be helpful for a digitized workplace. The FPN will start the new era of digital transformation. Keywords - Federal Parliament, Digitalization, Secretariat, Parliamentarian, Legislature, Information, and Communication Technology","698":"How to fast and accurately assess the severity level of COVID-19 is an essential problem, when millions of people are suffering from the pandemic around the world. Currently, the chest CT is regarded as a popular and informative imaging tool for COVID-19 diagnosis. However, we observe that there are two issues -- weak annotation and insufficient data that may obstruct automatic COVID-19 severity assessment with CT images. To address these challenges, we propose a novel three-component method, i.e., 1) a deep multiple instance learning component with instance-level attention to jointly classify the bag and also weigh the instances, 2) a bag-level data augmentation component to generate virtual bags by reorganizing high confidential instances, and 3) a self-supervised pretext component to aid the learning process. We have systematically evaluated our method on the CT images of 229 COVID-19 cases, including 50 severe and 179 non-severe cases. Our method could obtain an average accuracy of 95.8%, with 93.6% sensitivity and 96.4% specificity, which outperformed previous works.","699":"The most serious threat to ecosystems is the global climate change fueled by the uncontrolled increase in carbon emissions. In this project, we use mean field control and mean field game models to analyze and inform the decisions of electricity producers on how much renewable sources of production ought to be used in the presence of a carbon tax. The trade-off between higher revenues from production and the negative externality of carbon emissions is quantified for each producer who needs to balance in real time reliance on reliable but polluting (fossil fuel) thermal power stations versus investing in and depending upon clean production from uncertain wind and solar technologies. We compare the impacts of these decisions in two different scenarios: 1) the producers are competitive and hopefully reach a Nash Equilibrium; 2) they cooperate and reach a Social Optimum. We first prove that both problems have a unique solution using forward-backward systems of stochastic differential equations. We then illustrate with numerical experiments the producers' behavior in each scenario. We further introduce and analyze the impact of a regulator in control of the carbon tax policy, and we study the resulting Stackelberg equilibrium with the field of producers.","700":"The SARS-CoV-2 outbreak changed the everyday life of almost all the people over the world.Currently, we are facing with the problem of containing the spread of the virus both using the more effective forced lockdown, which has the drawback of slowing down the economy of the involved countries, and by identifying and isolating the positive individuals, which, instead, is an hard task in general due to the lack of information. For this specific disease, the identificato of the infected is particularly challenging since there exists cathegories, namely the asymptomatics, who are positive and potentially contagious, but do not show any of the symptoms of SARS-CoV-2. Until the developement and distribution of a vaccine is not yet ready, we need to design ways of selecting those individuals which are most likely infected, given the limited amount of tests which are available each day. In this paper, we make use of available data collected by the so called contact tracing apps to develop an algorithm, namely PPTO, that identifies those individuals that are most likely positive and, therefore, should be tested. While in the past these analysis have been conducted by centralized algorithms, requiring that all the app users data are gathered in a single database, our protocol is able to work on a device level, by exploiting the communication of anonymized information to other devices.","701":"Drug discovery and development is a complex and costly process. Machine learning approaches are being investigated to help improve the effectiveness and speed of multiple stages of the drug discovery pipeline. Of these, those that use Knowledge Graphs (KG) have promise in many tasks, including drug repurposing, drug toxicity prediction and target gene-disease prioritisation. In a drug discovery KG, crucial elements including genes, diseases and drugs are represented as entities, whilst relationships between them indicate an interaction. However, to construct high-quality KGs, suitable data is required. In this review, we detail publicly available sources suitable for use in constructing drug discovery focused KGs. We aim to help guide machine learning and KG practitioners who are interested in applying new techniques to the drug discovery field, but who may be unfamiliar with the relevant data sources. The datasets are selected via strict criteria, categorised according to the primary type of information contained within and are considered based upon what information could be extracted to build a KG. We then present a comparative analysis of existing public drug discovery KGs and a evaluation of selected motivating case studies from the literature. Additionally, we raise numerous and unique challenges and issues associated with the domain and its datasets, whilst also highlighting key future research directions. We hope this review will motivate KGs use in solving key and emerging questions in the drug discovery domain.","702":"Abusive language on online platforms is a major societal problem, often leading to important societal problems such as the marginalisation of underrepresented minorities. There are many different forms of abusive language such as hate speech, profanity, and cyber-bullying, and online platforms seek to moderate it in order to limit societal harm, to comply with legislation, and to create a more inclusive environment for their users. Within the field of Natural Language Processing, researchers have developed different methods for automatically detecting abusive language, often focusing on specific subproblems or on narrow communities, as what is considered abusive language very much differs by context. We argue that there is currently a dichotomy between what types of abusive language online platforms seek to curb, and what research efforts there are to automatically detect abusive language. We thus survey existing methods as well as content moderation policies by online platforms in this light, and we suggest directions for future work.","703":"Starting in early 2020, the novel coronavirus disease (COVID-19) severely affected the U.S., causing substantial changes in the operations of bulk power systems and electricity markets. In this paper, we develop a data-driven analysis to substantiate the pandemic's impacts from the perspectives of power system security, electric power generation, electric power demand and electricity prices. Our results suggest that both electric power demand and electricity prices have discernibly dropped during the COVID-19 pandemic. Geographical variances in the impact are observed and quantified, and the bulk power market and power system operations in the northeast region are most severely affected. All the data sources, assessment criteria, and analysis codes reported in this paper are available on a GitHub repository.","704":"COVID-19 has resulted in a global health crisis that may become even more acute over the upcoming months. One of the main reasons behind the current rapid growth of COVID-19 in the U.S. population is the limited availability of testing kits and the relatively-high cost of screening tests. In this draft, we demonstrate the effectiveness of group testing (pooling) ideas to accelerate testing for COVID-19. This draft is semi-tutorial in nature and is written for a broad audience with interest in mathematical formulations relevant to COVID-19 testing. Therefore, ideas are presented through illustrative examples rather than through purely theoretical formulations. The focus is also on pools of size less than 64 such as what is practical with current RT-PCR technology.","705":"The very first case of corona-virus illness was recorded on 30 January 2020, in India and the number of infected cases, including the death toll, continues to rise. In this paper, we present short-term forecasts of COVID-19 for 28 Indian states and five union territories using real-time data from 30 January to 21 April 2020. Applying Holt's second-order exponential smoothing method and autoregressive integrated moving average (ARIMA) model, we generate 10-day ahead forecasts of the likely number of infected cases and deaths in India for 22 April to 1 May 2020. Our results show that the number of cumulative cases in India will rise to 36335.63 [PI 95% (30884.56, 42918.87)], concurrently the number of deaths may increase to 1099.38 [PI 95% (959.77, 1553.76)] by 1 May 2020. Further, we have divided the country into severity zones based on the cumulative cases. According to this analysis, Maharashtra is likely to be the most affected states with around 9787.24 [PI 95% (6949.81, 13757.06)] cumulative cases by 1 May 2020. However, Kerala and Karnataka are likely to shift from the red zone (i.e. highly affected) to the lesser affected region. On the other hand, Gujarat and Madhya Pradesh will move to the red zone. These results mark the states where lockdown by 3 May 2020, can be loosened.","706":"The COVID-19 pandemic is one of the most pressing issues at present. A question which is particularly important for governments and policy makers is the following: Does the virus spread in the same way in different countries? Or are there significant differences in the development of the epidemic? In this paper, we devise new inference methods that allow to detect differences in the development of the COVID-19 epidemic across countries in a statistically rigorous way. In our empirical study, we use the methods to compare the outbreak patterns of the epidemic in a number of European countries.","707":"The outbreak of coronavirus disease 2019 (COVID-19) has led to significant challenges for schools, workplaces and communities to return to operations during the pandemic, requiring policymakers to balance individuals' safety and operational efficiency. In this paper, we present our work using mixed-integer programming and simulation for redesigning routes and bus schedules for University of Michigan (UM)'s campus bus system during the COVID-19 pandemic. We propose a hub-and-spoke design and utilize real data of student activities to identify hub locations and bus stops to be used in the new routes. Using the same total number of buses to operate, each new bus route has 50\\% or fewer seats being used and takes maximumly 15 minutes, to reduce disease transmission through expiratory aerosol. We sample a variety of scenarios that cover variations of peak demand, social-distancing requirements, and break-down buses, to demonstrate the system resiliency of the new routes and schedules via simulation. The new bus routes are implemented and used by all UM campuses during the academic year 2020-2021, to ensure social distancing and short travel time. Our approach can be generalized to redesign public transit systems with social distancing requirement during the pandemic to reduce passengers' infection risk.","708":"Differential privacy (DP) is a widely-accepted and widely-applied notion of privacy based on worst-case analysis. Often, DP classifies most mechanisms without external noise as non-private [Dwork et al., 2014], and external noises, such as Gaussian noise or Laplacian noise [Dwork et al., 2006], are introduced to improve privacy. In many real-world applications, however, adding external noise is undesirable and sometimes prohibited. For example, presidential elections often require a deterministic rule to be used [Liu et al., 2020], and small noises can lead to dramatic decreases in the prediction accuracy of deep neural networks, especially the underrepresented classes [Bagdasaryan et al., 2019]. In this paper, we propose a natural extension and relaxation of DP following the worst average-case idea behind the celebrated smoothed analysis [Spielman and Teng, 2004]. Our notion, the smoothed DP, can effectively measure the privacy leakage of mechanisms without external noises under realistic settings. We prove several strong properties of the smoothed DP, including composability, robustness to post-processing and etc. We proved that any discrete mechanism with sampling procedures is more private than what DP predicts. In comparison, many continuous mechanisms with sampling procedures are still non-private under smoothed DP. Experimentally, we first verified that the discrete sampling mechanisms are private in real-world elections. Then, we apply the smoothed DP notion on quantized gradient descent, which indicates some neural networks can be private without adding any extra noises. We believe that these results contribute to the theoretical foundation of realistic privacy measures beyond worst-case analysis.","709":"Strong social distancing restrictions have been crucial to controlling the COVID-19 outbreak thus far, and the next question is when and how to relax these restrictions. A sequential timing of relaxing restrictions across groups is explored in order to identify policies that simultaneously reduce health risks and economic stagnation relative to current policies. The goal will be to mitigate health risks, particularly among the most fragile sub-populations, while also managing the deleterious effect of restrictions on economic activity. The results of this paper show that a properly constructed sequential release of age-defined subgroups from strict social distancing protocols can lead to lower overall fatality rates than the simultaneous release of all individuals after a lockdown. The optimal release policy, in terms of minimizing overall death rate, must be sequential in nature, and it is important to properly time each step of the staggered release. This model allows for testing of various timing choices for staggered release policies, which can provide insights that may be helpful in the design, testing, and planning of disease management policies for the ongoing COVID-19 pandemic and future outbreaks.","710":"Diagnostic tests are almost never perfect. Studies quantifying their performance use knowledge of the true health status, measured with a reference diagnostic test. Researchers commonly assume that the reference test is perfect, which is not the case in practice. When the assumption fails, conventional studies identify\"apparent\"performance or performance with respect to the reference, but not true performance. This paper provides the smallest possible bounds on the measures of true performance - sensitivity (true positive rate) and specificity (true negative rate), or equivalently false positive and negative rates, in standard settings. Implied bounds on policy-relevant parameters are derived: 1) Prevalence in screened populations; 2) Predictive values. Methods for inference based on moment inequalities are used to construct uniformly consistent confidence sets in level over a relevant family of data distributions. Emergency Use Authorization (EUA) and independent study data for the BinaxNOW COVID-19 antigen test demonstrate that the bounds can be very informative. Analysis reveals that the estimated false negative rates for symptomatic and asymptomatic patients are up to 3.89 and 5.42 times higher than the frequently cited\"apparent\"false negative rate.","711":"The employment status of billions of people has been affected by the COVID epidemic around the Globe. New evidence is needed on how to mitigate the job market crisis, but there exists only a handful of studies mostly focusing on developed countries. We fill in this gap in the literature by using novel data from Ukraine, a transition country in Eastern Europe, which enacted strict quarantine policies early on. We model four binary outcomes to identify respondents (i) who are not working during quarantine, (ii) those who are more likely to work from home, (iii) respondents who are afraid of losing a job, and, finally, (iv) survey participants who have savings for 1 month or less if quarantine is further extended. Our findings suggest that respondents employed in public administration, programming and IT, as well as highly qualified specialists, were more likely to secure their jobs during the quarantine. Females, better educated respondents, and those who lived in Kyiv were more likely to work remotely. Working in the public sector also made people more confident about their future employment perspectives. Although our findings are limited to urban households only, they provide important early evidence on the correlates of job market outcomes, expectations, and financial security, indicating potential deterioration of socio-economic inequalities.","712":"A self-exciting spatio-temporal point process is fitted to incident data from the UK National Traffic Information Service to model the rates of primary and secondary accidents on the M25 motorway in a 12-month period during 2017-18. This process uses a background component to represent primary accidents, and a self-exciting component to represent secondary accidents. The background consists of periodic daily and weekly components, a spatial component and a long-term trend. The self-exciting components are decaying, unidirectional functions of space and time. These components are determined via kernel smoothing and likelihood estimation. Temporally, the background is stable across seasons with a daily double peak structure reflecting commuting patterns. Spatially, there are two peaks in intensity, one of which becomes more pronounced during the study period. Self-excitation accounts for 6-7% of the data with associated time and length scales around 100 minutes and 1 kilometre respectively. In-sample and out-of-sample validation are performed to assess the model fit. When we restrict the data to incidents that resulted in large speed drops on the network, the results remain coherent.","713":"Modeling the spatiotemporal nature of the spread of infectious diseases can provide useful intuition in understanding the time-varying aspect of the disease spread and the underlying complex spatial dependency observed in people's mobility patterns. Besides, the county level multiple related time series information can be leveraged to make a forecast on an individual time series. Adding to this challenge is the fact that real-time data often deviates from the unimodal Gaussian distribution assumption and may show some complex mixed patterns. Motivated by this, we develop a deep learning-based time-series model for probabilistic forecasting called Auto-regressive Mixed Density Dynamic Diffusion Network(ARM3Dnet), which considers both people's mobility and disease spread as a diffusion process on a dynamic directed graph. The Gaussian Mixture Model layer is implemented to consider the multimodal nature of the real-time data while learning from multiple related time series. We show that our model, when trained with the best combination of dynamic covariate features and mixture components, can outperform both traditional statistical and deep learning models in forecasting the number of Covid-19 deaths and cases at the county level in the United States.","714":"Many developers and organizations implement apps for Android, the most widely used operating system for mobile devices. Common problems developers face are the various hardware devices, customized Android variants, and frequent updates, forcing them to implement workarounds for the different versions and variants of Android APIs used in practice. In this paper, we contribute the Android Compatibility checkS dataSet (AndroidCompass) that comprises changes to compatibility checks developers use to enforce workarounds for specific Android versions in their apps. We extracted 80,324 changes to compatibility checks from 1,394 apps by analyzing the version histories of 2,399 projects from the F-Droid catalog. With AndroidCompass, we aim to provide data on when and how developers introduced or evolved workarounds to handle Android incompatibilities. We hope that AndroidCompass fosters research to deal with version incompatibilities, address potential design flaws, identify security concerns, and help derive solutions for other developers, among others-helping researchers to develop and evaluate novel techniques, and Android app as well as operating-system developers in engineering their software.","715":"A meaningful and deep understanding of the human aspects of software engineering (SE) requires psychological constructs to be considered. Psychology theory can facilitate the systematic and sound development as well as the adoption of instruments (e.g., psychological tests, questionnaires) to assess these constructs. In particular, to ensure high quality, the psychometric properties of instruments need evaluation. In this paper, we provide an introduction to psychometric theory for the evaluation of measurement instruments for SE researchers. We present guidelines that enable using existing instruments and developing new ones adequately. We conducted a comprehensive review of the psychology literature framed by the Standards for Educational and Psychological Testing. We detail activities used when operationalizing new psychological constructs, such as item pooling, item review, pilot testing, item analysis, factor analysis, statistical property of items, reliability, validity, and fairness in testing and test bias. We provide an openly available example of a psychometric evaluation based on our guideline. We hope to encourage a culture change in SE research towards the adoption of established methods from psychology. To improve the quality of behavioral research in SE, studies focusing on introducing, validating, and then using psychometric instruments need to be more common.","716":"Automated Deduction in Geometry (ADG) is a forum to exchange ideas and views, to present research results and progress, and to demonstrate software tools at the intersection between geometry and automated deduction. Relevant topics include (but are not limited to): polynomial algebra, invariant and coordinate-free methods; probabilistic, synthetic, and logic approaches, techniques for automated geometric reasoning from discrete mathematics, combinatorics, and numerics; interactive theorem proving in geometry; symbolic and numeric methods for geometric computation, geometric constraint solving, automated generation\/reasoning and manipulation with diagrams; design and implementation of geometry software, automated theorem provers, special-purpose tools, experimental studies; applications of ADG in mechanics, geometric modelling, CAGD\/CAD, computer vision, robotics and education. Traditionally, the ADG conference is held every two years. The previous editions of ADG were held in Nanning in 2018, Strasbourg in 2016, Coimbra in 2014, Edinburgh in 2012, Munich in 2010, Shanghai in 2008, Pontevedra in 2006, Gainesville in 2004, Hagenberg in 2002, Zurich in 2000, Beijing in 1998, and Toulouse in 1996. The 13th edition of ADG was supposed to be held in 2020 in Hagenberg, Austria, but due to the COVID-19 pandemic, it was postponed for 2021, and held online (still hosted by RISC Institute, Hagenberg, Austria), September 15-17, 2021 (https:\/\/www.risc.jku.at\/conferences\/adg2021).","717":"Monotone matrices play a key role in the convergence theory of regular splittings and different types of weak regular splittings. If monotonicity fails, then it is difficult to guarantee the convergence of the above-mentioned classes of matrices. In such a case, $K$-monotonicity is sufficient for the convergence of $K$-regular and $K$-weak regular splittings, where $K$ is a proper cone in $\\mathbb{R}^n$. However, the convergence theory of a two-stage iteration scheme in general proper cone setting is a gap in the literature. Especially, the same study for weak regular splittings of type II (even if in standard proper cone setting, i.e., $K=\\mathbb{R}^n_+$), is open. To this end, we propose convergence theory of two-stage iterative scheme for $K$-weak regular splittings of both types in the proper cone setting. We provide some sufficient conditions which guarantee that the induced splitting from a two-stage iterative scheme is a $K$-regular splitting and then establish some comparison theorems. We also study $K$-monotone convergence theory of the stationary two-stage iterative method in case of a $K$-weak regular splitting of type II. The most interesting and important part of this work is on $M$-matrices appearing in the Covid-19 pandemic model. Finally, numerical computations are performed using the proposed technique to compute the next generation matrix involved in the pandemic model.","718":"Emergency Departments (EDs) overcrowding is a well recognized worldwide phenomenon. The consequences range from long waiting times for visits and treatment of patients up to life-threatening health conditions. The international community is devoting greater and greater efforts to analyze this phenomenon aiming at reducing waiting times, improving the quality of the service. Within this framework, we propose a Discrete Event Simulation (DES) model to study the patient flows through a medium-size ED located in a region of Central Italy recently hit by a severe earthquake. In particular, our aim is to simulate unusual ED conditions, corresponding to critical events (like a natural disaster) that cause a sudden spike in the number of patient arrivals. The availability of detailed data concerning the ED processes enabled to build an accurate DES model and to perform extensive scenario analyses. The model provides a valid decision support system for the ED managers also in defining specific emergency plans to be activated in case of mass casualty disasters.","719":"Redox flow batteries (RFBs) have gained popularity as large-scale energy storage systems for wind and solar powered grids. Modern RFB systems are based on highly corrosive and\/or flammable electrolytes. Organic redox active species for RFBs are gaining commercial traction, but there is a trade-off in choosing aqueous or non-aqueous electrolytes in terms of rate capabilities, energy density, safety and cost. While modification of organic redox molecules to mitigate these issues is prevalent in the literature, the search for novel electrolyte systems is scarce. Here we present microemulsion-based electrolytes as an alternative for the next generation of organic RFBs. Micro-emulsion electrolytes (MEs) offer the advantages of decoupled solubility and ionic conductivity, broad electrochemical windows, non-flammability, simple fabrication modes and low cost of constituent chemicals. The electrochemical properties of organic redox species in MEs have been investigated for RFB requirements and a proof-of-concept flow cell with a widely studied redox system and a commercial membrane is shown. The compositions of MEs can be tailored to the redox system, making them a platform of electrolytes for all organic redox systems under consideration and benefit.","720":"The significance of social media has increased manifold in the past few decades as it helps people from even the most remote corners of the world to stay connected. With the advent of technology, digital media has become more relevant and widely used than ever before and along with this, there has been a resurgence in the circulation of fake news and tweets that demand immediate attention. In this paper, we describe a novel Fake News Detection system that automatically identifies whether a news item is\"real\"or\"fake\", as an extension of our work in the CONSTRAINT COVID-19 Fake News Detection in English challenge. We have used an ensemble model consisting of pre-trained models followed by a statistical feature fusion network , along with a novel heuristic algorithm by incorporating various attributes present in news items or tweets like source, username handles, URL domains and authors as statistical feature. Our proposed framework have also quantified reliable predictive uncertainty along with proper class output confidence level for the classification task. We have evaluated our results on the COVID-19 Fake News dataset and FakeNewsNet dataset to show the effectiveness of the proposed algorithm on detecting fake news in short news content as well as in news articles. We obtained a best F1-score of 0.9892 on the COVID-19 dataset, and an F1-score of 0.9073 on the FakeNewsNet dataset.","721":"We introduce a high-dimensional factor model with time-varying loadings. We cover both stationary and nonstationary factors to increase the possibilities of applications. We propose an estimation procedure based on two stages. First, we estimate common factors by principal components. In the second step, considering the estimated factors as observed, the time-varying loadings are estimated by an iterative generalized least squares procedure using wavelet functions. We investigate the finite sample features by some Monte Carlo simulations. Finally, we apply the model to study the Nord Pool power market's electricity prices and loads.","722":"This paper introduces the Sequential Monte Carlo Transformer, an original approach that naturally captures the observations distribution in a recurrent architecture. The keys, queries, values and attention vectors of the network are considered as the unobserved stochastic states of its hidden structure. This generative model is such that at each time step the received observation is a random function of these past states in a given attention window. In this general state-space setting, we use Sequential Monte Carlo methods to approximate the posterior distributions of the states given the observations, and then to estimate the gradient of the log-likelihood. We thus propose a generative model providing a predictive distribution, instead of a single-point estimate.","723":"In recent years, the problem of misinformation on the web has become widespread across languages, countries, and various social media platforms. Although there has been much work on automated fake news detection, the role of images and their variety are not well explored. In this paper, we investigate the roles of image and text at an earlier stage of the fake news detection pipeline, called claim detection. For this purpose, we introduce a novel dataset, MM-Claims, which consists of tweets and corresponding images over three topics: COVID-19, Climate Change and broadly Technology. The dataset contains roughly 86000 tweets, out of which 3400 are labeled manually by multiple annotators for the training and evaluation of multimodal models. We describe the dataset in detail, evaluate strong unimodal and multimodal baselines, and analyze the potential and drawbacks of current models.","724":"We introduce systematic tests exploiting robust statistical and behavioral patterns in trading to detect fake transactions on 29 cryptocurrency exchanges. Regulated exchanges feature patterns consistently observed in financial markets and nature; abnormal first-significant-digit distributions, size rounding, and transaction tail distributions on unregulated exchanges reveal rampant manipulations unlikely driven by strategy or exchange heterogeneity. We quantify the wash trading on each unregulated exchange, which averaged over 70% of the reported volume. We further document how these fabricated volumes (trillions of dollars annually) improve exchange ranking, temporarily distort prices, and relate to exchange characteristics (e.g., age and userbase), market conditions, and regulation.","725":"Noise pollution is considered to be the third most hazardous pollution after air and water pollution by the World Health Organization (WHO). Short as well as long-term exposure to noise pollution has several adverse effects on humans, ranging from psychiatric disorders such as anxiety and depression, hypertension, hormonal dysfunction, and blood pressure rise leading to cardiovascular disease. One of the major sources of noise pollution is road traffic. The WHO reports that around 40% of Europe's population are currently exposed to high noise levels. This study investigates noise pollution in Dublin, Ireland before and after the lockdown imposed as a result of the COVID-19 pandemic. The analysis was performed using 2020 hourly data from 12 noise monitoring stations. More than 80% of stations recorded high noise levels for more that 60% of the time before the lockdown in Dublin. However, a significant reduction in average and minimum noise levels was observed at all stations during the lockdown period and this can be attributed to reductions in both road and air traffic movements.","726":"The student peer-group is one of the most important influences on student development. Group work is essential for creating positive learning experiences, especially in remote-learning where student interactions are more challenging. While the benefits of study groups are established, students from underrepresented communities often face challenges in finding social support for their education when compared with those from majority groups. We present a system for flexible and inclusive study group formation that can scale to thousands of students. Our focus is on long-term study groups that persist throughout the semester and beyond. Students are periodically provided opportunities to obtain a new study group if they feel their current group is not a good fit. In contrast to prior work that generates single-use groups, our process enables continuous refinement of groups for each student, which in turn informs our algorithm for future iterations. We trialed our approach in a 1000+ student introductory Electrical Engineering and Computer Science course that was conducted entirely online during the COVID-19 pandemic. We found that of all students matched to study groups through our algorithm, a large majority felt comfortable asking questions (78%) and sharing ideas (74%) with their group. Students from underrepresented backgrounds were more likely to request software-matching for study groups when compared with students from majority groups. However, underrepresented students that we did match into study groups had group experiences that were just as successful as students from' majority groups. Students in engaged, regularly participating study groups had more positive results across all other indicators of the study group experience, and certain positive group experiences were associated with higher exam scores overall.","727":"MRI-guided radiation treatment planning is widely applied because of its superior soft-tissue contrast and no ionization radiation compared to CT-based planning. In this regard, synthetic CT (sCT) images should be generated from the patients MRI scans if radiation treatment planning is sought. Among the different available methods for this purpose, the deep learning algorithms have and do outperform their conventional counterparts. In this study, we investigated the performance of some most popular deep learning architectures including eCNN, U-Net, GAN, V-Net, and Res-Net for the task of sCT generation. As a baseline, an atlas-based method is implemented to which the results of the deep learning-based model are compared. A dataset consisting of 20 co-registered MR-CT pairs of the male pelvis is applied to assess the different sCT production methods' performance. The mean error (ME), mean absolute error (MAE), Pearson correlation coefficient (PCC), structural similarity index (SSIM), and peak signal-to-noise ratio (PSNR) metrics were computed between the estimated sCT and the ground truth (reference) CT images. The visual inspection revealed that the sCTs produced by eCNN, V-Net, and ResNet, unlike the other methods, were less noisy and greatly resemble the ground truth CT image. In the whole pelvis region, the eCNN yielded the lowest MAE (26.03-8.85 HU) and ME (0.82-7.06 HU), and the highest PCC metrics were yielded by the eCNN (0.93-0.05) and ResNet (0.91-0.02) methods. The ResNet model had the highest PSNR of 29.38-1.75 among all models. In terms of the Dice similarity coefficient, the eCNN method revealed superior performance in major tissue identification (air, bone, and soft tissue). All in all, the eCNN and ResNet deep learning methods revealed acceptable performance with clinically tolerable quantification errors.","728":"The COVID-19 pandemic has caused a massive economic shock across the world due to business interruptions and shutdowns from social-distancing measures. To evaluate the socio-economic impact of COVID-19 on individuals, a micro-economic model is developed to estimate the direct impact of distancing on household income, savings, consumption, and poverty. The model assumes two periods: a crisis period during which some individuals experience a drop in income and can use their precautionary savings to maintain consumption; and a recovery period, when households save to replenish their depleted savings to pre-crisis level. The San Francisco Bay Area is used as a case study, and the impacts of a lockdown are quantified, accounting for the effects of unemployment insurance (UI) and the CARES Act federal stimulus. Assuming a shelter-in-place period of three months, the poverty rate would temporarily increase from 17.1% to 25.9% in the Bay Area in the absence of social protection, and the lowest income earners would suffer the most in relative terms. If fully implemented, the combination of UI and CARES could keep the increase in poverty close to zero, and reduce the average recovery time, for individuals who suffer an income loss, from 11.8 to 6.7 months. However, the severity of the economic impact is spatially heterogeneous, and certain communities are more affected than the average and could take more than a year to recover. Overall, this model is a first step in quantifying the household-level impacts of COVID-19 at a regional scale. This study can be extended to explore the impact of indirect macroeconomic effects, the role of uncertainty in households' decision-making and the potential effect of simultaneous exogenous shocks (e.g., natural disasters).","729":"This paper investigates the cointegration between possible determinants of crude oil futures prices during the COVID-19 pandemic period. We perform comparative analysis of WTI and newly-launched Shanghai crude oil futures (SC) via the Autoregressive Distributed Lag (ARDL) model and Quantile Autoregressive Distributed Lag (QARDL) model. The empirical results confirm that economic policy uncertainty, stock markets, interest rates and coronavirus panic are important drivers of WTI futures prices. Our findings also suggest that the US and China's stock markets play vital roles in movements of SC futures prices. Meanwhile, CSI300 stock index has a significant positive short-run impact on SC futures prices while S\\&P500 prices possess a positive nexus with SC futures prices both in long-run and short-run. Overall, these empirical evidences provide practical implications for investors and policymakers.","730":"Cybercrime is continuously growing in numbers and becoming more sophisticated. Currently, there are various monetisation and money laundering methods, creating a huge, underground economy worldwide. A clear indicator of these activities is online marketplaces which allow cybercriminals to trade their stolen assets and services. While traditionally these marketplaces are available through the dark web, several of them have emerged in the surface web. In this work, we perform a longitudinal analysis of a surface web marketplace. The information was collected through targeted web scrapping that allowed us to identify hundreds of merchants' profiles for the most widely used surface web marketplaces. In this regard, we discuss the products traded in these markets, their prices, their availability, and the exchange currency. This analysis is performed in an automated way through a machine learning-based pipeline, allowing us to quickly and accurately extract the needed information. The outcomes of our analysis evince that illegal practices are leveraged in surface marketplaces and that there are not effective mechanisms towards their takedown at the time of writing.","731":"We consider online resource allocation under a typical non-profit setting, where limited or even scarce resources are administered by a not-for-profit organization like a government. We focus on the internal-equity by assuming that arriving requesters are homogeneous in terms of their external factors like demands but heterogeneous for their internal attributes like demographics. Specifically, we associate each arriving requester with one or several groups based on their demographics (i.e., race, gender, and age), and we aim to design an equitable distributing strategy such that every group of requesters can receive a fair share of resources proportional to a preset target ratio. We present two LP-based sampling algorithms and investigate them both theoretically (in terms of competitive-ratio analysis) and experimentally based on real COVID-19 vaccination data maintained by the Minnesota Department of Health. Both theoretical and numerical results show that our LP-based sampling strategies can effectively promote equity, especially when the arrival population is disproportionately represented, as observed in the early stage of the COVID-19 vaccine rollout.","732":"Vaccination hesitancy is a major obstacle to achieving and maintaining herd immunity. It is therefore of prime importance for public health authorities to understand the dynamics of an anti-vaccine opinion in the population. We introduce a novel mathematical model of opinion dynamics with spatial reinforcement, which can generate echo chambers, i.e. opinion bubbles in which information that is incompatible with one's entrenched worldview, is likely disregarded. In a first mathematical part, we scale the model both to a deterministic limit and to a weak-effects limit, and obtain bifurcations, phase transitions, and the invariant measure. In a second part, we fit our model to measles and meningococci vaccination coverage across 413 districts in Germany. We reveal that strong echo chambers explain the occurrence and persistence of the anti-vaccination opinion. We predict and compare the effectiveness of different policies aimed at influencing opinion dynamics in order to increase vaccination uptake.","733":"When a free falling ping-pong ball collides on a horizontal surface, it loses kinetic energy. The ratio between the height reached by the ball after the collision and the initial height is called restitution coefficient. A method to measure it by using a home-made cathetometer was proposed during the Olimpiadi di Fisica 2018. In this paper we show how to measure it also by using the PhyPhox app and Arduino board.","734":"University buildings are significant closed built environments for COVID-19 spreading. As universities prepare to re-start in-class activities, students' adherence to physical distancing requirements is a priority topic. While physical distancing in classrooms can be easily managed, the movement of students inside common spaces can pose higher risks due to individuals' proximity. This paper provides an experimental analysis of unidirectional student flow inside a case-study university building, by investigating students' movements and grouping behaviour according to physical distancing requirements. Results show general adherence with the minimum required physical distancing guidance, but some spaces, such as corridors, pose higher exposure than doorways. Their width, in combination with group behaviour, affects the students' capacity to keep the recommended distance. Furthermore, students report higher perceived vulnerability while moving along corridors. Evidence-based results can support decision-makers in understanding individuals' exposure in universities and researchers in developing behavioural models in preparation of future outbreaks and pandemics.","735":"The covid19 pandemic is a global emergency that badly impacted the economies of various countries. Covid19 hit India when the growth rate of the country was at the lowest in the last 10 years. To semantically analyze the impact of this pandemic on the economy, it is curial to have an ontology. CIDO ontology is a well standardized ontology that is specially designed to assess the impact of coronavirus disease and utilize its results for future decision forecasting for the government, industry experts, and professionals in the field of various domains like research, medical advancement, technical innovative adoptions, and so on. However, this ontology does not analyze the impact of the Covid19 pandemic on the Indian banking sector. On the other side, Covid19IBO ontology has been developed to analyze the impact of the Covid19 pandemic on the Indian banking sector but this ontology does not reflect complete information of Covid19 data. Resultantly, users cannot get all the relevant information about Covid19 and its impact on the Indian economy. This article aims to extend the CIDO ontology to show the impact of Covid19 on the Indian economy sector by reusing the concepts from other data sources. We also provide a simplified schema matching approach that detects the overlapping information among the ontologies. The experimental analysis proves that the proposed approach has reasonable results.","736":"Analysing patterns of engagement among citizen science participants can provide important insights into the organisation and practice of individual citizen science projects. In particular, methods from statistics and network science can be used to understand different types of user behaviour and user interactions to help the further implementation and organization of community efforts. Using publicly available data from the iNaturalist community and their yearly City Nature Challenges (CNC) from 2017-2020 as an example; we showcase computational methods to explore the spatio-temporal evolution of this citizen science community that typically interacts in a hybrid offline-online way. In particular, we investigate the user types present in the community along with their interactions, finding significant differences in usage-behavior on both the level of engagement and the types of community tasks\/roles and how they interact with the network of contributors. We expect that these computational analysis strategies will be useful to gain further understanding of other citizen science communities and projects.","737":"Crowd-sourcing is commonly adopted for dialog data collection. However, it is highly costly and time-consuming, and the collected data is limited in scale and topic coverage. In this paper, aiming to generate emotional support conversations, we propose exploiting large-scale pre-trained language models for data augmentation, and provide key findings in our pilot exploration. Our adopted approach leverages the 6B-parameter GPT-J model and utilizes publicly available dialog posts to trigger conversations on various topics. Then we construct AugESC, a machine-augmented dataset for emotional support conversation. It is two orders of magnitude larger than the original ESConv dataset in scale, covers more diverse topics, and is shown to be of high quality by human evaluation. Lastly, we demonstrate with interactive evaluation that AugESC can further enhance dialog models tuned on ESConv to handle various conversation topics and to provide significantly more effective emotional support.","738":"The pandemic of COVID-19 has caused millions of infections, which has led to a great loss all over the world, socially and economically. Due to the false-negative rate and the time-consuming of the conventional Reverse Transcription Polymerase Chain Reaction (RT-PCR) tests, diagnosing based on X-ray images and Computed Tomography (CT) images has been widely adopted. Therefore, researchers of the computer vision area have developed many automatic diagnosing models based on machine learning or deep learning to assist the radiologists and improve the diagnosing accuracy. In this paper, we present a review of these recently emerging automatic diagnosing models. 70 models proposed from February 14, 2020, to July 21, 2020, are involved. We analyzed the models from the perspective of preprocessing, feature extraction, classification, and evaluation. Based on the limitation of existing models, we pointed out that domain adaption in transfer learning and interpretability promotion would be the possible future directions.","739":"Against the backdrop of a social media reckoning, this paper seeks to demonstrate the potential of social tools to build virtuous behaviours online. We must assume that human behaviour is flawed, the truth can be elusive, and as communities we must commit to mechanisms to encourage virtuous social digital behaviours. Societies that use social platforms should be inclusive, responsive to evidence, limit punitive actions and allow productive discord and respectful disagreement. Social media success, we argue, is in the hypothesis. Documents are valuable to the degree that they are evidence in service of, or to challenge an idea for a purpose. We outline how a Bayesian social platform can facilitate virtuous behaviours to build evidence-based collective rationality. The chapter outlines the epistemic architecture of the platform's algorithms and user interface in conjunction with explicit community management to ensure psychological safety. The BetterBeliefs platform rewards users who demonstrate epistemically virtuous behaviours and exports evidence-based propositions for decision-making. A Bayesian social network can make virtuous ideas powerful.","740":"Two Higgs doublet models with an additional pseudoscalar particle coupling to the Standard Model and to a new stable, neutral particle, provide an attractive and fairly minimal route to solving the problem of Dark Matter. They have been the subject of several searches at the LHC. We study the impact of existing LHC measurements on such models, first in the benchmark regions addressed by searches and then after relaxing some of their assumptions and broadening the parameter ranges considered. In each case we study how the new parameters change the potentially visible signatures at the LHC, and identify which of these signatures should already have had a significant impact on existing measurements. This allows us to set some first constraints on a number of so far unstudied scenarios.","741":"We propose a realistic model for the evolution of the COVID-19 pandemic subject to the lockdown and quarantine measures, which takes into account the time-delay for recovery or death processes. The dynamic equations for the entire process are derived by adopting a kinetic-type reaction approach. More specifically, the lockdown and the quarantine measures are modelled by some kind of inhibitor reactions where susceptible and infected individuals can be trapped into inactive states. The dynamics for the recovered people is obtained by accounting people who are only traced back to hospitalised infected people. To get the evolution equation we take inspiration from the Michaelis- Menten enzyme-substrate reaction model (the so-called MM reaction) where the enzyme is associated to the available hospital beds, the substrate to the infected people, and the product to the recovered people, respectively. In other words, everything happens as if the hospitals beds act as a catalyser in the hospital recovery process. Of course, in our case the reverse MM reactions has no sense in our case and, consequently, the kinetic constant is equal to zero. Finally, the O.D.E.s for people tested positive to COVID-19 is simply modelled by the following kinetic scheme S+I=>2I with I=>R or I=>D, with S, I, R, and D denoting the compartments Susceptible, Infected, Recovered, and Deceased people, respectively. The resulting kinetic-type equation} provide the O.D.E.s, for elementary reaction steps, describing the number of the infected people, the total number of the recovered people previously hospitalised, subject to the lockdown and the quarantine measure, and the total number of deaths. The model foresees also the second wave of Infection by Coronavirus. The tests carried out on real data for Belgium, France and Germany confirmed the correctness of our model.","742":"Almost every country in the world is battling to limit the spread of COVID-19. As the world strives to get an effective medication to control the disease, appropriate intervention measures, for now, remains one of the effective methods to reduce the spread of the disease. Optimal control strategies have proven to be an effective method in curtailing the spread of infectious diseases. In this paper, a model has been formulated to study transmission dynamics of the disease. Basic properties of the model such as the basic reproduction number, equilibrium points and stability of the equilibrium points have been determined. Sensitivity analysis was carried on to determine the impact of the model parameters on the basic reproduction number. We also introduced a compartment for the deceased and examined the behaviour of COVID-19 related deaths. The numerical simulation prediction is consistent with real data from Ghana for the period March 2020 to March 2021. The simulation revealed the disease had less impact on the population during the first seven months of the outbreak. To help contain the spread of the disease, time dependent optimal controls were incorporated into the model and Pontryagin maximum principle was used to characterize vital conditions of the optimal control model. Numerical simulations of the optimal control model showed that, combination of optimal preventive strategies such as nose mask and vaccination are effective to significantly decrease the number of COVID-19 cases in different compartments of the model. Vaccination decreases the susceptibility to the disease whereas mask usage preserved the susceptible population from extinction.","743":"With the Covid-19 pandemic an urgent need to simulate social distancing arises. The Optimal Steps Model (OSM) is a pedestrian locomotion model that operationalizes an individual's need for personal space. We present new parameter values for personal space in the Optimal Steps Model to simulate social distancing in the pedestrian dynamics simulator Vadere. Our approach is pragmatic. We consider two use cases: in the first we demand that a set social distance must never be violated. In the second the social distance must be kept only on average. For each use case we conduct simulation studies in a typical bottleneck scenario and measure contact times, that is, violations of the social distance rule. We derive rules of thumb for suitable parameter choices in dependency of the desired social distance. We test the rules of thumb for the social distances 1.5m and 2.0m and observe that the new parameter values indeed lead to the desired social distancing. Thus, the rules of thumb will quickly enable Vadere users to conduct their own studies without understanding the intricacies of the OSM implementation and without extensive parameter adjustment.","744":"Provided widespread vaccination will bring the COVID-19 pandemic under full control worldwide, the contrast to climate change and the energy transition as one of its main actions will return at the top of national and international policy agendas. This paper employs multivariate diffusion models to investigate and quantitatively assess the competitive power of renewable energy technologies and their perspectives along the invoked energy transition. The study was conducted for the period 1965-2019 on a number of selected case studies, that were considered critically representative of the current transition process in view of their energy and political context. The dynamic relationship between renewable technologies and natural gas has been at the core of the analysis, trying to establish whether gas could be considered as a bridging technology or a lock-in. The main findings show that in all the analyzed countries RETs have exerted a strongly competitive effect towards gas. In most cases, gas is found to have a bridging role, aiding the uptake of renewables.","745":"In the era of big data, standard analysis tools may be inadequate for making inference and there is a growing need for more efficient and innovative ways to collect, process, analyze and interpret the massive and complex data. We provide an overview of challenges in big data problems and describe how innovative analytical methods, machine learning tools and metaheuristics can tackle general healthcare problems with a focus on the current pandemic. In particular, we give applications of modern digital technology, statistical methods, data platforms and data integration systems to improve diagnosis and treatment of diseases in clinical research and novel epidemiologic tools to tackle infection source problems, such as finding Patient Zero in the spread of epidemics. We make the case that analyzing and interpreting big data is a very challenging task that requires a multi-disciplinary effort to continuously create more effective methodologies and powerful tools to transfer data information into knowledge that enables informed decision making.","746":"This paper explores how well deep learning models trained on chest CT images can diagnose COVID-19 infected people in a fast and automated process. To this end, we adopt advanced deep network architectures and propose a transfer learning strategy using custom-sized input tailored for each deep architecture to achieve the best performance. We conduct extensive sets of experiments on two CT image datasets, namely the SARS-CoV-2 CT-scan and the COVID19-CT. The obtained results show superior performances for our models compared with previous studies, where our best models achieve average accuracy, precision, sensitivity, specificity and F1 score of 99.4%, 99.6%, 99.8%, 99.6% and 99.4% on the SARS-CoV-2 dataset; and 92.9%, 91.3%, 93.7%, 92.2% and 92.5% on the COVID19-CT dataset, respectively. Furthermore, we apply two visualization techniques to provide visual explanations for the models' predictions. The visualizations show well-separated clusters for CT images of COVID-19 from other lung diseases, and accurate localizations of the COVID-19 associated regions.","747":"Recently, there has been an increased attention towards innovating, enhancing, building, and deploying applications of speech signal processing for providing assistance and relief to human mankind from the Coronavirus (COVID-19) pandemic. Many AI with speech initiatives are taken to combat with the present situation and also to create a safe and secure environment for the future. This paper summarises all these efforts taken by the re-search community towards helping the individuals and the society in the fight against COVID-19 over the past 3-4 months using speech signal processing. We also summarise the deep techniques used in this direction to come up with capable solutions in a short span of time. This paper further gives an overview of the contributions from non-speech modalities that may complement or serve as inspiration for audio and speech analysis. In addition, we discuss our observations with respect to solution usability, challenges, and the significant technology achievements.","748":"One desired outcome of introductory physics instruction is that students will develop facility with reasoning quantitatively about physical phenomena. Little research has been done regarding how students develop the algebraic concepts and skills involved in reasoning productively about physics quantities, which is different from either understanding of physics concepts or problem-solving abilities. We introduce the Physics Inventory of Quantitative Literacy (PIQL) as a tool for measuring quantitative literacy, a foundation of mathematical reasoning, in the context of introductory physics. We present the development of the PIQL and evidence of its validity for use in calculus-based introductory physics courses. Unlike concept inventories, the PIQL is a reasoning inventory, and can be used to assess reasoning over the span of students' instruction in introductory physics. Although mathematical reasoning associated with the PIQL is taught in prior mathematics courses, pre\/post test scores reveal that this reasoning isn't readily used by most students in physics, nor does it develop as part of physics instruction--even in courses that use high-quality, research-based curricular materials. As has been the case with many inventories in physics education, we expect use of the PIQL to support the development of instructional strategies and materials--in this case, designed to meet the course objective that all students become quantitatively literate in introductory physics.","749":"We present FACESEC, a framework for fine-grained robustness evaluation of face recognition systems. FACESEC evaluation is performed along four dimensions of adversarial modeling: the nature of perturbation (e.g., pixel-level or face accessories), the attacker's system knowledge (about training data and learning architecture), goals (dodging or impersonation), and capability (tailored to individual inputs or across sets of these). We use FACESEC to study five face recognition systems in both closed-set and open-set settings, and to evaluate the state-of-the-art approach for defending against physically realizable attacks on these. We find that accurate knowledge of neural architecture is significantly more important than knowledge of the training data in black-box attacks. Moreover, we observe that open-set face recognition systems are more vulnerable than closed-set systems under different types of attacks. The efficacy of attacks for other threat model variations, however, appears highly dependent on both the nature of perturbation and the neural network architecture. For example, attacks that involve adversarial face masks are usually more potent, even against adversarially trained models, and the ArcFace architecture tends to be more robust than the others.","750":"Over the past decade, fake news and misinformation have turned into a major problem that has impacted different aspects of our lives, including politics and public health. Inspired by natural human behavior, we present an approach that automates the detection of fake news. Natural human behavior is to cross-check new information with reliable sources. We use Natural Language Processing (NLP) and build a machine learning (ML) model that automates the process of cross-checking new information with a set of predefined reliable sources. We implement this for Twitter and build a model that flags fake tweets. Specifically, for a given tweet, we use its text to find relevant news from reliable news agencies. We then train a Random Forest model that checks if the textual content of the tweet is aligned with the trusted news. If it is not, the tweet is classified as fake. This approach can be generally applied to any kind of information and is not limited to a specific news story or a category of information. Our implementation of this approach gives a $70\\%$ accuracy which outperforms other generic fake-news classification models. These results pave the way towards a more sensible and natural approach to fake news detection.","751":"Dripping, jetting and tip streaming have been studied up to a certain point separately by both fluid mechanics and microfluidics communities, the former focusing on fundamental aspects while the latter on applications. Here, we intend to review this field from a global perspective by considering and linking the two sides of the problem. In the first part, we present the theoretical model used to study interfacial flows arising in droplet-based microfluidics, paying attention to three elements commonly present in applications: viscoelasticity, electric fields and surfactants. We review both classical and current results about the stability of jets affected by these elements. Mechanisms leading to the breakup of jets to produce drops are reviewed as well, including some recent advances in this field. We also consider the relatively scarce theoretical studies on the emergence and stability of tip streaming flows. In the second part of this review, we focus on axisymmetric microfluidic configurations which can operate on the dripping and jetting modes either in a direct (standard) way or via tip streaming. We present the dimensionless parameters characterizing these configurations, the scaling laws which allow predicting the size of the resulting droplets and bubbles, as well as those delimiting the parameter windows where tip streaming can be found. Special attention is paid to electrospray and flow focusing, two of the techniques more frequently used in continuous drop production microfluidics. We aim to connect experimental observations described in this section of topics with fundamental and general aspects described in the first part of the review. This work closes with some prospects at both fundamental and practical levels.","752":"Fast and stable fluid simulations are an essential prerequisite for applications ranging from computer aided aerodynamic design of automobiles or airplanes to simulations of physical effects in CGI to research in meteorology. Recent differentiable fluid simulations allow gradient based methods to optimize e.g. fluid control systems in an informed manner. Solving the partial differential equations governed by the dynamics of the underlying physical systems, however, is a challenging task and current numerical approximation schemes still come at high computational costs. In this work, we propose an unsupervised framework that allows powerful deep neural networks to learn the dynamics of incompressible fluids end to end on a grid-based representation. For this purpose, we introduce a loss function that penalizes residuals of the incompressible Navier Stokes equations. After training, the framework yields models that are capable of fast and differentiable fluid simulations and can handle various fluid phenomena such as the Magnus effect and K\\'arm\\'an vortex streets. Besides demonstrating its real-time capability on a GPU, we exploit our approach in a control optimization scenario.","753":"We look at optimal liability-driven portfolios in a family of fat-tailed and extremal risk measures, especially in the context of pension fund and insurance fixed cashflow liability profiles, but also those arising in derivatives books such as delta one books or options books in the presence of stochastic volatilities. In the extremal limit, we recover a new tail risk measure, Extreme Deviation (XD), an extremal risk measure significantly more sensitive to extremal returns than CVaR. Resulting optimal portfolios optimize the return per unit of XD, with portfolio weights consisting of a liability hedging contribution, and a risk contribution seeking to generate positive risk-adjusted return. The resulting allocations are analyzed qualitatively and quantitatively in a number of different limits.","754":"In a recent paper, Jean Gaudart and colleagues studied the factors associated with the spatial heterogeneity of the first wave of COVID-19 in France. We make some critical comments on their work which may be useful for future, similar studies.","755":"Deep learning is gaining instant popularity in computer aided diagnosis of COVID-19. Due to the high sensitivity of Computed Tomography (CT) to this disease, CT-based COVID-19 detection with visual models is currently at the forefront of medical imaging research. Outcomes published in this direction are frequently claiming highly accurate detection under deep transfer learning. This is leading medical technologists to believe that deep transfer learning is the mainstream solution for the problem. However, our critical analysis of the literature reveals an alarming performance disparity between different published results. Hence, we conduct a systematic thorough investigation to analyze the effectiveness of deep transfer learning for COVID-19 detection with CT images. Exploring 14 state-of-the-art visual models with over 200 model training sessions, we conclusively establish that the published literature is frequently overestimating transfer learning performance for the problem, even in the prestigious scientific sources. The roots of overestimation trace back to inappropriate data curation. We also provide case studies that consider more realistic scenarios, and establish transparent baselines for the problem. We hope that our reproducible investigation will help in curbing hype-driven claims for the critical problem of COVID-19 diagnosis, and pave the way for a more transparent performance evaluation of techniques for CT-based COVID-19 detection.","756":"First identified in Wuhan, China, in December 2019, the outbreak of COVID-19 has been declared as a global emergency in January, and a pandemic in March 2020 by the World Health Organization (WHO). Along with this pandemic, we are also experiencing an\"infodemic\"of information with low credibility such as fake news and conspiracies. In this work, we present ReCOVery, a repository designed and constructed to facilitate the studies of combating such information regarding COVID-19. We first broadly search and investigate ~2,000 news publishers, from which 61 are identified with extreme [high or low] levels of credibility. By inheriting the credibility of the media on which they were published, a total of 2,029 news articles on coronavirus, published from January to May 2020, are collected in the repository, along with 140,820 tweets that reveal how these news articles are spread on the social network. The repository provides multimodal information of news articles on coronavirus, including textual, visual, temporal, and network information. The way that news credibility is obtained allows a trade-off between dataset scalability and label accuracy. Extensive experiments are conducted to present data statistics and distributions, as well as to provide baseline performances for predicting news credibility so that future methods can be directly compared. Our repository is available at http:\/\/coronavirus-fakenews.com, which will be timely updated.","757":"Contact tracing is an important instrument for national health services to fight epidemics. As part of the COVID-19 situation, many proposals have been made for scaling up contract tracing capacities with the help of smartphone applications, an important but highly critical endeavor due to the privacy risks involved in such solutions. Extending our previously expressed concern, we clearly articulate in this article, the functional and non-functional requirements that any solution has to meet, when striving to serve, not mere collections of individuals, but the whole of a nation, as required in face of such potentially dangerous epidemics. We present a critical information infrastructure, PriLock, a fully-open preliminary architecture proposal and design draft for privacy preserving contact tracing, which we believe can be constructed in a way to fulfill the former requirements. Our architecture leverages the existing regulated mobile communication infrastructure and builds upon the concept of\"checks and balances\", requiring a majority of independent players to agree to effect any operation on it, thus preventing abuse of the highly sensitive information that must be collected and processed for efficient contact tracing. This is enforced with a largely decentralised layout and highly resilient state-of-the-art technology, which we explain in the paper, finishing by giving a security, dependability and resilience analysis, showing how it meets the defined requirements, even while the infrastructure is under attack.","758":"In this paper I show, with a rich and systematized diet of examples, that many contra-classical logics can be presented as variants of FDE, obtained by modifying at least one of the truth or falsity conditions of some connective. Then I argue that using Dunn semantics provides a clear understanding of the source of contra-classicality, namely, connectives that have either the classical truth or the classical falsity condition of another connective. This requires a fine-grained analysis of the sorts of modifications that can be made to an evaluation condition, analysis which I offer here as well.","759":"Blood oxygen saturation (SpO$_2$) is an essential indicator of respiratory functionality and is receiving increasing attention during the COVID-19 pandemic. Clinical findings show that it is possible for COVID-19 patients to have significantly low SpO$_2$ before any obvious symptoms. The prevalence of cameras has motivated researchers to investigate methods for monitoring SpO$_2$ using videos. Most prior schemes involving smartphones are contact-based: They require a fingertip to cover the phone's camera and the nearby light source to capture re-emitted light from the illuminated tissue. In this paper, we propose the first convolutional neural network based noncontact SpO$_2$ estimation scheme using smartphone cameras. The scheme analyzes the videos of a participant's hand for physiological sensing, which is convenient and comfortable, and can protect their privacy and allow for keeping face masks on. We design our neural network architectures inspired by the optophysiological models for SpO$_2$ measurement and demonstrate the explainability by visualizing the weights for channel combination. Our proposed models outperform the state-of-the-art model that is designed for contact-based SpO$_2$ measurement, showing the potential of our proposed method to contribute to public health. We also analyze the impact of skin type and the side of a hand on SpO$_2$ estimation performance.","760":"With the ease of access to information, and its rapid dissemination over the internet (both velocity and volume), it has become challenging to filter out truthful information from fake ones. The research community is now faced with the task of automatic detection of fake news, which carries real-world socio-political impact. One such research contribution came in the form of the Constraint@AAA12021 Shared Task on COVID19 Fake News Detection in English. In this paper, we shed light on a novel method we proposed as a part of this shared task. Our team introduced an approach to combine topical distributions from Latent Dirichlet Allocation (LDA) with contextualized representations from XLNet. We also compared our method with existing baselines to show that XLNet + Topic Distributions outperforms other approaches by attaining an F1-score of 0.967.","761":"Contact tracing apps have become one of the main approaches to control and slow down the spread of COVID-19 and ease up lockdown measures. While these apps can be very effective in stopping the transmission chain and saving lives, their adoption remains under the expected critical mass. The public debate about contact tracing apps emphasizes general privacy reservations and is conducted at an expert level, but lacks the user perspective related to actual designs. To address this gap, we explore user preferences for contact tracing apps using market research techniques, and specifically conjoint analysis. Our main contributions are empirical insights into individual and group preferences, as well as insights for prescriptive design. While our results confirm the privacy-preserving design of most European contact tracing apps, they also provide a more nuanced understanding of acceptable features. Based on market simulation and variation analysis, we conclude that adding goal-congruent features will play an important role in fostering mass adoption.","762":"India accounts for 11% of maternal deaths globally where a woman dies in childbirth every fifteen minutes. Lack of access to preventive care information is a significant problem contributing to high maternal morbidity and mortality numbers, especially in low-income households. We work with ARMMAN, a non-profit based in India, to further the use of call-based information programs by early-on identifying women who might not engage on these programs that are proven to affect health parameters positively.We analyzed anonymized call-records of over 300,000 women registered in an awareness program created by ARMMAN that uses cellphone calls to regularly disseminate health related information. We built robust deep learning based models to predict short term and long term dropout risk from call logs and beneficiaries' demographic information. Our model performs 13% better than competitive baselines for short-term forecasting and 7% better for long term forecasting. We also discuss the applicability of this method in the real world through a pilot validation that uses our method to perform targeted interventions.","763":"We analyse the distribution and the flows between different types of employment (self-employment, temporary, and permanent), unemployment, education, and other types of inactivity, with particular focus on the duration of the school-to-work transition (STWT). The aim is to assess the impact of the COVID-19 pandemic in Italy on the careers of individuals aged 15-34. We find that the pandemic worsened an already concerning situation of higher unemployment and inactivity rates and significantly longer STWT duration compared to other EU countries, particularly for females and residents in the South of Italy. In the midst of the pandemic, individuals aged 20-29 were less in (permanent and temporary) employment and more in the NLFET (Neither in the Labour Force nor in Education or Training) state, particularly females and non Italian citizens. We also provide evidence of an increased propensity to return to schooling, but most importantly of a substantial prolongation of the STWT duration towards permanent employment, mostly for males and non Italian citizens. Our contribution lies in providing a rigorous estimation and analysis of the impact of COVID-19 on the carriers of young individuals in Italy, which has not yet been explored in the literature.","764":"The so-called block-term decomposition (BTD) tensor model, especially in its rank-$(L_r,L_r,1)$ version, has been recently receiving increasing attention due to its enhanced ability of representing systems and signals that are composed of \\emph{block} components of rank higher than one, a scenario encountered in numerous and diverse applications. Its uniqueness and approximation have thus been thoroughly studied. The challenging problem of estimating the BTD model structure, namely the number of block terms (rank) and their individual (block) ranks, is of crucial importance in practice and has only recently started to attract significant attention. In data-streaming scenarios and\/or big data applications, where the tensor dimension in one of its modes grows in time or can only be processed incrementally, it is essential to be able to perform model selection and computation in a recursive (incremental\/online) manner. To date there is only one such work in the literature concerning the (general rank-$(L,M,N)$) BTD model, which proposes an incremental method, however with the BTD rank and block ranks assumed to be a-priori known and time invariant. In this preprint, a novel approach to rank-$(L_r,L_r,1)$ BTD model selection and tracking is proposed, based on the idea of imposing column sparsity jointly on the factors and estimating the ranks as the numbers of factor columns of nonnegligible magnitude. An online method of the alternating iteratively reweighted least squares (IRLS) type is developed and shown to be computationally efficient and fast converging, also allowing the model ranks to change in time. Its time and memory efficiency are evaluated and favorably compared with those of the batch approach. Simulation results are reported that demonstrate the effectiveness of the proposed scheme in both selecting and tracking the correct BTD model.","765":"We report results from searches for anisotropic stochastic gravitational-wave backgrounds using data from the first three observing runs of the Advanced LIGO and Advanced Virgo detectors. For the first time, we include Virgo data in our analysis and run our search with a new efficient pipeline called {\\tt PyStoch} on data folded over one sidereal day. We use gravitational-wave radiometry (broadband and narrow band) to produce sky maps of stochastic gravitational-wave backgrounds and to search for gravitational waves from point sources. A spherical harmonic decomposition method is employed to look for gravitational-wave emission from spatially-extended sources. Neither technique found evidence of gravitational-wave signals. Hence we derive 95\\% confidence-level upper limit sky maps on the gravitational-wave energy flux from broadband point sources, ranging from $F_{\\alpha, \\Theta}<{\\rm (0.013 - 7.6)} \\times 10^{-8} {\\rm erg \\, cm^{-2} \\, s^{-1} \\, Hz^{-1}},$ and on the (normalized) gravitational-wave energy density spectrum from extended sources, ranging from $\\Omega_{\\alpha, \\Theta}<{\\rm (0.57 - 9.3)} \\times 10^{-9} \\, {\\rm sr^{-1}}$, depending on direction ($\\Theta$) and spectral index ($\\alpha$). These limits improve upon previous limits by factors of $2.9 - 3.5$. We also set 95\\% confidence level upper limits on the frequency-dependent strain amplitudes of quasimonochromatic gravitational waves coming from three interesting targets, Scorpius X-1, SN 1987A and the Galactic Center, with best upper limits range from $h_0<{\\rm (1.7-2.1)} \\times 10^{-25},$ a factor of $\\geq 2.0$ improvement compared to previous stochastic radiometer searches.","766":"In the context of the Covid-19 pandemic, many were quick to spread deceptive information. I investigate here how reasoning in Description Logics (DLs) can detect inconsistencies between trusted medical sources and not trusted ones. The not-trusted information comes in natural language (e.g.\"Covid-19 affects only the elderly\"). To automatically convert into DLs, I used the FRED converter. Reasoning in Description Logics is then performed with the Racer tool.","767":"Two-minute science (2-minute science) is a science communication project initiated and supported by early-career Greek astrophysicists. With this endeavor, which started in December 2020, we try to bridge the gap between the scientific community and the public. This project is based on the simple idea of writing short articles with an approximate reading time of two minutes. These articles cover several topics and their difficulty scales to cover a broad audience range, from young students to experienced adults. We support the idea of<<ask an expert>>in Astrophysics in Greece, where any reader can pose a question. We offer the appropriate answer either by writing it ourselves or by contacting the field experts from the Greek astronomical society. Furthermore, our previous science communication experience leads us to design educational activities for students and\/or adults based on pedagogical means. A successful one was an<<escape-zoom>>titled<<Escape to Other Worlds>>, a digital version of an escape room. Further activities are Astronomy workshops for teenagers, online talks to schools, and our participation in a scientific podcast to trigger the public interest in Astrophysics. We communicate this work through social media, where several thousands of people already follow our work.","768":"Early classification of time series has been extensively studied for minimizing class prediction delay in time-sensitive applications such as healthcare and finance. A primary task of an early classification approach is to classify an incomplete time series as soon as possible with some desired level of accuracy. Recent years have witnessed several approaches for early classification of time series. As most of the approaches have solved the early classification problem with different aspects, it becomes very important to make a thorough review of the existing solutions to know the current status of the area. These solutions have demonstrated reasonable performance in a wide range of applications including human activity recognition, gene expression based health diagnostic, industrial monitoring, and so on. In this paper, we present a systematic review of current literature on early classification approaches for both univariate and multivariate time series. We divide various existing approaches into four exclusive categories based on their proposed solution strategies. The four categories include prefix based, shapelet based, model based, and miscellaneous approaches. The authors also discuss the applications of early classification in many areas including industrial monitoring, intelligent transportation, and medical. Finally, we provide a quick summary of the current literature with future research directions.","769":"Air pollution has been on continuous rise with increase in industrialization in metropolitan cities of the world. Several measures including strict climate laws and reduction in the number of vehicles were implemented by several nations. The COVID-19 pandemic provided a great opportunity to understand the daily human activities effect on air pollution. Majority nations restricted industrial activities and vehicular traffic to a large extent as a measure to restrict COVID-19 spread. In this paper, we analyzed the impact of such COVID19-induced lockdown on the air quality of the city of New Delhi, India. We analyzed the average concentration of common gaseous pollutants viz. sulfur dioxide (SO$_2$), ozone (O$_3$), nitrogen dioxide (NO$_2$), and carbon monoxide (CO). These concentrations were obtained from the tropospheric column of Sentinel-5P (an earth observation satellite of European Space Agency) data. We observed that the city observed a significant drop in the level of atmospheric pollutant's concentration for all the major pollutants as a result of strict lockdown measures. Such findings are also validated with pollutant data obtained from ground-based monitoring stations. We observed that near-surface pollutant concentration dropped significantly by 50% for PM$_{2.5}$, 71.9% for NO$_2$, and 88% for CO, after the lockdown period. Such studies would pave the path for implementing future air pollution control measures by environmentalists.","770":"This record contains the proceedings of the 2020 Workshop on Assessing, Explaining, and Conveying Robot Proficiency for Human-Robot Teaming, which was held in conjunction with the 2020 ACM\/IEEE International Conference on Human-Robot Interaction (HRI). This workshop was originally scheduled to occur in Cambridge, UK on March 23, but was moved to a set of online talks due to the COVID-19 pandemic.","771":"Background and Objective:Currently, the whole world is facing a pandemic disease, novel Coronavirus also known as COVID-19, which spread in more than 200 countries with around 3.3 million active cases and 4.4 lakh deaths approximately. Due to rapid increase in number of cases and limited supply of testing kits, availability of alternative diagnostic method is necessary for containing the spread of COVID-19 cases at an early stage and reducing the death count. For making available an alternative diagnostic method, we proposed a deep neural network based diagnostic method which can be easily integrated with mobile devices for detection of COVID-19 and viral pneumonia using Chest X-rays (CXR) images. Methods:In this study, we have proposed a method named COVIDLite, which is a combination of white balance followed by Contrast Limited Adaptive Histogram Equalization (CLAHE) and depth-wise separable convolutional neural network (DSCNN). In this method, white balance followed by CLAHE is used as an image preprocessing step for enhancing the visibility of CXR images and DSCNN trained using sparse cross entropy is used for image classification with lesser parameters and significantly lighter in size, i.e., 8.4 MB without quantization. Results:The proposed COVIDLite method resulted in improved performance in comparison to vanilla DSCNN with no pre-processing. The proposed method achieved higher accuracy of 99.58% for binary classification, whereas 96.43% for multiclass classification and out-performed various state-of-the-art methods. Conclusion:Our proposed method, COVIDLite achieved exceptional results on various performance metrics. With detailed model interpretations, COVIDLite can assist radiologists in detecting COVID-19 patients from CXR images and can reduce the diagnosis time significantly.","772":"A precise measurement of the differential cross-sections $d\\sigma\/d\\Omega$ and the linearly polarized photon beam asymmetry $\\Sigma_3$ for Compton scattering on the proton below pion threshold has been performed with a tagged photon beam and almost $4\\pi$ detector at the Mainz Microtron. The incident photons were produced by the recently upgraded Glasgow-Mainz photon tagging facility and impinged on a cryogenic liquid hydrogen target, with the scattered photons detected in the Crystal Ball\/TAPS set-up. Using the highest statistics Compton scattering data ever measured on the proton along with two effective field theories (both covariant baryon and heavy-baryon) and one fixed-$t$ dispersion relation model, constraining the fits with the Baldin sum rule, we have obtained the proton electric and magnetic polarizabilities with unprecedented precision: \\begin{align*}&{}\\alpha_{E1} = 10.99 \\pm 0.16 \\pm 0.47 \\pm 0.17 \\pm 0.34&{}\\beta_{M1} = 3.14 \\pm 0.21 \\pm 0.24 \\pm 0.20 \\pm 0.35 \\end{align*} in units of $10^{-4}$\\,fm$^3$ where the errors are statistical, systematic, spin polarizability dependent and model dependent.","773":"Psychological safety has been postulated as a key factor for the success of agile software development teams, yet there is a lack of empirical studies investigating the role of psychological safety in this context. The present study examines how work design characteristics of software development teams (autonomy, task interdependence, and role clarity) influence psychological safety and, further, how psychological safety impacts team performance, either directly or indirectly through team reflexivity. We test our model using survey data from 236 team members in 43 software development teams in Norway. Our results show that autonomy boosts psychological safety in software teams, and that psychological safety again has a positive effect on team reflexivity and a direct effect on team performance.","774":"We introduce a model of the diffusion of an epidemic with demographically heterogeneous agents interacting socially on a spatially structured network. Contagion-risk averse agents respond behaviorally to the diffusion of the infections by limiting their social interactions. Schools and workplaces also respond by allowing students and employees to attend and work remotely. The spatial structure induces local herd immunities along socio-demographic dimensions, which significantly affect the dynamics of infections. We study several non-pharmaceutical interventions; e.g., i) lockdown rules, which set thresholds on the spread of the infection for the closing and reopening of economic activities; ii) neighborhood lockdowns, leveraging granular (neighborhood-level) information to improve the effectiveness public health policies; iii) selective lockdowns, which restrict social interactions by location (in the network) and by the demographic characteristics of the agents. Substantiating a\"Lucas critique\"argument, we assess the cost of naive discretionary policies ignoring agents and firms' behavioral responses.","775":"On March 10, 2020, Italy imposed a national lockdown to curtail the spread of COVID-19. Here we estimate that, fourteen days after the implementation of the strategy, the net reproduction number has dropped below the epidemic threshold - estimated range 0.4-0.7. Our findings provide a timeline of the effectiveness of the implemented lockdown, which is relevant for a large number of countries that followed Italy in enforcing similar measures.","776":"Systematic relations between multiple objects that occur in various fields can be represented as networks. Real-world networks typically exhibit complex topologies whose structural properties are key factors in characterizing and further exploring the networks themselves. Uncertainty, modelling procedures and measurement difficulties raise often insurmountable challenges in fully characterizing most of the known real-world networks; hence, the necessity to predict their unknown elements from the limited data currently available in order to estimate possible future relations and\/or to unveil unmeasurable relations. In this work, we propose a deep learning approach to this problem based on Graph Convolutional Networks for predicting networks while preserving their original structural properties. The study reveals that this method can preserve scale-free and small-world properties of complex networks when predicting their unknown parts, a feature lacked by the up-to-date conventional methods. An external validation realized by testing the approach on biological networks confirms the results, initially obtained on artificial data. Moreover, this process provides new insights into the retainability of network structure properties in network prediction. We anticipate that our work could inspire similar approaches in other research fields as well, where unknown mechanisms behind complex systems need to be revealed by combining machine-based and experiment-based methods.","777":"COVID-19 frequently provokes pneumonia, which can be diagnosed using imaging exams. Chest X-ray (CXR) is often useful because it is cheap, fast, widespread, and uses less radiation. Here, we demonstrate the impact of lung segmentation in COVID-19 identification using CXR images and evaluate which contents of the image influenced the most. Semantic segmentation was performed using a U-Net CNN architecture, and the classification using three CNN architectures (VGG, ResNet, and Inception). Explainable Artificial Intelligence techniques were employed to estimate the impact of segmentation. A three-classes database was composed: lung opacity (pneumonia), COVID-19, and normal. We assessed the impact of creating a CXR image database from different sources, and the COVID-19 generalization from one source to another. The segmentation achieved a Jaccard distance of 0.034 and a Dice coefficient of 0.982. The classification using segmented images achieved an F1-Score of 0.88 for the multi-class setup, and 0.83 for COVID-19 identification. In the cross-dataset scenario, we obtained an F1-Score of 0.74 and an area under the ROC curve of 0.9 for COVID-19 identification using segmented images. Experiments support the conclusion that even after segmentation, there is a strong bias introduced by underlying factors from different sources.","778":"In late 2019, COVID-19, a severe respiratory disease, emerged, and since then, the world has been facing a deadly pandemic caused by it. This ongoing pandemic has had a significant effect on different aspects of societies. The uncertainty around the number of daily cases made it difficult for decision-makers to control the outbreak. Deep Learning models have proved that they can come in handy in many real-world problems such as healthcare ones. However, they require a lot of data to learn the features properly and output an acceptable solution. Since COVID-19 has been a lately emerged disease, there was not much data available, especially in the first stage of the pandemic, and this shortage of data makes it challenging to design an optimized model. To overcome these problems, we first introduce a new dataset with augmented features and then forecast COVID-19 cases with a new approach, using an evolutionary neural architecture search with Binary Bat Algorithm (BBA) to generate an optimized deep recurrent network. Finally, to show our approach's effectiveness, we conducted a comparative study on Iran's COVID-19 daily cases. The results prove our approach's capability to generate an accurate deep architecture to forecast the pandemic cases, even in the early stages with limited data.","779":"Irrespective of the success of the deep learning-based mixed-domain transfer learning approach for solving various Natural Language Processing tasks, it does not lend a generalizable solution for detecting misinformation from COVID-19 social media data. Due to the inherent complexity of this type of data, caused by its dynamic (context evolves rapidly), nuanced (misinformation types are often ambiguous), and diverse (skewed, fine-grained, and overlapping categories) nature, it is imperative for an effective model to capture both the local and global context of the target domain. By conducting a systematic investigation, we show that: (i) the deep Transformer-based pre-trained models, utilized via the mixed-domain transfer learning, are only good at capturing the local context, thus exhibits poor generalization, and (ii) a combination of shallow network-based domain-specific models and convolutional neural networks can efficiently extract local as well as global context directly from the target data in a hierarchical fashion, enabling it to offer a more generalizable solution.","780":"Edge video analytics is becoming the solution to many safety and management tasks. Its wide deployment, however, must first address the tension between inference accuracy and resource (compute\/network) cost. This has led to the development of video analytics pipelines (VAPs), which reduce resource cost by combining DNN compression\/speedup techniques with video processing heuristics. Our measurement study on existing VAPs, however, shows that today's methods for evaluating VAPs are incomplete, often producing premature conclusions or ambiguous results. This is because each VAP's performance varies substantially across videos and time (even under the same scenario) and is sensitive to different subsets of video content characteristics. We argue that accurate VAP evaluation must first characterize the complex interaction between VAPs and video characteristics, which we refer to as VAP performance clarity. We design and implement Yoda, the first VAP benchmark to achieve performance clarity. Using primitive-based profiling and a carefully curated benchmark video set, Yoda builds a performance clarity profile for each VAP to precisely define its accuracy\/cost tradeoff and its relationship with video characteristics. We show that Yoda substantially improves VAP evaluations by (1) providing a comprehensive, transparent assessment of VAP performance and its dependencies on video characteristics; (2) explicitly identifying fine-grained VAP behaviors that were previously hidden by large performance variance; and (3) revealing strengths\/weaknesses among different VAPs and new design opportunities.","781":"The construction and application of knowledge graphs have seen a rapid increase across many disciplines in recent years. Additionally, the problem of uncovering relationships between developments in the COVID-19 pandemic and social media behavior is of great interest to researchers hoping to curb the spread of the disease. In this paper we present a knowledge graph constructed from COVID-19 related tweets in the Los Angeles area, supplemented with federal and state policy announcements and disease spread statistics. By incorporating dates, topics, and events as entities, we construct a knowledge graph that describes the connections between these useful information. We use natural language processing and change point analysis to extract tweet-topic, tweet-date, and event-date relations. Further analysis on the constructed knowledge graph provides insight into how tweets reflect public sentiments towards COVID-19 related topics and how changes in these sentiments correlate with real-world events.","782":"Introduced by Kiefer and Wolfowitz \\cite{KW56}, the nonparametric maximum likelihood estimator (NPMLE) is a widely used methodology for learning mixture odels and empirical Bayes estimation. Sidestepping the non-convexity in mixture likelihood, the NPMLE estimates the mixing distribution by maximizing the total likelihood over the space of probability measures, which can be viewed as an extreme form of overparameterization. In this paper we discover a surprising property of the NPMLE solution. Consider, for example, a Gaussian mixture model on the real line with a subgaussian mixing distribution. Leveraging complex-analytic techniques, we show that with high probability the NPMLE based on a sample of size $n$ has $O(\\log n)$ atoms (mass points), significantly improving the deterministic upper bound of $n$ due to Lindsay \\cite{lindsay1983geometry1}. Notably, any such Gaussian mixture is statistically indistinguishable from a finite one with $O(\\log n)$ components (and this is tight for certain mixtures). Thus, absent any explicit form of model selection, NPMLE automatically chooses the right model complexity, a property we term \\emph{self-regularization}. Extensions to other exponential families are given. As a statistical application, we show that this structural property can be harnessed to bootstrap existing Hellinger risk bound of the (parametric) MLE for finite Gaussian mixtures to the NPMLE for general Gaussian mixtures, recovering a result of Zhang \\cite{zhang2009generalized}.","783":"A question of global concern regarding the sustainable future of humankind stems from the effect due to aerosols on the global climate. The quantification of atmospheric aerosols and their relationship to climatic impacts are key to understanding the dynamics of climate forcing and to improve our knowledge about climate change. Due to its response to precipitation, temperature, topography and human activity, one of the most dynamical atmospheric regions is the atmospheric boundary layer (ABL): ABL aerosols have a sizable impact on the evolution of the radiative forcing of climate change, human health, food security, and, ultimately, on the local and global economy. The identification of ABL pattern behaviour requires constant monitoring and the application of instrumental and computational methods for its detection and analysis. Here, we show a new method for the retrieval of ABL top arising from light detection and ranging (LiDAR) signals, by training a convolutional neural network in a supervised manner; forcing it to learn how to retrieve such a dynamical parameter on real, non-ideal conditions and in a fully automated, unsupervised way. Our findings pave the way for a full integration of LiDAR elastic, inelastic, and depolarisation signal processing, and provide a novel approach for real-time quantitative sensing of aerosols.","784":"While environmental, social, and governance (ESG) trading activity has been a distinctive feature of financial markets, the debate if ESG scores can also convey information regarding a company's riskiness remains open. Regulatory authorities, such as the European Banking Authority (EBA), have acknowledged that ESG factors can contribute to risk. Therefore, it is important to model such risks and quantify what part of a company's riskiness can be attributed to the ESG scores. This paper aims to question whether ESG scores can be used to provide information on (tail) riskiness. By analyzing the (tail) dependence structure of companies with a range of ESG scores, that is within an ESG rating class, using high-dimensional vine copula modelling, we are able to show that risk can also depend on and be directly associated with a specific ESG rating class. Empirical findings on real-world data show positive not negligible ESG risks determined by ESG scores, especially during the 2008 crisis.","785":"We propose an end-to-end architecture for multivariate time-series prediction that integrates a spatial-temporal graph neural network with a matrix filtering module. This module generates filtered (inverse) correlation graphs from multivariate time series before inputting them into a GNN. In contrast with existing sparsification methods adopted in graph neural network, our model explicitly leverage time-series filtering to overcome the low signal-to-noise ratio typical of complex systems data. We present a set of experiments, where we predict future sales from a synthetic time-series sales dataset. The proposed spatial-temporal graph neural network displays superior performances with respect to baseline approaches, with no graphical information, and with fully connected, disconnected graphs and unfiltered graphs.","786":"These proceedings accompany the Belle II talk in the Diversity and Inclusion parallel session delivered during ICHEP 2020. This marks the first external presentation by the Belle II Collaboration, in which we present some of our data and self-reported statistics regarding diversity and inclusion. We also present Belle II's current and planned activities to aid and improve diversity and inclusion. We find that there is still a lot to be done to improve the social working environment and population representation within our collaboration and within high energy physics.","787":"Acute Respiratory Distress Syndrome (ARDS) is a very severe syndrome leading to respiratory failure and subsequent mortality. Sepsis is one of the leading causes of ARDS. Thus, extracellular bacteria play an important role in the pathophysiology of ARDS. Overactivated neutrophils are the major effector cells in ARDS. Thus, extracellular bacteria triggered TH17-like innate immunity with neutrophil activation might accounts for the etiology of ARDS. Here, microarray analysis was employed to describe TH17-like innate immunity-related cytokine including TGF-{\\beta} and IL-6 up-regulation in whole blood of ARDS patients. It was found that the innate TH17-related TLR1,2,4,5,8, HSP70, G-CSF, GM-CSF, complements, defensin, PMN chemokines, cathepsins, Fc receptors, NCFs, FOS, JunB, CEBPs, NFkB, and leukotriene B4 are all up-regulated. TGF-{\\beta} secreting Treg cells play important roles in lung fibrosis. Up-regulation of Treg associated STAT5B and TGF-{\\beta} with down-regulation of MHC genes, TCR genes, and co-stimulation molecule CD86 are noted. Key TH17 transcription factors, STAT3 and ROR{\\alpha}, are down-regulated. Thus, the full adaptive TH17 helper CD4 T cells may not be successfully triggered. Many fibrosis promoting genes are also up-regulated including MMP8, MMP9, FGF13, TIMP1, TIMP2, PLOD1, P4HB, P4HA1, PDGFC, HMMR, HS2ST1, CHSY1, and CSGALNACT. Failure to induce successful adaptive immunity could also attribute to ARDS pathogenesis. Thus, ARDS is actually a TH17-like and Treg immune disorder.","788":"Information management has enter a completely new era, quantum era. However, there exists a lack of sufficient theory to extract truly useful quantum information and transfer it to a form which is intuitive and straightforward for decision making. Therefore, based on the quantum model of mass function, a fortified dual check system is proposed to ensure the judgment generated retains enough high accuracy. Moreover, considering the situations in real life, everything takes place in an observable time interval, then the concept of time interval is introduced into the frame of the check system. The proposed model is very helpful in disposing uncertain quantum information in this paper. And some applications are provided to verify the rationality and correctness of the proposed method.","789":"The causal explanation of image misclassifications is an understudied niche, which can potentially provide valuable insights in model interpretability and increase prediction accuracy. This study trains CIFAR-10 on six modern CNN architectures, including VGG16, ResNet50, GoogLeNet, DenseNet161, MobileNet V2, and Inception V3, and explores the misclassification patterns using conditional confusion matrices and misclassification networks. Two causes are identified and qualitatively distinguished: morphological similarity and non-essential information interference. The former cause is not model dependent, whereas the latter is inconsistent across all six models. To reduce the misclassifications caused by non-essential information interference, this study erases the pixels within the bonding boxes anchored at the top 5% pixels of the saliency map. This method first verifies the cause; then by directly modifying the cause it reduces the misclassification. Future studies will focus on quantitatively differentiating the two causes of misclassifications, generalizing the anchor-box based inference modification method to reduce misclassification, exploring the interactions of the two causes in misclassifications.","790":"Multi-scale biomedical knowledge networks are expanding with emerging experimental technologies that generates multi-scale biomedical big data. Link prediction is increasingly used especially in bipartite biomedical networks to identify hidden biological interactions and relationshipts between key entities such as compounds, targets, gene and diseases. We propose a Graph Neural Networks (GNN) method, namely Graph Pair based Link Prediction model (GPLP), for predicting biomedical network links simply based on their topological interaction information. In GPLP, 1-hop subgraphs extracted from known network interaction matrix is learnt to predict missing links. To evaluate our method, three heterogeneous biomedical networks were used, i.e. Drug-Target Interaction network (DTI), Compound-Protein Interaction network (CPI) from NIH Tox21, and Compound-Virus Inhibition network (CVI). Our proposed GPLP method significantly outperforms over the state-of-the-art baselines. In addition, different network incompleteness is analysed with our devised protocol, and we also design an effective approach to improve the model robustness towards incomplete networks. Our method demonstrates the potential applications in other biomedical networks.","791":"Europe's contemporary political landscape has been shaped by massive shifts in recent decades caused by geopolitical upheavals such as Brexit and now, COVID-19. The way in which policy makers respond to the current pandemic could have large effects on how the world looks after the pandemic subsides. We aim to investigate complex questions post COVID-19 around the relationships and intersections concerning nationalism, religiosity, and anti-immigrant sentiment from a socio-cognitive perspective by applying a mixed-method approach (survey and modelling); in a context where unprecedented contagion threats have caused huge instability. There are still significant gaps in the scholarly literature on populism and nationalism. In particular, there is a lack of attention to the role of evolved human psychology in responding to persistent threats, which can fall into four broad categories in the literature: predation (threats to one's life via being eaten or killed in some other way), contagion (threats to one's life via physical infection), natural (threats to one's life via natural disasters), and social (threats to one's life by destroying social standing). These threats have been discussed in light of their effects on religion and other forms of behaviour, but they have not been employed to study nationalist and populist behaviours. In what follows, two studies are presented that begin to fill this gap in the literature. The first is a survey used to inform our theoretical framework and explore the different possible relationships in an online sample. The second is a study of a computer simulation. Both studies (completed in 2020) found very clear effects among the relevant variables, enabling us to identify trends that require further explanation and research as we move toward models that can adequately inform policy discussions.","792":"We investigate the evolution of epidemics over dynamical networks when nodes choose to interact with others in a selfish and decentralized manner. Specifically, we analyze the susceptible-asymptomatic-infected-recovered (SAIR) epidemic in the framework of activity-driven networks with heterogeneous node degrees and time-varying activation rates, and derive both individual and degree-based mean-field approximations of the exact state evolution. We then present a game-theoretic model where nodes choose their activation probabilities in a strategic manner using current state information as feedback, and characterize the quantal response equilibrium (QRE) of the proposed setting. We then consider the activity-driven susceptible-infected-susceptible (SIS) epidemic model, characterize equilibrium activation probabilities and analyze epidemic evolution in closed-loop. Our numerical results provide compelling insights into epidemic evolution under game-theoretic activation. Specifically, for the SAIR epidemic, we show that under suitable conditions, the epidemic can persist, as any decrease in infected proportion is counteracted by an increase in activity rates by the nodes. For the SIS epidemic, we show that in regimes where there is an endemic state, the infected proportion could be significantly smaller under game-theoretic activation if the loss upon infection is sufficiently high.","793":"Emerged in Wuhan city of China in December 2019, COVID-19 continues to spread rapidly across the world despite authorities having made available a number of vaccines. While the coronavirus has been around for a significant period of time, people and authorities still feel the need for awareness due to the mutating nature of the virus and therefore varying symptoms and prevention strategies. People and authorities resort to social media platforms the most to share awareness information and voice out their opinions due to their massive outreach in spreading the word in practically no time. People use a number of languages to communicate over social media platforms based on their familiarity, language outreach, and availability on social media platforms. The entire world has been hit by the coronavirus and India is the second worst-hit country in terms of the number of active coronavirus cases. India, being a multilingual country, offers a great opportunity to study the outreach of various languages that have been actively used across social media platforms. In this study, we aim to study the dataset related to COVID-19 collected in the period between February 2020 to July 2020 specifically for regional languages in India. This could be helpful for the Government of India, various state governments, NGOs, researchers, and policymakers in studying different issues related to the pandemic. We found that English has been the mode of communication in over 64% of tweets while as many as twelve regional languages in India account for approximately 4.77% of tweets.","794":"The environmental performance of shared micromobility services compared to private alternatives has never been assessed using an integrated modal Life Cycle Assessment (LCA) relying on field data. Such an LCA is conducted on three shared micromobility services in Paris - bikes, second-generation e-scooters, and e-mopeds - and their private alternatives. Global warming potential, primary energy consumption, and the three endpoint damages are calculated. Sensitivity analyses on vehicle lifespan, shipping, servicing distance, and electricity mix are conducted. Electric micromobility ranks between active modes and personal ICE modes. Its impacts are globally driven by vehicle manufacturing. Ownership does not affect directly the environmental performance: the vehicle lifetime mileage does. Assessing the sole carbon footprint leads to biased environmental decision-making, as it is not correlated to the three damages: multicriteria LCA is mandatory to preserve the planet. Finally, a major change of paradigm is needed to eco-design modern transportation policies.","795":"The relationship between epidemiology, mathematical modeling and computational tools allows to build and test theories on the development and battling of a disease. This PhD thesis is motivated by the study of epidemiological models applied to infectious diseases in an Optimal Control perspective, giving particular relevance to Dengue. Dengue is a subtropical and tropical disease transmitted by mosquitoes, that affects about 100 million people per year and is considered by the World Health Organization a major concern for public health. The mathematical models developed and tested in this work, are based on ordinary differential equations that describe the dynamics underlying the disease, including the interaction between humans and mosquitoes. An analytical study is made related to equilibrium points, their stability and basic reproduction number. The spreading of Dengue can be attenuated through measures to control the transmission vector, such as the use of specific insecticides and educational campaigns. Since the development of a potential vaccine has been a recent global bet, models based on the simulation of a hypothetical vaccination process in a population are proposed. Based on Optimal Control theory, we have analyzed the optimal strategies for using these controls, and respective impact on the reduction\/eradication of the disease during an outbreak in the population, considering a bioeconomic approach. The formulated problems are numerically solved using direct and indirect methods. The first discretize the problem turning it into a nonlinear optimization problem. Indirect methods use the Pontryagin Maximum Principle as a necessary condition to find the optimal curve for the respective control. In these two strategies several numerical software packages are used.","796":"The rapid outbreak of COVID-19 has caused humanity to come to a stand-still and brought with it a plethora of other problems. COVID-19 is the first pandemic in history when humanity is the most technologically advanced and relies heavily on social media platforms for connectivity and other benefits. Unfortunately, fake news and misinformation regarding this virus is also available to people and causing some massive problems. So, fighting this infodemic has become a significant challenge. We present our solution for the\"Constraint@AAAI2021 - COVID19 Fake News Detection in English\"challenge in this work. After extensive experimentation with numerous architectures and techniques, we use eight different transformer-based pre-trained models with additional layers to construct a stacking ensemble classifier and fine-tuned them for our purpose. We achieved 0.979906542 accuracy, 0.979913119 precision, 0.979906542 recall, and 0.979907901 f1-score on the test dataset of the competition.","797":"Suppressing SARS-CoV-2 will likely require the rapid identification and isolation of infected individuals, on an ongoing basis. RT-PCR (reverse transcription polymerase chain reaction) tests are accurate but costly, making regular testing of every individual expensive. The costs are a challenge for all countries and particularly for developing countries. Cost reductions can be achieved by combining samples and testing them in groups. We propose an algorithm for grouping subsamples, prior to testing, based on the geometry of a hypercube. At low prevalence, this testing procedure uniquely identifies infected individuals in a small number of tests. We discuss the optimal group size and explain why, given the highly infectious nature of the disease, parallel searches are preferred. We report proof of concept experiments in which a positive sample was detected even when diluted a hundred-fold with negative samples. Using these methods, the costs of mass testing could be reduced by a factor of ten to a hundred or more. If infected individuals are quickly and effectively quarantined, the prevalence will fall and so will the costs of regularly testing everyone. Such a strategy provides a possible pathway to the longterm elimination of SARS-CoV-2. Field trials of our approach are now under way in Rwanda and initial data from these are reported here.","798":"Influence competition finds its significance in many applications, such as marketing, politics and public events like COVID-19. Existing work tends to believe that the stronger influence will always win and dominate nearly the whole network, i.e.,\"winner takes all\". However, this finding somewhat contradicts with our common sense that many competing products are actually coexistent, e.g., Android vs. iOS. This contradiction naturally raises the question: will the winner take all? To answer this question, we make a comprehensive study into influence competition by identifying two factors frequently overlooked by prior art: (1) the incomplete observation of real diffusion networks; (2) the existence of information overload and its impact on user behaviors. To this end, we attempt to recover possible diffusion links based on user similarities, which are extracted by embedding users into a latent space. Following this, we further derive the condition under which users will be overloaded, and formulate the competing processes where users' behaviors differ before and after information overload. By establishing the explicit expressions of competing dynamics, we disclose that information overload acts as the critical\"boundary line\", before which the\"winner takes all\"phenomenon will definitively occur, whereas after information overload the share of influences gradually stabilizes and is jointly affected by their initial spreading conditions, influence powers and the advent of overload. Numerous experiments are conducted to validate our theoretical results where favorable agreement is found. Our work sheds light on the intrinsic driving forces behind real-world dynamics, thus providing useful insights into effective information engineering.","799":"There remains an urgent need to identify existing drugs that might be suitable for treating patients suffering from COVID-19 infection. Drugs rarely act at a single molecular target, with off target effects often being responsible for undesirable side effects and sometimes, beneficial synergy between targets for a specific illness. Off target activities have also led to blockbuster drugs in some cases, e.g. Viagra for erectile dysfunction and Minoxidil for male pattern hair loss. Drugs already in use or in clinical trials plus approved natural products constitute a rich resource for discovery of therapeutic agents that can be repurposed for existing and new conditions, based on the rationale that they have already been assessed for safety in man. A key question then is how to rapidly and efficiently screen such compounds for activity against new pandemic pathogens such as COVID-19. Here we show how a fast and robust computational process can be used to screen large libraries of drugs and natural compounds to identify those that may inhibit the main protease of SARS-Cov-2 (3CL pro, Mpro). We show how the resulting shortlist of candidates with strongest binding affinities is highly enriched in compounds that have been independently identified as potential antivirals against COVID-19. The top candidates also include a substantial number of drugs and natural products not previously identified as having potential COVID-19 activity, thereby providing additional targets for experimental validation. This in silico screening pipeline may also be useful for repurposing of existing drugs and discovery of new drug candidates against other medically important pathogens and for use in future pandemics.","800":"The negative effects of misinformation filter bubbles in adaptive systems have been known to researchers for some time. Several studies investigated, most prominently on YouTube, how fast a user can get into a misinformation filter bubble simply by selecting wrong choices from the items offered. Yet, no studies so far have investigated what it takes to burst the bubble, i.e., revert the bubble enclosure. We present a study in which pre-programmed agents (acting as YouTube users) delve into misinformation filter bubbles by watching misinformation promoting content (for various topics). Then, by watching misinformation debunking content, the agents try to burst the bubbles and reach more balanced recommendation mixes. We recorded the search results and recommendations, which the agents encountered, and analyzed them for the presence of misinformation. Our key finding is that bursting of a filter bubble is possible, albeit it manifests differently from topic to topic. Moreover, we observe that filter bubbles do not truly appear in some situations. We also draw a direct comparison with a previous study. Sadly, we did not find much improvements in misinformation occurrences, despite recent pledges by YouTube.","801":"We have learned to live with many potentially deadly viruses for which there is no vaccine, no immunity, and no cure. We do not live in constant fear of these viruses, instead, we have learned how to outsmart them and reduce the harm they cause. A new mathematical model that combines the spread of diseases that do not confer immunity together with the evolution of human behaviors indicates that we may be able to fight new diseases with the same type of strategy we use to fight viruses like HIV.","802":"Faster-than-Nyquist (FTN) signaling is a promising non-orthogonal pulse modulation technique that can improve the spectral efficiency (SE) of next generation communication systems at the expense of higher detection complexity to remove the introduced inter-symbol interference (ISI). In this paper, we investigate the detection problem of ultra high-order quadrature-amplitude modulation (QAM) FTN signaling where we exploit a mathematical programming technique based on the alternating directions multiplier method (ADMM). The proposed ADMM sequence estimation (ADMMSE) FTN signaling detector demonstrates an excellent trade-off between performance and computational effort enabling successful detection and SE gains for QAM modulation orders as high as 64K (65,536). The complexity of the proposed ADMMSE detector is polynomial in the length of the transmit symbols sequence and its sensitivity to the modulation order increases only logarithmically. Simulation results show that for 16-QAM, the proposed ADMMSE FTN signaling detector achieves comparable SE gains to the generalized approach semidefinite relaxation-based sequence estimation (GASDRSE) FTN signaling detector, but at an experimentally evaluated much lower computational time. Simulation results additionally show SE gains for modulation orders starting from 4-QAM, or quadrature phase shift keying (QPSK), up to and including 64K-QAM when compared to conventional Nyquist signaling. The very low computational effort required makes the proposed ADMMSE detector a practically promising FTN signaling detector for both low order and ultra high-order QAM FTN signaling systems.","803":"Quantum Information Technologies and a Practical Module is a new course we launch at Shanghai Jiao Tong University targeting at the undergraduate students who major in a variety of engineering disciplines. We develop a holistic curriculum for quantum computing covering the quantum hardware, quantum algorithms and applications. The quantum computing approaches include the universal digital quantum computing, analog quantum computing and the hybrid quantum-classical variational quantum computing that is tailored to the noisy intermediate-scale quantum (NISQ) technologies nowadays. Besides, we set a practical module to bring student closer to the real industry needs. The students would form a team of three to use any quantum approach to solve a problem in fields like optimization, finance, machine learning, chemistry and biology. Further, this course is selected into the Jiao Tong Global Virtual Classroom Initiative, so that it is open to global students in Association of Pacific Rim Universities at the same time with the offline students, in a specifically updated classroom. The efforts in curriculum development, practical module setting and blended learning make this course a good case study for education on quantum sciences and technologies.","804":"Analysis pipelines commonly use high-level technologies that are popular when created, but are unlikely to be readable, executable, or sustainable in the long term. A set of criteria is introduced to address this problem: Completeness (no execution requirement beyond a minimal Unix-like operating system, no administrator privileges, no network connection, and storage primarily in plain text); modular design; minimal complexity; scalability; verifiable inputs and outputs; version control; linking analysis with narrative; and free and open source software. As a proof of concept, we introduce\"Maneage\"(Managing data lineage), enabling cheap archiving, provenance extraction, and peer verification that has been tested in several research publications. We show that longevity is a realistic requirement that does not sacrifice immediate or short-term reproducibility. The caveats (with proposed solutions) are then discussed and we conclude with the benefits for the various stakeholders. This article is itself a Maneage'd project (project commit 54e4eb2).","805":"Many countries have passed their first COVID-19 epidemic peak. Traditional epidemiological models describe this as a result of non-pharmaceutical interventions that pushed the growth rate below the recovery rate. In this new phase of the pandemic many countries show an almost linear growth of confirmed cases for extended time-periods. This new containment regime is hard to explain by traditional models where infection numbers either grow explosively until herd immunity is reached, or the epidemic is completely suppressed (zero new cases). Here we offer an explanation of this puzzling observation based on the structure of contact networks. We show that for any given transmission rate there exists a critical number of social contacts, $D_c$, below which linear growth and low infection prevalence must occur. Above $D_c$ traditional epidemiological dynamics takes place, as e.g. in SIR-type models. When calibrating our corresponding model to empirical estimates of the transmission rate and the number of days being contagious, we find $D_c\\sim 7.2$. Assuming realistic contact networks with a degree of about 5, and assuming that lockdown measures would reduce that to household-size (about 2.5), we reproduce actual infection curves with a remarkable precision, without fitting or fine-tuning of parameters. In particular we compare the US and Austria, as examples for one country that initially did not impose measures and one that responded with a severe lockdown early on. Our findings question the applicability of standard compartmental models to describe the COVID-19 containment phase. The probability to observe linear growth in these is practically zero.","806":"Parental control apps, which are mobile apps that allow parents to monitor and restrict their children's activities online, are becoming increasingly adopted by parents as a means of safeguarding their children's online safety. However, it is not clear whether these apps are always beneficial or effective in what they aim to do; for instance, the overuse of restriction and surveillance has been found to undermine parent-child relationship and children's sense of autonomy. In this work, we investigate this gap, asking specifically: how might children's and parents' perceptions be related to how parental control features were designed? To investigate this question, we conducted an analysis of 58 top Android parental control apps designed for the purpose of promoting children's online safety, finding three major axes of variation in how key restriction and monitoring features were realised: granularity, feedback\/transparency, and parent-child communications support. To relate these axes to perceived benefits and problems, we then analysed 3264 app reviews to identify references to aspects of the each of the axes above, to understand children's and parents' views of how such dimensions related to their experiences with these apps. Our findings led towards 1) an understanding of how parental control apps realise their functionalities differently along three axes of variation, 2) an analysis of exactly the ways that such variation influences children's and parents' perceptions, respectively of the usefulness or effectiveness of these apps, and finally 3) an identification of design recommendations and opportunities for future apps by contextualising our findings within existing digital parenting theories.","807":"The introduction of COVID-19 lockdown measures and an outlook on return to normality are demanding societal changes. Among the most pressing questions is how individuals adjust to the pandemic. This paper examines the emotional responses to the pandemic in a repeated-measures design. Data (n=1698) were collected in April 2020 (during strict lockdown measures) and in April 2021 (when vaccination programmes gained traction). We asked participants to report their emotions and express these in text data. Statistical tests revealed an average trend towards better adjustment to the pandemic. However, clustering analyses suggested a more complex heterogeneous pattern with a well-coping and a resigning subgroup of participants. Linguistic computational analyses uncovered that topics and n-gram frequencies shifted towards attention to the vaccination programme and away from general worrying. Implications for public mental health efforts in identifying people at heightened risk are discussed. The dataset is made publicly available.","808":"We report findings from a case study of a large agile information systems development (ISD) organization`s sudden transformation to distributed, digital work in the context of the Covid-19 pandemic. It seeks to understand how knowledge creation and sharing changes. The findings show various forms of distance being introduced, digital tool usage, increased task orientation, and variations across teams. To analyze the findings, we use the concepts of large-scale collaborations and sociability. Large-scale collaboration offers a socio-technical perspective on tackling distributed knowledge sharing and creation in the presence of multiple, loosely coupled partners using digital tools for collaboration. We show what the digital tools afford using the concept of sociability. We discuss how distributed digital practices make teams more task-oriented and that creating and maintaining sociability, a key issue for knowledge sharing in agile ISD organizations, require relation oriented communication during practical problem solving using digital tools.","809":"The outbreak of novel coronavirus disease 2019 (COVID-19) has already infected millions of people and is still rapidly spreading all over the globe. Most COVID-19 patients suffer from lung infection, so one important diagnostic method is to screen chest radiography images, e.g., X-Ray or CT images. However, such examinations are time-consuming and labor-intensive, leading to limited diagnostic efficiency. To solve this issue, AI-based technologies, such as deep learning, have been used recently as effective computer-aided means to improve diagnostic efficiency. However, one practical and critical difficulty is the limited availability of annotated COVID-19 data, due to the prohibitive annotation costs and urgent work of doctors to fight against the pandemic. This makes the learning of deep diagnosis models very challenging. To address this, motivated by that typical pneumonia has similar characteristics with COVID-19 and many pneumonia datasets are publicly available, we propose to conduct domain knowledge adaptation from typical pneumonia to COVID-19. There are two main challenges: 1) the discrepancy of data distributions between domains; 2) the task difference between the diagnosis of typical pneumonia and COVID-19. To address them, we propose a new deep domain adaptation method for COVID-19 diagnosis, namely COVID-DA. Specifically, we alleviate the domain discrepancy via feature adversarial adaptation and handle the task difference issue via a novel classifier separation scheme. In this way, COVID-DA is able to diagnose COVID-19 effectively with only a small number of COVID-19 annotations. Extensive experiments verify the effectiveness of COVID-DA and its great potential for real-world applications.","810":"We propose a mechanism to allocate slots fairly at congested airports. This mechanism: (a) ensures that the slots are allocated according to the true valuations of airlines, (b) provides fair opportunities to the flights connecting remote cities to large airports, and (c) controls the number of flights in each slot to minimize congestion. The mechanism draws inspiration from economic theory. It allocates the slots based on an affine maximizer allocation rule and charges payments to the airlines such that they are incentivized to reveal their true valuations. The allocation also optimizes the occupancy of every slot to keep them as uncongested as possible. The formulation solves an optimal integral solution in strongly polynomial time. We conduct experiments on the data collected from two major airports in India. We also compare our results with existing allocations and also with the allocations based on the International Air Transport Association (IATA) guidelines. The computational results show that the social utility generated using our mechanism is 20-30% higher than IATA and current allocations.","811":"We address the diffusion of information about the COVID-19 with a massive data analysis on Twitter, Instagram, YouTube, Reddit and Gab. We analyze engagement and interest in the COVID-19 topic and provide a differential assessment on the evolution of the discourse on a global scale for each platform and their users. We fit information spreading with epidemic models characterizing the basic reproduction numbers $R_0$ for each social media platform. Moreover, we characterize information spreading from questionable sources, finding different volumes of misinformation in each platform. However, information from both reliable and questionable sources do not present different spreading patterns. Finally, we provide platform-dependent numerical estimates of rumors' amplification.","812":"Obesity is a complex disease and its prevalence depends on multiple factors related to the local socioeconomic, cultural and urban context of individuals. Many obesity prevention strategies and policies, however, are horizontal measures that do not depend on context-specific evidence. In this paper we present an overview of BigO (http:\/\/bigoprogram.eu), a system designed to collect objective behavioral data from children and adolescent populations as well as their environment in order to support public health authorities in formulating effective, context-specific policies and interventions addressing childhood obesity. We present an overview of the data acquisition, indicator extraction, data exploration and analysis components of the BigO system, as well as an account of its preliminary pilot application in 33 schools and 2 clinics in four European countries, involving over 4,200 participants.","813":"While algorithm audits are growing rapidly in commonality and public importance, relatively little scholarly work has gone toward synthesizing prior work and strategizing future research in the area. This systematic literature review aims to do just that, following PRISMA guidelines in a review of over 500 English articles that yielded 62 algorithm audit studies. The studies are synthesized and organized primarily by behavior (discrimination, distortion, exploitation, and misjudgement), with codes also provided for domain (e.g. search, vision, advertising, etc.), organization (e.g. Google, Facebook, Amazon, etc.), and audit method (e.g. sock puppet, direct scrape, crowdsourcing, etc.). The review shows how previous audit studies have exposed public-facing algorithms exhibiting problematic behavior, such as search algorithms culpable of distortion and advertising algorithms culpable of discrimination. Based on the studies reviewed, it also suggests some behaviors (e.g. discrimination on the basis of intersectional identities), domains (e.g. advertising algorithms), methods (e.g. code auditing), and organizations (e.g. Twitter, TikTok, LinkedIn) that call for future audit attention. The paper concludes by offering the common ingredients of successful audits, and discussing algorithm auditing in the context of broader research working toward algorithmic justice.","814":"In this work we describe a token-based solution to Contact Tracing via Distributed Point Functions (DPF) and, more generally, Function Secret Sharing (FSS). The key idea behind the solution is that FSS natively supports secure keyword search on raw sets of keywords without a need for processing the keyword sets via a data structure for set membership. Furthermore, the FSS functionality enables adding up numerical payloads associated with multiple matches without additional interaction. These features make FSS an attractive tool for lightweight privacy-preserving searching on a database of tokens belonging to infected individuals.","815":"We report discovery of new antiferromagnetic semimetal EuZnSb$_2$, obtained and studied in the form of single crystals. Electric resistivity, magnetic susceptibility and heat capacity indicate antiferromagnetic order of Eu with $T_N$ = 20 K. The effective moment of Eu$^{2+}$ inferred from the magnetization and specific heat measurement is 3.5 $\\mu_B$, smaller than the theoretical value of Eu$^{2+}$ due to presence of both Eu$^{3+}$ and Eu$^{2+}$. Magnetic field-dependent resistivity measurements suggest dominant quasi two dimensional Fermi surfaces whereas the first-principle calculations point to the presence of Dirac fermions. Therefore, EuZnSb$_2$ could represent the first platform to study the interplay of dynamical charge fluctuations, localized magnetic 4$f$ moments and Dirac states with Sb orbital character.","816":"Insufficient Social Cost of Carbon (SCC) estimation methods and short-term decision-making horizons have hindered the ability of carbon emitters to properly correct for the negative externalities of climate change, as well as the capacity of nations to balance economic and climate policy. To overcome these limitations, we introduce Retrospective Social Cost of Carbon Updating (ReSCCU), a novel mechanism that corrects for these limitations as empirically measured evidence is collected. To implement ReSCCU in the context of carbon taxation, we propose Retroactive Carbon Pricing (ReCaP), a market mechanism in which polluters offload the payment of ReSCCU adjustments to insurers. To alleviate systematic risks and minimize government involvement, we introduce the Private ReCaP (PReCaP) prediction market, which could see real-world implementation based on the engagement of a few high net-worth individuals or independent institutions.","817":"In this paper, we introduce a new control-theoretic paradigm for mitigating the spread of a virus. To this end, our discrete-time controller, aims to reduce the number of new daily deaths, and consequently, the cumulative number of deaths. In contrast to much of the existing literature, we do not rely on a potentially complex virus transmission model whose equations must be customized to the\"particulars\"of the pandemic at hand. For new viruses such as Covid-19, the epidemiology driving the modelling process may not be well known and model estimation with limited data may be unreliable. With this motivation in mind, the new paradigm described here is data-driven and, to a large extent, we avoid modelling difficulties by concentrating on just two key quantities which are common to pandemics: the doubling time, denoted by $d(k)$ and the peak day denoted by $\\theta(k)$. Our numerical studies to date suggest that our appealingly simple model can provide a reasonable fit to real data. Given that time is of the essence during the ongoing global health crisis, the intent of this paper is to introduce this new paradigm to control practitioners and describe a number of new research directions suggested by our current results.","818":"We discuss the criticality in the stochastic SIR model for infectious diseases. We adopt the path-integral formalism for the propagation of infections among susceptible, infectious, and removed individuals, and perform the perturbative and nonperturbative analyses to evaluate the critical value of the basic reproduction number ${\\cal R}$. In the perturbation theory, we calculate the mean values and the variances of the number of infectious individuals near the initial time, and find that the critical value ${\\cal R}_{\\text{c}}=1\/3$-$2\/3$ should be adopted in order to suppress the stochastic spread of infections sufficiently. In the nonperturbative approach, we derive the effective potential by integrating out the stochastic fluctuations, and obtain the effective Euler-Lagrange equations for the time-evolution of the numbers of susceptible, infectious, and removed individuals. From the asymptotic behaviors for a long time, we find that the critical value ${\\cal R}_{\\text{c}}=2\/3$ should be adopted for the sufficient convergence of infections. We also find that the endemic state can be generated dynamically by the stochastic fluctuation which is absent in the conventional SIR model. Those analyses show that the critical value of the basic reproduction number should be less than one, against the usually known critical value ${\\cal R}_{\\text{c}}=1$, when the stochastic fluctuations are taken into account in the SIR model.","819":"We quantify the impact of COVID-19 vaccination on psychological well-being using information from a large-scale panel survey representative of the UK population. Exploiting exogenous variation in the timing of vaccinations, we find that vaccination increases psychological well-being (GHQ-12) by 0.12 standard deviation, compensating for around one-half of the overall decrease caused by the pandemic. This effect persists for at least two months, and it is associated with a decrease in the perceived likelihood of contracting COVID-19 and higher engagement in social activities. The improvement is 1.5 times larger for mentally distressed individuals, supporting the prioritization of this group in vaccination roll-outs.","820":"News sharing on social networks reveals how information disseminates among users. This process, constrained by user preferences and social ties, plays a key role in the formation of public opinion. In this work we analyze news sharing behavior on Twitter as bipartite news-user networks for two consecutive years of Argentinian major media outlets. Analysis of news networks revealed modular structure driven by semantic similarity of news content and homophilic media interactions, producing a fragmented media agenda. In particular, the two largest communities present a persistently polarized political leaning, reflected in the consumption of ideologically homogeneous groups of media outlets. On the other hand, the user networks show modules driven by similar profiles of news consumption, where core individuals have less diversified profiles, suggesting that the observed polarization could arise from lack of diversity to media exposure.","821":"This article presents the potential use of the Self-Sovereign Identities (SSI), combining with Distributed Ledger Technologies (DLT), to improve the privacy and control of health data. The paper presents the SSI technology, lists the prominent use cases of decentralized identities in the health area, and discusses an effective blockchain-based architecture. The main contributions of the article are: (i) mapping SSI general and abstract concepts, e.g., issuers and holders, to the health domain concepts, e.g., physicians and patients; (ii) creating a correspondence between the SSI interactions, e.g., issue and verify a credential, and the US standardized set of health use cases; (iii) presenting and instantiating an architecture to deal with the use cases mentioned, effectively organizing the data in a user-centric way, that uses well-known SSI and Blockchain technologies.","822":"The rapid spreading of SARS-CoV-2 and its dramatic consequences, are forcing policymakers to take strict measures in order to keep the population safe. At the same time, societal and economical interactions are to be safeguarded. A wide spectrum of containment measures have been hence devised and implemented, in different countries and at different stages of the pandemic evolution. Mobility towards workplace or retails, public transit usage and permanence in residential areas constitute reliable tools to indirectly photograph the actual grade of the imposed containment protocols. In this paper, taking Italy as an example, we will develop and test a deep learning model which can forecast various spreading scenarios based on different mobility indices, at a regional level. We will show that containment measures contribute to\"flatten the curve\"and quantify the minimum time frame necessary for the imposed restrictions to result in a perceptible impact, depending on their associated grade.","823":"Medical imaging AI systems such as disease classification and segmentation are increasingly inspired and transformed from computer vision based AI systems. Although an array of adversarial training and\/or loss function based defense techniques have been developed and proved to be effective in computer vision, defending against adversarial attacks on medical images remains largely an uncharted territory due to the following unique challenges: 1) label scarcity in medical images significantly limits adversarial generalizability of the AI system; 2) vastly similar and dominant fore- and background in medical images make it hard samples for learning the discriminating features between different disease classes; and 3) crafted adversarial noises added to the entire medical image as opposed to the focused organ target can make clean and adversarial examples more discriminate than that between different disease classes. In this paper, we propose a novel robust medical imaging AI framework based on Semi-Supervised Adversarial Training (SSAT) and Unsupervised Adversarial Detection (UAD), followed by designing a new measure for assessing systems adversarial risk. We systematically demonstrate the advantages of our robust medical imaging AI system over the existing adversarial defense techniques under diverse real-world settings of adversarial attacks using a benchmark OCT imaging data set.","824":"The data for the Ebola outbreak that occurred in 2014-2016 in three countries of West Africa are analysed within a common framework. The analysis is made using the results of an agent based Susceptible-Infected-Removed (SIR) model on a Euclidean network, where nodes at a distance $l$ are connected with probability $P(l) \\propto l^{-\\delta }$, $\\delta$ determining the range of the interaction, in addition to nearest neighbors. The cumulative (total) density of infected population here has the form $R(t) = \\frac{a\\exp(t\/T)}{1+c\\exp(t\/T)}$, where the parameters depend on $\\delta$ and the infection probability $q$. This form is seen to fit well with the data. Using the best fitting parameters, the time at which the peak is reached is estimated and is shown to be consistent with the data. We also show that in the Euclidean model, one can choose $\\delta$ and $q$ values which reproduce the data for the three countries qualitatively. These choices are correlated with population density, control schemes and other factors. Comparing the real data and the results from the model one can also estimate the size of the actual population susceptible to the disease. Rescaling the real data a reasonably good quantitative agreement with the simulation results is obtained.","825":"As humanity struggles to contain the global COVID-19 pandemic, privacy concerns are emerging regarding confinement, tracing and testing. The scientific debate concerning privacy of the COVID-19 tracing efforts has been intense, especially focusing on the choice between centralised and decentralised tracing apps. The privacy concerns regarding COVID-19 testing, however, have not received as much attention even though the privacy at stake is arguably even higher. COVID-19 tests require the collection of samples. Those samples possibly contain viral material but inevitably also human DNA. Patient DNA is not necessary for the test but it is technically impossible to avoid collecting it. The unlawful preservation, or misuse, of such samples at a massive scale may hence disclose patient DNA information with far-reaching privacy consequences. Inspired by the cryptographic concept of\"Indistinguishability under Chosen Plaintext Attack\", this paper poses the blueprint of novel types of tests allowing to detect viral presence without leaving persisting traces of the patient's DNA. Authors are listed in alphabetical order.","826":"COVID-19 has created a global pandemic with high morbidity and mortality in 2020. Novel coronavirus (nCoV), also known as Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV2), is responsible for this deadly disease. International Committee on Taxonomy of Viruses (ICTV) has declared that nCoV is highly genetically similar to SARS-CoV epidemic in 2003 (89% similarity). Limited number of clinically validated Human-nCoV protein interaction data is available in the literature. With this hypothesis, the present work focuses on developing a computational model for nCoV-Human protein interaction network, using the experimentally validated SARS-CoV-Human protein interactions. Initially, level-1 and level-2 human spreader proteins are identified in SARS-CoV-Human interaction network, using Susceptible-Infected-Susceptible (SIS) model. These proteins are considered as potential human targets for nCoV bait proteins. A gene-ontology based fuzzy affinity function has been used to construct the nCoV-Human protein interaction network at 99.98% specificity threshold. This also identifies the level-1 human spreaders for COVID-19 in human protein-interaction network. Level-2 human spreaders are subsequently identified using the SIS model. The derived host-pathogen interaction network is finally validated using 7 potential FDA listed drugs for COVID-19 with significant overlap between the known drug target proteins and the identified spreader proteins.","827":"We present an image classifier based on the CheXNet and a transfer learning stage to classify chest X-Ray images according to three labels: COVID-19, viral pneumonia and normal. CheXNet is a DenseNet121 that has been trained twice, firstly on ImageNet and then, for classification of pneumonia and other 13 chest diseases, over a large chest X-Ray database (ChestX- ray14). The proposed network reached a test accuracy of 97.8% and, for the COVID-19 class, of 98.3%. In order to clarify the modus operandi of the network, we used Layer Wise Relevance Propagation (LRP) to generate heat maps, indicating an analytical path for future research on diagnosis.","828":"Influencing (and being influenced by) others through social networks is fundamental to all human societies. Whether this happens through the diffusion of rumors, opinions, or viruses, identifying the diffusion source (i.e., the person that initiated it) is a problem that has attracted much research interest. Nevertheless, existing literature has ignored the possibility that the source might strategically modify the network structure (by rewiring links or introducing fake nodes) to escape detection. Here, without restricting our analysis to any particular diffusion scenario, we close this gap by evaluating two mechanisms that hide the source-one stemming from the source's actions, the other from the network structure itself. This reveals that sources can easily escape detection, and that removing links is far more effective than introducing fake nodes. Thus, efforts should focus on exposing concealed ties rather than planted entities; such exposure would drastically improve our chances of detecting the diffusion source.","829":"We describe a formal approach based on graphical causal models to identify the\"root causes\"of the change in the probability distribution of variables. After factorizing the joint distribution into conditional distributions of each variable, given its parents (the\"causal mechanisms\"), we attribute the change to changes of these causal mechanisms. This attribution analysis accounts for the fact that mechanisms often change independently and sometimes only some of them change. Through simulations, we study the performance of our distribution change attribution method. We then present a real-world case study identifying the drivers of the difference in the income distribution between men and women.","830":"The novel COVID-19 pandemic is a current, major global health threat. Up till now, there is no fully approved pharmacological treatment or a vaccine. In this study, simple mathematical models were employed to examine the dynamics of transmission and control of COVID-19 taking into consideration social distancing and community awareness. Both situations of homogeneous and nonhomogeneous population were considered. Based on the calculations, a sufficient degree of social distancing based on its reproductive ratio is found to be effective in controlling COVID-19, even in the absence of a vaccine. With a vaccine, social distancing minimizes the sufficient vaccination rate to control the disease. Community awareness also has a great impact in eradicating the virus transmission. The model is simulated on small-world networks and the role of social distancing in controlling the infection is explained.","831":"In this work, we study high-dimensional mean estimation under user-level differential privacy, and attempt to design an $(\\epsilon,\\delta)$-differentially private mechanism using as few users as possible. In particular, we provide a nearly optimal trade-off between the number of users and the number of samples per user required for private mean estimation, even when the number of users is as low as $O(\\frac{1}{\\epsilon}\\log\\frac{1}{\\delta})$. Interestingly our bound $O(\\frac{1}{\\epsilon}\\log\\frac{1}{\\delta})$ on the number of users is independent of the dimension, unlike the previous work that depends polynomially on the dimension, solving a problem left open by Amin et al.~(ICML'2019). Our mechanism enjoys robustness up to the point that even if the information of $49\\%$ of the users are corrupted, our final estimation is still approximately accurate. Finally, our results also apply to a broader range of problems such as learning discrete distributions, stochastic convex optimization, empirical risk minimization, and a variant of stochastic gradient descent via a reduction to differentially private mean estimation.","832":"Since coronavirus has shown up, inaccessibility of legitimate clinical resources is at its peak, like the shortage of specialists, healthcare workers, lack of proper equipment and medicines. The entire medical fraternity is in distress, which results in numerous individuals demise. Due to unavailability, people started taking medication independently without appropriate consultation, making the health condition worse than usual. As of late, machine learning has been valuable in numerous applications, and there is an increase in innovative work for automation. This paper intends to present a drug recommender system that can drastically reduce specialists heap. In this research, we build a medicine recommendation system that uses patient reviews to predict the sentiment using various vectorization processes like Bow, TFIDF, Word2Vec, and Manual Feature Analysis, which can help recommend the top drug for a given disease by different classification algorithms. The predicted sentiments were evaluated by precision, recall, f1score, accuracy, and AUC score. The results show that classifier LinearSVC using TFIDF vectorization outperforms all other models with 93% accuracy.","833":"We analyze the global structure of the world-wide air transportation network, a critical infrastructure with an enormous impact on local, national, and international economies. We find that the world-wide air transportation network is a scale-free small-world network. In contrast to the prediction of scale-free network models, however, we find that the most connected cities are not necessarily the most central, resulting in anomalous values of the centrality. We demonstrate that these anomalies arise because of the multi-community structure of the network. We identify the communities in the air transportation network and show that the community structure cannot be explained solely based on geographical constraints, and that geo-political considerations have to be taken into account. We identify each city's global role based on its pattern of inter- and intra-community connections, which enables us to obtain scale-specific representations of the network.","834":"This paper proposes optimal lockdown management policies based on short-term prediction of active COVID-19 confirmed cases to ensure the availability of critical medical resources. The optimal time to start the lockdown from the current time is obtained after maximizing a cost function considering economic value subject to constraints of availability of medical resources, and maximum allowable value of daily growth rate and Test Positive Ratio. The estimated value of required medical resources is calculated as a function of total active cases. The predicted value of active cases is calculated using an adaptive short-term prediction model. The proposed approach can be easily implementable by a local authority. An optimal lockdown case study for Delhi during the second wave in the month of April 2021 is presented using the proposed formulation.","835":"This article surveys the use of algorithmic systems to support decision-making in the public sector. Governments adopt, procure, and use algorithmic systems to support their functions within several contexts -- including criminal justice, education, and benefits provision -- with important consequences for accountability, privacy, social inequity, and public participation in decision-making. We explore the social implications of municipal algorithmic systems across a variety of stages, including problem formulation, technology acquisition, deployment, and evaluation. We highlight several open questions that require further empirical research.","836":"Coronavirus disease 2019 (Covid-19) is highly contagious with limited treatment options. Early and accurate diagnosis of Covid-19 is crucial in reducing the spread of the disease and its accompanied mortality. Currently, detection by reverse transcriptase polymerase chain reaction (RT-PCR) is the gold standard of outpatient and inpatient detection of Covid-19. RT-PCR is a rapid method, however, its accuracy in detection is only ~70-75%. Another approved strategy is computed tomography (CT) imaging. CT imaging has a much higher sensitivity of ~80-98%, but similar accuracy of 70%. To enhance the accuracy of CT imaging detection, we developed an open-source set of algorithms called CovidCTNet that successfully differentiates Covid-19 from community-acquired pneumonia (CAP) and other lung diseases. CovidCTNet increases the accuracy of CT imaging detection to 90% compared to radiologists (70%). The model is designed to work with heterogeneous and small sample sizes independent of the CT imaging hardware. In order to facilitate the detection of Covid-19 globally and assist radiologists and physicians in the screening process, we are releasing all algorithms and parametric details in an open-source format. Open-source sharing of our CovidCTNet enables developers to rapidly improve and optimize services, while preserving user privacy and data ownership.","837":"With social media being a major force in information consumption, accelerated propagation of fake news has presented new challenges for platforms to distinguish between legitimate and fake news. Effective fake news detection is a non-trivial task due to the diverse nature of news domains and expensive annotation costs. In this work, we address the limitations of existing automated fake news detection models by incorporating auxiliary information (e.g., user comments and user-news interactions) into a novel reinforcement learning-based model called \\textbf{RE}inforced \\textbf{A}daptive \\textbf{L}earning \\textbf{F}ake \\textbf{N}ews \\textbf{D}etection (REAL-FND). REAL-FND exploits cross-domain and within-domain knowledge that makes it robust in a target domain, despite being trained in a different source domain. Extensive experiments on real-world datasets illustrate the effectiveness of the proposed model, especially when limited labeled data is available in the target domain.","838":"Due to the convenience of access-on-demand to information and business solutions, mobile apps have become an important asset in the digital world. In the context of the Covid-19 pandemic, app developers have joined the response effort in various ways by releasing apps that target different user bases (e.g., all citizens or journalists), offer different services (e.g., location tracking or diagnostic-aid), provide generic or specialized information, etc. While many apps have raised some concerns by spreading misinformation or even malware, the literature does not yet provide a clear landscape of the different apps that were developed. In this study, we focus on the Android ecosystem and investigate Covid-related Android apps. In a best-effort scenario, we attempt to systematically identify all relevant apps and study their characteristics with the objective to provide a first taxonomy of Covid-related apps, broadening the relevance beyond the implementation of contact tracing. Overall, our study yields a number of empirical insights that contribute to enlarge the knowledge on Covid-related apps: (1) Developer communities contributed rapidly to the Covid-19, with apps released as early as February 2020; (2) Covid-related apps deliver digital tools to users (e.g., health diaries), serve to broadcast information to users (e.g., spread statistics), and collect data from users (e.g., for tracing); (3) Covid-related apps are less complex than standard apps; (4) they generally do not seem to leak sensitive data; (5) in the majority of cases, Covid-related apps are released by entities with past experience on the market, mostly official government entities or public health organizations.","839":"Software engineering is not only about technical solutions. To a large extent, it is also concerned with organizational issues, project management, and human behavior. There are serious gender issues that can severely limit the participation of women in science and engineering careers. It is claimed that women lead differently than men and are more collaboration-oriented, communicative, and less aggressive than their male counterparts. However, when talking with women in technology companies' leadership roles, a list of problems women face grows fast. We invite women in software engineering management roles to answer the questions from an empathy map canvas. We used thematic analysis for coding the answers and group the codes into themes. From the analysis, we identified seven themes that support us to list the main challenges they face in their careers.","840":"Using gravitational wave observations to search for deviations from general relativity in the strong-gravity regime has become an important research direction. Chern Simons (CS) gravity is one of the most frequently studied parity-violating models of strong gravity. It is known that the Kerr black-hole is not a solution for CS gravity. At the same time, the only rotating solution available in the literature for dynamical CS (dCS) gravity is the slow-rotating case most accurately known to quadratic order in spin. In this work, for the slow-rotating case (accurate to first order in spin), we derive the linear perturbation equations governing the metric and the dCS field accurate to linear order in spin and quadratic order in the CS coupling parameter ($\\alpha$) and obtain the quasi-normal mode (QNM) frequencies. After confirming the recent results of Wagle et al. (2021), we find an additional contribution to the eigenfrequency correction at the leading perturbative order of $\\alpha^2$. Unlike Wagle et al., we also find corrections to frequencies in the polar sector. We compute these extra corrections by evaluating the expectation values of the perturbative potential on unperturbed QNM wavefunctions along a contour deformed into the complex-$r$ plane. For $\\alpha=0.1 M^2$, we obtain the ratio of the imaginary parts of the dCS correction to the GR correction in the first QNM frequency (in the polar sector) to be $0.263$ implying significant change. For the $(2,2)-$mode, the dCS corrections make the imaginary part of the first QNM of the fundamental mode less negative, thereby decreasing the decay rate. Our results, along with future gravitational wave observations, can be used to test for dCS gravity and further constrain the CS coupling parameters. [abridged]","841":"Triangulum, M33, is a low mass, relatively undisturbed spiral galaxy that offers a new regime in which to test models of dynamical heating. In spite of its proximity, the dynamical heating history of M33 has not yet been well constrained. In this work, we present the TREX Survey, the largest stellar spectroscopic survey across the disk of M33. We present the stellar disk kinematics as a function of age to study the past and ongoing dynamical heating of M33. We measure line of sight velocities for ~4,500 disk stars. Using a subset, we divide the stars into broad age bins using Hubble Space Telescope and Canada-France-Hawaii-Telescope photometric catalogs: massive main sequence stars and helium burning stars (~80 Myr), intermediate mass asymptotic branch stars (~1 Gyr), and low mass red giant branch stars (~4 Gyr). We compare the stellar disk dynamics to that of the gas using existing HI, CO, and Halpha kinematics. We find that the disk of M33 has relatively low velocity dispersion (~16 km\/s), and unlike in the Milky Way and Andromeda galaxies, there is no strong trend in velocity dispersion as a function of stellar age. The youngest disk stars are as dynamically hot as the oldest disk stars and are dynamically hotter than predicted by most M33 like low mass simulated analogs in Illustris. The velocity dispersion of the young stars is highly structured, with the large velocity dispersion fairly localized. The cause of this high velocity dispersion is not evident from the observations and simulated analogs presented here.","842":"For building successful Machine Learning (ML) systems, it is imperative to have high quality data and well tuned learning models. But how can one assess the quality of a given dataset? And how can the strengths and weaknesses of a model on a dataset be revealed? Our new tool PyHard employs a methodology known as Instance Space Analysis (ISA) to produce a hardness embedding of a dataset relating the predictive performance of multiple ML models to estimated instance hardness meta-features. This space is built so that observations are distributed linearly regarding how hard they are to classify. The user can visually interact with this embedding in multiple ways and obtain useful insights about data and algorithmic performance along the individual observations of the dataset. We show in a COVID prognosis dataset how this analysis supported the identification of pockets of hard observations that challenge ML models and are therefore worth closer inspection, and the delineation of regions of strengths and weaknesses of ML models.","843":"Finder networks in general, and Apple's Find My network in particular, can pose a grave threat to users' privacy and even health if these networks are abused for stalking. Apple's release of the AirTag, a very affordable tracker covered by the nearly ubiquitous Find My network, amplified this issue. While Apple provides a stalking detection feature within its ecosystem, billions of Android users are still left in the dark. Apple recently released the Android app\"Tracker Detect,\"which does not deliver a convincing feature set for stalking protection. We reverse engineer Apple's tracking protection in iOS and discuss its features regarding stalking detection. We design\"AirGuard\"and release it as an Android app to protect against abuse by Apple tracking devices. We compare the performance of our solution with the Apple-provided one in iOS and study the use of AirGuard in the wild over multiple weeks using data contributed by tens of thousands of active users.","844":"The world is currently witnessing dangerous shifts in the epidemic of emerging SARS-CoV-2, the causative agent of (COVID-19) coronavirus. The infection, and death numbers reported by World Health Organization (WHO) about this epidemic forecasts an increasing threats to the lives of people and the economics of countries. The greatest challenge that most governments are currently suffering from is the lack of a precise mechanism to detect unknown infected cases and predict the infection risk of COVID-19 virus. In response to mitigate this challenge, this study proposes a novel innovative approach for mitigating big challenges of (COVID-19) coronavirus propagation and contagion. This study propose a blockchain-based framework which investigate the possibility of utilizing peer-to peer, time stamping, and decentralized storage advantages of blockchain to build a new system for verifying and detecting the unknown infected cases of COVID-19 virus. Moreover, the proposed framework will enable the citizens to predict the infection risk of COVID-19 virus within conglomerates of people or within public places through a novel design of P2P-Mobile Application. The proposed approach is forecasted to produce an effective system able to support governments, health authorities, and citizens to take critical decision regarding the infection detection, infection prediction, and infection avoidance. The framework is currently being developed and implemented as a new system consists of four components, Infection Verifier Subsystem, Blockchain platform, P2P-Mobile Application, and Mass-Surveillance System. This four components work together for detecting the unknown infected cases and predicting and estimating the infection Risk of Corona Virus (COVID-19).","845":"Coronaviruses are enveloped, non-segmented positive-sense RNA viruses that have the largest genome among RNA viruses. The genome contains a large replicase ORF encodes nonstructural proteins (NSPs), structural and accessory genes. NSP15 is a nidoviral RNA uridylate-specific endoribonuclease (NendoU) has C-terminal catalytic domain. The endoribonuclease activity of NSP15 interferes with the innate immune response of the host. Here, we screened Selleckchem Natural product database of compounds against the NSP15, Thymopentin and Oleuropein showed highest binding energies. The binding of these molecules was further validated by Molecular dynamic simulation and found very stable complexes. These drugs might serve as effective counter molecules in the reduction of virulence of this virus. Future validation of both these inhibitors are worth consideration for patients being treated for COVID -19.","846":"Software development is information-dense knowledge work that requires collaboration with other developers and awareness of artifacts such as work items, pull requests, and files. With the speed of development increasing, information overload is a challenge for people developing and maintaining these systems. Finding information and people is difficult for software engineers, especially when they work in large software systems or have just recently joined a project. In this paper, we build a large scale data platform named Nalanda platform, which contains two subsystems: 1. A large scale socio-technical graph system, named Nalanda graph system 2. A large scale recommendation system, named Nalanda index system that aims at satisfying the information needs of software developers. The Nalanda graph is an enterprise scale graph with data from 6,500 repositories, with 37,410,706 nodes and 128,745,590 edges. On top of the Nalanda graph system, we built software analytics applications including a newsfeed named MyNalanda, and based on organic growth alone, it has Daily Active Users (DAU) of 290 and Monthly Active Users (MAU) of 590. A preliminary user study shows that 74% of developers and engineering managers surveyed are favorable toward continued use of the platform for information discovery. The Nalanda index system constitutes two indices: artifact index and expert index. It uses the socio-technical graph (Nalanda graph system) to rank the results and provide better recommendations to software developers. A large scale quantitative evaluation shows that the Nalanda index system provides recommendations with an accuracy of 78% for the top three recommendations.","847":"The increasing availability of curated citation data provides a wealth of resources for analyzing and understanding the intellectual influence of scientific publications. In the field of statistics, current studies of citation data have mostly focused on the interactions between statistical journals and papers, limiting the measure of influence to mainly within statistics itself. In this paper, we take the first step towards understanding the impact statistics has made on other scientific fields in the era of Big Data. By collecting comprehensive bibliometric data from the Web of Science database for selected statistical journals, we investigate the citation trends and compositions of citing fields over time to show that their diversity has been increasing. Furthermore, we use the local clustering technique involving personalized PageRank with conductance for size selection to find the most relevant statistical research area for a given external topic of interest. We provide theoretical guarantees for the procedure and, through a number of case studies, show the results from our citation data align well with our knowledge and intuition about these external topics. Overall, we have found that the statistical theory and methods recently invented by the statistics community have made increasing impact on other scientific fields.","848":"We demonstrate an approach to replicate and forecast the spread of the SARS-CoV-2 (COVID-19) pandemic using the toolkit of probabilistic programming languages (PPLs). Our goal is to study the impact of various modeling assumptions and motivate policy interventions enacted to limit the spread of infectious diseases. Using existing compartmental models we show how to use inference in PPLs to obtain posterior estimates for disease parameters. We improve popular existing models to reflect practical considerations such as the under-reporting of the true number of COVID-19 cases and motivate the need to model policy interventions for real-world data. We design an SEI3RD model as a reusable template and demonstrate its flexibility in comparison to other models. We also provide a greedy algorithm that selects the optimal series of policy interventions that are likely to control the infected population subject to provided constraints. We work within a simple, modular, and reproducible framework to enable immediate cross-domain access to the state-of-the-art in probabilistic inference with emphasis on policy interventions. We are not epidemiologists; the sole aim of this study is to serve as an exposition of methods, not to directly infer the real-world impact of policy-making for COVID-19.","849":"Recent work in fair machine learning has proposed dozens of technical definitions of algorithmic fairness and methods for enforcing these definitions. However, we still lack an understanding of how to develop machine learning systems with fairness criteria that reflect relevant stakeholders' nuanced viewpoints in real-world contexts. To address this gap, we propose a framework for eliciting stakeholders' subjective fairness notions. Combining a user interface that allows stakeholders to examine the data and the algorithm's predictions with an interview protocol to probe stakeholders' thoughts while they are interacting with the interface, we can identify stakeholders' fairness beliefs and principles. We conduct a user study to evaluate our framework in the setting of a child maltreatment predictive system. Our evaluations show that the framework allows stakeholders to comprehensively convey their fairness viewpoints. We also discuss how our results can inform the design of predictive systems.","850":"In this work, the problem of localizing ground devices (GDs) is studied comparing the performance of four range-free (RF) localization algorithms that use a mobile anchor (MA). All the investigated algorithms are based on the so-called heard\/not-heard (HnH) method, which allows the GDs to detect the MA at the border of their antenna communication radius. Despite the simplicity of this method, its efficacy in terms of accuracy is poor because it relies on the antenna radius that continuously varies under different conditions. Usually, the antenna radius declared by the manufacturer does not fully characterize the actual antenna radiation pattern. In this paper, the radiation pattern of the commercial DecaWave DWM1001 Ultra-Wide-Band (UWB) antennas is observed in a real test-bed at different altitudes for collecting more information and insights on the antenna radius. The compared algorithms are then tested using both the observed and the manufacturer radii. The experimental accuracy is close to the expected theoretical one only when the antenna pattern is actually omnidirectional. However, typical antennas have strong pattern irregularities that decrease the accuracy. For improving the performance, we propose range-based (RB) variants of the compared algorithms in which, instead of using the observed or the manufacturer radii, the actual measured distances between the MA and the GD are used. The localization accuracy tremendously improves confirming that the knowledge of the exact antenna pattern is essential for any RF algorithm.","851":"We are presenting COVID-19Base, a knowledgebase highlighting the biomedical entities related to COVID-19 disease based on literature mining. To develop COVID-19Base, we mine the information from publicly available scientific literature and related public resources. We considered seven topic-specific dictionaries, including human genes, human miRNAs, human lncRNAs, diseases, Protein Databank, drugs, and drug side effects, are integrated to mine all scientific evidence related to COVID-19. We have employed an automated literature mining and labeling system through a novel approach to measure the effectiveness of drugs against diseases based on natural language processing, sentiment analysis, and deep learning. To the best of our knowledge, this is the first knowledgebase dedicated to COVID-19, which integrates such large variety of related biomedical entities through literature mining. Proper investigation of the mined biomedical entities along with the identified interactions among those, reported in COVID-19Base, would help the research community to discover possible ways for the therapeutic treatment of COVID-19.","852":"Artificial intelligence (AI) provides a promising substitution for streamlining COVID-19 diagnoses. However, concerns surrounding security and trustworthiness impede the collection of large-scale representative medical data, posing a considerable challenge for training a well-generalised model in clinical practices. To address this, we launch the Unified CT-COVID AI Diagnostic Initiative (UCADI), where the AI model can be distributedly trained and independently executed at each host institution under a federated learning framework (FL) without data sharing. Here we show that our FL model outperformed all the local models by a large yield (test sensitivity \/specificity in China: 0.973\/0.951, in the UK: 0.730\/0.942), achieving comparable performance with a panel of professional radiologists. We further evaluated the model on the hold-out (collected from another two hospitals leaving out the FL) and heterogeneous (acquired with contrast materials) data, provided visual explanations for decisions made by the model, and analysed the trade-offs between the model performance and the communication costs in the federated training process. Our study is based on 9,573 chest computed tomography scans (CTs) from 3,336 patients collected from 23 hospitals located in China and the UK. Collectively, our work advanced the prospects of utilising federated learning for privacy-preserving AI in digital health.","853":"Modern intelligent urban mobility applications are underpinned by large-scale, multivariate, spatiotemporal data streams. Working with this data presents unique challenges of data management, processing and presentation that is often overlooked by researchers. Therefore, in this work we present an integrated data management and processing framework for intelligent urban mobility systems currently in use by our partner transit agencies. We discuss the available data sources and outline our cloud-centric data management and stream processing architecture built upon open-source publish-subscribe and NoSQL data stores. We then describe our data-integrity monitoring methods. We then present a set of visualization dashboards designed for our transit agency partners. Lastly, we discuss how these tools are currently being used for AI-driven urban mobility applications that use these tools.","854":"Twitter has become a vital social media platform while an ample amount of malicious Twitter bots exist and induce undesirable social effects. Successful Twitter bot detection proposals are generally supervised, which rely heavily on large-scale datasets. However, existing benchmarks generally suffer from low levels of user diversity, limited user information and data scarcity. Therefore, these datasets are not sufficient to train and stably benchmark bot detection measures. To alleviate these problems, we present TwiBot-20, a massive Twitter bot detection benchmark, which contains 229,573 users, 33,488,192 tweets, 8,723,736 user property items and 455,958 follow relationships. TwiBot-20 covers diversified bots and genuine users to better represent the real-world Twittersphere. TwiBot-20 also includes three modals of user information to support both binary classification of single users and community-aware approaches. To the best of our knowledge, TwiBot-20 is the largest Twitter bot detection benchmark to date. We reproduce competitive bot detection methods and conduct a thorough evaluation on TwiBot-20 and two other public datasets. Experiment results demonstrate that existing bot detection measures fail to match their previously claimed performance on TwiBot-20, which suggests that Twitter bot detection remains a challenging task and requires further research efforts.","855":"Precisely defining the terminology is the first step in scientific communication. Developing neural text generation models for definition generation can circumvent the labor-intensity curation, further accelerating scientific discovery. Unfortunately, the lack of large-scale terminology definition dataset hinders the process toward definition generation. In this paper, we present a large-scale terminology definition dataset Graphine covering 2,010,648 terminology definition pairs, spanning 227 biomedical subdisciplines. Terminologies in each subdiscipline further form a directed acyclic graph, opening up new avenues for developing graph-aware text generation models. We then proposed a novel graph-aware definition generation model Graphex that integrates transformer with graph neural network. Our model outperforms existing text generation models by exploiting the graph structure of terminologies. We further demonstrated how Graphine can be used to evaluate pretrained language models, compare graph representation learning methods and predict sentence granularity. We envision Graphine to be a unique resource for definition generation and many other NLP tasks in biomedicine.","856":"In this work we look at several mathematical models that have been constructed during the present pandemic to address dfferent issues of importance to public health policies about epidemic scenarios and thier causes. We start by briefly reviewing the most basic properties of the classic Kermack-McKendrick models and then proceed to look at some generalizations and their applications to contact structures, cocirculation of viral infections, growth patterns of epidemic curves, characterization of probability distributions and passage times necessary to parametrize the compartments that are added to the basic Kermack-McKendrick model. All of these examples revolve around the idea that a system of differential equations constructed for an specifc epidemiological problem, has as central and main theoretical and conceptual support the epidemiological, medical and biological context that motivates its construction and analysis. Fitting differential models to data without a sound perspective on the biological feasibility of the mathematical model will provide an statistical significant fit but will also fall short of providing useful information for the management, mitigation and eventual control of the epidemic of SARS-CoV-2.","857":"Optimization of vaccine allocations among different segments of a heterogeneous population is important for enhancing the effectiveness of vaccination campaigns in reducing the burden of epidemics. Intuitively, it would seem that allocations designed to minimize infections should prioritize those with the highest risk of being infected and infecting others. This prescription is well supported by vaccination theory, e.g., when the vaccination campaign aims to reach herd immunity. In this work, we show, however, that for vaccines providing partial protection (leaky vaccines) and for sufficiently high values of the basic reproduction number, intuition is overturned: the optimal allocation for minimizing the number of infections prioritizes the vaccination of those who are least likely to be infected. Furthermore, we show that this phenomenon occurs at a range of basic reproduction numbers relevant for the currently circulating strains of SARS-CoV-19. The work combines numerical investigations, asymptotic analysis for a general model, and complete mathematical analysis in a simple two-group model. The results point to important considerations in managing vaccination campaigns for infections with high transmissibility.","858":"Learning visual concepts from raw images without strong supervision is a challenging task. In this work, we show the advantages of prototype representations for understanding and revising the latent space of neural concept learners. For this purpose, we introduce interactive Concept Swapping Networks (iCSNs), a novel framework for learning concept-grounded representations via weak supervision and implicit prototype representations. iCSNs learn to bind conceptual information to specific prototype slots by swapping the latent representations of paired images. This semantically grounded and discrete latent space facilitates human understanding and human-machine interaction. We support this claim by conducting experiments on our novel data set\"Elementary Concept Reasoning\"(ECR), focusing on visual concepts shared by geometric objects.","859":"We show that the nonstandard limiting distribution of HAR test statistics under fixed-b asymptotics is not pivotal (even after studentization) when the data are nonstationarity. It takes the form of a complicated function of Gaussian processes and depends on the integrated local long-run variance and on on the second moments of the relevant series (e.g., of the regressors and errors for the case of the linear regression model). Hence, existing fixed-b inference methods based on stationarity are not theoretically valid in general. The nuisance parameters entering the fixed-b limiting distribution can be consistently estimated under small-b asymptotics but only with nonparametric rate of convergence. Hence, We show that the error in rejection probability (ERP) is an order of magnitude larger than that under stationarity and is also larger than that of HAR tests based on HAC estimators under conventional asymptotics. These theoretical results reconcile with recent finite-sample evidence in Casini (2021) and Casini, Deng and Perron (2021) who showing that fixed-b HAR tests can perform poorly when the data are nonstationary. They can be conservative under the null hypothesis and have non-monotonic power under the alternative hypothesis irrespective of how large the sample size is.","860":"Towards the end of 2019, Wuhan experienced an outbreak of novel coronavirus, which soon spread all over the world, resulting in a deadly pandemic that infected millions of people around the globe. The government and public health agencies followed many strategies to counter the fatal virus. However, the virus severely affected the social and economic lives of the people. In this paper, we extract and study the opinion of people from the top five worst affected countries by the virus, namely USA, Brazil, India, Russia, and South Africa. We propose a deep language-independent Multilevel Attention-based Conv-BiGRU network (MACBiG-Net), which includes embedding layer, word-level encoded attention, and sentence-level encoded attention mechanism to extract the positive, negative, and neutral sentiments. The embedding layer encodes the sentence sequence into a real-valued vector. The word-level and sentence-level encoding is performed by a 1D Conv-BiGRU based mechanism, followed by word-level and sentence-level attention, respectively. We further develop a COVID-19 Sentiment Dataset by crawling the tweets from Twitter. Extensive experiments on our proposed dataset demonstrate the effectiveness of the proposed MACBiG-Net. Also, attention-weights visualization and in-depth results analysis shows that the proposed network has effectively captured the sentiments of the people.","861":"This paper presents a project, named VIDAR-19, able to extract automatically diseases from the CORD-19 dataset, and also diseases which might be considered as risk factors. The project relies on the ICD-11 classification of diseases maintained by the WHO. This nomenclature is used as a data source of the extraction mechanism, and also as the repository for the results. Developed for the COVID-19, the project has the ability to extract diseases at risk and to calculate relevant indicators. The outcome of the project is presented in a dashboard which enables the user to explore graphically diseases at risk which are put back in the classification hierarchy. Beyond the COVID-19, VIDAR has much broader applications and might be directly used for any corpus dealing with other pathologies.","862":"Fighting the ongoing COVID-19 infodemic has been declared as one of the most important focus areas by the World Health Organization since the onset of the COVID-19 pandemic. While the information that is consumed and disseminated consists of promoting fake cures, rumors, and conspiracy theories to spreading xenophobia and panic, at the same time there is information (e.g., containing advice, promoting cure) that can help different stakeholders such as policy-makers. Social media platforms enable the infodemic and there has been an effort to curate the content on such platforms, analyze and debunk them. While a majority of the research efforts consider one or two aspects (e.g., detecting factuality) of such information, in this study we focus on a multifaceted approach, including an API,\\url{https:\/\/app.swaggerhub.com\/apis\/yifan2019\/Tanbih\/0.8.0\/} and a demo system,\\url{https:\/\/covid19.tanbih.org}, which we made freely and publicly available. We believe that this will facilitate researchers and different stakeholders. A screencast of the API services and demo is available.\\url{https:\/\/youtu.be\/zhbcSvxEKMk}","863":"We use European football performance data to select teams to form the proposed European football Super League, using only unsupervised techniques. We first used random forest regression to select important variables predicting goal difference, which we used to calculate the Euclidian distances between teams. Creating a Laplacian eigenmap, we bisected the Fielder vector to identify the five major European football leagues' natural clusters. Our results showed how an unsupervised approach could successfully identify four clusters based on five basic performance metrics: shots, shots on target, shots conceded, possession, and pass success. The top two clusters identify those teams who dominate their respective leagues and are the best candidates to create the most competitive elite super league.","864":"There has been great concern in the UK that people from the BAME (Black And Minority Ethnic) community have a far higher risk of dying from Covid19 than those of other ethnicities. However, the overall fatalities data from the Government's ONS (Office of National Statistics) most recent report on deaths by religion shows that Jews (very few of whom are classified as BAME) have a much higher risk than those of religions (Hindu, Sikh, Muslim) with predominantly BAME people. This apparently contradictory result is, according to the ONS statistical analysis, implicitly explained by age as the report claims that, when 'adjusted for age' Muslims have the highest fatality risk. However, the report fails to provide the raw data to support this. There are many factors other than just age that must be incorporated into any analysis of the observed data before making definitive conclusions about risk based on religion\/ethnicity. We propose the need for a causal model for this. If we discount unknown genetic factors, then religion and ethnicity have NO impact at all on a person's Covid19 death risk once we know their age, underlying medical conditions, work\/living conditions, and extent of social distancing.","865":"The classification of dark matter halos as isolated hosts or subhalos is critical for our understanding of structure formation and the galaxy-halo connection. Most commonly, subhalos are defined to reside inside a spherical overdensity boundary such as the virial radius. The resulting host-subhalo relations depend sensitively on the somewhat arbitrary overdensity threshold, but the impact of this dependence is rarely quantified. The recently proposed splashback radius tends to be larger and to include more subhalos than even the largest spherical overdensity boundaries. We systematically investigate the dependence of the subhalo fraction on the radius definition and show that it can vary by factors of unity between different spherical overdensity definitions. Using splashback radii can yet double the abundance of subhalos compared to the virial definition. We also quantify the abundance of flyby (or backsplash) halos, hosts that used to be subhalos in the past. We show that the majority of these objects are mislabeled satellites that are naturally classified as subhalos when we use the splashback radius. We compare our results to self-similar universes to show that the subhalo and flyby fractions are not universal with redshift and cosmology.","866":"All pandemics are local; so learning about the impacts of pandemics on public health and related societal issues at granular levels is of great interest. COVID-19 is affecting everyone in the globe and mask wearing is one of the few precautions against it. To quantify people's perception of mask effectiveness and to prevent the spread of COVID-19 for small areas, we use Understanding America Study's (UAS) survey data on COVID-19 as our primary data source. Our data analysis shows that direct survey-weighted estimates for small areas could be highly unreliable. In this paper we develop a synthetic estimation method to estimate proportions of mask effectiveness for small areas using a logistic model that combines information from multiple data sources. We select our working model using an extensive data analysis facilitated by a new variable selection criterion for survey data and benchmarking ratios. We propose a Jackknife method to estimate variance of our proposed estimator. From our data analysis. it is evident that our proposed synthetic method outperforms direct survey-weighted estimator with respect to commonly used evaluation measures.","867":"Situations in which no scientific consensus has been reached due to either insufficient, inconclusive or contradicting findings place strain on governments and public organizations which are forced to take action under circumstances of uncertainty. In this chapter, we focus on the case of COVID-19, its effects on children and the public debate around the reopening of schools. The aim is to better understand the relationship between policy interventions in the face of an uncertain and rapidly changing knowledge landscape and the subsequent use of scientific information in public debates related to the policy interventions. Our approach is to combine scientific information from journal articles and preprints with their appearance in the popular media, including social media. First, we provide a picture of the different scientific areas and approaches, by which the effects of COVID-19 on children are being studied. Second, we identify news media and social media attention around the COVID-19 scientific output related to children and schools. We focus on policies and media responses in three countries: Spain, South Africa and the Netherlands. These countries have followed very different policy actions with regard to the reopening of schools and represent very different policy approaches to the same problem. We analyse the activity in (social) media around the debate between COVID-19, children and school closures by focusing on the use of references to scientific information in the debate. Finally, we analyse the dominant topics that emerge in the news outlets and the online debates. We draw attention to illustrative cases of miscommunication related to scientific output and conclude the chapter by discussing how information from scientific publication, the media and policy actions shape the public discussion in the context of a global health pandemic.","868":"The COVID-19 pandemic has shaken the world unprecedentedly, where it has affected the vast global population both socially and economically. The pandemic has also opened our eyes to the many threats that novel virus infections can pose for humanity. While numerous unknowns are being investigated in terms of the distributed damage that the virus can do to the human body, recent studies have also shown that the infection can lead to lifelong sequelae that could affect other parts of the body, and one example is the brain. As part of this work, we investigate how viral infection can affect the brain by modelling and simulating a neuron's behaviour under demyelination that is affected by the cytokine storm. We quantify the effects of cytokine-induced demyelination on the propagation of action potential signals within a neuron. We used information and communication theory analysis on the signal propagated through the axonal pathway under different intensity levels of demyelination to analyse these effects. Our simulations demonstrate that virus-induced degeneration can play a role in the signal power and spiking rate and the probability of releasing neurotransmitters and compromising the propagation and processing of information between the neurons. We also propose a transfer function that models these attenuation effects that degenerates the action potential, where this model has the potential to be used as a framework for the analysis of virus-induced neurodegeneration that can pave the way to improved understanding of virus-induced demyelination.","869":"We study the influence of the rate of the attainment of herd immunity (HI), in the absence of an approved vaccine, on the vulnerable population. We essentially ask the question: how hard the evolution towards the desired herd immunity could be on the life of the vulnerables? We employ mathematical modelling (chemical network theory) and cellular automata based computer simulations to study the human cost of an epidemic spread and an effective strategy to introduce HI. Implementation of different strategies to counter the spread of the disease requires a certain degree of quantitative understanding of the time dependence of the outcome. In this paper, our main objective is to gather understanding of the dependence of outcome on the rate of progress of HI. We generalize the celebrated SIR model (Susceptible-Infected-Removed) by compartmentalizing the susceptible population into two categories- (i) vulnerables and (ii) resilients, and study dynamical evolution of the disease progression. We achieve such a classification by employing different rates of recovery of vulnerables vis-a-vis resilients. We obtain the relative fatality of these two sub-categories as a function of the percentages of the vulnerable and resilient population, and the complex dependence on the rate of attainment of herd immunity. Our results quantify the adverse effects on the recovery rates of vulnerables in the course of attaining the herd immunity. We find the important result that a slower attainment of the HI is relatively less fatal. However, a slower progress towards HI could be complicated by many intervening factors.","870":"The shortage of high-quality teachers is one of the biggest educational problems faced by underdeveloped areas. With the development of information and communication technologies (ICTs), China has begun a remote co-teaching intervention program using ICTs for rural classes, forming a unique co-teaching classroom. We conducted semi-structured interviews with nine remote urban teachers and twelve local rural teachers. We identified the remote co-teaching classes standard practices and co-teachers collaborative work process. We also found that remote teachers high-quality class directly impacted local teachers and students. Furthermore, interestingly, local teachers were also actively involved in making indirect impacts on their students by deeply coordinating with remote teachers and adapting the resources offered by the remote teachers. We conclude by summarizing and discussing the challenges faced by teachers, lessons learned from the current program, and related design implications to achieve a more adaptive and sustainable ICT4D program design.","871":"Cardiovascular diseases (CVDs) are one of the most common chronic illnesses that affect peoples health. Early detection of CVDs can reduce mortality rates by preventing or reducing the severity of the disease. Machine learning algorithms are a promising method for identifying risk factors. This paper proposes a proposed recursive feature elimination-based gradient boosting (RFE-GB) algorithm in order to obtain accurate heart disease prediction. The patients health record with important CVD features has been analyzed for the evaluation of the results. Several other machine learning methods were also used to build the prediction model, and the results were compared with the proposed model. The results of this proposed model infer that the combined recursive feature elimination and gradient boosting algorithm achieves the highest accuracy (89.7 %). Further, with an area under the curve of 0.84, the proposed RFE-GB algorithm was found superior and had obtained a substantial gain over other techniques. Thus, the proposed RFE-GB algorithm will serve as a prominent model for CVD estimation and treatment.","872":"Generalized zero-shot learning (GZSL) is a technique to train a deep learning model to identify unseen classes using the attribute. In this paper, we put forth a new GZSL technique that improves the GZSL classification performance greatly. Key idea of the proposed approach, henceforth referred to as semantic feature extraction-based GZSL (SE-GZSL), is to use the semantic feature containing only attribute-related information in learning the relationship between the image and the attribute. In doing so, we can remove the interference, if any, caused by the attribute-irrelevant information contained in the image feature. To train a network extracting the semantic feature, we present two novel loss functions, 1) mutual information-based loss to capture all the attribute-related information in the image feature and 2) similarity-based loss to remove unwanted attribute-irrelevant information. From extensive experiments using various datasets, we show that the proposed SE-GZSL technique outperforms conventional GZSL approaches by a large margin.","873":"Many people consider news articles to be a reliable source of information on current events. However, due to the range of factors influencing news agencies, such coverage may not always be impartial. Media bias, or slanted news coverage, can have a substantial impact on public perception of events, and, accordingly, can potentially alter the beliefs and views of the public. The main data gap in current research on media bias detection is a robust, representative, and diverse dataset containing annotations of biased words and sentences. In particular, existing datasets do not control for the individual background of annotators, which may affect their assessment and, thus, represents critical information for contextualizing their annotations. In this poster, we present a matrix-based methodology to crowdsource such data using a self-developed annotation platform. We also present MBIC (Media Bias Including Characteristics) - the first sample of 1,700 statements representing various media bias instances. The statements were reviewed by ten annotators each and contain labels for media bias identification both on the word and sentence level. MBIC is the first available dataset about media bias reporting detailed information on annotator characteristics and their individual background. The current dataset already significantly extends existing data in this domain providing unique and more reliable insights into the perception of bias. In future, we will further extend it both with respect to the number of articles and annotators per article.","874":"Due to privacy concerns of users and law enforcement in data security and privacy, it becomes more and more difficult to share data among organizations. Data federation brings new opportunities to the data-related cooperation among organizations by providing abstract data interfaces. With the development of cloud computing, organizations store data on the cloud to achieve elasticity and scalability for data processing. The existing data placement approaches generally only consider one aspect, which is either execution time or monetary cost, and do not consider data partitioning for hard constraints. In this paper, we propose an approach to enable data processing on the cloud with the data from different organizations. The approach consists of a data federation platform named FedCube and a Lyapunov-based data placement algorithm. FedCube enables data processing on the cloud. We use the data placement algorithm to create a plan in order to partition and store data on the cloud so as to achieve multiple objectives while satisfying the constraints based on a multi-objective cost model. The cost model is composed of two objectives, i.e., reducing monetary cost and execution time. We present an experimental evaluation to show our proposed algorithm significantly reduces the total cost (up to 69.8\\%) compared with existing approaches.","875":"Network embedding techniques are powerful to capture structural regularities in networks and to identify similarities between their local fabrics. However, conventional network embedding models are developed for static structures, commonly consider nodes only and they are seriously challenged when the network is varying in time. Temporal networks may provide an advantage in the description of real systems, but they code more complex information, which could be effectively represented only by a handful of methods so far. Here, we propose a new method of event embedding of temporal networks, called weg2vec, which builds on temporal and structural similarities of events to learn a low dimensional representation of a temporal network. This projection successfully captures latent structures and similarities between events involving different nodes at different times and provides ways to predict the final outcome of spreading processes unfolding on the temporal structure.","876":"The SIR evolutionary model predicts too sharp a decrease of the fractions of people infected with COVID-19 in France after the start of the national lockdown, compared to what is observed. I fit the daily hospital data: arrivals in regular and critical care units, releases and deaths, using extended SEIR models. These involve ratios of evolutionary timescales to branching fractions, assumed uniform throughout a country, and the basic reproduction number, $R_0$, before and during the national lockdown, for each region of France. The joint-region Bayesian analysis allows precise evaluations of the time\/fraction ratios and pre-hospitalized fractions. The hospital data are well fit by the models, except the arrivals in critical care, which decrease faster than predicted, indicating better treatment over time. Averaged over France, the analysis yields $R_0$= 3.4$\\pm$0.1 before the lockdown and 0.65$\\pm$0.04 (90% c.l.) during the lockdown, with small regional variations. On 11 May 2020, the Infection Fatality Rate in France was 4 $\\pm$1% (90% c.l.), while the Feverish vastly outnumber the Asymptomatic, contrary to the early phases. Without the lockdown nor social distancing, over 2 million deaths from COVID-19 would have occurred throughout France, while a lockdown that would have been enforced 10 days earlier would have led to less than 1000 deaths. The fraction of immunized people reached a plateau below 1% throughout France (3% in Paris) by late April 2020 (95% c.l.), suggesting a lack of herd immunity. The widespread availability of face masks on 11 May, when the lockdown was partially lifted, should keep $R_0$ below unity if at least 46% of the population wear them outside their home. Otherwise, without enhanced other social distancing, a second wave is inevitable and cause the number of deaths to triple between early May and October (if $R_0$=1.2) or even late June (if $R_0$=2).","877":"Cloud computing enables remote execution of users tasks. The pervasive adoption of cloud computing in smart cities services and applications requires timely execution of tasks adhering to Quality of Services (QoS). However, the increasing use of computing servers exacerbates the issues of high energy consumption, operating costs, and environmental pollution. Maximizing the performance and minimizing the energy in a cloud data center is challenging. In this paper, we propose a performance and energy optimization bi-objective algorithm to tradeoff the contradicting performance and energy objectives. An evolutionary algorithm-based multi-objective optimization is for the first time proposed using system performance counters. The performance of the proposed model is evaluated using a realistic cloud dataset in a cloud computing environment. Our experimental results achieve higher performance and lower energy consumption compared to a state of the art algorithm.","878":"AI and Machine Learning can offer powerful tools to help in the fight against Covid-19. In this paper we present a study and a concrete tool based on machine learning to predict the prognosis of hospitalised patients with Covid-19. In particular we address the task of predicting the risk of death of a patient at different times of the hospitalisation, on the base of some demographic information, chest X-ray scores and several laboratory findings. Our machine learning models use ensembles of decision trees trained and tested using data from more than 2000 patients. An experimental evaluation of the models shows good performance in solving the addressed task.","879":"Because of the current COVID-19 pandemic with its increasing fears among people, it has triggered several health complications such as depression and anxiety. Such complications have not only affected the developed countries but also developing countries such as Nepal. These complications can be understood from peoples' tweets\/comments posted online after their proper analysis and sentiment classification. Nevertheless, owing to the limited number of tokens\/words in each tweet, it is always crucial to capture multiple information associated with them for their better understanding. In this study, we, first, represent each tweet by combining both syntactic and semantic information, called hybrid features. The syntactic information is generated from the bag of words method, whereas the semantic information is generated from the combination of the fastText-based (ft) and domain-specific (ds) methods. Second, we design a novel multi-channel convolutional neural network (MCNN), which ensembles the multiple CNNs, to capture multi-scale information for better classification. Last, we evaluate the efficacy of both the proposed feature extraction method and the MCNN model classifying tweets into three sentiment classes (positive, neutral and negative) on NepCOV19Tweets dataset, which is the only public COVID-19 tweets dataset in Nepali language. The evaluation results show that the proposed hybrid features outperform individual feature extraction methods with the highest classification accuracy of 69.7% and the MCNN model outperforms the existing methods with the highest classification accuracy of 71.3% during classification.","880":"Background. Forecasting the time of forthcoming pandemic reduces the impact of diseases by taking precautionary steps such as public health messaging and raising the consciousness of doctors. With the continuous and rapid increase in the cumulative incidence of COVID-19, statistical and outbreak prediction models including various machine learning (ML) models are being used by the research community to track and predict the trend of the epidemic, and also in developing appropriate strategies to combat and manage its spread. Methods. In this paper, we present a comparative analysis of various ML approaches including Support Vector Machine, Random Forest, K-Nearest Neighbor and Artificial Neural Network in predicting the COVID-19 outbreak in the epidemiological domain. We first apply the autoregressive distributed lag (ARDL) method to identify and model the short and long-run relationships of the time-series COVID-19 datasets. That is, we determine the lags between a response variable and its respective explanatory time series variables as independent variables. Then, the resulting significant variables concerning their lags are used in the regression model selected by the ARDL for predicting and forecasting the trend of the epidemic. Results. Statistical measures i.e., Root Mean Square Error (RMSE), Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) are used for model accuracy. The values of MAPE for the best selected models for confirmed, recovered and deaths cases are 0.407, 0.094 and 0.124 respectively, which falls under the category of highly accurate forecasts. In addition, we computed fifteen days ahead forecast for the daily deaths, recover, and confirm patients and the cases fluctuated across time in all aspects. Besides, the results reveal the advantages of ML algorithms for supporting decision making of evolving short term policies.","881":"While Alexa can perform over 100,000 skills on paper, its capability covers only a fraction of what is possible on the web. To reach the full potential of an assistant, it is desirable that individuals can create skills to automate their personal web browsing routines. Many seemingly simple routines, however, such as monitoring COVID-19 stats for their hometown, detecting changes in their child's grades online, or sending personally-addressed messages to a group, cannot be automated without conventional programming concepts such as conditional and iterative evaluation. This paper presents VASH (Voice Assistant Scripting Helper), a new system that empowers users to create useful web-based virtual assistant skills without learning a formal programming language. With VASH, the user demonstrates their task of interest in the browser and issues a few voice commands, such as naming the skills and adding conditions on the action. VASH turns these multi-modal specifications into skills that can be invoked invoice on a virtual assistant. These skills are represented in a formal programming language we designed called WebTalk, which supports parameterization, function invocation, conditionals, and iterative execution. VASH is a fully working prototype that works on the Chrome browser on real-world websites. Our user study shows that users have many web routines they wish to automate, 81% of which can be expressed using VASH. We found that VASH Is easy to learn, and that a majority of the users in our study want to use our system.","882":"Tractography from high-dimensional diffusion magnetic resonance imaging (dMRI) data allows brain's structural connectivity analysis. Recent dMRI studies aim to compare connectivity patterns across subject groups and disease populations to understand subtle abnormalities in the brain's white matter connectivity and distributions of biologically sensitive dMRI derived metrics. Existing software products focus solely on the anatomy, are not intuitive or restrict the comparison of multiple subjects. In this paper, we present the design and implementation of FiberStars, a visual analysis tool for tractography data that allows the interactive visualization of brain fiber clusters combining existing 3D anatomy with compact 2D visualizations. With FiberStars, researchers can analyze and compare multiple subjects in large collections of brain fibers using different views. To evaluate the usability of our software, we performed a quantitative user study. We asked domain experts and non-experts to find patterns in a tractography dataset with either FiberStars or an existing dMRI exploration tool. Our results show that participants using FiberStars can navigate extensive collections of tractography faster and more accurately. All our research, software, and results are available openly.","883":"Recently the first laser spectroscopy measurement of the radioactive RaF molecule has been reported [Nature 581, 396 (2020)]. This and similar molecules are considered to search for the New Physics effects. The radium nucleus is of interest as it is octupole-deformed and has close levels of opposite parity. The preparation of such experiments can be simplified if there are reliable theoretical predictions. It is shown that the accurate prediction of the hyperfine structure of the RaF molecule requires to take into account the finite magnetization distribution inside the radium nucleus. For atoms, this effect is known as the Bohr-Weisskopf (BW) effect. Its magnitude depends on the model of the nuclear magnetization distribution which is usually not well known. We show that it is possible to express the nuclear magnetization distribution contribution to the hyperfine structure constant in terms of one magnetization distribution dependent parameter: BW matrix element for $1s$-state of the corresponding hydrogen-like ion. This parameter can be extracted from the accurate experimental and theoretical electronic structure data for an ion, atom, or molecule without the explicit treatment of any nuclear magnetization distribution model. This approach can be applied to predict the hyperfine structure of atoms and \\textit{molecules} and allows one to separate the nuclear and electronic correlation problems. It is employed to calculate the finite nuclear magnetization distribution contribution to the hyperfine structure of the $^{225}$Ra$^+$ cation and $^{225}$RaF molecule. For the ground state of the $^{225}$RaF molecule, this contribution achieves 4\\%.","884":"Different retail and e-commerce companies are facing the challenge of assembling large numbers of time-critical picking orders that include both small-line and multi-line orders. To reduce unproductive picker working time as in traditional picker-to-parts warehousing systems, different solutions are proposed in the literature and in practice. For example, in a mixed-shelves storage policy, items of the same stock keeping unit are spread over several shelves in a warehouse; or automated guided vehicles (AGVs) are used to transport the picked items from the storage area to packing stations instead of human pickers. This is the first paper to combine both solutions, creating what we call AGV-assisted mixed-shelves picking systems. We model the new integrated order batching and routing problem in such systems as an extended multi-depot vehicle routing problem with both three-index and two-commodity network flow formulations. Due to the complexity of the integrated problem, we develop a novel variable neighborhood search algorithm to solve the integrated problem more efficiently. We test our methods with different sizes of instances, and conclude that the mixed-shelves storage policy is more suitable than the usual storage policy in AGV-assisted mixed-shelves systems for orders with different sizes of order lines (saving up to 62% on driving distances for AGVs). Our variable neighborhood search algorithm provides optimal solutions within an acceptable computational time.","885":"Few-shot learning is an established topic in natural images for years, but few work is attended to histology images, which is of high clinical value since well-labeled datasets and rare abnormal samples are expensive to collect. Here, we facilitate the study of few-shot learning in histology images by setting up three cross-domain tasks that simulate real clinics problems. To enable label-efficient learning and better generalizability, we propose to incorporate contrastive learning (CL) with latent augmentation (LA) to build a few-shot system. CL learns useful representations without manual labels, while LA transfers semantic variations of the base dataset in an unsupervised way. These two components fully exploit unlabeled training data and can scale gracefully to other label-hungry problems. In experiments, we find i) models learned by CL generalize better than supervised learning for histology images in unseen classes, and ii) LA brings consistent gains over baselines. Prior studies of self-supervised learning mainly focus on ImageNet-like images, which only present a dominant object in their centers. Recent attention has been paid to images with multi-objects and multi-textures. Histology images are a natural choice for such a study. We show the superiority of CL over supervised learning in terms of generalization for such data and provide our empirical understanding for this observation. The findings in this work could contribute to understanding how the model generalizes in the context of both representation learning and histological image analysis. Code is available.","886":"In this paper we provide an account of how we ported a text and data mining course online in summer 2020 as a result of the COVID-19 pandemic and how we improved it in a second pilot run. We describe the course, how we adapted it over the two pilot runs and what teaching techniques we used to improve students' learning and community building online. We also provide information on the relentless feedback collected during the course which helped us to adapt our teaching from one session to the next and one pilot to the next. We discuss the lessons learned and promote the use of innovative teaching techniques applied to the digital such as digital badges and pair programming in break-out rooms for teaching Natural Language Processing courses to beginners and students with different backgrounds.","887":"Plasma wakefield acceleration (PWA) has shown illustrious progress over the past two decades of active research and resulted in an impressive demonstration of O(10 GeV) particle acceleration in O(1 m) long single structures. While already potentially sufficient for some applications, like, e.g., FELs, the traditional laser- and beam-driven acceleration in gaseous plasma faces enormous challenges when it comes to the design of the PWA-based O(1-10 TeV) high energy $e^+e^-$ colliders due to the complexity of energy staging, low average geometric gradients, and unprecedented transverse and longitudinal stability requirements. Channeling acceleration in solid-state plasma of crystals or nanostructures, e.g., carbon nanotubes (CNTs) or alumna honeycomb holes, has the promise of ultra-high accelerating gradients O(1-10 TeV\/m), continuous focusing of channeling particles without need of staging, and ultimately small equilibrium beam emittances naturally obtained while accelerating.","888":"Computing Education Research (CER) is critical for supporting the increasing number of students who need to learn computing skills. To systematically advance knowledge, publications must be clear enough to support replications, meta-analyses, and theory-building. The goal of this study is to characterize the reporting of empiricism in CER literature by identifying whether publications include information to support replications, meta-analyses, and theory building. The research questions are: RQ1) What percentage of papers in CER venues have empirical evaluation? RQ2) What are the characteristics of the empirical evaluation? RQ3) Do the papers with empirical evaluation follow reporting norms (both for inclusion and for labeling of key information)? We conducted an SLR of 427 papers published during 2014 and 2015 in five CER venues: SIGCSE TS, ICER, ITiCSE, TOCE, and CSE. We developed and applied the CER Empiricism Assessment Rubric. Over 80% of papers had some form of empirical evaluation. Quantitative evaluation methods were the most frequent. Papers most frequently reported results on interventions around pedagogical techniques, curriculum, community, or tools. There was a split in papers that had some type of comparison between an intervention and some other data set or baseline. Many papers lacked properly reported research objectives, goals, research questions, or hypotheses, description of participants, study design, data collection, and threats to validity. CER authors are contributing empirical results to the literature; however, not all norms for reporting are met. We encourage authors to provide clear, labeled details about their work so readers can use the methodologies and results for replications and meta-analyses. As our community grows, our reporting of CER should mature to help establish computing education theory to support the next generation of computing learners.","889":"The COVID-19 pandemic has affected people's lives around the world on an unprecedented scale. We intend to investigate hoarding behaviors in response to the pandemic using large-scale social media data. First, we collect hoarding-related tweets shortly after the outbreak of the coronavirus. Next, we analyze the hoarding and anti-hoarding patterns of over 42,000 unique Twitter users in the United States from March 1 to April 30, 2020, and dissect the hoarding-related tweets by age, gender, and geographic location. We find the percentage of females in both hoarding and anti-hoarding groups is higher than that of the general Twitter user population. Furthermore, using topic modeling, we investigate the opinions expressed towards the hoarding behavior by categorizing these topics according to demographic and geographic groups. We also calculate the anxiety scores for the hoarding and anti-hoarding related tweets using a lexical approach. By comparing their anxiety scores with the baseline Twitter anxiety score, we reveal further insights. The LIWC anxiety mean for the hoarding-related tweets is significantly higher than the baseline Twitter anxiety mean. Interestingly, beer has the highest calculated anxiety score compared to other hoarded items mentioned in the tweets.","890":"Due to the different losses caused by various photovoltaic (PV) array faults, accurate diagnosis of fault types is becoming increasingly important. Compared with a single one, multiple PV stations collect sufficient fault samples, but their data is not allowed to be shared directly due to potential conflicts of interest. Therefore, federated learning can be exploited to train a collaborative fault diagnosis model. However, the modeling efficiency is seriously affected by the model update mechanism since each PV station has a different computing capability and amount of data. Moreover, for the safe and stable operation of the PV system, the robustness of collaborative modeling must be guaranteed rather than simply being processed on a central server. To address these challenges, a novel asynchronous decentralized federated learning (ADFL) framework is proposed. Each PV station not only trains its local model but also participates in collaborative fault diagnosis by exchanging model parameters to improve the generalization without losing accuracy. The global model is aggregated distributedly to avoid central node failure. By designing the asynchronous update scheme, the communication overhead and training time are greatly reduced. Both the experiments and numerical simulations are carried out to verify the effectiveness of the proposed method.","891":"In a number of application domains, one observes a sequence of network data; for example, repeated measurements between users interactions in social media platforms, financial correlation networks over time, or across subjects, as in multi-subject studies of brain connectivity. One way to analyze such data is by stacking networks into a third-order array or tensor. We propose a principal components analysis (PCA) framework for sequence network data, based on a novel decomposition for semi-symmetric tensors. We derive efficient algorithms for computing our proposed\"Coupled CP\"decomposition and establish estimation consistency of our approach under an analogue of the spiked covariance model with rates the same as the matrix case up to a logarithmic term. Our framework inherits many of the strengths of classical PCA and is suitable for a wide range of unsupervised learning tasks, including identifying principal networks, isolating meaningful changepoints or outliers across observations, and for characterizing the\"variability network\"of the most varying edges. Finally, we demonstrate the effectiveness of our proposal on simulated data and on examples from political science and financial economics. The proof techniques used to establish our main consistency results are surprisingly straight-forward and may find use in a variety of other matrix and tensor decomposition problems.","892":"Stellar feedback in the form of radiation pressure and magnetically-driven collimated outflows may limit the maximum mass that a star can achieve and affect the star-formation efficiency of massive pre-stellar cores. Here we present a series of 3D adaptive mesh refinement radiation-magnetohydrodynamic simulations of the collapse of initially turbulent, massive pre-stellar cores. Our simulations include radiative feedback from both the direct stellar and dust-reprocessed radiation fields, and collimated outflow feedback from the accreting stars. We find that protostellar outflows punches holes in the dusty circumstellar gas along the star's polar directions, thereby increasing the size of optically thin regions through which radiation can escape. Precession of the outflows as the star's spin axis changes due to the turbulent accretion flow further broadens the outflow, and causes more material to be entrained. Additionally, the presence of magnetic fields in the entrained material leads to broader entrained outflows that escape the core. We compare the injected and entrained outflow properties and find that the entrained outflow mass is a factor of $\\sim$3 larger than the injected mass and the momentum and energy contained in the entrained material are $\\sim$25% and $\\sim$5% of the injected momentum and energy, respectively. As a result, we find that, when one includes both outflows and radiation pressure, the former are a much more effective and important feedback mechanism, even for massive stars with significant radiative outputs.","893":"Existing tensor completion formulation mostly relies on partial observations from a single tensor. However, tensors extracted from real-world data are often more complex due to: (i) Partial observation: Only a small subset (e.g., 5%) of tensor elements are available. (ii) Coarse observation: Some tensor modes only present coarse and aggregated patterns (e.g., monthly summary instead of daily reports). In this paper, we are given a subset of the tensor and some aggregated\/coarse observations (along one or more modes) and seek to recover the original fine-granular tensor with low-rank factorization. We formulate a coupled tensor completion problem and propose an efficient Multi-resolution Tensor Completion model (MTC) to solve the problem. Our MTC model explores tensor mode properties and leverages the hierarchy of resolutions to recursively initialize an optimization setup, and optimizes on the coupled system using alternating least squares. MTC ensures low computational and space complexity. We evaluate our model on two COVID-19 related spatio-temporal tensors. The experiments show that MTC could provide 65.20% and 75.79% percentage of fitness (PoF) in tensor completion with only 5% fine granular observations, which is 27.96% relative improvement over the best baseline. To evaluate the learned low-rank factors, we also design a tensor prediction task for daily and cumulative disease case predictions, where MTC achieves 50% in PoF and 30% relative improvements over the best baseline.","894":"Population dynamics are often affected by sudden environmental perturbations. Parameters of stochastic models are often imprecise due to various uncertainties. In this paper, we formulate a stochastic multimolecular biochemical reaction model that includes L\\'{e}vy jumps and interval parameters. Firstly, we prove the existence and uniqueness of the positive solution. Moreover, the threshold between extinction and persistence of the reaction is obtained. Finally, some simulations are carried out to demonstrate our theoretical results.","895":"This paper is devoted to the study of a stochastic epidemiological model which is a variant of the SIR model to which we add an extra factor in the transition rate from susceptible to infected accounting for the inflow of infection due to immigration or environmental sources of infection. This factor yields the formation of new clusters of infections, without having to specify a priori and explicitly their date and place of appearance.We establish an {exact deterministic description} for such stochastic processes on 1D lattices (finite lines, semi-infinite lines, infinite lines, circles) by showing that the probability of infection at a given point in space and time can be obtained as the solution of a deterministic ODE system on the lattice. Our results allow stochastic initial conditions and arbitrary spatio-temporal heterogeneities on the parameters.We then apply our results to some concrete situations and obtain useful qualitative results and explicit formulae on the macroscopic dynamics and also the local temporal behavior of each individual. In particular, we provide a fine analysis of some aspects of cluster formation through the study of {patient-zero problems} and the effects of {time-varying point sources}.Finally, we show that the space-discrete model gives rise to new space-continuous models, which are either ODEs or PDEs, depending on the rescaling regime assumed on the parameters.","896":"Starting with the exact factorization of the molecular wavefunction, this paper presents the results from the numerical implementation in nonadiabatic molecular dynamics of the recently proposed bohmion method. Within the context of quantum hydrodynamics, we introduce a regularized nuclear Bohm potential admitting solutions comprising a train of $\\delta$-functions which provide a finite-dimensional sampling of the hydrodynamic flow paths. The bohmion method inherits all the basic conservation laws from its underlying variational structure and captures electronic decoherence. After reviewing the general theory, the method is applied to the well-known Tully models, which are used here as benchmark problems. In the present case of study, we show that the new method accurately reproduces both electronic decoherence and nuclear population dynamics.","897":"Older adults have been hit disproportionally hard by the COVID-19 pandemic. One critical way for older adults to minimize the negative impact of COVID-19 and future pandemics is to stay informed about its latest information, which has been increasingly presented through online interactive visualizations (e.g., live dashboards and websites). Thus, it is imperative to understand how older adults interact with and comprehend online COVID-19 interactive visualizations and what challenges they might encounter to make such visualizations more accessible to older adults. We adopted a user-centered approach by inviting older adults to interact with COVID-19 interactive visualizations while at the same time verbalizing their thought processes using a think-aloud protocol. By analyzing their think-aloud verbalizations, we identified four types of thought processes representing how older adults comprehended the visualizations and uncovered the challenges they encountered. Furthermore, we also identified the challenges they encountered with seven common types of interaction techniques adopted by the visualizations. Based on the findings, we present design guidelines for making interactive visualizations more accessible to older adults.","898":"Knowledge graphs (KGs) have become the preferred technology for representing, sharing and adding knowledge to modern AI applications. While KGs have become a mainstream technology, the RDF\/SPARQL-centric toolset for operating with them at scale is heterogeneous, difficult to integrate and only covers a subset of the operations that are commonly needed in data science applications. In this paper we present KGTK, a data science-centric toolkit designed to represent, create, transform, enhance and analyze KGs. KGTK represents graphs in tables and leverages popular libraries developed for data science applications, enabling a wide audience of developers to easily construct knowledge graph pipelines for their applications. We illustrate the framework with real-world scenarios where we have used KGTK to integrate and manipulate large KGs, such as Wikidata, DBpedia and ConceptNet.","899":"The coronavirus (COVID-19) pandemic has significantly altered our lifestyles as we resort to minimize the spread through preventive measures such as social distancing and quarantine. An increasingly worrying aspect is the gap between the exponential disease spread and the delay in adopting preventive measures. This gap is attributed to the lack of awareness about the disease and its preventive measures. Nowadays, social media platforms (ie., Twitter) are frequently used to create awareness about major events, including COVID-19. In this paper, we use Twitter to characterize public awareness regarding COVID-19 by analyzing the information flow in the most affected countries. Towards that, we collect more than 46K trends and 622 Million tweets from the top twenty most affected countries to examine 1) the temporal evolution of COVID-19 related trends, 2) the volume of tweets and recurring topics in those trends, and 3) the user sentiment towards preventive measures. Our results show that countries with a lower pandemic spread generated a higher volume of trends and tweets to expedite the information flow and contribute to public awareness. We also observed that in those countries, the COVID-19 related trends were generated before the sharp increase in the number of cases, indicating a preemptive attempt to notify users about the potential threat. Finally, we noticed that in countries with a lower spread, users had a positive sentiment towards COVID-19 preventive measures. Our measurements and analysis show that effective social media usage can influence public behavior, which can be leveraged to better combat future pandemics.","900":"A randomized trial allows estimation of the causal effect of an intervention compared to a control in the overall population and in subpopulations defined by baseline characteristics. Often, however, clinical questions also arise regarding the treatment effect in subpopulations of patients, which would experience clinical or disease related events post-randomization. Events that occur after treatment initiation and potentially affect the interpretation or the existence of the measurements are called intercurrent events in the ICH E9(R1) guideline. If the intercurrent event is a consequence of treatment, randomization alone is no longer sufficient to meaningfully estimate the treatment effect. Analyses comparing the subgroups of patients without the intercurrent events for intervention and control will not estimate a causal effect. This is well known, but post-hoc analyses of this kind are commonly performed in drug development. An alternative approach is the principal stratum strategy, which classifies subjects according to their potential occurrence of an intercurrent event on both study arms. We illustrate with examples that questions formulated through principal strata occur naturally in drug development and argue that approaching these questions with the ICH E9(R1) estimand framework has the potential to lead to more transparent assumptions as well as more adequate analyses and conclusions. In addition, we provide an overview of assumptions required for estimation of effects in principal strata. Most of these assumptions are unverifiable and should hence be based on solid scientific understanding. Sensitivity analyses are needed to assess robustness of conclusions.","901":"SARS-CoV-2 (COVID-19), a positive single stranded RNA virus, member of corona virus family, is spreading its tentacles across the world due to lack of drugs at present. Being associated with cough, fever, and respiratory distress, this disease caused more than 15 % mortality worldwide. Due to its vital role in virus replication, Mpro\/3CLpro has recently been regarded as a suitable target for drug design. The current study focused on the inhibitory activity of Calotropin, a component from milk of Calotropis gigantean, against Mpro protein from SARS-CoV-2. Till date there is no work is undertaken on in-silico analysis of this compound against Mpro of COVID-19 protein. In the present study, molecular docking studies were conducted by using Patchdock tool. Protein Interactions tool was used for protein interactions. The calculated parameters such as docking score indicated effective binding of Calotropin to Mpro protein. Interactions results indicated that, Mpro\/ Calotropin complexes forms hydrophobic interactions. Therefore, Calotropin may represent potential herbal treatment to act as COVID-19 Mpro inhibitor. However, further research is necessary to investigate their potential medicinal use.","902":"Many COVID-19 patients developed prolonged symptoms after the infection, including fatigue, delirium, and headache. The long-term health impact of these conditions is still not clear. It is necessary to develop a way to follow up with these patients for monitoring their health status to support timely intervention and treatment. In the lack of sufficient human resources to follow up with patients, we propose a novel smart chatbot solution backed with machine learning to collect information (i.e., generating digital diary) in a personalized manner. In this article, we describe the design framework and components of our prototype.","903":"This paper aims to automatically detect COVID-19 patients by analysing the acoustic information embedded in coughs. COVID-19 affects the respiratory system, and, consequently, respiratory-related signals have the potential to contain salient information for the task at hand. We focus on analysing the spectrogram representations of coughing samples with the aim to investigate whether COVID-19 alters the frequency content of these signals. Furthermore, this work also assesses the impact of gender in the automatic detection of COVID-19. To extract deep learnt representations of the spectrograms, we compare the performance of a cough-specific, and a Resnet18 pre-trained Convolutional Neural Network (CNN). Additionally, our approach explores the use of contextual attention, so the model can learn to highlight the most relevant deep learnt features extracted by the CNN. We conduct our experiments on the dataset released for the Cough Sound Track of the DiCOVA 2021 Challenge. The best performance on the test set is obtained using the Resnet18 pre-trained CNN with contextual attention, which scored an Area Under the Curve (AUC) of 70.91 at 80% sensitivity.","904":"Federated learning (FL) has emerged as a promising privacy-aware paradigm that allows multiple clients to jointly train a model without sharing their private data. Recently, many studies have shown that FL is vulnerable to membership inference attacks (MIAs) that can distinguish the training members of the given model from the non-members. However, existing MIAs ignore the source of a training member, i.e., the information of which client owns the training member, while it is essential to explore source privacy in FL beyond membership privacy of examples from all clients. The leakage of source information can lead to severe privacy issues. For example, identification of the hospital contributing to the training of an FL model for COVID-19 pandemic can render the owner of a data record from this hospital more prone to discrimination if the hospital is in a high risk region. In this paper, we propose a new inference attack called source inference attack (SIA), which can derive an optimal estimation of the source of a training member. Specifically, we innovatively adopt the Bayesian perspective to demonstrate that an honest-but-curious server can launch an SIA to steal non-trivial source information of the training members without violating the FL protocol. The server leverages the prediction loss of local models on the training members to achieve the attack effectively and non-intrusively. We conduct extensive experiments on one synthetic and five real datasets to evaluate the key factors in an SIA, and the results show the efficacy of the proposed source inference attack.","905":"The coronavirus disease 2019 (COVID-19) pandemic has quickly become a global public health crisis unseen in recent years. It is known that the structure of the human contact network plays an important role in the spread of transmissible diseases. In this work, we study a structure aware model of COVID-19 CGEM. This model becomes similar to the classical compartment-based models in epidemiology if we assume the contact network is a Erdos-Renyi (ER) graph, i.e. everyone comes into contact with everyone else with the same probability. In contrast, CGEM is more expressive and allows for plugging in the actual contact networks, or more realistic proxies for it. Moreover, CGEM enables more precise modelling of enforcing and releasing different non-pharmaceutical intervention (NPI) strategies. Through a set of extensive experiments, we demonstrate significant differences between the epidemic curves when assuming different underlying structures. More specifically we demonstrate that the compartment-based models are overestimating the spread of the infection by a factor of 3, and under some realistic assumptions on the compliance factor, underestimating the effectiveness of some of NPIs, mischaracterizing others (e.g. predicting a later peak), and underestimating the scale of the second peak after reopening.","906":"As a reaction to the high infectiousness and lethality of the COVID-19 virus, countries around the world have adopted drastic policy measures to contain the pandemic. However, it remains unclear which effect these measures, so-called non-pharmaceutical interventions (NPIs), have on the spread of the virus. In this article, we use machine learning and apply drift detection methods in a novel way to predict the time lag of policy interventions with respect to the development of daily case numbers of COVID-19 across 9 European countries and 28 US states. Our analysis shows that there are, on average, more than two weeks between NPI enactment and a drift in the case numbers.","907":"We propose an on-chip mid-infrared (MIR) photonic spectroscopy platform for aerosol characterization to obtain highly discriminatory information on the chemistry of aerosol particles. Sensing of aerosols is crucial for various environmental, climactic, warfare threat detection, and pulmonary healthcare applications. Further, there are a number of unintended situations for potential exposure to bioaerosols such as viruses, bacteria, and fungi. For instance, the current pandemic scenario of COVID-19 occurring across the world. Currently, chemical characterization of aerosols is performed using FTIR spectroscopy yielding chemical fingerprinting because most of the vibrational and rotational transitions of chemical molecules fall in the MIR range; and Raman spectroscopy. Both techniques use free space bench-top geometries. Here, we propose miniaturized on-chip MIR photonics-based aerosol spectroscopy consisting of a broadband spiral-waveguide sensor that significantly enhances particle-light interaction to improve sensitivity. The spiral waveguides are made of a chalcogenide glass material (Ge23Sb7S70) which shows a broad transparency over IR range. We demonstrate the sensing of N-methyl aniline-based aerosol particles with the device. We anticipate that the sensor will readily complement existing photonic resonator-based particle sizing and counting techniques to develop a unified framework for on-chip integrated photonic aerosol spectroscopy.","908":"Simulation-based Inference (SBI) is a widely used set of algorithms to learn the parameters of complex scientific simulation models. While primarily run on CPUs in HPC clusters, these algorithms have been shown to scale in performance when developed to be run on massively parallel architectures such as GPUs. While parallelizing existing SBI algorithms provides us with performance gains, this might not be the most efficient way to utilize the achieved parallelism. This work proposes a new algorithm, that builds on an existing SBI method - Approximate Bayesian Computation with Sequential Monte Carlo(ABC-SMC). This new algorithm is designed to utilize the parallelism not only for performance gain, but also toward qualitative benefits in the learnt parameters. The key idea is to replace the notion of a single 'step-size' hyperparameter, which governs how the state space of parameters is explored during learning, with step-sizes sampled from a tuned Beta distribution. This allows this new ABC-SMC algorithm to more efficiently explore the state-space of the parameters being learnt. We test the effectiveness of the proposed algorithm to learn parameters for an epidemiology model running on a Tesla T4 GPU. Compared to the parallelized state-of-the-art SBI algorithm, we get similar quality results in $\\sim 100$x fewer simulations and observe ~80x lower run-to-run variance across 10 independent trials.","909":"Mapping of spatial hotspots, i.e., regions with significantly higher rates of generating cases of certain events (e.g., disease or crime cases), is an important task in diverse societal domains, including public health, public safety, transportation, agriculture, environmental science, etc. Clustering techniques required by these domains differ from traditional clustering methods due to the high economic and social costs of spurious results (e.g., false alarms of crime clusters). As a result, statistical rigor is needed explicitly to control the rate of spurious detections. To address this challenge, techniques for statistically-robust clustering (e.g., scan statistics) have been extensively studied by the data mining and statistics communities. In this survey we present an up-to-date and detailed review of the models and algorithms developed by this field. We first present a general taxonomy for statistically-robust clustering, covering key steps of data and statistical modeling, region enumeration and maximization, and significance testing. We further discuss different paradigms and methods within each of the key steps. Finally, we highlight research gaps and potential future directions, which may serve as a stepping stone in generating new ideas and thoughts in this growing field and beyond.","910":"Since March 2020, companies nationwide have started work from home (WFH) due to the rapid increase of confirmed COVID-19 cases in an attempt to help prevent the coronavirus from spreading and rescue the economy from the pandemic. Many organizations have conducted surveys to understand people's opinions towards WFH. However, the findings are limited due to small sample size and the dynamic topics over time. This study aims to understand the U.S. public opinions on working from home during the COVID-19 pandemic. We conduct a large-scale social media study using Twitter data to portrait different groups who have positive\/negative opinions about WFH. We perform an ordinary least squares regression to investigate the relationship between the sentiment about WFH and user characteristics including gender, age, ethnicity, median household income, and population density. To better understand public opinion, we use latent Dirichlet allocation to extract topics and discover how tweet contents relate to people's attitudes. These findings provide evidence that sentiment about WFH varies across user characteristics. Furthermore, the content analysis sheds light on the nuanced differences in sentiment and reveals disparities relate to WFH.","911":"Zines are a form of small-circulation self-produced publication often akin to a magazine. This free-form medium has a long history and has been used as means for personal or intimate expression, as a way for marginalized people to describe issues that are important to them, and as a venue for graphical experimentation. It would seem then that zines would make an ideal vehicle for the recent interest in applying feminist or humanist ideas to visualization. Yet, there has been little work combining visualization and zines. In this paper we explore the potential of this intersection by analyzing examples of zines that use data graphics and by describing the pedagogical value that they can have in a visualization classroom. In doing so, we argue that there are plentiful opportunities for visualization research and practice in this rich intersectional-medium.","912":"The ability to determine what parts of objects and surfaces people touch as they go about their daily lives would be useful in understanding how the COVID-19 virus spreads. To determine whether a person has touched an object or surface using visual data, images, or videos, is a hard problem. Computer vision 3D reconstruction approaches project objects and the human body from the 2D image domain to 3D and perform 3D space intersection directly. However, this solution would not meet the accuracy requirement in applications due to projection error. Another standard approach is to train a neural network to infer touch actions from the collected visual data. This strategy would require significant amounts of training data to generalize over scale and viewpoint variations. A different approach to this problem is to identify whether a person has touched a defined object. In this work, we show that the solution to this problem can be straightforward. Specifically, we show that the contact between an object and a static surface can be identified by projecting the object onto the static surface through two different viewpoints and analyzing their 2D intersection. The object contacts the surface when the projected points are close to each other; we call this cross view projection consistency. Instead of doing 3D scene reconstruction or transfer learning from deep networks, a mapping from the surface in the two camera views to the surface space is the only requirement. For planar space, this mapping is the Homography transformation. This simple method can be easily adapted to real-life applications. In this paper, we apply our method to do office occupancy detection for studying the COVID-19 transmission pattern from an office desk in a meeting room using the contact information.","913":"Activity reductions in early 2020 due to the Coronavirus Disease 2019 pandemic led to unprecedented decreases in carbon dioxide (CO2) emissions. Despite their record size, the resulting atmospheric signals are smaller than and obscured by climate variability in atmospheric transport and biospheric fluxes, notably that related to the 2019-2020 Indian Ocean Dipole. Monitoring CO2 anomalies and distinguishing human and climatic causes thus remains a new frontier in Earth system science. We show, for the first time, that the impact of short-term, regional changes in fossil fuel emissions on CO2 concentrations was observable from space. Starting in February and continuing through May, column CO2 over many of the World's largest emitting regions was 0.14 to 0.62 parts per million less than expected in a pandemic-free scenario, consistent with reductions of 3 to 13 percent in annual, global emissions. Current spaceborne technologies are therefore approaching levels of accuracy and precision needed to support climate mitigation strategies with future missions expected to meet those needs.","914":"New facilities of the 2020s, such as the High Luminosity Large Hadron Collider (HL-LHC), will be relevant through at least the 2030s. This means that their software efforts and those that are used to analyze their data need to consider sustainability to enable their adaptability to new challenges, longevity, and efficiency, over at least this period. This will help ensure that this software will be easier to develop and maintain, that it remains available in the future on new platforms, that it meets new needs, and that it is as reusable as possible. This report discusses a virtual half-day workshop on\"Software Sustainability and High Energy Physics\"that aimed 1) to bring together experts from HEP as well as those from outside to share their experiences and practices, and 2) to articulate a vision that helps the Institute for Research and Innovation in Software for High Energy Physics (IRIS-HEP) to create a work plan to implement elements of software sustainability. Software sustainability practices could lead to new collaborations, including elements of HEP software being directly used outside the field, and, as has happened more frequently in recent years, to HEP developers contributing to software developed outside the field rather than reinventing it. A focus on and skills related to sustainable software will give HEP software developers an important skill that is essential to careers in the realm of software, inside or outside HEP. The report closes with recommendations to improve software sustainability in HEP, aimed at the HEP community via IRIS-HEP and the HEP Software Foundation (HSF).","915":"Electronic Health Records (EHRs) are a valuable asset to facilitate clinical research and point of care applications; however, many challenges such as data privacy concerns impede its optimal utilization. Generative Adversarial Networks (GANs) show great promise in generating synthetic EHR data by learning underlying data distributions while achieving excellent performance and addressing these challenges. This work aims to review the major developments in various applications of GANs for EHRs and provides an overview of the proposed methodologies. For this purpose, we combine perspectives from healthcare applications and machine learning techniques in terms of source datasets and the fidelity and privacy evaluation of the generated synthetic datasets. We also compile a list of the metrics and datasets used by the reviewed works, which can be utilized as benchmarks for future research in the field. We conclude by discussing challenges in GANs for EHRs development and proposing recommended practices. We hope that this work motivates novel research development directions in the intersection of healthcare and machine learning.","916":"We discovered that the time evolution of the inverse fractional daily growth of new infections, N\/dN, in the current outbreak of COVID-19 is accurately described by a universal function, namely the two-parameter Gumbel cumulative function, in all countries that we have investigated. While the two Gumbel parameters, as determined bit fits to the data, vary from country to country (and even within different regions of the same country), reflecting the diversity and efficacy of the adopted containment measures, the functional form of the evolution of N\/dN appears to be universal. The result of the fit in a given region or country appears to be stable against variations of the selected time interval. This makes it possible to robustly estimate the two parameters from the data data even over relatively small time periods. In turn, this allows one to predict with large advance and well-controlled confidence levels, the time of the peak in the daily new infections, its magnitude and duration (hence the total infections), as well as the time when the daily new infections decrease to a pre-set value (e.g. less than about 2 new infections per day per million people), which can be very useful for planning the reopening of economic and social activities. We use this formalism to predict and compare these key features of the evolution of the COVID-19 disease in a number of countries and provide a quantitative assessment of the degree of success in in their efforts to countain the outbreak.","917":"While the COVID-19 pandemic continues to be as complex as ever, the collection and exchange of data in the light of fighting coronavirus poses a major challenge for privacy systems around the globe. The disease's size and magnitude is not uncommon but it appears to be at the point of hysteria surrounding it. Consequently, in a very short time, extreme measures for dealing with the situation appear to have become the norm. Any such actions affect the privacy of individuals in particular. For some cases, there is intensive monitoring of the whole population while the medical data of those diagnosed with the virus is commonly circulated through institutions and nations. This may well be in the interest of saving the world from a deadly disease, but is it really appropriate and right? Although creative solutions have been implemented in many countries to address the issue, proponents of privacy are concerned that technologies will eventually erode privacy, while regulators and privacy supporters are worried about what kind of impact this could bring. While that tension has always been present, privacy has been thrown into sharp relief by the sheer urgency of containing an exponentially spreading virus. The essence of this dilemma indicates that establishing the right equilibrium will be the best solution. The jurisprudence concerning cases regarding the willingness of public officials to interfere with the constitutional right to privacy in the interests of national security or public health has repeatedly proven that a reasonable balance can be reached.","918":"The aims of this study are to identify risk factors and develop a composite risk factor of initial stage of COVID-19 pandemic in regency level in Indonesia. Three risk factors, i.e., exposure, transmission and susceptibility, are investigated. Multivariate regression, and Canonical correlation analysis are implemented to measure the association between the risk factors and the initial stage of reported COVID -19 cases. The result reveals strong correlation between the composite risk factor and the number of COVID-19 cases at the initial stage of pandemic. The influence of population density, percentage of people commuting, international exposures, and number of public places which prone to COVID-19 transmission are observed. Large regencies and cities, mostly in Java, have high risk score. The largest risk score owned by regencies that are part of the Jakarta Metropolitan Area.","919":"High hospitalization rates due to the global spread of Covid-19 bring about a need for improvements to classical triaging workflows. To this end, convolutional neural networks (CNNs) can effectively differentiate critical from non-critical images so that critical cases may be addressed quickly, so long as there exists some representative image for the illness. Presented is a conglomerate neural network system consisting of multiple VGG16 CNNs; the system trains on weighted skin disease images re-labelled as critical or non-critical, to then attach to input images a critical index between 0 and 10. A critical index offers a more comprehensive rating system compared to binary critical\/non-critical labels. Results for batches of input images run through the trained network are promising. A batch is shown being re-ordered by the proposed architecture from most critical to least critical roughly accurately.","920":"This paper aims to study the fight against COVID-19 in Bangladesh and digital intervention initiatives. To achieve the purpose of our research, we conducted a methodical review of online content. We have reviewed the first digital intervention that COVID-19 has been used to fight against worldwide. Then we reviewed the initiatives that have been taken in Bangladesh. Our paper has shown that while Bangladesh can take advantage of the digital intervention approach, it will require rigorous collaboration between government organizations and universities to get the most out of it. Public health can become increasingly digital in the future, and we are reviewing international alignment requirements. This exploration also focused on the strategies for controlling, evaluating, and using digital technology to strengthen epidemic management and future preparations for COVID-19.","921":"The Coronavirus disease 2019 (COVID-19) has affected several million people. With the outbreak of the epidemic, many researchers are devoting themselves to the COVID-19 screening system. The standard practices for rapid risk screening of COVID-19 are the CT imaging or RT-PCR (real-time polymerase chain reaction). However, these methods demand professional efforts of the acquisition of CT images and saliva samples, a certain amount of waiting time, and most importantly prohibitive examination fee in some countries. Recently, some literatures have shown that the COVID-19 patients usually accompanied by ocular manifestations consistent with the conjunctivitis, including conjunctival hyperemia, chemosis, epiphora, or increased secretions. After more than four months study, we found that the confirmed cases of COVID-19 present the consistent ocular pathological symbols; and we propose a new screening method of analyzing the eye-region images, captured by common CCD and CMOS cameras, could reliably make a rapid risk screening of COVID-19 with very high accuracy. We believe a system implementing such an algorithm should assist the triage management or the clinical diagnosis. To further evaluate our algorithm and approved by the Ethics Committee of Shanghai public health clinic center of Fudan University, we conduct a study of analyzing the eye-region images of 303 patients (104 COVID-19, 131 pulmonary, and 68 ocular patients), as well as 136 healthy people. Remarkably, our results of COVID-19 patients in testing set consistently present similar ocular pathological symbols; and very high testing results have been achieved in terms of sensitivity and specificity. We hope this study can be inspiring and helpful for encouraging more researches in this topic.","922":"The worldwide refugee crisis is a major current challenge, affecting the health and education of millions of families with children due to displacement. Despite the various challenges and risks of migration practices, numerous refugee families have access to interactive technologies during these processes. The aim of this ongoing study is to explore the role of technologies in the transitions of refugee families in Scotland. Based on Tudge's ecocultural theory, a qualitative case-study approach has been adopted. Semi-structured interviews have been conducted with volunteers who work with refugee families in a big city in Scotland, and proxy observations of young children were facilitated remotely by their refugee parents. A preliminary overview of the participants' insights of the use and role of technology for transitioning into a new culture is provided here.","923":"Importance: Alternative methods for hospital utilization forecasting, essential information in hospital crisis planning, are necessary in a novel pandemic when traditional data sources such as disease testing are limited. Objective: Determine whether mandatory daily employee symptom attestation data can be used as syndromic surveillance to forecast COVID-19 hospitalizations in the communities where employees live. Design: Retrospective cohort study. Setting: Large academic hospital network of 10 hospitals accounting for a total of 2,384 beds and 136,000 discharges in New England. Participants: 6,841 employees working on-site of Hospital 1 from April 2, 2020 to November 4, 2020, who live in the 10 hospitals' service areas. Interventions: Mandatory, daily employee self-reported symptoms were collected using an automated text messaging system. Main Outcomes: Mean absolute error (MAE) and weighted mean absolute percentage error (WMAPE) of 7 day forecasts of daily COVID-19 hospital census at each hospital. Results: 6,841 employees, with a mean age of 40.8 (SD = 13.6), 8.8 years of service (SD = 10.4), and 74.8% were female (n = 5,120), living in the 10 hospitals' service areas. Our model has an MAE of 6.9 COVID-19 patients and a WMAPE of 1.5% for hospitalizations for the entire hospital network. The individual hospitals had an MAE that ranged from 0.9 to 4.5 patients (WMAPE ranged from 2.1% to 16.1%). At Hospital 1, a doubling of the number of employees reporting symptoms (which corresponds to 4 additional employees reporting symptoms at the mean for Hospital 1) is associated with a 5% increase in COVID-19 hospitalizations at Hospital 1 in 7 days (95% CI: (0.02, 0.07)). Conclusions: We found that a real-time employee health attestation tool used at a single hospital could be used to predict subsequent hospitalizations in 7 days at hospitals throughout a larger hospital network in New England.","924":"Recent works in Reinforcement Learning (RL) combine model-free (Mf)-RL algorithms with model-based (Mb)-RL approaches to get the best from both: asymptotic performance of Mf-RL and high sample-efficiency of Mb-RL. Inspired by these works, we propose a hierarchical framework that integrates online learning for the Mb-trajectory optimization with off-policy methods for the Mf-RL. In particular, two loops are proposed, where the Dynamic Mirror Descent based Model Predictive Control (DMD-MPC) is used as the inner loop to obtain an optimal sequence of actions. These actions are in turn used to significantly accelerate the outer loop Mf-RL. We show that our formulation is generic for a broad class of MPC based policies and objectives, and includes some of the well-known Mb-Mf approaches. Based on the framework we define two algorithms to increase sample efficiency of Off Policy RL and to guide end to end RL algorithms for online adaption respectively. Thus we finally introduce two novel algorithms: Dynamic-Mirror Descent Model Predictive RL(DeMoRL), which uses the method of elite fractions for the inner loop and Soft Actor-Critic (SAC) as the off-policy RL for the outer loop and Dynamic-Mirror Descent Model Predictive Layer(DeMo Layer), a special case of the hierarchical framework which guides linear policies trained using Augmented Random Search(ARS). Our experiments show faster convergence of the proposed DeMo RL, and better or equal performance compared to other Mf-Mb approaches on benchmark MuJoCo control tasks. The DeMo Layer was tested on classical Cartpole and custom-built Quadruped trained using Linear Policy.","925":"Contrary to expectations that the increased connectivity offered by the internet and particularly Online Social Networks (OSNs) would result in broad consensus on contentious issues, we instead frequently observe the formation of polarised echo chambers, in which only one side of an argument is entertained. These can progress to filter bubbles, actively filtering contrasting opinions, resulting in vulnerability to misinformation and increased polarisation on social and political issues. These have real-world effects when they spread offline, such as vaccine hesitation and violence. This work seeks to develop a better understanding of how echo chambers manifest in different discussions dealing with different issues over an extended period of time. We explore the activities of two groups of polarised accounts across three Twitter discussions in the Australian context. We found Australian Twitter accounts arguing against marriage equality in 2017 were more likely to support the notion that arsonists were the primary cause of the 2019\/2020 Australian bushfires, and those supporting marriage equality argued against that arson narrative. We also found strong evidence that the stance people took on marriage equality in 2017 did not predict their political stance in discussions around the Australian federal election two years later. Although mostly isolated from each other, we observe that in certain situations the polarised groups may interact with the broader community, which offers hope that the echo chambers may be reduced with concerted outreach to members.","926":"We report on a search for compact binary coalescences where at least one binary component has a mass between 0.2 $M_\\odot$ and 1.0 $M_\\odot$ in Advanced LIGO and Advanced Virgo data collected between 1 April 2019 1500 UTC and 1 October 2019 1500 UTC. We extend previous analyses in two main ways: we include data from the Virgo detector and we allow for more unequal mass systems, with mass ratio $q \\geq 0.1$. We do not report any gravitational-wave candidates. The most significant trigger has a false alarm rate of 0.14 $\\mathrm{yr}^{-1}$. This implies an upper limit on the merger rate of subsolar binaries in the range $[220-24200] \\mathrm{Gpc}^{-3} \\mathrm{yr}^{-1}$, depending on the chirp mass of the binary. We use this upper limit to derive astrophysical constraints on two phenomenological models that could produce subsolar-mass compact objects. One is an isotropic distribution of equal-mass primordial black holes. Using this model, we find that the fraction of dark matter in primordial black holes is $f_\\mathrm{PBH} \\equiv \\Omega_\\mathrm{PBH} \/ \\Omega_\\mathrm{DM} \\lesssim 6\\%$. The other is a dissipative dark matter model, in which fermionic dark matter can collapse and form black holes. The upper limit on the fraction of dark matter black holes depends on the minimum mass of the black holes that can be formed: the most constraining result is obtained at $M_\\mathrm{min}=1 M_\\odot$, where $f_\\mathrm{DBH} \\equiv \\Omega_\\mathrm{PBH} \/ \\Omega_\\mathrm{DM} \\lesssim 0.003\\%$. These are the tightest limits on spinning subsolar-mass binaries to date.","927":"We study a multi-group SAIRS-type epidemic model with vaccination. The role of asymptomatic and symptomatic infectious individuals is explicitly considered in the transmission pattern of the disease among the groups in which the population is divided. This is a natural extension of the homogeneous mixing SAIRS model with vaccination studied in Ottaviano et. al (2021). We provide a global stability analysis for the model. We determine the value of the basic reproduction number $\\mathcal{R}_0$ and prove that the disease-free equilibrium is globally asymptotically stable if $\\mathcal{R}_0<1$. In the case of the SAIRS model without vaccination, we prove the global asymptotic stability of the disease-free equilibrium also when $\\mathcal{R}_0=1$. Moreover, if $\\mathcal{R}_0>1$, the disease-free equilibrium is unstable and a unique endemic equilibrium exists. First, we investigate the local asymptotic stability of the endemic equilibrium and subsequently its global stability, for two variations of the original model. Last, we provide numerical simulations to compare the epidemic spreading on different networks topologies.","928":"We investigate networks formed by keywords in tweets and study their community structure. Based on datasets of tweets mined from over seven hundred political figures in the U.S. and Canada, we hypothesize that such Twitter keyword networks exhibit a small number of communities. Our results are further reinforced by considering via so-called pseudo-tweets generated randomly and using AI-based language generation software. We speculate as to the possible origins of the small community hypothesis and further attempts at validating it.","929":"Data and data sources have become increasingly essential in recent decades. Scientists and researchers require more data to deploy AI approaches as the field continues to improve. In recent years, the rapid technological advancements have had a significant impact on human existence. One major field for collecting data is satellite technology. With the fast development of various satellite sensor equipment, synthetic aperture radar (SAR) images have become an important source of data for a variety of research subjects, including environmental studies, urban studies, coastal extraction, water sources, etc. Change detection and coastline detection are both achieved using SAR pictures. However, speckle noise is a major problem in SAR imaging. Several solutions have been offered to address this issue. One solution is to expose SAR images to spatial fuzzy clustering. Another solution is to separate speech. This study utilises the spatial function to overcome speckle noise and cluster the SAR images with the highest achieved accuracy. The spatial function is proposed in this work since the likelihood of data falling into one cluster is what this function is all about. When the spatial function is employed to cluster data in fuzzy logic, the clustering outcomes improve. The proposed clustering technique is us","930":"In spite of the fact that efficient compression methods for dense two-dimensional flow fields would be very useful for modern video codecs, hardly any research has been performed in this area so far. Our paper addresses this problem by proposing the first lossy diffusion-based codec for this purpose. It keeps only a few flow vectors on a coarse grid. Additionally stored edge locations ensure the accurate representation of discontinuities. In the decoding step, the missing information is recovered by homogeneous diffusion inpainting that incorporates the stored edges as reflecting boundary conditions. In spite of the simple nature of this codec, our experiments show that it achieves remarkable quality for compression ratios up to 800 : 1.","931":"To limit the spread of the novel coronavirus, governments across the world implemented extraordinary physical distancing policies, such as stay-at-home orders, and numerous studies aim to estimate their effects. Many statistical and econometric methods, such as difference-in-differences, leverage repeated measurements and variation in timing to estimate policy effects, including in the COVID-19 context. While these methods are less common in epidemiology, epidemiologic researchers are well accustomed to handling similar complexities in studies of individual-level interventions.\"Target trial emulation\"emphasizes the need to carefully design a non-experimental study in terms of inclusion and exclusion criteria, covariates, exposure definition, and outcome measurement -- and the timing of those variables. We argue that policy evaluations using group-level longitudinal (\"panel\") data need to take a similar careful approach to study design, which we refer to as\"policy trial emulation.\"This is especially important when intervention timing varies across jurisdictions; the main idea is to construct target trials separately for each\"treatment cohort\"(states that implement the policy at the same time) and then aggregate. We present a stylized analysis of the impact of state-level stay-at-home orders on total coronavirus cases. We argue that estimates from panel methods -- with the right data and careful modeling and diagnostics -- can help add to our understanding of many policies, though doing so is often challenging.","932":"On 26 January 2021, India witnessed a national embarrassment from the demographic least expected from - farmers. People across the nation watched in horror as a pseudo-patriotic mob of farmers stormed capital Delhi and vandalized the national pride- Red Fort. Investigations that followed the event revealed the existence of a social media trail that led to the likes of such an event. Consequently, it became essential and necessary to archive this trail for social media analysis - not only to understand the bread-crumbs that are dispersed across the trail but also to visualize the role played by misinformation and fake news in this event. In this paper, we propose the tractor2twitter dataset which contains around 0.05 million tweets that were posted before, during, and after this event. Also, we benchmark our dataset with an Explainable AI ML model for classification of each tweet into either of the three categories - disinformation, misinformation, and opinion.","933":"Blended learning (BL) is a recent tread among many options that can best fit learners' needs, regardless of time and place. This study aimed to discover students' perceptions of BL and the challenges faced by them while using technology. This quantitative study used data gathered from 300 students enrolled in four public universities in the Sindh province of Pakistan. the finding shows that students were compatible with the use of technology, and it has a positive effect on their academic experience. The study also showed that the use of technology encourages peer collaboration. The challenges found include: neither teacher support nor a training program was provided to the students for the course which needed to shift from a traditional face to face paradigm to a blended format, a lake of space lies with skills in a laboratory assistants for the courses with a blended format and as shortage of high tech computer laboratories \/ computer units to run these courses. Therefore, it is recommended that the authorities must develop and incorporate a comprehensive mechanism for the effective implementation of BL in the learning teaching-learning process heads of the departments should also provide additional computing infrastructure to their departments.","934":"The economic downturn and the air travel crisis triggered by the recent coronavirus pandemic pose a substantial threat to the new consumer class of many emerging economies. In Brazil, considerable improvements in social inclusion have fostered the emergence of hundreds of thousands of first-time fliers over the past decades. We apply a two-step regression methodology in which the first step consists of identifying air transport markets characterized by greater social inclusion, using indicators of the local economies' income distribution, credit availability, and access to the Internet. In the second step, we inspect the drivers of the plunge in air travel demand since the pandemic began, differentiating markets by their predicted social inclusion intensity. After controlling for potential endogeneity stemming from the spread of COVID-19 through air travel, our results suggest that short and low-density routes are among the most impacted airline markets and that business-oriented routes are more impacted than leisure ones. Finally, we estimate that a market with 1 per cent higher social inclusion is associated with a 0.153 per cent to 0.166 per cent more pronounced decline in demand during the pandemic. Therefore, markets that have benefited from greater social inclusion in the country may be the most vulnerable to the current crisis.","935":"The novel coronavirus disease, named COVID-19, emerged in China in December 2019, and has rapidly spread around the world. It is clearly urgent to fight COVID-19 at global scale. The development of methods for identifying drug uses based on phenotypic data can improve the efficiency of drug development. However, there are still many difficulties in identifying drug applications based on cell picture data. This work reported one state-of-the-art machine learning method to identify drug uses based on the cell image features of 1024 drugs generated in the LINCS program. Because the multi-dimensional features of the image are affected by non-experimental factors, the characteristics of similar drugs vary greatly, and the current sample number is not enough to use deep learning and other methods are used for learning optimization. As a consequence, this study is based on the supervised ITML algorithm to convert the characteristics of drugs. The results show that the characteristics of ITML conversion are more conducive to the recognition of drug functions. The analysis of feature conversion shows that different features play important roles in identifying different drug functions. For the current COVID-19, Chloroquine and Hydroxychloroquine achieve antiviral effects by inhibiting endocytosis, etc., and were classified to the same community. And Clomiphene in the same community inibited the entry of Ebola Virus, indicated a similar MoAs that could be reflected by cell image.","936":"We consider a random process on recursive trees, with three types of events. Vertices give birth at a constant rate (growth), each edge may be removed independently (fragmentation of the tree) and clusters are frozen with a rate proportional to their size (isolation of connected component). A phase transition occurs when the isolation is able to stop the growth fragmentation process and cause extinction. When the process survives, we characterize its growth and prove that the empirical measure of clusters a.s. converges to a limit law on recursive trees. This approach exploits the branching structure associated to the size of clusters, which is inherited from the splitting property of random recursive trees. This issue is motivated by the control of epidemics and contact-tracing where clusters correspond to subtrees of infected individuals that can be identified and isolated.","937":"The VirtualCube system is a 3D video conference system that attempts to overcome some limitations of conventional technologies. The key ingredient is VirtualCube, an abstract representation of a real-world cubicle instrumented with RGBD cameras for capturing the 3D geometry and texture of a user. We design VirtualCube so that the task of data capturing is standardized and significantly simplified, and everything can be built using off-the-shelf hardware. We use VirtualCubes as the basic building blocks of a virtual conferencing environment, and we provide each VirtualCube user with a surrounding display showing life-size videos of remote participants. To achieve real-time rendering of remote participants, we develop the V-Cube View algorithm, which uses multi-view stereo for more accurate depth estimation and Lumi-Net rendering for better rendering quality. The VirtualCube system correctly preserves the mutual eye gaze between participants, allowing them to establish eye contact and be aware of who is visually paying attention to them. The system also allows a participant to have side discussions with remote participants as if they were in the same room. Finally, the system sheds lights on how to support the shared space of work items (e.g., documents and applications) and track the visual attention of participants to work items.","938":"Actually, after one year it is recognized that the evolution of COVID-19 is different in each country or region around the world. In this paper, we do a revision to the date about COVID-19 evolution in Mexico, we explain where the main epicenter and states with most high impact. Mexico has a particular geographical position in the American continent because it is a natural bridge between the USA and Latin America, that represents a special point of propagation because between other facts this virus is transported by people of different nationalities migrating to the USA. The research in this paper helps to understand why Mexico is one of the countries with the most high mortality impact by this new virus and how the lockdown works in the population. Finally, we give a practical perspective as this evolution is a complex system.","939":"High-quality data is critical to train performant Machine Learning (ML) models, highlighting the importance of Data Quality Management (DQM). Existing DQM schemes often cannot satisfactorily improve ML performance because, by design, they are oblivious to downstream ML tasks. Besides, they cannot handle various data quality issues (especially those caused by adversarial attacks) and have limited applications to only certain types of ML models. Recently, data valuation approaches (e.g., based on the Shapley value) have been leveraged to perform DQM; yet, empirical studies have observed that their performance varies considerably based on the underlying data and training process. In this paper, we propose a task-driven, multi-purpose, model-agnostic DQM framework, DataSifter, which is optimized towards a given downstream ML task, capable of effectively removing data points with various defects, and applicable to diverse models. Specifically, we formulate DQM as an optimization problem and devise a scalable algorithm to solve it. Furthermore, we propose a theoretical framework for comparing the worst-case performance of different DQM strategies. Remarkably, our results show that the popular strategy based on the Shapley value may end up choosing the worst data subset in certain practical scenarios. Our evaluation shows that DataSifter achieves and most often significantly improves the state-of-the-art performance over a wide range of DQM tasks, including backdoor, poison, noisy\/mislabel data detection, data summarization, and data debiasing.","940":"Understanding a medical conversation between a patient and a physician poses a unique natural language understanding challenge since it combines elements of standard open ended conversation with very domain specific elements that require expertise and medical knowledge. Summarization of medical conversations is a particularly important aspect of medical conversation understanding since it addresses a very real need in medical practice: capturing the most important aspects of a medical encounter so that they can be used for medical decision making and subsequent follow ups. In this paper we present a novel approach to medical conversation summarization that leverages the unique and independent local structures created when gathering a patient's medical history. Our approach is a variation of the pointer generator network where we introduce a penalty on the generator distribution, and we explicitly model negations. The model also captures important properties of medical conversations such as medical knowledge coming from standardized medical ontologies better than when those concepts are introduced explicitly. Through evaluation by doctors, we show that our approach is preferred on twice the number of summaries to the baseline pointer generator model and captures most or all of the information in 80% of the conversations making it a realistic alternative to costly manual summarization by medical experts.","941":"The increasing need for reducing greenhouse gas emissions and the drive for green cities have promoted the use of electric vehicles due to their environmental benefits. In fact, countries have set their own targets and are offering incentives for people to purchase EVs as opposed to traditional gasoline-powered cars. Manufacturers have been hastily deploying charging stations to meet the charging requirements of the EVs on the road. This rapid deployment has contributed to the EV ecosystem's lack of proper security measures, raising multiple questions related to the power grid security and vulnerability. In this paper, we offer a complete examination of the EV ecosystem from the vulnerability to the attacks and finally the solutions. We start by examining the existing vulnerabilities in the EV ecosystem that can be exploited to control the EV charging and launch attacks against the power grid. We then discuss the non-linear nature of the EV charging load and simulate multiple attacks that can be launched against the power grid using these EVs. EV loads have high reactive power demand which can have a larger impact on the grid compared to residential loads. We perform simulations on two power grids and demonstrate that while the grid can recover after a 48 MW attack utilizing traditional residential loads, a smaller 30 MW EV load attack can completely destabilize the system. Finally, we suggest several patches for the existing vulnerabilities and discuss two methods aimed at detecting EV attacks.","942":"Distance correlation is a new class of multivariate dependence coefficients applicable to random vectors of arbitrary and not necessarily equal dimension. Distance covariance and distance correlation are analogous to product-moment covariance and correlation, but generalize and extend these classical bivariate measures of dependence. Distance correlation characterizes independence: it is zero if and only if the random vectors are independent. The notion of covariance with respect to a stochastic process is introduced, and it is shown that population distance covariance coincides with the covariance with respect to Brownian motion; thus, both can be called Brownian distance covariance. In the bivariate case, Brownian covariance is the natural extension of product-moment covariance, as we obtain Pearson product-moment covariance by replacing the Brownian motion in the definition with identity. The corresponding statistic has an elegantly simple computing formula. Advantages of applying Brownian covariance and correlation vs the classical Pearson covariance and correlation are discussed and illustrated.","943":"Kiosks are a popular self-service option in many fast-food restaurants, they save time for the visitors and save labor for the fast-food chains. In this paper, we propose an effective design of a kiosk shopping cart recommender system that combines a language model as a vectorizer and a neural network-based classifier. The model performs better than other models in offline tests and exhibits performance comparable to the best models in A\/B\/C tests.","944":"The COVID-19 is sweeping the world with deadly consequences. Its contagious nature and clinical similarity to other pneumonias make separating subjects contracted with COVID-19 and non-COVID-19 viral pneumonia a priority and a challenge. However, COVID-19 testing has been greatly limited by the availability and cost of existing methods, even in developed countries like the US. Intrigued by the wide availability of routine blood tests, we propose to leverage them for COVID-19 testing using the power of machine learning. Two proven-robust machine learning model families, random forests (RFs) and support vector machines (SVMs), are employed to tackle the challenge. Trained on blood data from 208 moderate COVID-19 subjects and 86 subjects with non-COVID-19 moderate viral pneumonia, the best result is obtained in an SVM-based classifier with an accuracy of 84%, a sensitivity of 88%, a specificity of 80%, and a precision of 92%. The results are found explainable from both machine learning and medical perspectives. A privacy-protected web portal is set up to help medical personnel in their practice and the trained models are released for developers to further build other applications. We hope our results can help the world fight this pandemic and welcome clinical verification of our approach on larger populations.","945":"In India, post the demonetization exercise in 2016, digital payments have become extremely popular. Among them, the volume of transactions using Paytm wallets and UPI (Unified Payment Interface) have grown manifold. The lockdowns due to COVID-19 Pandemic have furthered this trend. Side by side, crypto-currencies such as bitcoin are also gaining traction. Many countries are considering issuing a Digital Currency via their Central Banks. In this paper, we propose a novel Decentralized Digital Currency System (DDCS) that makes use of Merkle Hash-Trees as Authenticated Data Structures. DDCS uses a Ledger-less, distributed, peer-to-peer architecture. We name the proposed currency $\\delta$-Money. $\\delta$-Money is intended as a replacement for physical currency and has in-built security features that rival crypto-currencies. Transactions using $\\delta$-Money happen in a disintermediated manner but with post-facto reconciliation. In place of Central Bank-issued Digital Currency (CBDC), we envisage a scenario where multiple Payment Banks issue digital currencies that have stable valuations without being subject to either volatility or perennial devaluation.","946":"We use the stellar evolution code MESA-binary and follow the evolution of six exoplanets to determine their potential role in the future evolution of their parent star on the red giant branch (RGB) and on the asymptotic giant branch (AGB). We limit this study to planets with orbits that have semi-major axis of 1AU0.25, and having a parent star of mass M>1Mo. We find that the star HIP 75458 will engulf its planet HIP75458 b during its RGB phase. The planet will remove the envelope and terminate the RGB evolution, leaving a bare helium core of mass 0.4Mo that will evolve to form a helium white dwarf. Only in one system out of six, the planet beta Pic c will enter the envelope of its parent star during the AGB phase. For that to occur, we have to reduce the wind mass-loss rate by a factor of about four from its commonly used value. This strengthens an early conclusion, which was based on exoplanets with circular orbits, that states that to have a non-negligible fraction of AGB stars that engulf planets we should consider lower wind mass-loss rates of isolated AGB stars (before they are spun-up by a companion). Such an engulfed planet might lead to the shaping of the AGB mass-loss geometry to form an elliptical planetary nebula.","947":"In a circular economy, tracking the flow of second-life components for quality control is critical. Tokenization can enhance the transparency of the flow of second-life components. However, simple tokenization does not correspond to real economic models and lacks the ability to finely manage complex business processes. In particular, existing systems have to take into account the different roles of the parties in the supply chain. Based on the Algorand blockchain, we propose a role-based token management scheme, which can achieve authentication, synthesis, circulation, and reuse of these second-life components in a trustless environment. The proposed scheme not only achieves fine-grained and scalable second-life component management, but also enables on-chain trading, subsidies, and green-bond issuance. Furthermore, we implemented and performed scalability tests for the proposed architecture on Algorand blockchain using its smart contracts and Algorand Standard Assets (ASA). The open-source implementation, tests, along with results are available on our Github page.","948":"Responding to the U.S. opioid crisis requires a holistic approach supported by evidence from linking and analyzing multiple data sources. This paper discusses how 20 available resources can be combined to answer pressing public health questions related to the crisis. It presents a network view based on U.S. geographical units and other standard concepts, crosswalked to communicate the coverage and interlinkage of these resources. These opioid-related datasets can be grouped by four themes: (1) drug prescriptions, (2) opioid related harms, (3) opioid treatment workforce, jobs, and training, and (4) drug policy. An interactive network visualization was created and is freely available online; it lets users explore key metadata, relevant scholarly works, and data interlinkages in support of informed decision making through data analysis.","949":"Open-retrieval question answering systems are generally trained and tested on large datasets in well-established domains. However, low-resource settings such as new and emerging domains would especially benefit from reliable question answering systems. Furthermore, multilingual and cross-lingual resources in emergent domains are scarce, leading to few or no such systems. In this paper, we demonstrate a cross-lingual open-retrieval question answering system for the emergent domain of COVID-19. Our system adopts a corpus of scientific articles to ensure that retrieved documents are reliable. To address the scarcity of cross-lingual training data in emergent domains, we present a method utilizing automatic translation, alignment, and filtering to produce English-to-all datasets. We show that a deep semantic retriever greatly benefits from training on our English-to-all data and significantly outperforms a BM25 baseline in the cross-lingual setting. We illustrate the capabilities of our system with examples and release all code necessary to train and deploy such a system.","950":"Let $G$ be a bipartite graph where every node has a strict ranking of its neighbors. For every node, its preferences over neighbors extend naturally to preferences over matchings. Matching $N$ is more popular than matching $M$ if the number of nodes that prefer $N$ to $M$ is more than the number that prefer $M$ to $N$. A maximum matching $M$ in $G$ is a\"popular max-matching\"if there is no maximum matching in $G$ that is more popular than $M$. Such matchings are relevant in applications where the set of admissible solutions is the set of maximum matchings and we wish to find a best maximum matching as per node preferences. It is known that a popular max-matching always exists in $G$. Here we show a compact extended formulation for the popular max-matching polytope. So when there are edge costs, a min-cost popular max-matching in $G$ can be computed in polynomial time. This is in contrast to the min-cost popular matching problem which is known to be NP-hard. We also consider Pareto-optimality, which is a relaxation of popularity, and show that computing a min-cost Pareto-optimal matching\/max-matching is NP-hard.","951":"Out of the numerous hazards posing a threat to sustainable environmental conditions in the 21st century, only a few have a graver impact than air pollution. Its importance in determining the health and living standards in urban settings is only expected to increase with time. Various factors ranging from emissions from traffic and power plants, household emissions, natural causes are known to be primary causal agents or influencers behind rising air pollution levels. However, the lack of large scale data involving the major factors has hindered the research on the causes and relations governing the variability of the different air pollutants. Through this work, we introduce a large scale city-wise dataset for exploring the relationships among these agents over a long period of time. We analyze and explore the dataset to bring out inferences which we can derive by modeling the data. Also, we provide a set of benchmarks for the problem of estimating or forecasting pollutant levels with a set of diverse models and methodologies. Through our paper, we seek to provide a ground base for further research into this domain that will demand critical attention of ours in the near future.","952":"We present a high-contrast imaging search for Pa$\\beta$ line emission from protoplanets in the PDS~70 system with Keck\/OSIRIS integral field spectroscopy. We applied the high-resolution spectral differential imaging technique to the OSIRIS $J$-band data but did not detect the Pa$\\beta$ line at the level predicted using the parameters of \\cite{Hashimoto2020}. This lack of Pa$\\beta$ emission suggests the MUSE-based study may have overestimated the line width of H$\\alpha$. We compared our Pa$\\beta$ detection limits with the previous H$\\alpha$ flux and H$\\beta$ limits and estimated $A_{\\rm V}$ to be $\\sim0.9$ and 2.0 for PDS~70~b and c respectively. In particular, PDS~70~b's $A_{\\rm V}$ is much smaller than implied by high-contrast near-infrared studies, which suggests the infrared-continuum photosphere and the hydrogen-emitting regions exist at different heights above the forming planet.","953":"Despite substantial advances in global measles vaccination, measles disease burden remains high in many low- and middle-income countries. A key public health strategy for controling measles in such high-burden settings is to conduct supplementary immunization activities (SIAs) in the form of mass vaccination campaigns, in addition to delivering scheduled vaccination through routine immunization (RI) programs. To achieve balanced implementations of RI and SIAs, robust measurement of sub-national RI-specific coverage is crucial. In this paper, we develop a space-time smoothing model for estimating RI-specific coverage of the first dose of measles-containing-vaccines (MCV1) at sub-national level using complex survey data. The application that motivated this work is estimation of the RI-specific MCV1 coverage in Nigeria's 36 states and the Federal Capital Territory. Data come from four Demographic and Health Surveys, three Multiple Indicator Cluster Surveys, and two National Nutrition and Health Surveys conducted in Nigeria between 2003 and 2018. Our method incorporates information from the SIA calendar published by the World Health Organization and accounts for the impact of SIAs on the overall MCV1 coverage, as measured by cross-sectional surveys. The model can be used to analyze data from multiple surveys with different data collection schemes and construct coverage estimates with uncertainty that reflects the various sampling designs. Implementation of our method can be done efficiently using integrated nested Laplace approximation (INLA).","954":"Contact tracing is a well-established and effective approach for the containment of the spread of infectious diseases. While Bluetooth-based contact tracing method using phones has become popular recently, these approaches suffer from the need for a critical mass adoption to be effective. In this paper, we present WiFiTrace, a network-centric approach for contact tracing that relies on passive WiFi sensing with no client-side involvement. Our approach exploits WiFi network logs gathered by enterprise networks for performance and security monitoring, and utilizes them for reconstructing device trajectories for contact tracing. Our approach is specifically designed to enhance the efficacy of traditional methods, rather than to supplant them with new technology. We designed an efficient graph algorithm to scale our approach to large networks with tens of thousands of users. The graph-based approach outperforms an indexed PostgresSQL in memory by at least 4.5X without any index update overheads or blocking. We have implemented a full prototype of our system and deployed it on two large university campuses. We validated our approach and demonstrate its efficacy using case studies and detailed experiments using real-world WiFi datasets.","955":"We are investigating deterministic SIS dynamics on large networks starting from only a few infected individuals. Under mild assumptions we show that any two epidemic curves { on the same network and with the same parameters { are almost identical up to time translation when initial conditions are small enough regardless of how infections are distributed at the beginning. The limit of an epidemic starting from the infinite past with infinitesimal prevalence is identified as the nontrivial eternal solution connecting the disease free state with the endemic equilibrium. Our framework covers several benchmark models including the N-Intertwined Mean Field Approximation (NIMFA) and the Inhomogeneous Mean Field Approximation (IMFA).","956":"During a pandemic in which aerosol and droplet transmission is possible, the demand for masks that meet medical or workplace standards can prevent most individuals or organizations from obtaining suitable protection. Cloth masks are widely believed to impede droplet and aerosol transmission but most are constructed from materials with unknown filtration efficiency, airflow resistance and water resistance. Further, there has been no clear guidance on the most important performance metrics for the materials used by the general public (as opposed to high-risk healthcare settings). Here we provide data on a range of common fabrics that might be used to construct masks. None of the materials were suitable for masks meeting the N95 NIOSH standard, but many could provide useful filtration (>90%) of 3 micron particles (a plausible challenge size for human generated aerosols), with low pressure drop. These were: nonwoven sterile wraps, dried baby wipes and some double-knit cotton materials. Decontamination of N95 masks using isopropyl alcohol produces the expected increase in particle penetration, but for 3 micron particles, filtration efficiency is still well above 95%. Tightly woven thin fabrics, despite having the visual appearance of a good particle barrier, had remarkably low filtration efficiency and high pressure drop. These differences in filtration performance can be partly explained by the material structure; the better structures expose individual fibers to the flow while the poor materials may have small fundamental fibers but these are in tightly bundled yarns. The fit and use of the whole mask are critical factors not addressed in this work. Despite the complexity of the design of a very good mask, it is clear that for the larger aerosol particles, any mask will provide substantial protection to the wearer and those around them.","957":"The first goal of this study is to quantify the magnitude and spatial variability of air quality changes in the US during the COVID-19 pandemic. We focus on two federally regulated pollutants, nitrogen dioxide (NO2), and fine particulate matter (PM2.5). Observed concentrations at all available ground monitoring sites (240 and 480 for NO2 and PM2.5, respectively) were compared between April 2020 and April of the prior five years, 2015-2019, as the baseline. Large statistically significant decreases in NO2 concentrations were found at more than 65% of the monitoring sites, with an average drop of 2 ppb when compared to the mean of the previous five years. The same patterns are confirmed by satellite-derived NO2 column totals from NASA OMI. PM2.5 concentrations from the ground monitoring sites, however, were more likely to be higher. The second goal of this study is to explain the different responses of the two pollutants during the COVID-19 pandemic. The hypothesis put forward is that the shelter-in-place measures affected peoples' driving patterns most dramatically, thus passenger vehicle NO2 emissions were reduced. Commercial vehicles and electricity demand for all purposes remained relatively unchanged, thus PM2.5 concentrations did not drop significantly. To establish a correlation between the observed NO2 changes and the extent to which people were sheltering in place, we use a mobility index, which was produced and made public by Descartes Labs. This mobility index aggregates cell phone usage at the county level to capture changes in human movement over time. We found a strong correlation between the observed decreases in NO2 concentrations and decreases in human mobility. By contrast, no discernible pattern was detected between mobility and PM2.5 concentrations changes, suggesting that decreases in personal-vehicle traffic alone may not be effective at reducing PM2.5 pollution.","958":"Blind Descent uses constrained but, guided approach to learn the weights. The probability density function is non-zero in the infinite space of the dimension (case in point: Gaussians and normal probability distribution functions). In Blind Descent paper, some of the implicit ideas involving layer by layer training and filter by filter training (with different batch sizes) were proposed as probable greedy solutions. The results of similar experiments are discussed. Octave (and proposed PyTorch variants) source code of the experiments of this paper can be found at https:\/\/github.com\/PrasadNR\/Attempted-Blind-Constrained-Descent-Experiments-ABCDE- . This is compared against the ABCDE derivatives of the original PyTorch source code of https:\/\/github.com\/akshat57\/Blind-Descent .","959":"We estimate changes in the rates of five FBI Part 1 crime (homicide, auto theft, burglary, robbery, and larceny) during the COVID-19 pandemic from March through December 2020. Using publicly available weekly crime count data from 29 of the 70 largest cities in the U.S. from January 2018 through December 2020, three different linear regression model specifications are used to detect changes. One detects whether crime trends in four 2020 pre- and post-pandemic periods differ from those in 2018 and 2019. A second looks in more detail at the spring 2020 lockdowns to detect whether crime trends changed over successive biweekly periods into the lockdown. The third uses a city-level openness index that we created for the purpose of examining whether the degree of openness was associated with changing crime rates. For homicide and auto theft, we find significant increases during all or most of the pandemic. By contrast, we find significant declines in robbery and larceny during all or part of the pandemic and no significant changes in burglary over the course of the pandemic. Only larceny rates fluctuated with the degree of each city's lockdown. It is unusual for crime rates to move in different directions, and the reasons for the mixed findings for these five Part 1 Index crimes, one with no change, two with sustained increases, and two with sustained decreases, are not yet known. We hypothesize that the reasons may be related to changes in opportunity, and the pandemic provides unique opportunities for future research to better understand the forces impacting crime rates. In the absence of a clear understanding of the mechanisms by which the pandemic affected crime, in the spirit of evidence-based crime policy, we caution against advancing policy at this time based on lessons learned from the pandemic\"natural experiment.\"","960":"In this paper we introduce a simple modal logic framework to reason about the expertise of an information source. In the framework, a source is an expert on a proposition $p$ if they are able to correctly determine the truth value of $p$ in any possible world. We also consider how information may be false, but true after accounting for the lack of expertise of the source. This is relevant for modelling situations in which information sources make claims beyond their domain of expertise. We use non-standard semantics for the language based on an expertise set with certain closure properties. It turns out there is a close connection between our semantics and S5 epistemic logic, so that expertise can be expressed in terms of knowledge at all possible states. We use this connection to obtain a sound and complete axiomatisation.","961":"In order to model an epidemic, different approaches can be adopted. Mainly, the deterministic approach and the stochastic one. Recently, a huge amount of literature has been published using the two approaches. The aim of this paper is to illustrate the usual framework used for commonly adopted compartmental models in epidemiology and introduce variant analytic and numerical tools that interfere on each one of those models, as well as the general related types of existing, ongoing and future possible contributions.","962":"General equilibrium macroeconomic models are a core tool used by policymakers to understand a nation's economy. They represent the economy as a collection of forward-looking actors whose behaviours combine, possibly with stochastic effects, to determine global variables (such as prices) in a dynamic equilibrium. However, standard semi-analytical techniques for solving these models make it difficult to include the important effects of heterogeneous economic actors. The COVID-19 pandemic has further highlighted the importance of heterogeneity, for example in age and sector of employment, in macroeconomic outcomes and the need for models that can more easily incorporate it. We use techniques from reinforcement learning to solve such models incorporating heterogeneous agents in a way that is simple, extensible, and computationally efficient. We demonstrate the method's accuracy and stability on a toy problem for which there is a known analytical solution, its versatility by solving a general equilibrium problem that includes global stochasticity, and its flexibility by solving a combined macroeconomic and epidemiological model to explore the economic and health implications of a pandemic. The latter successfully captures plausible economic behaviours induced by differential health risks by age.","963":"We introduce GACELA, a generative adversarial network (GAN) designed to restore missing musical audio data with a duration ranging between hundreds of milliseconds to a few seconds, i.e., to perform long-gap audio inpainting. While previous work either addressed shorter gaps or relied on exemplars by copying available information from other signal parts, GACELA addresses the inpainting of long gaps in two aspects. First, it considers various time scales of audio information by relying on five parallel discriminators with increasing resolution of receptive fields. Second, it is conditioned not only on the available information surrounding the gap, i.e., the context, but also on the latent variable of the conditional GAN. This addresses the inherent multi-modality of audio inpainting at such long gaps and provides the option of user-defined inpainting. GACELA was tested in listening tests on music signals of varying complexity and gap durations ranging from 375~ms to 1500~ms. While our subjects were often able to detect the inpaintings, the severity of the artifacts decreased from unacceptable to mildly disturbing. GACELA represents a framework capable to integrate future improvements such as processing of more auditory-related features or more explicit musical features.","964":"The World Health Organization (WHO) announced that COVID-19 was a pandemic disease on the 11th of March as there were 118K cases in several countries and territories. Numerous researchers worked on forecasting the number of confirmed cases since anticipating the growth of the cases helps governments adopting knotty decisions to ease the lockdowns orders for their countries. These orders help several people who have lost their jobs and support gravely impacted businesses. Our research aims to investigate the relation between Google search trends and the spreading of the novel coronavirus (COVID-19) over countries worldwide, to predict the number of cases. We perform a correlation analysis on the keywords of the related Google search trends according to the number of confirmed cases reported by the WHO. After that, we applied several machine learning techniques (Multiple Linear Regression, Non-negative Integer Regression, Deep Neural Network), to forecast the number of confirmed cases globally based on historical data as well as the hybrid data (Google search trends). Our results show that Google search trends are highly associated with the number of reported confirmed cases, where the Deep Learning approach outperforms other forecasting techniques. We believe that it is not only a promising approach for forecasting the confirmed cases of COVID-19, but also for similar forecasting problems that are associated with the related Google trends.","965":"This cross-sectional study characterizes users' attitudes towards the face mask requirements introduced by the Russian government as a response to the COVID-19 pandemic. We study how they relate to other users' characteristics such as age, gender, and political attitudes. Our results indicate that men and elder individuals - demographic groups that are most vulnerable to COVID-19 -- underestimate the benefits of wearing face masks. We also discovered that users in opposition to the Russian government highly approve of this anti-COVID-19 measure -- an oppositionist will approve of the face mask requirements with the probability of 0.95. For those who support the Russian government, the odds of approval are merely 0.45.","966":"Methanol, one of the simplest complex organic molecules in the Interstellar Medium (ISM), has been shown to be present and extended in cold environments such as starless cores. We aim at studying methanol emission across several starless cores and investigate the physical conditions at which methanol starts to be efficiently formed, as well as how the physical structure of the cores and their surrounding environment affect its distribution. Methanol and C$^{18}$O emission lines at 3 mm have been observed with the IRAM 30m telescope within the large program\"Gas phase Elemental abundances in Molecular CloudS\"(GEMS) towards 66 positions across 12 starless cores in the Taurus Molecular Cloud. A non-LTE radiative transfer code was used to compute the column densities in all positions. We then used state-of-the-art chemical models to reproduce our observations. We have computed N(CH$_3$OH)\/N(C$^{18}$O) column density ratios for all the observed offsets, and two different behaviours can be recognised: the cores where the ratio peaks at the dust peak, and the cores where the ratio peaks with a slight offset with respect to the dust peak ($\\sim $10000 AU). We suggest that the cause of this behaviour is the irradiation on the cores due to protostars nearby which accelerate energetic particles along their outflows. The chemical models, which do not take into account irradiation variations, can reproduce fairly well the overall observed column density of methanol, but cannot reproduce the two different radial profiles observed. We confirm the substantial effect of the environment onto the distribution of methanol in starless cores. We suggest that the clumpy medium generated by protostellar outflows might cause a more efficient penetration of the interstellar radiation field in the molecular cloud and have an impact on the distribution of methanol in starless cores.","967":"Child Sexual Abuse Media (CSAM) is any visual record of a sexually-explicit activity involving minors. CSAM impacts victims differently from the actual abuse because the distribution never ends, and images are permanent. Machine learning-based solutions can help law enforcement quickly identify CSAM and block digital distribution. However, collecting CSAM imagery to train machine learning models has many ethical and legal constraints, creating a barrier to research development. With such restrictions in place, the development of CSAM machine learning detection systems based on file metadata uncovers several opportunities. Metadata is not a record of a crime, and it does not have legal restrictions. Therefore, investing in detection systems based on metadata can increase the rate of discovery of CSAM and help thousands of victims. We propose a framework for training and evaluating deployment-ready machine learning models for CSAM identification. Our framework provides guidelines to evaluate CSAM detection models against intelligent adversaries and models' performance with open data. We apply the proposed framework to the problem of CSAM detection based on file paths. In our experiments, the best-performing model is based on convolutional neural networks and achieves an accuracy of 0.97. Our evaluation shows that the CNN model is robust against offenders actively trying to evade detection by evaluating the model against adversarially modified data. Experiments with open datasets confirm that the model generalizes well and is deployment-ready.","968":"We introduce a digital twin of the classical compartmental SIR (Susceptible, Infected, Recovered) epidemic model and study the interrelation between the digital twin and the system. In doing so, we use Stieltjes derivatives to feed the data from the real system to the virtual model which, in return, improves it in real time. As a byproduct of the model, we present a precise mathematical definition of solution to the problem. We also analyze the existence and uniqueness of solutions, introduce the concept of Main Digital Twin and present some numerical simulations with real data of the COVID-19 epidemic, showing the accuracy of the proposed ideas.","969":"COVID-19 vaccine hesitancy has increased concerns about vaccine uptake required to overcome the pandemic and protect public health. A critical factor associated with anti-vaccine attitudes is the information shared on social media. In this work, we investigate misinformation communities and narratives that can contribute to COVID-19 vaccine hesitancy. During the pandemic, anti-science and political misinformation\/conspiracies have been rampant on social media. Therefore, we investigate misinformation and conspiracy groups and their characteristic behaviours in Twitter data collected on COVID-19 vaccines. We identify if any suspicious coordinated efforts are present in promoting vaccine misinformation, and find two suspicious groups - one promoting a 'Great Reset' conspiracy which suggests that the pandemic is orchestrated by world leaders to take control of the economy, with vaccine related misinformation and strong anti-vaccine and anti-social messages such as no lock-downs; and another promoting the Bioweapon theory. Misinformation promoted is largely from the anti-vaccine and far-right communities in the 3-core of the retweet graph, with its tweets proportion of conspiracy and questionable sources to reliable sources being much higher. In comparison with the mainstream and health news, the right-leaning community is more influenced by the anti-vaccine and far-right communities, which is also reflected in the disparate vaccination rates in left and right U.S. states. The misinformation communities are also more vocal, either in vaccine or other discussions, relative to remaining communities, besides other behavioral differences.","970":"The classic searches for supersymmetry have not given any strong indication for new physics. Therefore CMS is designing dedicated searches to target the more difficult and specific supersymmetry scenarios. This contribution present three such recent searches based on 13 TeV proton-proton collisions recorded with the CMS detector in 2016, 2017 and 2018: a search for heavy gluinos cascading via heavy next-to-lightest neutralino in final states with boosted Z bosons and missing transverse momentum; a search for compressed supersymmetry in final states with soft taus; and a search for compressed, long-lived charginos in hadronic final states with disappearing tracks.","971":"\\'{E}.\\~Ghys proved that the linking numbers of modular knots and the\"missing\"trefoil $K_{2,3}$ in $S^3$ coincide with the values of a highly ubiquitous function called the Rademacher symbol for ${\\rm SL}_2\\mathbb{Z}$. In this paper, we replace ${\\rm SL}_2\\mathbb{Z}=\\Gamma_{2,3}$ by the triangle group $\\Gamma_{p,q}$ for any coprime pair $(p,q)$ of integers with $2\\leq p<q$. We invoke the theory of harmonic Maass forms for $\\Gamma_{p,q}$ to introduce the notion of the Rademacher symbol $\\psi_{p,q}$, and provide several characterizations. Among other things, we generalize Ghys's theorem for modular knots around any\"missing\"torus knot $K_{p,q}$ in $S^3$ and in a lens space.","972":"We are currently facing a highly critical case of a world-wide pandemic. The novel coronavirus (SARS-CoV-2, a.k.a. COVID-19) has proved to be extremely contagious and the original outbreak from Asia has now spread to all continents. This situation will fruitfully profit from the study in regards of the spread of the virus, assessing effective countermeasures to weight the impact of the adopted strategies. The standard Susceptible-Infectious-Recovered (SIR) model is a very successful and widely used mathematical model for predicting the spread of an epidemic. We adopt the SIR model on a random network and extend the model to include control strategies {\\em lockdown} and {\\em testing} -- two often employed mitigation strategies. The ability of these strategies in controlling the pandemic spread is investigated by varying the effectiveness with which they are implemented. The possibility of a second outbreak is evaluated in detail after the mitigation strategies are withdrawn. We notice that, in any case, a sudden interruption of such mitigation strategies will likely induce a resurgence of a second outbreak, whose peak will be correlated to the number of susceptible individuals. In fact, we find that a population will remain vulnerable to the infection until the herd immunity is achieved. We also test our model with real statistics and information on the epidemic spread in South Korea, Germany, and New York and find a remarkable agreement with the simulation data.","973":"Human beings have been affected by disasters from the beginning of life, bringing them many sad memories. In the long struggle against disaster, people have devised a variety of methods to train relevant participants in disaster relief capabilities. However, many traditional training methods, such as disaster exercises may not provide effective training to meet the need of today. Serious games provide an innovative approach to train participants in disaster relief, and a large number of Serious Games for Disaster Relief (SGDRs) have been developed to train disaster planning and rescue capabilities. At the same time, there is no systematics phase description for disaster relief, which cannot effectively guide participants' work and training in disaster relief. Therefore, this paper proposes a comprehensive and professional disaster relief classification framework according to different relief work in each stage of the disaster. Based on this framework, we review the functions and technologies of serious games in each classification, which can offer reliable guidance for researchers to better understand and use SGDRs. In addition, we analyze the serious games in each category, point out the limitations, and provide some valuable advice for developers on game design.","974":"Contact tracing is a key tool for managing epidemic diseases like HIV, tuberculosis, and COVID-19. Manual investigations by human contact tracers remain a dominant way in which this is carried out. This process is limited by the number of contact tracers available, who are often overburdened during an outbreak or epidemic. As a result, a crucial decision in any contact tracing strategy is, given a set of contacts, which person should a tracer trace next? In this work, we develop a formal model that articulates these questions and provides a framework for comparing contact tracing strategies. Through analyzing our model, we give provably optimal prioritization policies via a clean connection to a tool from operations research called a\"branching bandit\". Examining these policies gives qualitative insight into trade-offs in contact tracing applications.","975":"The ongoing Coronavirus Disease (COVID-19) pandemic highlights the interconnected-ness of our present-day globalized world. With social distancing policies in place, virtual communication has become an important source of (mis)information. As increasing number of people rely on social media platforms for news, identifying misinformation has emerged as a critical task in these unprecedented times. In addition to being malicious, the spread of such information poses a serious public health risk. To this end, we design a dashboard to track misinformation on popular social media news sharing platform - Twitter. Our dashboard allows visibility into the social media discussions around Coronavirus and the quality of information shared on the platform as the situation evolves. We collect streaming data using the Twitter API from March 1, 2020 to date and provide analysis of topic clusters and social sentiments related to important emerging policies such as\"#socialdistancing\"and\"#workfromhome\". We track emerging hashtags over time, and provide location and time sensitive analysis of sentiments. In addition, we study the challenging problem of misinformation on social media, and provide a detection method to identify false, misleading and clickbait contents from Twitter information cascades. The dashboard maintains an evolving list of detected misinformation cascades with the corresponding detection scores, accessible online athttps:\/\/ksharmar.github.io\/index.html.","976":"Symptom checkers have emerged as an important tool for collecting symptoms and diagnosing patients, minimizing the involvement of clinical personnel. We developed a machine-learning-backed system, SmartTriage, which goes beyond conventional symptom checking through a tight bi-directional integration with the electronic medical record (EMR). Conditioned on EMR-derived patient history, our system identifies the patient's chief complaint from a free-text entry and then asks a series of discrete questions to obtain relevant symptomatology. The patient-specific data are used to predict detailed ICD-10-CM codes as well as medication, laboratory, and imaging orders. Patient responses and clinical decision support (CDS) predictions are then inserted back into the EMR. To train the machine learning components of SmartTriage, we employed novel data sets of over 25 million primary care encounters and 1 million patient free-text reason-for-visit entries. These data sets were used to construct: (1) a long short-term memory (LSTM) based patient history representation, (2) a fine-tuned transformer model for chief complaint extraction, (3) a random forest model for question sequencing, and (4) a feed-forward network for CDS predictions. In total, our system supports 337 patient chief complaints, which together make up $>90\\%$ of all primary care encounters at Kaiser Permanente.","977":"We present a simple yet novel time series imputation technique with the goal of constructing an irregular time series that is uniform across every sample in a data set. Specifically, we fix a grid defined by the midpoints of non-overlapping bins (dubbed\"slices\") of observation times and ensure that each sample has values for all of the features at that given time. This allows one to both impute fully missing observations to allow uniform time series classification across the entire data and, in special cases, to impute individually missing features. To do so, we slightly generalize the well-known class imbalance algorithm SMOTE \\cite{smote} to allow component wise nearest neighbor interpolation that preserves correlations when there are no missing features. We visualize the method in the simplified setting of 2-dimensional uncoupled harmonic oscillators. Next, we use tSMOTE to train an Encoder\/Decoder long-short term memory (LSTM) model with Logistic Regression for predicting and classifying distinct trajectories of different 2D oscillators. After illustrating the the utility of tSMOTE in this context, we use the same architecture to train a clinical model for COVID-19 disease severity on an imputed data set. Our experiments show an improvement over standard mean and median imputation techniques by allowing a wider class of patient trajectories to be recognized by the model, as well as improvement over aggregated classification models.","978":"We reflect on two museum visiting experiences that adopted the strategy of interpersonalization in which one visitor creates an experience for another. In the Gift app, visitors create personal mini-tours for specific others. In Never let me go, one visitor controls the experience of another by sending them remote instructions as they follow them around the museum. By reflecting on the design of these experiences and their deployment in museums we show how interpersonalization can deliver engaging social visits in which visitors make their own interpretations. We contrast the approach to previous research in customization and algorithmic personalization. We reveal how these experiences relied on intimacy between pairs of visitors but also between visitors and the museum. We propose that interpersonalization requires museums to step back to make space for interpretation, but that this then raises the challenge of how to reintroduce the museum's own perspective. Finally, we articulate strategies and challenges for applying this approach.","979":"The quality of sleep has a deep impact on people's physical and mental health. People with insufficient sleep are more likely to report physical and mental distress, activity limitation, anxiety, and pain. Moreover, in the past few years, there has been an explosion of applications and devices for activity monitoring and health tracking. Signals collected from these wearable devices can be used to study and improve sleep quality. In this paper, we utilize the relationship between physical activity and sleep quality to find ways of assisting people improve their sleep using machine learning techniques. People usually have several behavior modes that their bio-functions can be divided into. Performing time series clustering on activity data, we find cluster centers that would correlate to the most evident behavior modes for a specific subject. Activity recipes are then generated for good sleep quality for each behavior mode within each cluster. These activity recipes are supplied to an activity recommendation engine for suggesting a mix of relaxed to intense activities to subjects during their daily routines. The recommendations are further personalized based on the subjects' lifestyle constraints, i.e. their age, gender, body mass index (BMI), resting heart rate, etc, with the objective of the recommendation being the improvement of that night's quality of sleep. This would in turn serve a longer-term health objective, like lowering heart rate, improving the overall quality of sleep, etc.","980":"Predicting the evolution of mortality rates plays a central role for life insurance and pension funds.Various stochastic frameworks have been developed to model mortality patterns taking into account the main stylized facts driving these patterns. However, relying on the prediction of one specific model can be too restrictive and lead to some well documented drawbacks including model misspecification, parameter uncertainty and overfitting. To address these issues we first consider mortality modelling in a Bayesian Negative-Binomial framework to account for overdispersion and the uncertainty about the parameter estimates in a natural and coherent way. Model averaging techniques are then considered as a response to model misspecifications. In this paper, we propose two methods based on leave-future-out validation which are compared to the standard Bayesian model averaging (BMA) based on marginal likelihood. An intensive numerical study is carried out over a large range of simulation setups to compare the performances of the proposed methodologies. An illustration is then proposed on real-life mortality datasets which includes a sensitivity analysis to a Covid-type scenario. Overall, we found that both methods based on out-of-sample criterion outperform the standard BMA approach in terms of prediction performance and robustness.","981":"Reopening a cold case, inspector Echelon, high-ranking in the Row Operations Center, is searching for a lost linear map, known to be nilpotent. When a partially decomposed matrix is unearthed, he reconstructs its reduced form, finding it singular. But were its roots nilpotent?","982":"Recent works suggest that striking a balance between maximizing idea stimulation and minimizing idea redundancy can elevate creativity in self-organizing social networks. We explore whether dispersing the visibility of idea generators can help achieve such a trade-off. We employ popularity signals (follower counts) of participants as an external source of variation in network structures, which we control across four randomized study conditions. We observe that popularity signals influence inspiration-seeking ties, partly by biasing people's perception of their peers' creativity. Networks that partially disperse the ideators' visibility using this external signal show reduced idea-redundancy and elevated creativity. However, extreme dispersal leads to inferior creativity by narrowing the range of idea stimulation. Our work holds future-of-work implications for elevating creativity.","983":"Understanding a visualization is a multi-level process. A reader must extract and extrapolate from numeric facts, understand how those facts apply to both the context of the data and other potential contexts, and draw or evaluate conclusions from the data. A well-designed visualization should support each of these levels of understanding. We diagnose levels of understanding of visualized data by adapting Bloom's taxonomy, a common framework from the education literature. We describe each level of the framework and provide examples for how it can be applied to evaluate the efficacy of data visualizations along six levels of knowledge acquisition - knowledge, comprehension, application, analysis, synthesis, and evaluation. We present three case studies showing that this framework expands on existing methods to comprehensively measure how a visualization design facilitates a viewer's understanding of visualizations. Although Bloom's original taxonomy suggests a strong hierarchical structure for some domains, we found few examples of dependent relationships between performance at different levels for our three case studies. If this level-independence holds across new tested visualizations, the taxonomy could serve to inspire more targeted evaluations of levels of understanding that are relevant to a communication goal.","984":"Machine learning techniques including neural networks are popular tools for materials and chemical scientists with applications that may provide viable alternative methods in the analysis of structure and energetics of systems ranging from crystals to biomolecules. However, efforts are less abundant for prediction of dynamics. Here we explore the ability of three well established recurrent neural network architectures for forecasting the energetics of a macromolecular polymer-lipid aggregate solvated in ethyl acetate at ambient conditions. Data models generated from recurrent neural networks are trained and tested on nanoseconds-long time series of the intra-macromolecules potential energy and their interaction energy with the solvent generated from Molecular Dynamics and containing half million points. Our exhaustive analyses convey that the three recurrent neural network investigated generate data models with limited capability of reproducing the energetic fluctuations and yielding short or long term energetics forecasts with underlying distribution of points inconsistent with the input series distributions. We propose an in silico experimental protocol consisting on forming an ensemble of artificial network models trained on an ensemble of series with additional features from time series containing pre-clustered time patterns of the original series. The forecast process improves by predicting a band of forecasted time series with a spread of values consistent with the molecular dynamics energy fluctuations span. However, the distribution of points from the band of forecasts is not optimal. Although the three inspected recurrent neural networks were unable of generating single models that reproduce the actual fluctuations of the inspected molecular system energies in thermal equilibrium at the nanosecond scale, the proposed protocol provides useful estimates of the molecular fate","985":"The use of social, anthropomorphic robots to support humans in various industries has been on the rise. During Human-Robot Interaction (HRI), physically interactive non-verbal behaviour is key for more natural interactions. Handshaking is one such natural interaction used commonly in many social contexts. It is one of the first non-verbal interactions which takes place and should, therefore, be part of the repertoire of a social robot. In this paper, we explore the existing state of Human-Robot Handshaking and discuss possible ways forward for such physically interactive behaviours.","986":"In the next few years, smart farming will reach each and every nook of the world. The prospects of using unmanned aerial vehicles (UAV) for smart farming are immense. However, the cost and the ease in controlling UAVs for smart farming might play an important role for motivating farmers to use UAVs in farming. Mostly, UAVs are controlled by remote controllers using radio waves. There are several technologies such as WiFi or ZigBee that are also used for controlling UAVs. However, Smart Bluetooth (also referred to as Bluetooth Low Energy) is a wireless technology used to transfer data over short distances. Bluetooth smart is cheaper than other technologies and has the advantage of being available on every smart phone. Farmers can use any smart phone to operate their respective UAVs along with Bluetooth Smart enabled agricultural sensors in the future. However, certain requirements and challenges need to be addressed before UAVs can be operated for smart agriculture-related applications. Hence, in this article, an attempt has been made to explore the types of sensors suitable for smart farming, potential requirements and challenges for operating UAVs in smart agriculture. We have also identified the future applications of using UAVs in smart farming.","987":"Several places across the world are experiencing a steep surge in COVID-19 infections. Face masks have become increasingly accepted as one of the most effective means for combating the spread of the disease, when used in combination with social-distancing and frequent hand-washing. However, there is an increasing trend of people substituting regular cloth or surgical masks with clear plastic face shields, and with masks equipped with exhalation valves. One of the factors driving this increased adoption is improved comfort compared to regular masks. However, there is a possibility that widespread public use of these alternatives to regular masks could have an adverse effect on mitigation efforts. To help increase public awareness regarding the effectiveness of these alternative options, we use qualitative visualizations to examine the performance of face shields and exhalation valves in impeding the spread of aerosol-sized droplets. The visualizations indicate that although face shields block the initial forward motion of the jet, the expelled droplets can move around the visor with relative ease and spread out over a large area depending on light ambient disturbances. Visualizations for a mask equipped with an exhalation port indicate that a large number of droplets pass through the exhale valve unfiltered, which significantly reduces its effectiveness as a means of source control. Our observations suggest that to minimize the community spread of COVID-19, it may be preferable to use high quality cloth or surgical masks that are of a plain design, instead of face shields and masks equipped with exhale valves.","988":"In this paper, we describe our proposed methodology to approach the predict+optimise challenge introduced in the IEEE CIS 3rd Technical Challenge. The predictive model employs an ensemble of LightGBM models and the prescriptive analysis employs mathematical optimisation to efficiently prescribe solutions that minimise the average cost over multiple scenarios. Our solutions ranked 1st in the optimisation and 2nd in the prediction challenge of the competition.","989":"In this paper, we introduce and study the problem of facility location along with the notion of \\emph{`social distancing'}. The input to the problem is the road network of a city where the nodes are the residential zones, edges are the road segments connecting the zones along with their respective distance. We also have the information about the population at each zone, different types of facilities to be opened and in which number, and their respective demands in each zone. The goal of the problem is to locate the facilities such that the people can be served and at the same time the total social distancing is maximized. We formally call this problem as the \\textsc{Social Distancing-Based Facility Location Problem}. We mathematically quantify social distancing for a given allocation of facilities and proposed an optimization model. As the problem is \\textsf{NP-Hard}, we propose a simulation-based and heuristic approach for solving this problem. A detailed analysis of both methods has been done. We perform an extensive set of experiments with synthetic datasets. From the results, we observe that the proposed heuristic approach leads to a better allocation compared to the simulation-based approach.","990":"Representation learning over temporal networks has drawn considerable attention in recent years. Efforts are mainly focused on modeling structural dependencies and temporal evolving regularities in Euclidean space which, however, underestimates the inherent complex and hierarchical properties in many real-world temporal networks, leading to sub-optimal embeddings. To explore these properties of a complex temporal network, we propose a hyperbolic temporal graph network (HTGN) that fully takes advantage of the exponential capacity and hierarchical awareness of hyperbolic geometry. More specially, HTGN maps the temporal graph into hyperbolic space, and incorporates hyperbolic graph neural network and hyperbolic gated recurrent neural network, to capture the evolving behaviors and implicitly preserve hierarchical information simultaneously. Furthermore, in the hyperbolic space, we propose two important modules that enable HTGN to successfully model temporal networks: (1) hyperbolic temporal contextual self-attention (HTA) module to attend to historical states and (2) hyperbolic temporal consistency (HTC) module to ensure stability and generalization. Experimental results on multiple real-world datasets demonstrate the superiority of HTGN for temporal graph embedding, as it consistently outperforms competing methods by significant margins in various temporal link prediction tasks. Specifically, HTGN achieves AUC improvement up to 9.98% for link prediction and 11.4% for new link prediction. Moreover, the ablation study further validates the representational ability of hyperbolic geometry and the effectiveness of the proposed HTA and HTC modules.","991":"The world-wide lockdown in response to the COVID-19 pandemic in year 2020 led to economic slowdown and large reduction of fossil fuel CO2 emissions, but it is unclear how much it would reduce atmospheric CO2 concentration, and whether it can be observed. We estimated that a 7.9% reduction in emissions for 4 months would result in a 0.25 ppm decrease in the Northern Hemisphere CO2, an increment that is within the capability of current CO2 analyzers, but is a few times smaller than natural CO2 variabilities caused by weather and the biosphere such as El Nino. We used a state-of-the-art atmospheric transport model to simulate CO2, driven by a new daily fossil fuel emissions dataset and hourly biospheric fluxes from a carbon cycle model forced with observed climate variability. Our results show a 0.13 ppm decrease in atmospheric column CO2 anomaly averaged over 50S-50N for the period February-April 2020 relative to a 10-year climatology. A similar decrease was observed by the carbon satellite GOSAT3. Using model sensitivity experiments, we further found that COVID, the biosphere and weather contributed 54%, 23%, and 23% respectively. This seemingly small change stands out as the largest sub-annual anomaly in the last 10 years. Measurements from global ground stations were analyzed. At city scale, on-road CO2 enhancement measured in Beijing shows reduction of 20-30 ppm, consistent with drastically reduced traffic during the lockdown. The ability of our current carbon monitoring systems in detecting the small and short-lasting COVID signal on the background of fossil fuel CO2 accumulated over the last two centuries is encouraging. The COVID-19 pandemic is an unintended experiment whose impact suggests that to keep atmospheric CO2 at a climate-safe level will require sustained effort of similar magnitude and improved accuracy and expanded spatiotemporal coverage of our monitoring systems.","992":"Patterns embody repeating phenomena, and, as such, they are partly but not fully detachable from their context. 'Design patterns' and 'pattern languages' are established methods for working with patterns. They have been applied in architecture, software engineering, and other design fields, but have so far seen little application in the field of future studies. We reimagine futures discourse and anticipatory practices using pattern methods. We focus specifically on processes for coordinating distributed projects, integrating multiple voices, and on play that builds capability to face what's yet to come. One of the advantages of the method as a whole is that it deals with local knowledge and does not subsume everything within one overall 'global' strategy, while nevertheless offering a way to communicate between contexts and disciplines.","993":"Social distancing, often in the form of lockdown, has been adopted by many countries as a way to contrast the spreading of COVID-19. We discuss the temporal aspects of social distancing in contrasting an epidemic diffusion. We argue that a strategy based uniquely on social distancing requires to maintain the relative measures for a very long time, while a more articulate strategy, which also uses early detection and prompt isolation, can be both more efficient on reducing the epidemic peak and allow to relax the social distancing measures after a much shorter time. We consider in more detail the situation in Italy, simulating the effect of different strategies through a recently introduced SIR-type epidemiological model. The short answer to the question in the title is:\"it depends on what else you do\".","994":"We build a deep-learning-based SEIR-AIM model integrating the classical Susceptible-Exposed-Infectious-Removed epidemiology model with forecast modules of infection, community mobility, and unemployment. Through linking Google's multi-dimensional mobility index to economic activities, public health status, and mitigation policies, our AI-assisted model captures the populace's endogenous response to economic incentives and health risks. In addition to being an effective predictive tool, our analyses reveal that the long-term effective reproduction number of COVID-19 equilibrates around one before mass vaccination using data from the United States. We identify a\"policy frontier\"and identify reopening schools and workplaces to be the most effective. We also quantify protestors' employment-value-equivalence of the Black Lives Matter movement and find that its public health impact to be negligible.","995":"This study deals with the estimation of the trajectory of coronavirus COVID-19 adhering to respiratory droplets projected horizontally, considering the geographical altitude. The size of viruses and respiratory droplets is the factor that determines the trajectory of the microparticles in a viscous medium such as air; For this purpose, a graphical comparison of the diameters and masses of the microparticles produced in respiratory activity has been made. The estimation of the vertical movement of the microparticles through the air is based on Stokes' Law, it was determined that respiratory droplets smaller than 10 {\\mu}m in diameter have very small speeds, in practice they are floating for a few seconds before evaporating in the air; Regarding the horizontal displacement of respiratory droplets, frames from Scharfman et al. to determine its scope. In the case of a sneeze, the respiratory droplets can reach a distance of 1.65 m in 1 s, stopping rapidly until reaching 1.71 m in 2 seconds, then an analysis of the effect of geographical altitude on the movement of the micro-droplets was made, determining minimal change in kinematic variables.","996":"We propose a partial identification method for estimating disease prevalence from serology studies. Our data are results from antibody tests in some population sample, where the test parameters, such as the true\/false positive rates, are unknown. Our method scans the entire parameter space, and rejects parameter values using the joint data density as the test statistic. The proposed method is conservative for marginal inference, in general, but its key advantage over more standard approaches is that it is valid in finite samples even when the underlying model is not point identified. Moreover, our method requires only independence of serology test results, and does not rely on asymptotic arguments, normality assumptions, or other approximations. We use recent Covid-19 serology studies in the US, and show that the parameter confidence set is generally wide, and cannot support definite conclusions. Specifically, recent serology studies from California suggest a prevalence anywhere in the range 0%-2% (at the time of study), and are therefore inconclusive. However, this range could be narrowed down to 0.7%-1.5% if the actual false positive rate of the antibody test was indeed near its empirical estimate (~0.5%). In another study from New York state, Covid-19 prevalence is confidently estimated in the range 13%-17% in mid-April of 2020, which also suggests significant geographic variation in Covid-19 exposure across the US. Combining all datasets yields a 5%-8% prevalence range. Our results overall suggest that serology testing on a massive scale can give crucial information for future policy design, even when such tests are imperfect and their parameters unknown.","997":"We propose a two-step framework for predicting the implied volatility surface over time without static arbitrage. In the first step, we select features to represent the surface and predict them over time. In the second step, we use the predicted features to construct the implied volatility surface using a deep neural network (DNN) model by incorporating constraints that prevent static arbitrage. We consider three methods to extract features from the implied volatility data: principal component analysis, variational autoencoder and sampling the surface, and we predict these features using LSTM. Using a long time series of implied volatility data for S\\&P500 index options to train our models, we find two feature construction methods, sampling the surface and variational autoencoders combined with DNN for surface construction, are the best performers in out-of-sample prediction. In particular, they outperform a classical method substantially. Furthermore, the DNN model for surface construction not only removes static arbitrage, but also significantly reduces the prediction error compared with a standard interpolation method. Our framework can also be used to simulate the dynamics of the implied volatility surface without static arbitrage.","998":"The importance of the working document is that it allows the analysis of the information and the status of cases associated with (SARS-CoV-2) COVID-19 as open data at the municipal, state and national level, with a daily record of patients, according to a age, sex, comorbidities, for the condition of (SARS-CoV-2) COVID-19 according to the following characteristics: a) Positive, b) Negative, c) Suspicious. Likewise, it presents information related to the identification of an outpatient and \/ or hospitalized patient, attending to their medical development, identifying: a) Recovered, b) Deaths and c) Active, in Phase 3 and Phase 4, in the five main population areas speaker of indigenous language in the State of Veracruz - Mexico. The data analysis is carried out through the application of a data mining algorithm, which provides the information, fast and timely, required for the estimation of Medical Care Scenarios of (SARS-CoV-2) COVID-19, as well as for know the impact on the indigenous language-speaking population in Mexico.","999":"The recovery phase of the COVID-19 pandemic requires careful planning and monitoring while people gradually return to work. Internet-of-Things (IoT) is widely regarded as a crucial tool to help combating COVID-19 pandemic in many areas and societies. In particular, the heterogeneous data captured by IoT solutions can inform policy making and quick responses to community events. This article introduces a novel IoT crowd monitoring solution which uses software defined networks (SDN) assisted WiFi access points as 24\/7 sensors to monitor and analyze the use of physical space. Prototypes and crowd behavior models are developed using over 500 million records captured on a university campus. Besides supporting informed decision at institution level, the results can be used by individual visitors to plan or schedule their access to facilities."},"input":{"0":"COVID Super Expert","1":"COVID Super Expert","2":"COVID Super Expert","3":"COVID Super Expert","4":"COVID Super Expert","5":"COVID Super Expert","6":"COVID Super Expert","7":"COVID Super Expert","8":"COVID Super Expert","9":"COVID Super Expert","10":"COVID Super Expert","11":"COVID Super Expert","12":"COVID Super Expert","13":"COVID Super Expert","14":"COVID Super Expert","15":"COVID Super Expert","16":"COVID Super Expert","17":"COVID Super Expert","18":"COVID Super Expert","19":"COVID Super Expert","20":"COVID Super Expert","21":"COVID Super Expert","22":"COVID Super Expert","23":"COVID Super Expert","24":"COVID Super Expert","25":"COVID Super Expert","26":"COVID Super Expert","27":"COVID Super Expert","28":"COVID Super Expert","29":"COVID Super Expert","30":"COVID Super Expert","31":"COVID Super Expert","32":"COVID Super Expert","33":"COVID Super Expert","34":"COVID Super Expert","35":"COVID Super Expert","36":"COVID Super Expert","37":"COVID Super Expert","38":"COVID Super Expert","39":"COVID Super Expert","40":"COVID Super Expert","41":"COVID Super Expert","42":"COVID Super Expert","43":"COVID Super Expert","44":"COVID Super Expert","45":"COVID Super Expert","46":"COVID Super Expert","47":"COVID Super Expert","48":"COVID Super Expert","49":"COVID Super Expert","50":"COVID Super Expert","51":"COVID Super Expert","52":"COVID Super Expert","53":"COVID Super Expert","54":"COVID Super Expert","55":"COVID Super Expert","56":"COVID Super Expert","57":"COVID Super Expert","58":"COVID Super Expert","59":"COVID Super Expert","60":"COVID Super Expert","61":"COVID Super Expert","62":"COVID Super Expert","63":"COVID Super Expert","64":"COVID Super Expert","65":"COVID Super Expert","66":"COVID Super Expert","67":"COVID Super Expert","68":"COVID Super Expert","69":"COVID Super Expert","70":"COVID Super Expert","71":"COVID Super Expert","72":"COVID Super Expert","73":"COVID Super Expert","74":"COVID Super Expert","75":"COVID Super Expert","76":"COVID Super Expert","77":"COVID Super Expert","78":"COVID Super Expert","79":"COVID Super Expert","80":"COVID Super Expert","81":"COVID Super Expert","82":"COVID Super Expert","83":"COVID Super Expert","84":"COVID Super Expert","85":"COVID Super Expert","86":"COVID Super Expert","87":"COVID Super Expert","88":"COVID Super Expert","89":"COVID Super Expert","90":"COVID Super Expert","91":"COVID Super Expert","92":"COVID Super Expert","93":"COVID Super Expert","94":"COVID Super Expert","95":"COVID Super Expert","96":"COVID Super Expert","97":"COVID Super Expert","98":"COVID Super Expert","99":"COVID Super Expert","100":"COVID Super Expert","101":"COVID Super Expert","102":"COVID Super Expert","103":"COVID Super Expert","104":"COVID Super Expert","105":"COVID Super Expert","106":"COVID Super Expert","107":"COVID Super Expert","108":"COVID Super Expert","109":"COVID Super Expert","110":"COVID Super Expert","111":"COVID Super Expert","112":"COVID Super Expert","113":"COVID Super Expert","114":"COVID Super Expert","115":"COVID Super Expert","116":"COVID Super Expert","117":"COVID Super Expert","118":"COVID Super Expert","119":"COVID Super Expert","120":"COVID Super Expert","121":"COVID Super Expert","122":"COVID Super Expert","123":"COVID Super Expert","124":"COVID Super Expert","125":"COVID Super Expert","126":"COVID Super Expert","127":"COVID Super Expert","128":"COVID Super Expert","129":"COVID Super Expert","130":"COVID Super Expert","131":"COVID Super Expert","132":"COVID Super Expert","133":"COVID Super Expert","134":"COVID Super Expert","135":"COVID Super Expert","136":"COVID Super Expert","137":"COVID Super Expert","138":"COVID Super Expert","139":"COVID Super Expert","140":"COVID Super Expert","141":"COVID Super Expert","142":"COVID Super Expert","143":"COVID Super Expert","144":"COVID Super Expert","145":"COVID Super Expert","146":"COVID Super Expert","147":"COVID Super Expert","148":"COVID Super Expert","149":"COVID Super Expert","150":"COVID Super Expert","151":"COVID Super Expert","152":"COVID Super Expert","153":"COVID Super Expert","154":"COVID Super Expert","155":"COVID Super Expert","156":"COVID Super Expert","157":"COVID Super Expert","158":"COVID Super Expert","159":"COVID Super Expert","160":"COVID Super Expert","161":"COVID Super Expert","162":"COVID Super Expert","163":"COVID Super Expert","164":"COVID Super Expert","165":"COVID Super Expert","166":"COVID Super Expert","167":"COVID Super Expert","168":"COVID Super Expert","169":"COVID Super Expert","170":"COVID Super Expert","171":"COVID Super Expert","172":"COVID Super Expert","173":"COVID Super Expert","174":"COVID Super Expert","175":"COVID Super Expert","176":"COVID Super Expert","177":"COVID Super Expert","178":"COVID Super Expert","179":"COVID Super Expert","180":"COVID Super Expert","181":"COVID Super Expert","182":"COVID Super Expert","183":"COVID Super Expert","184":"COVID Super Expert","185":"COVID Super Expert","186":"COVID Super Expert","187":"COVID Super Expert","188":"COVID Super Expert","189":"COVID Super Expert","190":"COVID Super Expert","191":"COVID Super Expert","192":"COVID Super Expert","193":"COVID Super Expert","194":"COVID Super Expert","195":"COVID Super Expert","196":"COVID Super Expert","197":"COVID Super Expert","198":"COVID Super Expert","199":"COVID Super Expert","200":"COVID Super Expert","201":"COVID Super Expert","202":"COVID Super Expert","203":"COVID Super Expert","204":"COVID Super Expert","205":"COVID Super Expert","206":"COVID Super Expert","207":"COVID Super Expert","208":"COVID Super Expert","209":"COVID Super Expert","210":"COVID Super Expert","211":"COVID Super Expert","212":"COVID Super Expert","213":"COVID Super Expert","214":"COVID Super Expert","215":"COVID Super Expert","216":"COVID Super Expert","217":"COVID Super Expert","218":"COVID Super Expert","219":"COVID Super Expert","220":"COVID Super Expert","221":"COVID Super Expert","222":"COVID Super Expert","223":"COVID Super Expert","224":"COVID Super Expert","225":"COVID Super Expert","226":"COVID Super Expert","227":"COVID Super Expert","228":"COVID Super Expert","229":"COVID Super Expert","230":"COVID Super Expert","231":"COVID Super Expert","232":"COVID Super Expert","233":"COVID Super Expert","234":"COVID Super Expert","235":"COVID Super Expert","236":"COVID Super Expert","237":"COVID Super Expert","238":"COVID Super Expert","239":"COVID Super Expert","240":"COVID Super Expert","241":"COVID Super Expert","242":"COVID Super Expert","243":"COVID Super Expert","244":"COVID Super Expert","245":"COVID Super Expert","246":"COVID Super Expert","247":"COVID Super Expert","248":"COVID Super Expert","249":"COVID Super Expert","250":"COVID Super Expert","251":"COVID Super Expert","252":"COVID Super Expert","253":"COVID Super Expert","254":"COVID Super Expert","255":"COVID Super Expert","256":"COVID Super Expert","257":"COVID Super Expert","258":"COVID Super Expert","259":"COVID Super Expert","260":"COVID Super Expert","261":"COVID Super Expert","262":"COVID Super Expert","263":"COVID Super Expert","264":"COVID Super Expert","265":"COVID Super Expert","266":"COVID Super Expert","267":"COVID Super Expert","268":"COVID Super Expert","269":"COVID Super Expert","270":"COVID Super Expert","271":"COVID Super Expert","272":"COVID Super Expert","273":"COVID Super Expert","274":"COVID Super Expert","275":"COVID Super Expert","276":"COVID Super Expert","277":"COVID Super Expert","278":"COVID Super Expert","279":"COVID Super Expert","280":"COVID Super Expert","281":"COVID Super Expert","282":"COVID Super Expert","283":"COVID Super Expert","284":"COVID Super Expert","285":"COVID Super Expert","286":"COVID Super Expert","287":"COVID Super Expert","288":"COVID Super Expert","289":"COVID Super Expert","290":"COVID Super Expert","291":"COVID Super Expert","292":"COVID Super Expert","293":"COVID Super Expert","294":"COVID Super Expert","295":"COVID Super Expert","296":"COVID Super Expert","297":"COVID Super Expert","298":"COVID Super Expert","299":"COVID Super Expert","300":"COVID Super Expert","301":"COVID Super Expert","302":"COVID Super Expert","303":"COVID Super Expert","304":"COVID Super Expert","305":"COVID Super Expert","306":"COVID Super Expert","307":"COVID Super Expert","308":"COVID Super Expert","309":"COVID Super Expert","310":"COVID Super Expert","311":"COVID Super Expert","312":"COVID Super Expert","313":"COVID Super Expert","314":"COVID Super Expert","315":"COVID Super Expert","316":"COVID Super Expert","317":"COVID Super Expert","318":"COVID Super Expert","319":"COVID Super Expert","320":"COVID Super Expert","321":"COVID Super Expert","322":"COVID Super Expert","323":"COVID Super Expert","324":"COVID Super Expert","325":"COVID Super Expert","326":"COVID Super Expert","327":"COVID Super Expert","328":"COVID Super Expert","329":"COVID Super Expert","330":"COVID Super Expert","331":"COVID Super Expert","332":"COVID Super Expert","333":"COVID Super Expert","334":"COVID Super Expert","335":"COVID Super Expert","336":"COVID Super Expert","337":"COVID Super Expert","338":"COVID Super Expert","339":"COVID Super Expert","340":"COVID Super Expert","341":"COVID Super Expert","342":"COVID Super Expert","343":"COVID Super Expert","344":"COVID Super Expert","345":"COVID Super Expert","346":"COVID Super Expert","347":"COVID Super Expert","348":"COVID Super Expert","349":"COVID Super Expert","350":"COVID Super Expert","351":"COVID Super Expert","352":"COVID Super Expert","353":"COVID Super Expert","354":"COVID Super Expert","355":"COVID Super Expert","356":"COVID Super Expert","357":"COVID Super Expert","358":"COVID Super Expert","359":"COVID Super Expert","360":"COVID Super Expert","361":"COVID Super Expert","362":"COVID Super Expert","363":"COVID Super Expert","364":"COVID Super Expert","365":"COVID Super Expert","366":"COVID Super Expert","367":"COVID Super Expert","368":"COVID Super Expert","369":"COVID Super Expert","370":"COVID Super Expert","371":"COVID Super Expert","372":"COVID Super Expert","373":"COVID Super Expert","374":"COVID Super Expert","375":"COVID Super Expert","376":"COVID Super Expert","377":"COVID Super Expert","378":"COVID Super Expert","379":"COVID Super Expert","380":"COVID Super Expert","381":"COVID Super Expert","382":"COVID Super Expert","383":"COVID Super Expert","384":"COVID Super Expert","385":"COVID Super Expert","386":"COVID Super Expert","387":"COVID Super Expert","388":"COVID Super Expert","389":"COVID Super Expert","390":"COVID Super Expert","391":"COVID Super Expert","392":"COVID Super Expert","393":"COVID Super Expert","394":"COVID Super Expert","395":"COVID Super Expert","396":"COVID Super Expert","397":"COVID Super Expert","398":"COVID Super Expert","399":"COVID Super Expert","400":"COVID Super Expert","401":"COVID Super Expert","402":"COVID Super Expert","403":"COVID Super Expert","404":"COVID Super Expert","405":"COVID Super Expert","406":"COVID Super Expert","407":"COVID Super Expert","408":"COVID Super Expert","409":"COVID Super Expert","410":"COVID Super Expert","411":"COVID Super Expert","412":"COVID Super Expert","413":"COVID Super Expert","414":"COVID Super Expert","415":"COVID Super Expert","416":"COVID Super Expert","417":"COVID Super Expert","418":"COVID Super Expert","419":"COVID Super Expert","420":"COVID Super Expert","421":"COVID Super Expert","422":"COVID Super Expert","423":"COVID Super Expert","424":"COVID Super Expert","425":"COVID Super Expert","426":"COVID Super Expert","427":"COVID Super Expert","428":"COVID Super Expert","429":"COVID Super Expert","430":"COVID Super Expert","431":"COVID Super Expert","432":"COVID Super Expert","433":"COVID Super Expert","434":"COVID Super Expert","435":"COVID Super Expert","436":"COVID Super Expert","437":"COVID Super Expert","438":"COVID Super Expert","439":"COVID Super Expert","440":"COVID Super Expert","441":"COVID Super Expert","442":"COVID Super Expert","443":"COVID Super Expert","444":"COVID Super Expert","445":"COVID Super Expert","446":"COVID Super Expert","447":"COVID Super Expert","448":"COVID Super Expert","449":"COVID Super Expert","450":"COVID Super Expert","451":"COVID Super Expert","452":"COVID Super Expert","453":"COVID Super Expert","454":"COVID Super Expert","455":"COVID Super Expert","456":"COVID Super Expert","457":"COVID Super Expert","458":"COVID Super Expert","459":"COVID Super Expert","460":"COVID Super Expert","461":"COVID Super Expert","462":"COVID Super Expert","463":"COVID Super Expert","464":"COVID Super Expert","465":"COVID Super Expert","466":"COVID Super Expert","467":"COVID Super Expert","468":"COVID Super Expert","469":"COVID Super Expert","470":"COVID Super Expert","471":"COVID Super Expert","472":"COVID Super Expert","473":"COVID Super Expert","474":"COVID Super Expert","475":"COVID Super Expert","476":"COVID Super Expert","477":"COVID Super Expert","478":"COVID Super Expert","479":"COVID Super Expert","480":"COVID Super Expert","481":"COVID Super Expert","482":"COVID Super Expert","483":"COVID Super Expert","484":"COVID Super Expert","485":"COVID Super Expert","486":"COVID Super Expert","487":"COVID Super Expert","488":"COVID Super Expert","489":"COVID Super Expert","490":"COVID Super Expert","491":"COVID Super Expert","492":"COVID Super Expert","493":"COVID Super Expert","494":"COVID Super Expert","495":"COVID Super Expert","496":"COVID Super Expert","497":"COVID Super Expert","498":"COVID Super Expert","499":"COVID Super Expert","500":"COVID Super Expert","501":"COVID Super Expert","502":"COVID Super Expert","503":"COVID Super Expert","504":"COVID Super Expert","505":"COVID Super Expert","506":"COVID Super Expert","507":"COVID Super Expert","508":"COVID Super Expert","509":"COVID Super Expert","510":"COVID Super Expert","511":"COVID Super Expert","512":"COVID Super Expert","513":"COVID Super Expert","514":"COVID Super Expert","515":"COVID Super Expert","516":"COVID Super Expert","517":"COVID Super Expert","518":"COVID Super Expert","519":"COVID Super Expert","520":"COVID Super Expert","521":"COVID Super Expert","522":"COVID Super Expert","523":"COVID Super Expert","524":"COVID Super Expert","525":"COVID Super Expert","526":"COVID Super Expert","527":"COVID Super Expert","528":"COVID Super Expert","529":"COVID Super Expert","530":"COVID Super Expert","531":"COVID Super Expert","532":"COVID Super Expert","533":"COVID Super Expert","534":"COVID Super Expert","535":"COVID Super Expert","536":"COVID Super Expert","537":"COVID Super Expert","538":"COVID Super Expert","539":"COVID Super Expert","540":"COVID Super Expert","541":"COVID Super Expert","542":"COVID Super Expert","543":"COVID Super Expert","544":"COVID Super Expert","545":"COVID Super Expert","546":"COVID Super Expert","547":"COVID Super Expert","548":"COVID Super Expert","549":"COVID Super Expert","550":"COVID Super Expert","551":"COVID Super Expert","552":"COVID Super Expert","553":"COVID Super Expert","554":"COVID Super Expert","555":"COVID Super Expert","556":"COVID Super Expert","557":"COVID Super Expert","558":"COVID Super Expert","559":"COVID Super Expert","560":"COVID Super Expert","561":"COVID Super Expert","562":"COVID Super Expert","563":"COVID Super Expert","564":"COVID Super Expert","565":"COVID Super Expert","566":"COVID Super Expert","567":"COVID Super Expert","568":"COVID Super Expert","569":"COVID Super Expert","570":"COVID Super Expert","571":"COVID Super Expert","572":"COVID Super Expert","573":"COVID Super Expert","574":"COVID Super Expert","575":"COVID Super Expert","576":"COVID Super Expert","577":"COVID Super Expert","578":"COVID Super Expert","579":"COVID Super Expert","580":"COVID Super Expert","581":"COVID Super Expert","582":"COVID Super Expert","583":"COVID Super Expert","584":"COVID Super Expert","585":"COVID Super Expert","586":"COVID Super Expert","587":"COVID Super Expert","588":"COVID Super Expert","589":"COVID Super Expert","590":"COVID Super Expert","591":"COVID Super Expert","592":"COVID Super Expert","593":"COVID Super Expert","594":"COVID Super Expert","595":"COVID Super Expert","596":"COVID Super Expert","597":"COVID Super Expert","598":"COVID Super Expert","599":"COVID Super Expert","600":"COVID Super Expert","601":"COVID Super Expert","602":"COVID Super Expert","603":"COVID Super Expert","604":"COVID Super Expert","605":"COVID Super Expert","606":"COVID Super Expert","607":"COVID Super Expert","608":"COVID Super Expert","609":"COVID Super Expert","610":"COVID Super Expert","611":"COVID Super Expert","612":"COVID Super Expert","613":"COVID Super Expert","614":"COVID Super Expert","615":"COVID Super Expert","616":"COVID Super Expert","617":"COVID Super Expert","618":"COVID Super Expert","619":"COVID Super Expert","620":"COVID Super Expert","621":"COVID Super Expert","622":"COVID Super Expert","623":"COVID Super Expert","624":"COVID Super Expert","625":"COVID Super Expert","626":"COVID Super Expert","627":"COVID Super Expert","628":"COVID Super Expert","629":"COVID Super Expert","630":"COVID Super Expert","631":"COVID Super Expert","632":"COVID Super Expert","633":"COVID Super Expert","634":"COVID Super Expert","635":"COVID Super Expert","636":"COVID Super Expert","637":"COVID Super Expert","638":"COVID Super Expert","639":"COVID Super Expert","640":"COVID Super Expert","641":"COVID Super Expert","642":"COVID Super Expert","643":"COVID Super Expert","644":"COVID Super Expert","645":"COVID Super Expert","646":"COVID Super Expert","647":"COVID Super Expert","648":"COVID Super Expert","649":"COVID Super Expert","650":"COVID Super Expert","651":"COVID Super Expert","652":"COVID Super Expert","653":"COVID Super Expert","654":"COVID Super Expert","655":"COVID Super Expert","656":"COVID Super Expert","657":"COVID Super Expert","658":"COVID Super Expert","659":"COVID Super Expert","660":"COVID Super Expert","661":"COVID Super Expert","662":"COVID Super Expert","663":"COVID Super Expert","664":"COVID Super Expert","665":"COVID Super Expert","666":"COVID Super Expert","667":"COVID Super Expert","668":"COVID Super Expert","669":"COVID Super Expert","670":"COVID Super Expert","671":"COVID Super Expert","672":"COVID Super Expert","673":"COVID Super Expert","674":"COVID Super Expert","675":"COVID Super Expert","676":"COVID Super Expert","677":"COVID Super Expert","678":"COVID Super Expert","679":"COVID Super Expert","680":"COVID Super Expert","681":"COVID Super Expert","682":"COVID Super Expert","683":"COVID Super Expert","684":"COVID Super Expert","685":"COVID Super Expert","686":"COVID Super Expert","687":"COVID Super Expert","688":"COVID Super Expert","689":"COVID Super Expert","690":"COVID Super Expert","691":"COVID Super Expert","692":"COVID Super Expert","693":"COVID Super Expert","694":"COVID Super Expert","695":"COVID Super Expert","696":"COVID Super Expert","697":"COVID Super Expert","698":"COVID Super Expert","699":"COVID Super Expert","700":"COVID Super Expert","701":"COVID Super Expert","702":"COVID Super Expert","703":"COVID Super Expert","704":"COVID Super Expert","705":"COVID Super Expert","706":"COVID Super Expert","707":"COVID Super Expert","708":"COVID Super Expert","709":"COVID Super Expert","710":"COVID Super Expert","711":"COVID Super Expert","712":"COVID Super Expert","713":"COVID Super Expert","714":"COVID Super Expert","715":"COVID Super Expert","716":"COVID Super Expert","717":"COVID Super Expert","718":"COVID Super Expert","719":"COVID Super Expert","720":"COVID Super Expert","721":"COVID Super Expert","722":"COVID Super Expert","723":"COVID Super Expert","724":"COVID Super Expert","725":"COVID Super Expert","726":"COVID Super Expert","727":"COVID Super Expert","728":"COVID Super Expert","729":"COVID Super Expert","730":"COVID Super Expert","731":"COVID Super Expert","732":"COVID Super Expert","733":"COVID Super Expert","734":"COVID Super Expert","735":"COVID Super Expert","736":"COVID Super Expert","737":"COVID Super Expert","738":"COVID Super Expert","739":"COVID Super Expert","740":"COVID Super Expert","741":"COVID Super Expert","742":"COVID Super Expert","743":"COVID Super Expert","744":"COVID Super Expert","745":"COVID Super Expert","746":"COVID Super Expert","747":"COVID Super Expert","748":"COVID Super Expert","749":"COVID Super Expert","750":"COVID Super Expert","751":"COVID Super Expert","752":"COVID Super Expert","753":"COVID Super Expert","754":"COVID Super Expert","755":"COVID Super Expert","756":"COVID Super Expert","757":"COVID Super Expert","758":"COVID Super Expert","759":"COVID Super Expert","760":"COVID Super Expert","761":"COVID Super Expert","762":"COVID Super Expert","763":"COVID Super Expert","764":"COVID Super Expert","765":"COVID Super Expert","766":"COVID Super Expert","767":"COVID Super Expert","768":"COVID Super Expert","769":"COVID Super Expert","770":"COVID Super Expert","771":"COVID Super Expert","772":"COVID Super Expert","773":"COVID Super Expert","774":"COVID Super Expert","775":"COVID Super Expert","776":"COVID Super Expert","777":"COVID Super Expert","778":"COVID Super Expert","779":"COVID Super Expert","780":"COVID Super Expert","781":"COVID Super Expert","782":"COVID Super Expert","783":"COVID Super Expert","784":"COVID Super Expert","785":"COVID Super Expert","786":"COVID Super Expert","787":"COVID Super Expert","788":"COVID Super Expert","789":"COVID Super Expert","790":"COVID Super Expert","791":"COVID Super Expert","792":"COVID Super Expert","793":"COVID Super Expert","794":"COVID Super Expert","795":"COVID Super Expert","796":"COVID Super Expert","797":"COVID Super Expert","798":"COVID Super Expert","799":"COVID Super Expert","800":"COVID Super Expert","801":"COVID Super Expert","802":"COVID Super Expert","803":"COVID Super Expert","804":"COVID Super Expert","805":"COVID Super Expert","806":"COVID Super Expert","807":"COVID Super Expert","808":"COVID Super Expert","809":"COVID Super Expert","810":"COVID Super Expert","811":"COVID Super Expert","812":"COVID Super Expert","813":"COVID Super Expert","814":"COVID Super Expert","815":"COVID Super Expert","816":"COVID Super Expert","817":"COVID Super Expert","818":"COVID Super Expert","819":"COVID Super Expert","820":"COVID Super Expert","821":"COVID Super Expert","822":"COVID Super Expert","823":"COVID Super Expert","824":"COVID Super Expert","825":"COVID Super Expert","826":"COVID Super Expert","827":"COVID Super Expert","828":"COVID Super Expert","829":"COVID Super Expert","830":"COVID Super Expert","831":"COVID Super Expert","832":"COVID Super Expert","833":"COVID Super Expert","834":"COVID Super Expert","835":"COVID Super Expert","836":"COVID Super Expert","837":"COVID Super Expert","838":"COVID Super Expert","839":"COVID Super Expert","840":"COVID Super Expert","841":"COVID Super Expert","842":"COVID Super Expert","843":"COVID Super Expert","844":"COVID Super Expert","845":"COVID Super Expert","846":"COVID Super Expert","847":"COVID Super Expert","848":"COVID Super Expert","849":"COVID Super Expert","850":"COVID Super Expert","851":"COVID Super Expert","852":"COVID Super Expert","853":"COVID Super Expert","854":"COVID Super Expert","855":"COVID Super Expert","856":"COVID Super Expert","857":"COVID Super Expert","858":"COVID Super Expert","859":"COVID Super Expert","860":"COVID Super Expert","861":"COVID Super Expert","862":"COVID Super Expert","863":"COVID Super Expert","864":"COVID Super Expert","865":"COVID Super Expert","866":"COVID Super Expert","867":"COVID Super Expert","868":"COVID Super Expert","869":"COVID Super Expert","870":"COVID Super Expert","871":"COVID Super Expert","872":"COVID Super Expert","873":"COVID Super Expert","874":"COVID Super Expert","875":"COVID Super Expert","876":"COVID Super Expert","877":"COVID Super Expert","878":"COVID Super Expert","879":"COVID Super Expert","880":"COVID Super Expert","881":"COVID Super Expert","882":"COVID Super Expert","883":"COVID Super Expert","884":"COVID Super Expert","885":"COVID Super Expert","886":"COVID Super Expert","887":"COVID Super Expert","888":"COVID Super Expert","889":"COVID Super Expert","890":"COVID Super Expert","891":"COVID Super Expert","892":"COVID Super Expert","893":"COVID Super Expert","894":"COVID Super Expert","895":"COVID Super Expert","896":"COVID Super Expert","897":"COVID Super Expert","898":"COVID Super Expert","899":"COVID Super Expert","900":"COVID Super Expert","901":"COVID Super Expert","902":"COVID Super Expert","903":"COVID Super Expert","904":"COVID Super Expert","905":"COVID Super Expert","906":"COVID Super Expert","907":"COVID Super Expert","908":"COVID Super Expert","909":"COVID Super Expert","910":"COVID Super Expert","911":"COVID Super Expert","912":"COVID Super Expert","913":"COVID Super Expert","914":"COVID Super Expert","915":"COVID Super Expert","916":"COVID Super Expert","917":"COVID Super Expert","918":"COVID Super Expert","919":"COVID Super Expert","920":"COVID Super Expert","921":"COVID Super Expert","922":"COVID Super Expert","923":"COVID Super Expert","924":"COVID Super Expert","925":"COVID Super Expert","926":"COVID Super Expert","927":"COVID Super Expert","928":"COVID Super Expert","929":"COVID Super Expert","930":"COVID Super Expert","931":"COVID Super Expert","932":"COVID Super Expert","933":"COVID Super Expert","934":"COVID Super Expert","935":"COVID Super Expert","936":"COVID Super Expert","937":"COVID Super Expert","938":"COVID Super Expert","939":"COVID Super Expert","940":"COVID Super Expert","941":"COVID Super Expert","942":"COVID Super Expert","943":"COVID Super Expert","944":"COVID Super Expert","945":"COVID Super Expert","946":"COVID Super Expert","947":"COVID Super Expert","948":"COVID Super Expert","949":"COVID Super Expert","950":"COVID Super Expert","951":"COVID Super Expert","952":"COVID Super Expert","953":"COVID Super Expert","954":"COVID Super Expert","955":"COVID Super Expert","956":"COVID Super Expert","957":"COVID Super Expert","958":"COVID Super Expert","959":"COVID Super Expert","960":"COVID Super Expert","961":"COVID Super Expert","962":"COVID Super Expert","963":"COVID Super Expert","964":"COVID Super Expert","965":"COVID Super Expert","966":"COVID Super Expert","967":"COVID Super Expert","968":"COVID Super Expert","969":"COVID Super Expert","970":"COVID Super Expert","971":"COVID Super Expert","972":"COVID Super Expert","973":"COVID Super Expert","974":"COVID Super Expert","975":"COVID Super Expert","976":"COVID Super Expert","977":"COVID Super Expert","978":"COVID Super Expert","979":"COVID Super Expert","980":"COVID Super Expert","981":"COVID Super Expert","982":"COVID Super Expert","983":"COVID Super Expert","984":"COVID Super Expert","985":"COVID Super Expert","986":"COVID Super Expert","987":"COVID Super Expert","988":"COVID Super Expert","989":"COVID Super Expert","990":"COVID Super Expert","991":"COVID Super Expert","992":"COVID Super Expert","993":"COVID Super Expert","994":"COVID Super Expert","995":"COVID Super Expert","996":"COVID Super Expert","997":"COVID Super Expert","998":"COVID Super Expert","999":"COVID Super Expert"},"gptj_fn":{"0":"The physical basis and algorithm of the ultraviolet germicidal irradiation (UVGI) scientific calculator are presented herein. The algorithm was implemented in the web application UVGI Scientific Calculator (found at https:\/\/covid-19-clean.org\/calculator-journal-report\/) that has been used to facilitate the engineering and design of air-purifying equipment being developed for use in hospitals, commercial and residential applications. Presented examples of the calculations illustrate how different factors of the construction of a UVGI air-purifier may influence the degree of virus inactivation. A comprehensive methodology for designing effective UVGI air-pumps is proposed based on the analysis of the effect of airflow resistance, air turbulent velocity, flow rate, and ambient temperature on the transmission of airborne pathogens. The practical prototype of the pump constructed from materials commonly available to disinfection tools has been tested in several experiments with various performance parameters. The results suggest that it is possible to make useful decisions regarding the design of efficient UVGI air-pump devices by considering more than one of the factors mentioned above. Therefore, the findings provide guidance for the development of future UVGI science calculators. To the best of our knowledge, this is the first time such a detailed review of the state-of","1":"Sensitivity Analysis (SA) is a useful tool to measure the impact of changes in model parameters on the infection dynamics, particularly to quantify the expected efficacy of disease control strategies. SA has only been applied to epidemic models at the population level, ignoring the effect of within-host virus-with-immune-system interactions on the disease spread. Connecting the scales from individual to population can help inform drug and vaccine development. Thus the value of understanding the impact of immunological parameters on epidemiological quantities. Here we consider an age-since-infection structured vector-host model, in which epidemiological parameters are formulated as functions of within-host virus and antibody densities, governed by an ODE system. We then use SA for these immuno-epidemiological models to investigate the impact of immunological parameters on population-level disease dynamics such as basic reproduction number, final size of the epidemic or the infectiousness at different phases of an outbreak. As a case study, we consider Rift Valley Fever Disease (RFVD) utilizing parameter estimations from prior studies. SA indicates that 1% increase in within-host pathogen growth rate can lead up to 8% increase in R0; up to 1% increase in steady-state infected host abundance, and up to","2":"In this paper, we estimate the impact of national lockdown on COVID-19 related total and daily deaths, per million people, in select European countries. In particular, we compare countries that imposed a nationwide lockdown (Treatment group); Belgium, Denmark, France, Germany, Italy, Norway, Spain, United Kingdom (UK), and the US, to Sweden (Control group) that did not impose national lockdown using a changes-in-changes (CIC) estimation model. The key advantage of the CIC model as compared to the standard difference-in-difference model is that CIC allows for mean and variance of the outcomes to change over time in the absence of any policy intervention, and CIC accounts for endogeneity in the choice of policy intervention. Our results indicate that in contrast to Sweden, which did not impose a national lockdown, Germany, and to some extent, the UK were the two countries where nationwide lockdown was implemented. This suggests that the effectiveness of the lockdown as a mitigation method was greater than the closure of schools or universities due to the shortage of manpower during the pandemic. For the other hand, the non-linear nature of the CIC model suggests that the effect of the lockdown was more pronounced in Italy, while both the number of","3":"We study pure exploration in multi-armed bandits with graph side-information. In particular, we consider the best arm (and near-best arm) identification problem in the fixed confidence setting under the assumption that the arm rewards are smooth with respect to a given arbitrary graph. This captures a range of real world pure-exploration scenarios where one often has information about the similarity of the options or actions under consideration. We propose a novel algorithm GRUB (GRaph based UcB) for this problem and provide a theoretical characterization of its performance that elicits the benefit of the graph-side information. We complement our theory with experimental results that show that capitalizing on available graph side information yields significant improvements over pure exploration methods that are unable to use this information. Our work highlights the importance of incorporating graph side information in state-of-the-art RL algorithms and offers new avenues for exploring rich graph-structured environments using this knowledge. Code is provided at https:\/\/github.com\/limengbinggz\/C2CL-RL.git.io\/. All source code is available at https:\/\/github.com\/limengbinggz\/KG4Vis - a Kaggle-based data set consisting of visualizations of subgraphs of trees and graphs","4":"In recent years, there has been a rapid growth in the application of machine learning techniques that leverage aviation data collected from commercial airline operations to improve safety. Anomaly detection and predictive maintenance have been the main targets for machine learning applications. However, this paper focuses on the identification of precursors, which is a relatively newer application. Precursors are events correlated with adverse events that happen prior to the adverse event itself. Therefore, precursor mining provides many benefits including understanding the reasons behind a safety incident and the ability to identify signatures, which can be tracked throughout a flight to alert the operators of the potential for an adverse event in the future. This work proposes using the multiple-instance learning (MIL) framework, a weakly supervised learning task, combined with carefully designed binary classifier leveraging a Multi-Head Convolutional Neural Network-Recurrent Neural Network (MHCNN-RNN) architecture. Multi-class classifiers are then created and compared, enabling the prediction of different adverse events for any given flight by combining binary classifiers, and by modifying the MHCNN-RNN to handle multi-output classification. Results obtained showed that the multiple binary classifiers perform better and are able to accurately forecast high speed and high path angle events during the approach phase","5":"Recently, public health officials have been trying to leverage predictive models to help university students avoid the spread of COVID-19. For this reason, they have introduced the concept of multi-armed bandit (MAB) games, which can provide insights into how to manage the risk of viral transmission while still allowing for some level of in person learning. In our paper, we present evidence from one of the largest undergraduate student halls across majors at a large US research university that there is indeed a significant increase in the number of tests administered each day and that these increases are strongly correlated with the performance of groups of students as measured by their final high school test scores. We also present information about the current state of the pandemic in terms of the total number of tests applied and the expected number of false positives. Based on these observations, we propose a novel intervention method called MAB-intervention, which consists of the following three components: 1) presenting proof of vaccination through an app, 2) updating parameters upon infection detection or development of new symptoms, and 3) providing incentives for students to refrain from certain actions regarding the use of those particular individuals. The overall solution relies on using a combination of interventions along with traditional testing practices such as physical distancing after being vaccinated. Vacc","6":"This article reports on a case study of AI-supported drug discovery with a focus on the role of model-agnostic adversarial behavior in this context. We develop a multi-level game framework for assessing the trustworthiness of disease diagnosis and treatment decisions made by such models, and perform quantitative and qualitative analyses of the evolution of the pandemic alongside the search for effective therapeutic agents. The core idea of our framework is to identity the trustworthy epidemiological model guided by human input, and to detect abnormal decision making by applying cognitive attentional resources. In particular, we define the acceptable risk level as the most important attribute influencing the final sensitivity of the model output regarding the amount of effort invested into finding an optimal solution. We demonstrate how our approach can be applied not only to problems where there are no existing data, but also to pure LAIRS cases where the origin of the virus is currently unknown. Finally, we discuss the implications of our work based on the use of AI-powered drug discovery. Our analysis shows that AI is well placed to support drug discovery, and that it can improve the identification of the trustworthy model guiding behavioral changes. Nevertheless, we identify potential threats and pitfalls associated with state-of-the-art approaches, and give some recommendations for prospective model developers.","7":"The risk of airborne infection for COVID-19 needs to be better understood and is especially urgent during the current pandemic. We present a method to determine the relative risk that can be readily deployed on either modelled or monitored CO$_2$ data and occupancy levels within an indoor space. Moreover, for spaces regularly, or consistently, occupied by the same group of people, e.g. an open-plan office or a school classroom, we establish protocols to assess the absolute risk of airborne infection of this regular attendance at work or school. In so doing, we are able to calculate the expected number of infections arising from a single regular attendee becoming infectious and remaining pre\/asymptomatic, i.e. we present a robust methodology to calculate the absolute reproductive number of these spaces. We demonstrate our model by calculating risks for both a modeller and by using monitored data recorded in a small naturally ventilated office. Results suggest that attendance at work is unlikely to significantly contribute to the pandemic if relatively quiet desk-based working conditions are carried out in the presence of adequate ventilation. However, these spaces are likely to contribute significantly to the pandemic if ventilation is inadequate and\/or activity levels increase. As ventilation is improved, the absolute reproductive numbers will decrease","8":"The COVID-19 pandemic has reshaped our world in a timescale much shorter than what we can understand. Particularities of SARS-CoV-2, such as its persistence in surfaces and the lack of a curative treatment or vaccine against COVID-19, have pushed authorities to apply restrictive policies to control its spreading. As data drove most of the decisions made in this global contingency, their quality is a critical variable for decision-making actors, and therefore should be carefully curated. In this work, we analyze the sources of error in typically reported epidemiological variables and usual tests used for diagnosis, and their impact on our understanding of COVID-19 spreading dynamics. We address the existence of different delays in the report of new cases, induced by the incubation time of the virus and testing-diagnosis time gaps, and other error sources related to the sensitivity\/specificity of the tests used to diagnose COVID-19. Using a statistically-based algorithm, we perform a temporal reclassification of cases to avoid delay-induced errors, building up new epidemiologic curves centered in the day where the contagion effectively occurred. We also statistically enhance the robustness behind the discharge\/recovery clinical criteria in the absence of a direct","9":"Many deep learning models have been proposed to address the issue of image data with great accuracy. However, such models require large training datasets for each task which are hard to obtain for time series imaging tasks. In this paper, we propose a novel method called self supervised learning (SSL) based on reinforcement learning and online feature clustering to generate accurate CT images from different views. We first introduce a general multi-view representation learning framework by using teacher forcing to representational transfer between various view pairs. Then, we apply the learned representations to cluster chest CT images via a distance metric with randomly generated initial embeddings. Finally, we use the clustered features to train a second classification model to predict whether the image represents a particular view. For one-dimensional (1D) inputs, we achieve a mean Dice score of 0.81$\\pm$0.03 across horizons ($3 \\times 10^{-2}$), while for two-dimensional (2D) inputs, the value increases to 0.99$\\pm$0.01 for the third dimension compared to 0.79$\\pm$0.04 for the second dimension. Extensive experiments demonstrate that our method outperforms state-of-the-art methods in terms of accuracy and speed","10":"We present automatically parameterised Fully Homomorphic Encryption (FHE), for encrypted neural network inference. We present and exemplify our inference over FHE compatible neural networks with our own open-source framework and reproducible step-by-step examples. We use the 4th generation Cheon, Kim, Kim and Song (CKKS) FHE scheme over fixed points provided by the Microsoft Simple Encrypted Arithmetic Library (MS-SEAL). We significantly enhance the usability and applicability of FHE in deep learning contexts, with a focus on the constituent graphs, traversal, and optimisation. We find that FHE is not a panacea for all privacy preserving machine learning (PPML) problems, and that certain limitations still remain, such as model training. However we also find that in certain contexts FHE is well suited for computing completely private predictions with neural networks. We focus on convolutional neural networks (CNNs), fashion-MNIST, and levelled FHE operations. The ability to privately compute sensitive problems more easily, while lowering the barriers to entry, can allow otherwise too-sensitive fields to begin advantaging themselves of performant third-party neural networks. Lastly we show encrypted deep learning, applied to a sensitive real world problem in","11":"Researchers have been investigating automated solutions for fact-checking in a variety of fronts. However, current approaches often overlook the fact that the amount of information released every day is escalating, and a large amount of them overlap with each other. Intending to accelerate fact-checking, we bridge this gap by grouping similar messages and summarizing them into aggregated claims. Specifically, we first clean a set of social media posts (e.g., tweets) and build a graph of all posts based on their semantics; Then, we perform two clustering methods to group the messages for further claim summarization. We evaluate the summaries both quantitatively with ROUGE scores and qualitatively with human evaluation. We also generate a graph of summaries to verify that there is no significant overlap among them. The results reduced 28,818 original messages to 700 summary claims, showing the potential to speed up the fact-check process by organizing and selecting representative claims from massive disorganized and redundant messages. We encourage future research to focus on developing semantic cluster extraction techniques as well as identifying relevant areas for fact-checking. These findings can help mitigate the negative impact of misinformation on our society at large. https:\/\/github.com\/anyleopeace\/covid19-claims-","12":"The widespread availability of large amounts of genomic data on the SARS-CoV-2 virus, as a result of the COVID-19 pandemic, has created an opportunity for researchers to analyze the disease at a level of detail unlike any virus before it. One one had, this will help biologists, policy makers and other authorities to make timely and appropriate decisions to control the spread of the coronavirus. On the other hand, such studies will help to more effectively deal with any possible future pandemic. Since the SARS-CoV-2 virus contains different variants, each of them having different mutations, performing any analysis on such data becomes a difficult task. It is well known that much of the variation in the SARS-CoV-2 genome happens disproportionately in the spike region of the genome sequence -- the relatively short region which codes for the spike protein(s). Hence, in this paper we propose an approach to cluster spike protein sequences in order to study the behavior of different known variants that are increasing at very high rate throughout the world. We use a k-mers based approach to first generate a fixed-length feature vector representation for the spike sequences. We then show that with the proper feature selection, we can efficiently and effectively cluster the spike sequences","13":"This article provides an overview on how to create (and deliver) digital resilience passports and digital health certificates in order to address the increasingly crucial role of cyber-criminals, hackers, bloggers, individuals and enterprises that use them for human trafficking, crime, financial crimes, etc., within national borders. We discuss various challenges associated with the creation of such digital resilience passports, including the design of the platform required for issuing credentials, the lack of standardization on the different types of digital resilience passport models, the insufficient data source validation criteria, and the usage of WSS2\/WSS3 standards for seamless portability. This article also considers the practical issues in the implementation of effective digital resilience passports and presents several recommendations for improving the security and privacy of smart healthcare systems involved in this application. Finally, we provide our personal view on what key research areas and potentials are needed to improve the digital resilience of societies across the globe. The purpose of this article is to demonstrate how NLP techniques can be used to enhance the protection of people's lives and assets against cybercrime, while considering their situational awareness. As shown in this paper, many of the most serious threats to individual and societal well-being occur through the criminal justice system and involve social media platforms or e-commerce companies","14":"The presence of concentration and temperature gradients in saline microdroplets evaporating directly in air makes them unsuitable for nucleation studies where homogeneous composition is required. This can be addressed by immersing the droplet in oil under regulated humidity and reducing the volume to the picoliter range. However, the evaporation dynamics of such a system is not well understood. In this work, we present evaporation models applicable for arrays of sessile microdroplets with dissolved solute submerged in a thin layer of oil. Our model accounts for the variable diffusion distance due to the presence of the oil film separating the droplet and air, the diffusive interaction of neighboring droplets, as well as the variation of the solution density and water activity due to the evolving solute concentration. Our model shows excellent agreement with experimental data for both pure water and NaCl solution. With this model, we demonstrate that assuming a constant evaporation rate and neglecting the diffusive interactions can lead to severe inaccuracies in the measurement of droplet concentration particularly during nucleation experiments. Given the significance of droplet evaporation in a wide array of scientific and industrial applications, the models and insights presented herein would be of great value to many fields of interest. Contact details","15":"We present our experience with the modernization on the GR-MHD code BHAC, aimed at improving its novel hybrid (MPI+OpenMP) parallelization scheme. In doing so, we showcase the use of performance profiling tools usable on x86 (Intel-based) architectures. Our performance characterization and threading analysis provided guidance in improving the concurrency and thus the efficiency of the OpenMP parallel regions. We assess scaling and communication patterns in order to identify and alleviate MPI bottlenecks, with both runtime switches and precise code interventions. The performance of optimized version of BHAC improved by $\\sim28\\%$, making it viable for scaling on several hundreds of supercomputer nodes. We finally test whether porting such optimizations to different hardware is likewise beneficial on the new architecture by running on ARM A64FX vector nodes. The results show that the fine-tuned optimized model outperforms both the baseline binary execution mode and the original TZ Search algorithm, while maintaining similar performances on the two GPU variants. This means that there is little overhead introduced by transferring knowledge from one system to another, which was previously thought to be infeasible. Finally, we see that using portable programming models such as C++ or CUDA can reduce the overhead even without","16":"In a paper of August 2013, I discussed the so-called SuperSpreader (SS) epidemic model and emphasized that it has dynamics differing greatly from the more-familiar uniform (or Poisson) textbook model. In that paper, SARS in 2003 was the representative instance and it was suggested that MERS may be another. In April 2014, MERS incident cases showed a spectacular spike (going from a handful in the previous April to more than 260 in that month of 2014) reminiscent of a figure I published nine months earlier. Here I refit the two-level and several variant SS models to incident data from January 1, 2013--April 30, 2014 and conclude that MERS will go pandemic (all other factors remaining the same). In addition, I discuss a number of model-realism and fitting methodology issues relevant to analysing SS epidemics. The results support the feasibility of some level of formal modelling of systems of epistemic random variables. This supports the validity of the Poisson process representation of SS incidence events and their associated time-varying probability distributions. Most importantly, this allows one to predict with considerable advance over time how these sporadic events will spread through time. A reasonably simple regression scheme can be built to estimate the parameters of the","17":"One regards spaces of trees as stratified spaces, to study distributions of phylogenetic trees. Stratified spaces with may have cycles, however spaces of trees with a fixed number of leafs are contractible. Spaces of trees with three and four leafs are spiders with three legs and four inches of Siamese space. We prove that the Cuntz-Pimsner $C^*$-algebra $\\mathcal{O}_\\text{(S)}(\\mathbb{Z}_{p>0})$-graded algebra can be given $128$ different leaves such that its multiplicity is not greater than $2+o(1)$. This implies that all but one of the N\\'eron--Severi groupoids on $(p,q)$ belong to $B_3^{k_c(\\cdot, \\cdot)}.$ In particular this gives an additional condition for the (unnecessary) constraint inferred by M.~Li, Chen, X.~Li, Ma, and Vit\\'anyi that the CUC exponent of $B_3^{k_c(\\cdot, \\cdot)}\\subseteq 2+o(1).$ A similar relation was also proved","18":"The analysis of longitudinal travel data enables investigating how mobility patterns vary across the population and identify the spatial properties thereof. The objective of this study is to identify the extent to which users explore different parts of the network as well as identify distinctive user groups in terms of the spatial extent of their mobility patterns. To this end, we propose two means for representing spatial mobility profiles and clustering travellers accordingly. We represent users patterns in terms of zonal visiting frequency profiles and grid-cells spatial extent heatmaps. We apply the proposed analysis to a large-scale multi-modal mobility data set from the public transport system in Stockholm, Sweden. We unravel three clusters - locals, commuters and explorers - that best describe the zonal visiting frequency and show that their composition varies considerably across users' place of residence and related demographics. We also identify 18 clusters of visiting spatial extent which form four groups that follow similar shapes of travel extent yet oriented in different directions. The approach proposed and applied in this study could be applied for any type of spatio-temporal individual travel demand data. Moreover, the findings highlighted in this study can be used by political decision makers to inform their interventions regarding targeted areas of human activity intervention. Modelling spatial mobility profiles and identifying potential spear points in demand change over time","19":"We study the formation of stratification in an incompressible fluid due to convective laminar flows in horizontal layers heated from the side. Medium and intensive modes of stationary laminar thermal, concentrational and thermoformational heating are considered, in which nonlinear flow features are manifested, which can radically change the flow structure and the characteristics of heat waves propagating through the layers. The solutions of the problems of laminar problems of convection show the features of the formation of layered structures, vertical temperature and concentration stratification depending on the determining dimensionless parameters. The metastable instability of the direction of the stratification vector (the location of the free surface) in weightlessness in the presence of capillary convection was shown. This suggests that the solution of the problem of laminar problems with conical inputs may be found under the conditions similar to those ones for the classical Rayleigh\u2013B\\'enard convection. The method is based on solving one dimensional system of ordinary differential equations via the finite element method, and it is therefore applicable to several other mechanisms related to multiphase mixing processes. In particular, the role of fluctuations in the mixing layer is taken into account. It is demonstrated that the numerical results obtained allow identifying the main mechanisms","20":"This article brings together two distinct, but related perspectives on playful museum experiences: Critical play and hybrid design. The article explores the challenges involved in combining these two perspectives, through the design of two hybrid museum experiences that aimed to facilitate critical play with\/in the collections of the Museum of Yugoslavia and the highly contested heritage they represent. Based on reflections from the design process as well as feedback from test users we describe a series of challenges: Challenging the norms of visitor behaviour, challenging the role of the artefact, and challenging the curatorial authority. In conclusion we outline some possible design strategies to address these problems. We also discuss some future research directions for this area which we hope will be able to translate into effective and enjoyable online content, and if successful, showcase the potential benefits of this approach by discussing how it might apply to other domains such as digital arts or museums. Keywords: Design, innovation, citizen science, game technology, social media, recommender systems, etc. This article is part of a longer term plan than originally intended and is presented here during 2020 in the form of an invited talk given at the University of Zagreb's Digital Art Gallery (DAG) on the topic of a recent work by our partner company Deep Learning. These results were","21":"Question Answering (QA) is a sub-field in natural language processing that has witnessed considerable growth after the advent of transformers. There have been notable advancements in QA datasets that have been proposed to challenge natural language processing models to improve human and existing model performance. Many pre-trained language models have proven to be incredibly effective at the task of extractive question answering. However, generalizability remains as a challenge for the majority of these models. That is, some datasets require models to reason more than others. In this paper, we train various pre-trained language models and fine-tune them on multiple question answering datasets of varying levels of difficulty to determine which of the models are capable of generalizing the most comprehensively across different datasets. Further, we propose a new architecture, BERT-BiLSTM, and compare it with other language models to determine if adding more bidirectionality can improve model performance. Using the F1-score as our metric, we find that the RoBERTa and BART pre-trained models perform the best across all datasets and that our BERT-BiLSTM model outperforms the baseline BERT model. Additional experiments validate the effectiveness of incorporating bidirectional reasoning into language models. The results demonstrate","22":"We present spectroscopic and photometric monitoring of NGC 3783 conducted throughout the first half of 2020. Time delays between the continuum variations and the response of the broad optical emission line were clearly detected, and we report reverberation measurements for H$\\beta$, HeII $\\lambda 4686$, H$\\gamma$, and H$\\delta$. From the time delay in the broad H$\\beta$ emission line and the line width in the variable portion of the spectrum, we derive a black hole mass of $M_{\\rm BH} = 2.34^{+0.43}_{-0.43} \\times 10^7$ M$_\\odot$. This is slightly smaller than, but consistent with, previous determinations. However, our significantly improved time sampling ($T_{\\rm med}=1.7$ days compared to $T_{\\rm med}=4.0$ days) has reduced the uncertainties on both the time delay and the derived mass by $\\sim 50$%. We also detect clear velocity-resolved time delays across the broad H$\\beta$ profile, with shorter lags in the line wings and a longer lag in the line core. Future modeling of the full velocity","23":"With noisy environment caused by fluoresence and additive white noise as well as complicated spectrum fingerprints, the identification of complex mixture materials remains a major challenge in Raman spectroscopy application. In this paper, we propose a new scheme based on a constant wavelet transform (CWT) and a deep network for classifying complex mixture. The scheme first transforms the noisy Raman spectrum to a two-dimensional scale map using CWT. A multi-label deep neural network model (MDNN) is then applied for classifying material. The proposed model accelerates the feature extraction and expands the feature graph using the global averaging pooling layer. The Sigmoid function is implemented in the last layer of the model. The MDNN model was trained, validated and tested with data collected from the samples prepared from substances in palm oil. During training and validating process, data augmentation is applied to overcome the imbalance of data and enrich the diversity of Raman spectra. From the test results, it is found that the MDNN model outperforms previously proposed deep neural network models in terms of Hamming loss, one error, coverage, ranking loss, average precision, F1 macro averaging and F1 micro averaging, respectively. The average detection time obtained from our model is 5","24":"To fight against the evolution of malware and its development, the specific methodologies that are applied by the malware analysts are crucial. Yet, this is something often overlooked in the relevant bibliography or in the formal and informal training of the relevant professionals. There are only two generic and all-encompassing structured methodologies for Malware Analysis (MA) - SAMA and MARE. The question is whether they are adequate and there is no need for another one or whether there is no such need at all. This paper will try to answer the above and it will contribute in the following ways: it will present, compare and dissect those two malware analysis methodologies, it will present their capacity for analysing modern malware by applying them on a random modern specimen and finally, it will conclude on whether there is a procedural optimization for malware analysis over the evolution of these two methodologies. To make the conclusion, the paper will first review which state of the art machine learning algorithms can be utilized for solving the problem of insufficient amount of data then, it will proceed to find a solution to the generalised MA problem, consisting of three components: a procedure, a human computer interaction system and a professional communication platform. Finally, based on the outcome of the study, it will propose a","25":"The disruption caused by the pandemic has called into question industrial norms and created an opportunity to reimagine the future of work. We discuss how this period of opportunity may be leveraged to bring about a future in which the workforce thrives rather than survives. Any coherent plan of such breadth must address the interaction of multiple technological, social, economic, and environmental systems. A shared language that facilitates communication across disciplinary boundaries can bring together stakeholders and facilitate a considered response. The origin story of cybernetics and the ideas posed therein serve to illustrate how we may better understand present complex challenges, to create a future of work that places human values at its core. Such a scenario redefines the concept of\"human-computer interaction\"and seeks to harness the computational power of cyberspace to transform societies' perceptions of labor, management, and innovation through data creation. We propose a framework for analyzing and discussing multidisciplinary societal change based on a combination of sociological, political, economical, and technical dimensions. Our aim is to offer a range of perspectives for managing and improving planetary health within our emerging ecosystem, while also pointing out the risks involved in moving away from fossil fuel-based generation. With this paper, we seek to provide visibility into the most important research trends focusing on the","26":"The application of Natural Language Processing (NLP) and data augmentation to improve the performance of a neural network for better detection of COVID-19 from patient-authored social media posts has been a promising direction. We propose a novel method, called HealthPrompt, to enhance healthcare workers' ability to identify patients with COVID-19 by using NLP techniques in conjunction with medical knowledge augmented through a learned model. In addition, we use adversarial training to make our model robust. Experiments were conducted on two datasets where we found that our approach outperformed various state-of-the-art methods when trying to detect COVID-19 disease from social media information. Moreover, we also found that our model's predictive capacity was much higher than a baseline classifier trained with handcrafted features and achieved up to 90% accuracy in detecting COVID-19 cases from chest X-ray images. Finally, we also evaluated our models thoroughly via both automated and human evaluation. The results indicate that our models have the potential to be used as a supplementary tool for frontline clinical doctors who are dealing with COVID-19 cases. Our code is available at https:\/\/github.com\/ethicslab\/healthprompt.git.io\/. For more details about this project","27":"Pooled testing offers an efficient solution to the unprecedented testing demands of the COVID-19 pandemic, although with potentially lower sensitivity and increased costs to implementation in some settings. Assessments of this trade-off typically assume pooled specimens are independent and identically distributed. Yet, in the context of COVID-19, these assumptions are often violated: testing done on networks (housemates, spouses, co-workers) captures correlated individuals, while infection risk varies substantially across time, place and individuals. Neglecting dependencies and heterogeneity may bias established optimality grids and induce a sub-optimal implementation of the procedure. As a lesson learned from this pandemic's first year, this paper highlights the necessity of integrating field sampling information with statistical modeling to efficiently optimize pooled testing. Using real data, we show that (a) greater gains can be achieved at low logistical cost by exploiting natural correlations (non-independence) between samples -- allowing improvements in sensitivity and efficiency of up to 30% and 90% respectively; and (b) these gains are robust despite substantial heterogeneity across pools (non-identical). Our modeling results complement and extend the observations of Barak et al (2021) who report an empirical sensitivity well beyond expectations. Finally, we provide an interactive tool","28":"As more and more people begin to wear masks due to current COVID-19 pandemic, existing face recognition systems may encounter severe performance degradation when recognizing masked faces. To figure out the impact of masks on face recognition model, we build a simple but effective tool to generate masked face images from unmasked faces automatically, and construct a new database called Masked LFW (MLFW) based on Cross-Age LFW (CALFW) database. The mask on the masked face generated by our method has good visual consistency with the original face. Moreover, we collect various mask types, covering most of the common styles appeared in the daily life, to achieve diverse generation effects. Considering realistic scenarios, we design three kinds of combinations of face pairs. The recognition accuracy of SOTA models declines 5%-16% on MLFW database compared with the accuracy on the original images. MLFW database can be viewed and downloaded at \\url{http:\/\/whdeng.cn\/mlfw}. Keywords: Face recognition, MLLF, CALFW, NLM, BERT, GAN, deep learning, transfer learning, etc.). Methods: We proposed a few keypoints based system for automatic recognition of masked faces. It includes two main parts: one","29":"We investigate spreading and recovery of disease in a square lattice, and in particular, emphasize the role of the initial distribution of infected patches in the network, on the progression of an endemic and initiation of a recovery process, if any, due to migration of both the susceptible and infected hosts. The disease starts in the lattice with three possible initial distribution patterns of infected and infection-free sites, infected core patches (ICP), infected peripheral patches (IPP) and randomly distributed infected patches (RDIP). Our results show that infection spreads monotonically in the lattice with increasing migration without showing any sign of recovery in the ICP case. In the IPP case, it follows a similar monotonic progression with increasing migration, however, a self-organized healing process starts for higher values of $m$. Encouragingly, for the initial RDIP arrangement, chances of recovering are much higher with lower migration rates than those for the random allocation case. An eigenvalue based semi-analytical study is made to determine the critical migration rate for realizing a stable infection-free lattice. It turns out that the initial fraction of infected patches plays an important role in determining the self-organized recovery probability. Sequential Monte Carlo simulations confirm that this phenomenon occurs for","30":"In cooperative bandits, a framework that captures essential features of collective sequential decision making, agents can minimize group regret, and thereby improve performance, by leveraging shared information. However, sharing information can be costly, which motivates developing policies that minimize group regret while also reducing the number of messages communicated by agents. Existing cooperative bandit algorithms obtain optimal performance when agents share information with their neighbors at \\textit{every time step}, i.e., full communication. This requires $\\Theta(T)$ number of messages, where $T$ is the time horizon of the decision problem. We propose \\textit{ComEx}, a novel cost-effective communication protocol in which the group achieves the same order of performance as full communication while communicating only $O(\\log T)$ number of messages. Our key step is developing a method to identify and only communicate the information crucial to achieving optimal performance. Further we propose novel algorithms for several benchmark cooperative bandit frameworks and show that our algorithms obtain \\textit{state-of-the-art} performance while consistently incurring a significantly smaller communication cost than existing algorithms. Finally, we provide a comprehensive theoretical analysis comparing both methods and establishing the connection between them. Code is available at https:\/\/github.com\/anyle","31":"Data privacy is a trending topic in the internet era. Given such importance, many challenges emerged in order to collect, manage, process, and publish data. In this sense, personal data have got attention, and many regulations emerged, such as GDPR in the European Union and LGPD in Brazil. This regulation provides a set of guidelines for companies and organizations collecting, storing, processing, and publishing data on individuals or within third parties. However, it does not provide any guidance regarding the specific regulatory requirements, which led to some criticism toward the use of GDPR and LDPE. Therefore, we propose a second-layer framework, called Data Governance System (DSS), consisting of two layers: 1) identification of high-level principles (e.g., with respect to what controls are considered), 2) design of objectives and associated constraints (e.g., publication time, user's consent, etc.), and 3) actual implementation of these components. We conducted several experiments using different datasets and validations to assess the performance of our proposed system. Our results show that the DSN architecture can help stakeholders identify their data sovereignty needs by evaluating both the topological and temporal properties of the data. Finally, we demonstrate the effectiveness of the proposed methodology through one case study","32":"On 2020 February 24, during their third observing run (\"O3\"), the Laser Interferometer Gravitational-wave Observatory and Virgo Collaboration (LVC) detected S200224ca: a candidate gravitational wave (GW) event produced by a binary black hole (BBH) merger. This event was one of the best-localized compact binary coalescences in history ($z = 0.01$), with all three components - two monochromatic GW waves from $l=m=2$ and one BBH wave propagating in LVC \\emph{i.e} through the Einstein Telescope (ET). The mass ratio of the two GW waves is determined by $\\approx$100\\% near the chirp mass $\\mathcal{M}_{\\rm eff} = 10^{-4} M_\\odot$, while the initial apparent horizon radius of the source in O3 has been measured to be $8.1 \\pm 1.5$ km, assuming it follows its electromagnetic counterpart. Assuming a uniform prior probability between 0 and 1 for each black hole's dimensionless spin magnitude, we are able to reproduce the observed luminosity curves of both events with a high degree of accuracy, providing strong constraints on","33":"The explosion in the availability of natural language data in the era of social media has given rise to a host of applications such as sentiment analysis and opinion mining. Simultaneously, the growing availability of precise geolocation information is enabling visualization of global phenomena such as environmental changes and disease propagation. Opportunities for tracking spatial variations in language use, however, have largely been overlooked, especially on small spatial scales. Here we explore the use of Twitter data with precise geolocation information to resolve spatial variations in language use on an urban scale down to single city blocks. We identify several categories of language tokens likely to show distinctive patterns of use and develop quantitative methods to visualize the spatial distributions associated with these patterns. Our analysis concentrates on comparison of contrasting pairs of Tweet distributions from the same category, each defined by a set of tokens. Our work shows that analysis of small-scale variations can provide unique information on correlations between language use and social context which are highly valuable to a wide range of fields from linguistic science and commercial advertising to social services. We also present evidence that our proposed method outperforms other well-established baselines in this case study. Further, we find that similar results can be obtained using state-of-the-art neural networks when training on equivalent array of input","34":"Robots in the real world should be able to adapt to unforeseen circumstances. Particularly in the context of tool use, robots may not have access to the tools they need for completing a task. In this paper, we focus on the problem of tool construction in the context of task planning. We seek to enable robots to construct replacements for missing tools using available objects, in order to complete the given task. We introduce the Feature Guided Search (FGS) algorithm that enables the application of existing heuristic search approaches in the context of task planning, to perform tool construction efficiently. FGS accounts for physical attributes of objects (e.g., shape, material) during the search for a valid task plan. Our results demonstrate that FGS significantly reduces the search effort over standard heuristic search approaches by approximately 93% for tool construction. Furthermore, FGS achieves high performance even when there are many more tools than available objects in the environment. Finally, we show that FGS can accurately reconstruct missing tools from previous tasks, yielding valuable insights into human behavior. Our code and data are available at https:\/\/github.com\/robot-planning-framework\/fgs.git.io. During future research, we will investigate how FGS can be applied to other contexts,","35":"The swift transitions in higher education after the COVID-19 outbreak identified a gap in the pedagogical support available to faculty. We propose a smart, knowledge-based chatbot that addresses issues of knowledge distillation and provides faculty with personalized recommendations. Our collaborative system crowdsources useful pedagogical practices and continuously filters recommendations based on theory and user feedback, thus enhancing the experiences of subsequent peers. We build a prototype for our local STEM faculty as a proof concept and receive favorable feedback that encourages us to extend our development and outreach, especially to underresourced faculty. We subsequently implement improvements and make them publicly available to all educators around the world through a web app. Through this article, we present both our case study and quantitative analyses of how these changes have impacted students, faculty, and university administrators. These analyses provide valuable information for optimizing the educational experience of every student, faculty, and university administrator across the globe. While we focus on the STEM field, we also discuss relevant aspects of the generalizability of our approach. In addition, we analyze the long-term implications of our work for other researchers, policymakers, and teachers who may want to design or refine our robot-supported teaching strategies. We are confident that our personalized recommendations will help improve the learning experience for everyone","36":"This study develops a framework for quantification of the impact of changes in population mobility due to social distancing on the COVID-19 infection growth rate. Using the Susceptible-Infected-Recovered (SIR) epidemiological model we establish that under some mild assumptions the growth rate of COVID-19 deaths is a time-delayed approximation of the growth rate of COVID-19 infections. We then hypothesize that the growth rate of COVID-19 infections is a function of socioeconomic factors, and identify the extent to which mobility influences the growth rate of COVID-19 deaths by state. The parameters of our statistical models are calibrated using data from several states, including California, Florida, Georgia, Maryland, Tennessee, Texas, Washington and Wisconsin. For each state, we estimate the dates at which the change in mobility was estimated. Our results show that social distancing has greatly changed the growth rate of COVID-19 deaths across all states, with rapid early implementations coming into play. Furthermore, the degree to which mobility varies between states is not widely believed to be correlated with death growth rates. Finally, there is significant variation in the onset and implementation of social distancing measures between states even after controlling for demographic and socioeconomic characteristics. One important","37":"CoVid-19 is primarily spread through airborne aerosols and droplets, with some possible routes of infection via pathogens contained in suspended respiratory tract fluid (saline). The main concern for human health is the risk of inhaling viruses or particles that may contain infectious agents, as well as the risk of contacting infected surfaces or objects contaminated by those agents. In this work we provide a comprehensive study on the transmission dynamics of SARS-CoV-2 in humans from the perspective of statistical physics. We show that the long range aerosol transmission is controlled by the breathing rate of each subject, regardless of the room volume and air velocity. However, the short range airborne transmission depends on multiple factors such as the number of viral copies in the respiratory tract, their initial velocity and gravity. Our results indicate that to minimize infection rates in high occupancy environments, it would be advantageous to apply early ventilation along with cautiousness regarding hygiene and wearing masks. This strategy provides a better balance between protecting patients from coronavirus disease 2019 (COVID-19) and other considerations related to public health policies. Additionally, our results show that if the goal is to keep the respiratory system from getting overwhelmed, then one should use the lowest air velocity possible. As an example, if the aim is","38":"We review previous approaches to nowcasting earthquakes and introduce new approaches based on deep learning using three distinct models based on recurrent neural networks and transformers. We discuss different choices for observables and measures presenting promising initial results for a region of Southern California from 1950-2020. Earthquake activity is predicted as a function of 0.1-degree spatial bins for time periods varying from two weeks to four years. The overall quality is measured by the Nash Sutcliffe Efficiency comparing the deviation of nowcast and observation with the variance over time in each spatial region. The software is available as open-source together with the preprocessed data from the USGS. We also discuss the applicability of this approach to other states and regions worldwide. Recurrent neural network based approaches are shown to be highly effective at forecasting peak values, valley locations, and sea surface temperature. They outperform conventional approaches such as linear regression and random forest methods. As the current track of these approaches, however, is not universally applicable to all areas of society due to differences in computation power and training datasets. Furthermore, we show that several practical applications of trained models can benefit from applying to high frequency images compared to simulations. To demonstrate the broad utility of our method, we use it to forecast the daily COVID-19","39":"Recent advancements in computational power and algorithms has enabled unabridged data (e.g., raw images or audio) to be used as input in some models (e.g., deep learning). However, the black box nature of such models reduces their likelihood of adoption by marketing scholars. Our paradigm of analysis, the Transparent Model of Unabridged Data (TMUD), enables researchers to investigate the inner workings of such black box models by incorporating an ex ante filtration module and an ex post experimentation module. We empirically demonstrate the TMUD by investigating the role of facial components and sexual dimorphism in face perceptions, which have implications for four marketing contexts: advertisement (perceptions of approachability, trustworthiness, and competence), brand (perceptions of whether a face represents a brand's typical customer), category (perceptions of whether a face represents a category's typical customer), and customer persona (perceptions of whether a face represents the persona of a brand's customer segment). Our results reveal new and useful findings that enrich the existing literature on face perception, most of which is based on abridged attributes (e.g., width of mouth). The TMUD has great potential to be a useful paradigm for generating theoretical insights and may encourage more marketing researchers","40":"Energy-efficient and reliable machine learning (ML) algorithms for building accurate ML models require substantial computation and communication resources. In this paper, we present a novel algorithm\/system architecture for energy-efficient and robust ML framework based on randomized neural network\/machine translation problem. With the combination of quantum and classical computing architectures, our system can achieve higher accuracy than any existing solution while using much less energy consumption and significantly improving the training time and memory utilization. We demonstrate the performance of our approach with several practical scenarios in both offline and cloud environments. The results show that by using suitable initial design choices, our system can outperform all previous state-of-the-art methods. Further, considering cost constraints, we show that our system can be adopted to different application domains, such as healthcare. Finally, we release our codebase, model weights, and benchmark datasets used in the research. They can be found at https:\/\/github.com\/FeiGSSS\/NEDL.git.io\/BLM4HMMR\/RBNN1KFJZYXOGEEV2\/ACDCBXMU5PQAUC, which shows its potential for large-scale real-world applications. Our work is expected to have a wider impact","41":"Antibodies are versatile proteins that bind to pathogens like viruses and stimulate the adaptive immune system. The specificity of antibody binding is determined by complementarity-determining regions (CDRs) at the tips of these Y-shaped proteins. In this paper, we propose a generative model to automatically design the CDRs of antibodies with enhanced binding specificity or neutralization capabilities. Previous generative approaches formulate protein design as a structure-conditioned sequence generation task, assuming the desired 3D structure is given a priori. In contrast, we propose to co-design the sequence and 3D structure of CDRs as graphs. Our model unravels a sequence autoregressively while iteratively refining its predicted global structure. The inferred structure in turn guides subsequent residue choices. For efficiency, we model the conditional dependence between residues inside and outside of a CDR in a coarse-grained manner. Our method achieves superior log-likelihood on the test set and outperforms previous baselines in designing antibodies capable of neutralizing the SARS-CoV-2 virus. Ablation studies confirm the superiority of our model in generating useful antibodies for COVID-19. Furthermore, we demonstrate that our work is able to generalize to unseen antibodies despite varying degrees of structural similarity","42":"The COVID-19 pandemic has affected all aspects of society, not only bringing health hazards, but also posing challenges to public order, governments and mental health. Moreover, it is the first one in history in which people from around the world uses social media to massively express their thoughts and concerns. This study aims at examining the stages of crisis response and recovery as a sociological problem by operationalizing a well-known model of crisis stages in terms of a psycho-linguistic analysis. Based on a large collection of Twitter data spanning from March to August 2020 in Argentina, we present a thematic analysis on the differences in language used in social media posts, and look at indicators that reveal the different stages of a crisis and the country response thereof. The analysis was combined with a study of the temporal prevalence of mental health conversations across the time span. Beyond the Argentinian case-study, the proposed approach and analyses can be applied to any public large-scale data. This approach can provide insights for the design of public health politics oriented to monitor and eventually intervene during the different stages of a crisis, and thus improve the adverse mental health effects on the population. Furthermore, it can offer methodological tools to analyze the effectiveness of government responses and identify the factors that drive psychological processes","43":"Twitter has been heavily used as an important channel for communicating and discussing about events in real-time. In such major events, many uninformative tweets are also published rapidly by many users, making it hard to follow the events. In this paper, we address this problem by investigating machine learning methods for automatically identifying informative tweets among those that are relevant to a target event. We examine both traditional approaches with a rich set of handcrafted features and state of the art approaches with automatically learned features. We further propose a hybrid model that leverages both the handcrafted features and the automatically learned ones. Our experiments on several large datasets of real-world events show that our approach significantly outperforms the baselines. Moreover, we qualitatively analyze the most discriminative features, showing that the polarity of a tweet's text is the main factor in deciding if the tweet is informative or not. Our findings motivate future research into automatic detection of informative tweets from social media platforms. To facilitate this, we release our collected dataset of 10,000 Tweets to foster future research. It can be found at https:\/\/github.com\/annahaensch\/covid19-tweets-ids. Besides, due to the high volume of the corpus, there is no need","44":"In statistics, researchers use Regression models for data analysis and prediction in many productive sectors (industry, business, academy, etc.). Regression models are mathematical functions representing an approximation of dependent variable $Y$ from n independent variables $X_i \\in X$. The literature presents many regression methods divided into single and multiple regressions. There are several procedures to generate regression models and sets of commercial and academic tools that implement these procedures. This work presents one open-source program called Regressor that makes models from a specific variation of polynomial regression. These models relate the independent variables to generate an approximation of the original output dependent data. In many tests, Regressor was able to build models five times more accurate than commercial tools. For example, it can produce a regression model with integer coefficients for 56 days, instead of 36 days, which has far better accuracy. Most importantly, we demonstrated that using pre-trained language models fine-tuned on domain data performs similarly for text classification tasks. Therefore, we also included some extra interpretability instructions in our manuscript. Source code and pre-trained weights are available at https:\/\/github.com\/annahaensch\/regressor. We encourage others to further develop their own research area by contributing to the growing community","45":"The number of companies opting for remote working has been increasing over the years, and Agile methodologies, such as Scrum, were adapted to mitigate the challenges caused by the distributed teams. However, the COVID-19 pandemic imposed a fully working from home context, which has never existed before. This paper investigation a two-phased Multi-Method study. In the first phase, we uncover how working from home impacted Scrum practitioners through a qualitative survey. Then, in the second phase, we propose a theoretical model that we test and generalize using Partial Least Squares - Structural Equation Modeling (PLS-SEM) through a sample study of 200 software practitioners who worked from home within Scrum projects. From assessing our model, we can conclude that all the latent variables are reliable and all the hypotheses are significant. Moreover, we performed an Importance-Performance Map Analysis (IPMA), highlighting the benefits of the home working environment and the use of Scrum for project success. We emphasize the importance of supporting the three innate psychological needs of autonomy, competence, and relatedness in the home working environment. We conclude that the home working environment and the use of Scrum both contribute to project success, with Scrum acting as a medi","46":"The main goal of this work is to present a new model able to deal with potentially misreported continuous time series. The proposed model is able to handle the autocorrelation structure in continuous time series data, which might be partially or totally underreported or overreported. Its performance is illustrated through a comprehensive simulation study considering several autocorrelation structures and two real data applications on human papillomavirus incidence in Girona (Catalunya, Spain) and COVID-19 outbreak in China. For each simulated dataset, we obtain the best results from our models by using the Bayesian optimization algorithm. We also illustrate how different factors may influence the behavior of the model in terms of computational efficiency and accuracy. One of the most important conclusions is that the autoregressive models are capable of generating reliable predictions for both continuous and discrete time series datasets. Such models could maybe serve as a first step towards the development of health prediction systems based on machine learning techniques. Our code is publicly available at https:\/\/github.com\/felipemaiapolo\/covid19_model_analysis. This website contains all the codes used to reproduce the experiments reported in this work.\\url{https:\/\/doi.org\/10.1151\/med","47":"The world is going through a challenging phase due to the disastrous effect caused by the COVID-19 pandemic on the healthcare system and the economy. The rate of spreading, post-COVID-19 symptoms, and the occurrence of new strands of COVID-19 have put the healthcare systems in disruption across the globe. Due to this, the task of accurately screening COVID-19 cases has become of utmost priority. Since the virus infects the respiratory system, Chest X-Ray is an imaging modality that is adopted extensively for the initial screening. We have performed a comprehensive study that uses CXR images to identify COVID-19 cases and realized the necessity of having a more generalizable model. We utilize MobileNetV2 architecture as the feature extractor and integrate it into Capsule Networks to construct a fully automated and lightweight model termed as MobileCaps. MobileCaps is trained and evaluated on the publicly available dataset with the model ensembling and Bayesian optimization strategies to efficiently classify CXR images of patients with COVID-19 from non-COVID pneumonia and healthy cases. The proposed model is further evaluated on two additional RT-PCR confirmed datasets to demonstrate the generalizability. We also introduce MobileCaps-S and leverage","48":"The daily routine of criminal investigators consists of a thorough analysis of highly complex and heterogeneous data of crime cases. Such data can consist of case descriptions, testimonies, criminal networks, spatial and temporal information, and virtually any other data that is relevant for the case. Criminal investigators work under heavy time pressure to analyze the data for relationships, propose and verify several hypotheses, and derive conclusions, while the data can be incomplete or inconsistent and is changed and updated throughout the investigation, as new findings are added to the case. Based on a four-year intense collaboration with criminalists, we present a conceptual design for a visual tool supporting the investigation workflow and Visilant, a web-based tool for the exploration and analysis of criminal data guided by the proposed design. Visilant aims to support namely the exploratory part of the investigation pipeline, from case overview, through exploration and hypothesis generation, to the case presentation. Visilant tracks the reasoning process and as the data is changing, it informs investigators which hypotheses are affected by the data change and should be revised. The tool was evaluated by senior criminology experts within two sessions and their feedback is summarized in the paper. Additional supplementary material contains the technical details and exemplary case study. We also performed a qualitative evaluation with three participants who regularly use","49":"We explore the value of weak labels in learning transferable representations for medical images. Compared to hand-labeled datasets, weak or inexact labels can be acquired in large quantities at significantly lower cost and can provide useful training signals for data-hungry models such as deep neural networks. We consider weak labels in the form of pseudo-labels and propose a semi-weakly supervised contrastive learning (SWCL) framework for representation learning using semi-weakly annotated images. Specifically, we train a semi-supervised model to propagate labels from a small dataset consisting of diverse image-level annotations to a large unlabeled dataset. Using the propagated labels, we generate a patch-level dataset for pretraining and formulate a multi-label contrastive learning objective to capture position-specific features encoded in each patch. We empirically validate the transfer learning performance of SWCL on seven public retinal fundus datasets, covering three disease classification tasks and two anatomical structure segmentation tasks. Our experiment results suggest that, under very low data regime, large-scale ImageNet pre-training on improved architecture remains a very strong baseline, and recently proposed self-supervised methods falter in segmentation tasks, possibly due to the strong invariant constraint imposed. Our method","50":"The recent COVID-19 pandemic has promoted vigorous scientific activity in an effort to understand, advice and control the pandemic. Data is now freely available at a staggering rate worldwide. Unfortunately, this unprecedented level of information contains a variety of data sources and formats, and the models do not always conform to the description of the data. Health officials have recognized the need for more accurate models that can adjust to sudden changes, such as produced by changes in behavior or social restrictions. In this work we formulate a model that fits a ``SIR''-type model concurrently with a statistical change detection test on the data. The result is a piece wise autonomous ordinary differential equation, whose parameters change at various points in time (automatically learned from the data). The main contributions of our model are: (a) providing interpretation of the parameters, (b) determining which parameters of the model are more important to produce changes in the spread of the disease, and (c) using data-driven discovery of sudden changes in the evolution of the pandemic. Together, these characteristics provide a new model that better describes the situation and thus, provides better quality of information for decision making. We further demonstrate how our approach can be used to refine relevant parameter values, which may then inform future","51":"Governments and organizations design performance-based research funding systems (PBFRS) for strategic aims, such as to selectively allocate scarce resources and stimulate research efficiency. In this work we analyze the relative change in research productivity of Italian universities after the introduction of such a system, featuring financial and reputational incentives. Using a bibliometric approach, we compare the relative research performance of universities before and after introduction of PBFRS, at the overall, discipline and field levels. The findings show convergence in the universities' performance, due above all to the remarkable improvement of the lowest performers. Geographically, the universities of the south (versus central and northern Italy) achieved the greatest improvement in relative performance. The methodology, and results, should be of use to university management and policy-makers. This analysis provides another confirmation of the importance of providing universities with financial incentives to improve their performance. At the same time, it demonstrates that financial incentives can help universities plan strategy and implement PBFRS, thus improving the evolution of higher education and scientific production. SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1007\/s10584-021-00748-2. RESULTS: The findings confirm the effectiveness of PBFRS in promoting academic growth","52":"The graph structure of biomedical data differs from those in typical knowledge graph benchmark tasks. A particular property of biomedical data is the presence of long-range dependencies, which can be captured by patterns described as logical rules. We propose a novel method that combines these rules with a neural multi-hop reasoning approach that uses reinforcement learning. We conduct an empirical study based on the real-world task of drug repurposing by formulating this task as a link prediction problem. We apply our method to the biomedical knowledge graph Hetionet and show that our approach outperforms several baseline methods. Furthermore, we demonstrate how our approach allows us to use it to identify important genes based on their genetic profiles alone. As a valuable resource, our approach enables new insights into the ways of building expert systems for link prediction. Our code has been made publicly available at https:\/\/github.com\/ml-jku\/Bioinformatics-based-learning-approach. Contact information for reproducibility is provided in https:\/\/gitlab.com\/dacs-hpi\/Covid-19-drug-repurposing. Further, we encourage any reader to examine the code for themselves or to take care of what they are reading. Finally, we discuss why our approach is","53":"The research examines the effect of cooking fuel choice on educational outcomes of adolescent children in rural India. Using multiple large-scale nationally representative datasets, we observe household solid fuel usage to adversely impact school attendance, years of schooling and age-appropriate grade progression among children. This inference is robust to alternative ways of measuring educational outcomes, other datasets, specifications and estimation techniques. Importantly, the effect is found to be more pronounced for females in comparison to the males highlighting the gendered nature of the impact. On exploring possible pathways, we find that the direct time substitution on account of solid fuel collection and preparation can explain the detrimental educational outcomes that include learning outcomes as well, even though we are unable to reject the health channel. In the light of the micro and macro level vulnerabilities posed by the COVID-19 outbreak, the paper recommends interventions that have the potential to fasten the household energy transition towards clean fuel in the post-covid world. The micro level vulnerability is caused by the lack of awareness about the ailment due to misinformation campaigns conducted via local media outlets to influence individual choices. Therefore, this research presents empirical evidence on the relationship between food insecurity and negative mental health outcomes such as depression given concerns about its increasing severity with regards to both genders and region across the globe","54":"As renewable resources gradually replace conventional generation based synchronous machines, the dynamics of the modern grid changes significantly and the system synchronous inertia decreases substantially. This transformation poses severe challenges for power system stability; for instance, it may lead to larger initial rate of change of frequency and increase frequency excursions. However, new opportunities also arise as novelconverter control techniques, so-called grid-forming strategies, show higher efficiency and faster response than conventional synchronous generators. They mainly involve virtual inertia (VI) emulation to mimic the behavior of synchronous machines. In this study, we developed a state-space model for the power system network with VI as a frequency regulation method. A reduced model based-norm algorithm (RMHA) considering the Fiedler mode impact was proposed in this paper to optimize the allocation of VI devices and improve power system frequency stability. Finally, case studies conducted on the IEEE 24-bus system demonstrate the efficacy of the proposed RMHA approach. The results indicate that the optimized VMAT strategy can greatly reduce the change of frequency and increase the capacity of the power system. For example, the optimal policy requires reducing the number of delayed cases by 20%, and increasing the reserve capacities of the power systems by 10% should result in additional benefits such as lower","55":"The men who took part in the nowcasts may have been mistaken about their own future, since they were only presented with information they already believed was false. This study presents evidence from a new perspective which adds to the previous studies based on public opinion polls. The focus is on the Netherlands, but results are relevant to any other country where these markets are open. By using social media data, we analyzed first the public perception of the Dutch news landscape at the early COVID-19 crisis, and then trying to understand how this image changed after the official introduction of the virus into the country. We found that the\"newborn\"conspiracy theory helped raise the initial negative public sentiment, but that its benefits became very small compared to the conspiracy theorists. We also found that the presence of the bot activity is more significant than the activity of other users, suggesting that it can be assumed that Twitter played a larger role in creating the illusion of having enough impact to sway public opinion. Finally, we tried to explain the positive effects of the measures taken by the government and the societal debate surrounding the acceptance of such measures. To conclude, we had initially thought that the measures taken by the government and the people's reaction to them were all encouraging and exciting, but upon reflection of the measures","56":"The COVID-19 pandemic has placed forecasting models at the forefront of health policy making. Predictions of mortality and hospitalization help governments meet planning and resource allocation challenges. In this paper, we consider the weekly forecasting of the cumulative mortality due to COVID-19 at the national and state level in the U.S. Optimal decision-making requires a forecast of a probability distribution, rather than just a single point forecast. Interval forecasts are also important, as they can support decision making and provide situational awareness. We consider the case where probabilistic forecasts have been provided by multiple forecasting teams, and we aggregate the forecasts to extract the wisdom of the crowd. With only limited information available regarding the historical accuracy of the forecasting teams, we consider aggregation (i.e. combining) methods that do not rely on a record of past accuracy. In this empirical paper, we evaluate the accuracy of aggregation methods that have been previously proposed for interval forecasts and predictions of probability distributions. These include the use of the simple average, the median, and trimming methods, which enable robust estimation and allow the aggregate forecast to reduce the impact of a tendency for the forecasting teams to be under- or overconfident. We use data that has been made publicly available from the CO","57":"The outbreak of COVID-19 has changed our lives in unprecedented ways. In the face of the projected catastrophic consequences, many countries have enacted social distancing measures in an attempt to limit the spread of the virus. Under these conditions, the Web has become an indispensable medium for information acquisition, communication, and entertainment. At the same time, unfortunately, the Web is being exploited for the dissemination of potentially harmful and disturbing content, such as the spread of conspiracy theories and hateful speech towards specific ethnic groups, in particular against Chinese people since COVID-19 is believed to have originated from China. In this paper, we make a first attempt to study the emergence of Sinophobic behavior on the Web during the outbreak of COVID-19. We collect two large-scale datasets from Twitter and 4chan's Politically Incorrect board (\/pol\/) over a time period of approximately five months and analyze them to investigate whether there is a rise or important differences with regard to the dissemination of Sinophobic content. We find that COVID-19 indeed drives the rise of Sinophobia on the Web and that the dissemination of Sinophobic content is a cross-platform phenomenon: it exists on fringe Web communities like \\dspol, and to a lesser extent on mainstream ones like Twitter. Also","58":"The increasing digitization of medical imaging enables machine learning based improvements in detecting, visualizing and segmenting lesions, easing the workload for medical experts. However, supervised machine learning requires reliable labelled data, which is is often difficult or impossible to collect or at least time consuming and thereby costly. Therefore methods requiring many resources, such as human expertise or funding, have been applied more regularly. Anomaly detection has proven to be effective for highlighting medically relevant anomalies on CT scans. To date, we are presenting a systematic review of literature with the aim of including anomaly detection techniques into the research community. We provide a comprehensive overview of recent papers by developing a hierarchical taxonomy containing two main approaches, 1) logical rule-based frameworks, based on either static or temporal characteristics of the lesion, 2) probabilistic feature-based frameworks, using either dynamic or contextual features exclusively. For each approach, we present concise results about training strategy, performance on different types of datasets (e.g., from CT to X-ray), challenges of applying AI approaches and their advantages over other tasks. The paper ends with a discussion on the current gaps and possible future directions. Our hope is that this will motivate further research into both areas! In addition, our website contains a collection of collected publications","59":"Online searches have been used to study different health-related behaviours, including monitoring disease outbreaks. An obvious caveat is that several reasons can motivate individuals to seek online information and models that are blind to people's motivations are of limited use and can even mislead. This is particularly true during extraordinary public health crisis, such as the ongoing pandemic, when fear, curiosity and many other reasons can lead individuals to search for health-related data, masking the disease-driven searches. However, while previous work has studied how individuals adjust their trust in web search results, we have largely focused on examining how social media users cope with uncertainty and imperfect information sources by adapting their trust in such crises. In this paper, we propose that a broader lens on uncertainty and imperfect information sources can enable us to understand how Facebook users balance their reliance on statistical evidence with their desire to avoid misinformation. We focus on two diseases, namely COVID-19 and flu. Leveraging more than 20 million posts related to these diseases from January 1st to May 15th, 2020, we analyse the distribution of links between vaccine-related and misinformation clusters across the two diseases. Specifically, we observe that vaccine-related pages receive greater attention after publication but that they tend to be less popular among all users. Moreover,","60":"The COVID-19 pandemic has been detrimental in terms of the number of fatalities and rising number of critical patients across the world. According to the UNDP (United National Development Programme) Socio-Economic programme, aimed at the COVID-19 crisis, the pandemic is far more than a health crisis: it is affecting societies and economies at their core. There has been greater developments recently in the chest X-ray-based imaging technique as part of the COVID-19 diagnosis especially using Convolution Neural Networks (CNN) for recognising and classifying images. However, given the limitation of supervised labelled imaging data, the classification and predictive risk modelling of medical diagnosis tend to compromise. This paper aims to identify and monitor the effects of COVID-19 on the human lungs by employing Deep Neural Networks on axial CT (Chest Computed Tomography) scan of lungs. We have adopted Mask RCNN, with ResNet50 and ResNet101 as its backbone, to segment the regions, affected by COVID-19 coronavirus. Using the regions of human lungs, where symptoms have manifested, the model classifies condition of the patient as either\"Mild\"or\"Alarming\". Moreover, the model is deployed on the Google Cloud Platform (","61":"Change point detection in time series has attracted substantial interest, but most of the existing results have been focused on detecting change points in the time domain. This paper considers the situation where nonlinear time series have potential changepoints in the state domain. We apply a density-weighted anti-symmetric kernel function to the state domain and therefore propose a nonparametric procedure to test the existence of changepoints. When the existence of changepoints is affirmative, we further introduce an algorithm to estimate the number of changepoints together with their locations. Theoretical properties of the proposed estimator are established and a simulation study is presented using data from the COVID-19 pandemic. Our method is then applied to detect changes in the daily number of new cases in Germany and Italy. In both countries, our approach successfully detects multiple changepoints in the state domain. Further, it is shown that our method can be used to reliably assess the existence of changepoints in complex longitudinal datasets. For example, our result indicates that the Italian mobility network (Ley de Protezione Civile) had a significant influence on the dynamics of the Covid-19 outbreak. Finally, since the rate of change in the total number of cases was found to have changed significantly over time,","62":"The hypothesis here states that neural network algorithms such as Multi-layer Perceptron (MLP) have higher accuracy in differentiating malicious and semi-structured phishing URLs. Compared to classical machine learning algorithms such as Logistic Regression and Multinomial Naive Bayes, the classical algorithms rely heavily on substantial corpus data training and machine learning experts' domain knowledge to perform complex feature engineering. MLP could perform non-linear separable multi-classes classification and focus less on corpus feature training. In addition, backpropagation weight adjustment could learn which features are more important in differentiating phishing from other attack types. The idea behind this hypothesis is to construct a deep learning framework with convolutional neural network architectures for both unsupervised and supervised learning tasks, along with some initial experiments involving human subjects. This study tries to evaluate the performance of state-of-the-art machine learning models on the MalImg malware dataset, using RT-PCR test images. It also discusses solutions to handle the problem of balanced classes distribution without adding any computational cost. Finally, it proposes a novel architecture for generating synthetic chest CT image datasets for further research. All these results show that modern deep learning techniques have achieved superior performance over traditional methods. They achieve $","63":"Supply Chains (SCs) are subject to disruptive events that potentially hinder the operational performance. Disruption Management Process (DMP) relies on the analysis of integrated heterogeneous data sources such as production scheduling, order management and logistics to evaluate the impact of disruptions on the SC. Existing approaches are limited as they address DMP process steps and corresponding data sources in a rather isolated manner which hurdles the systematic handling of a disruption originating anywhere in the SC. Thus, we propose MARE a framework for integrating Data Sources included in a Scenario-based HPC platform. The proposed framework enables the comparison between different scenarios based on the analysis of historical data from the perspective of any business logic or economic theory. Moreover, it allows to enhance the current capabilities of DMP steps by comparing their performance with relevant new features, including automated discovery of divergent subgroups among SCs. We illustrate the effectiveness of MARE across several case studies, namely a large food wholesaler\/processor facility run within a retail chain, a transportation network of oceanic origin, and an energy grid of electric vehicles. Our results show that MARE can be used to facilitate the integration of diverse datasets provided that the dataset distribution at the time of service is considered reasonably acceptable. In addition, due to the","64":"Many NLP applications, such as biomedical data and technical support, have 10-100 million tokens (~$10^7 to 10^9) in their training data. How should we train a language model on this scale? Most language modeling research considers either a small dataset with a closed vocabulary (like the standard 1 million token Penn Treebank), or the whole web with byte-pair encoding. We show that for our target setting in English, initialising and freezing input embeddings using in-domain data can improve language model performance by providing a useful representation of rare words, and this pattern holds across several different domains. In the process, we establish that the standard convention of tying input and output embeddings does not improve perplexity when initializing with embeddings trained on in-domain data. Finally, we provide a detailed analysis of the effect of domain-specific pre-trained models on downstream tasks. Our results indicate that providing domain-specific pre-trained models can benefit language modeling performance on downstream tasks, especially for rare words, while fine-tuning general language models on in-domain data yields smaller benefits. This indicates that measuring how well state-of-the-art language modeling methods perform on each downstream task is inconsistent with the primary goal of","65":"The COVID-19 pandemic has resulted in high rates of vaccine hesitancy, with concerns about the long-term effects of immunity. A critical factor associated with anti-vaccine attitudes is the information shared on social media. In this work, we investigate misinformation communities and narratives that can contribute to COVID-19 vaccine hesitancy. During the pandemic, anti-science and political misinformation\/conspiracies have been rampant on social media. Therefore, we investigate misinformation and conspiracy groups and their characteristic behaviours in Twitter data collected on COVID-19 vaccines. We identify if any suspicious coordinated efforts are present in promoting vaccine misinformation, and find two suspicious groups - one promoting a 'Great Reset' conspiracy which suggests that the pandemic is orchestrated by world leaders to take control of the economy, with vaccine related misinformation and strong anti-vaccine and anti-social messages such as no lock-downs; and another promoting the Bioweapon theory. Misinformation promoted is largely from the anti-vaccine and far-right communities in the 3-core of the retweet graph, with its tweets proportion of conspiracy and questionable sources to reliable sources being much higher. In comparison with mainstream and health news, the right-leaning community is more influenced by the anti-vacc","66":"Federal administrative tax data are invaluable for research, but because of privacy concerns, access to these data is typically limited to select agencies and a few individuals. An alternative to sharing microlevel data are validation servers, which allow individuals to query statistics without accessing the confidential data. This paper studies the feasibility of using differentially private (DP) methods to implement such a server. We provide an extensive study on existing DP methods for releasing tabular statistics, means, quantiles, and regression estimates. We also include new methodological adaptations to existing DP regression algorithms for using new data types and returning standard error estimates. We evaluate the selected methods based on the accuracy of the output for statistical analyses, using real administrative tax data obtained from the Internal Revenue Service Statistics of Income (SOI) Division. Our findings show that a validation server would be feasible for simple statistics but would struggle to produce accurate regression estimates and confidence intervals. We outline challenges and offer recommendations for future work on validation servers. This is the first comprehensive statistical study of DP methodology on a real, complex dataset, that has significant implications for the direction of a growing research field. The results presented here are useful as evidence for a potential pathway of distributional information processing in the public sector. The methodology studied can be applied generally to any other","67":"The world is going through one of the most dangerous pandemics of all time with the rapid spread of the novel coronavirus (SARS-CoV-2). The government scramble to seek antiviral treatment and vaccines to combat the disease, but the shortage of medical resources, especially in developing countries like Nigeria, made it difficult to provide sufficient help with the existing healthcare system. This study sought to develop a model that could estimate the number of patients infected with SARS-CoV-2 in a low-resource setting by using the data from the largest publicly available cohort of COVID-19 positive patients in Nigeria. The modeling approach adopted was based on the use of Bayesian optimization regression tree search applied to the entire dataset of positive individuals for the purpose of estimating the parameters of the model. The analysis revealed significant variables that affected the prediction of the number of cases infected with SARS-CoV-2. These included total population size, percentage of people infected with SARS-CoV-2, estimated range of age associated with infection, and the status of hospitalization among the patients who were hospitalized. The model also showed remarkable capacity for the detection of the demographic factors that significantly correlated with the risk of death of the patient. We chose the top performing","68":"We introduce a new model for contagion spread using a network of interacting finite memory two-color P\\'{o}lya urns, which we refer to as the Infinite Memory Interacting Poisson (IMIP) model. In this model, each agent in one of the states $\\pm 1$ ($\\infty$), $2 \\leq i \\leq N$, represents a different kind of exposed\/infected subpopula with respect to both their initial infection and their neighbors' susceptibility. The transition probabilities between those states are given by the kernel functions of the IMIP model. We establish analytically the well-definedness of the model and provide a microscopic derivation of the mean-field equation. We also study the properties of the non-stationary dynamics. We find that there are three distinct regions in parameter space where the curves of the averaged infected population grow either on a single scalar function or on a pairwise basis, with the first region being the most affected by the conflictive interaction among agents. Finally, we present numerical simulations to support our theoretical findings and further extend the analysis to more general cases. Our work incorporates results previously known from adaptive networks into the framework of random dynamical processes describing epidemic spreading and allows","69":"We consider the NP-complete problem of tracking paths in a graph, first introduced by Banik et. al. [3]. Given an undirected graph with a source $s$ and a destination $t$, find the smallest subset of vertices whose intersection with any $s-t$ path results in a unique sequence. In this paper, we show that this problem remains NP-complete when the graph is planar and we give a 4-approximation algorithm in this setting. We also show, via Courcelle's theorem, that it can be solved in linear time for graphs of bounded-clique width, when its clique decomposition is given in advance. Finally, we implement our approximation algorithm on real-world networks using a recently developed random walk testbed. Our experimental results show that our method outperforms the state-of-the-art heuristics, based on randomized early-outbreak strategies taken from literature. Overall, our work contributes to the long line of research on combinatorial online learning. Complementarily, the solution proposed here allows one to tailor the tuning parameters of a (time) series forecasting model to the specific network domain under consideration, e.g., for COVID-19 or automated mobility","70":"The high dependence on imported fuels and the potential for both climate change mitigation and economic diversification make Barbados' energy system particularly interesting for detailed transformation analysis. An open source energy system model is presented here for the analysis of a future Barbadian energy system. The model was applied in a scenario analysis, using a greenfield approach, to investigate cost-optimal and 100% renewable energy system configurations. Within the scenarios, the electrification of private passenger vehicles and cruise ships through shore-to-ship power supply was modelled to assess its impact on the energy system and the necessary investment in storage. Results show that for most scenarios of a system in 2030, a renewable energy share of over 80% is achieved in cost-optimal cases, even with a growing demand. The system's levelised costs of electricity range from 0.17 to 0.36 BBD\/kWh in the cost-optimal case and decrease only slightly for 100% renewable systems. Under the reasonable assumption of decreasing photovoltaic investments costs, system costs of a 100% system may be lower than the current costs. The results show that pumped hydro-storage is a no-regret option for the Barbadian power system design. Overall, the results highlight the great potential of renewable","71":"When the Covid-19 pandemic enters dangerous new phase, whether and when to take aggressive public health interventions to slow down the spread of COVID-19. To develop the artificial intelligence (AI) inspired methods for real-time forecasting and evaluating intervention strategies to curb the spread of Covid-19 in the World. A modified auto-encoder for modeling the transmission dynamics of the epidemics is developed and applied to the surveillance data of cumulative and new Covid-19 cases and deaths from WHO, as of March 16, 2020. The average errors of 5-step forecasting were 2.5%. The total peak number of cumulative cases and new cases, and the maximum number of cumulative cases in the world with later intervention (comprehensive public health intervention is implemented 4 weeks later) could reach 75,249,909, 10,086,085, and 255,392,154, respectively. The case ending time was January 10, 2021. However, the total peak number of cumulative cases and new cases and the maximum number of cumulative cases in the world with one week later intervention were reduced to 951,799, 108,853 and 1,530,276, respectively. Duration time of the Covid-19 spread would be reduced","72":"The Internet of Things, crowdsourcing, social media, public authorities, and other sources generate bigger and bigger data sets. Big and open data offers many benefits for emergency management, but also pose new challenges. This chapter will review the sources of big data and their characteristics. We then discuss potential benefits of big data for emergency management along with the technological and societal challenges it poses. We review central technologies for big-data storage and processing in general, before presenting the Spark big-data engine in more detail. Finally, we review ethical and societal threats that big data pose. Our work reviews past crises and disasters, and explores the social dimension of disaster events to understand how human groups characteristically self-organize to identify the common issues and concerns of society during catastrophic events. It combines the research agenda of crisis text mining and sentiment analysis with the operational stages of disaster events such as community detection, topic modeling, sentiment classification, and others. Many different fields of commercial tourism cities and states have been analyzed for this chapter's purpose, including hotel stays, travel information, tourism culture, and public utilities. The results of this chapter provide implications for future research on the application of big data analytics for emergency management and cover the areas of human life and social sciences. At the same time,","73":"We analysed publicly available data on place of occurrence of COVID-19 deaths from national statistical agencies in the UK between March 9 2020 and February 28 2021. We introduce a modified Weibull model that describes the deaths due to COVID-19 at a national and place of occurrence level. We observe similar trends in the UK where deaths due to COVID-19 first peak in Homes, followed by Hospitals and Care Homes 1-2 weeks later in the first and second waves. This is in line with the infectious period of the disease, indicating a possible transmission vehicle between the settings. Our results show that the first wave is characterised by fast growth and a slow reduction after the peak in deaths due to COVID-19. The second and third waves have the converse property, with slow growth and a rapid decrease from the peak. This difference may result from behavioural changes in the population (social distancing, masks, etc). Finally, we introduce a double logistic model to describe the dynamic proportion of COVID-19 deaths occurring in each setting. This analysis reveals that the proportion of COVID-19 deaths occurring in Care Homes increases from the start of the pandemic and past the peak in total number of COVID-19 deaths in the first wave.","74":"This paper proposes a new method for determining similarity and anomalies between time series, most practically effective in large collections of (likely related) time series, with a particular focus on measuring distances between structural breaks within such a collection. We consolidate and generalise a class of semi-metric distance measures, which we term MJ distances. Experiments on simulated data demonstrate that our proposed family of distances uncover similarity within collections of time series more effectively than measures such as the Hausdorff and Wasserstein metrics. Although our class of distances do not necessarily satisfy the triangle inequality requirement of a metric, we analyse the transitivity properties of respective distance matrices in various contextual scenarios. There, we demonstrate a trade-off between robust performance in the presence of outliers, and the triangle property property in the absence of outliers. For real world data, however, we show that both clustering algorithms allow for substantial improvement over pure baselines. A nice feature of our approach is that it can be applied to any existing measure or combination thereof that could offer improved insight into the behavior of this family of models. Finally, we illustrate three ways of analysing the distance and similarity matrices, namely via eigenvalue analysis, hierarchical clustering, and spectral clustering. The results from","75":"Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions; and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG. Finally","76":"In this study, we presented the results of analyzing the heart rate from the smartphone and obtained valuable insights based on the information contained in it. The\"MIcro-Surgical Anastomose Workflow recognition on training sessions\"(MISAW) challenge provided a data set consisting of 27 sequences of micro-surgical anastomosis on artificial blood vessels. This data set was composed of videos, kinematics, and workflow annotations described at three different granularity levels: phase, step, and activity. The participants were given the option to use kinematic data and videos to develop workflow recognition models. Four tasks were proposed to the participants: three of them were related to the recognition of surgical workflow at three different granularity levels, while the last one addressed the recognition of all granularity levels in the same model. One ranking was made for each task. We used the average application-dependent balanced accuracy (AD-Accuracy) as the evaluation metric. This takes unbalanced classes into account and it is more clinically relevant than a frame-by-frame score. Six teams, including a non-competing team, participated in at least one task. All models employed deep learning models, such as CNN or RNN. The best models achieved more than 95","77":"Recent progress in protein-protein docking has demonstrated impressive performance of existing models trained on protein interfaces. However, these methods predominantly utilize molecular dynamics (MD) or Monte Carlo (MC) sampling to achieve high-throughput and complex multi-agent interactions between proteins. In this work, we propose a novel parallel graph neural network (GNN) framework, namely BridgeDPI, for learning effective representations of protein interface states via MD and MC samplings. As such, our model incorporates both the dynamic structural connectivity and complementary functional relationships into one integrated representation space, enabling it to learn prominent conformational and compositional changes along with large numbers of mutations without much loss of interpretability. Additionally, we use an attention mechanism to reflect upon the learned features by incorporating several domain experts as prior nodes in the GNN architecture, thus leading to further improvement in downstream tasks. Our experiments demonstrate significant improvements over previous state-of-the-art methods on two challenging benchmark datasets for drug discovery related to COVID-19. Moreover, we show that our approach can be also applied to successfully predict residue-level binding events from the QM9 dataset. Finally, we provide extensive ablation studies to examine the importance of different components in the proposed framework. Our code and data are available at https","78":"Forecasting has emerged as an important component of informed, data-driven decision-making in a wide array of fields. We introduce a new data model for probabilistic predictions that encompasses a wide range of forecasting settings. This framework clearly defines the constituent parts of a probabilistic forecast and proposes one approach for representing these data elements. The data model is implemented in Zoltar, a new software application that stores forecasts using the data model and provides standardized API access to the data. In one real-time case study, an instance of the Zoltar web application was used to store, provide access to, and evaluate real-time forecast data on the order of 10$^7$ rows, provided by over 20 international research teams from academia and industry making forecasts of the COVID-19 outbreak in the US. Tools and data infrastructure for probabilistic forecasts, such as those introduced here, will play an increasingly important role in ensuring that future forecasting research adheres with a strict set of rigorous and reproducible standards. Finally, this paper offers our personal view on what aspects of a forecast should be included in formalizing its definition and providing illustrative examples of doing so. In particular, we argue that some fundamental concepts and tools of probability theory and statistics, and","79":"We begin this thesis with an extensive pedagogical introduction aimed at clarifying the foundations of the hierarchy problem. After introducing effective field theory, we discuss renormalization at length from a variety of perspectives. We focus on conceptual understanding and connections between approaches, while providing a plethora of examples for clarity. With that background we can then clearly understand the hierarchy problem, which is reviewed primarily by introducing and refuting common misconceptions thereof. We next discuss some of the beautiful classic frameworks to approach the issue. However, we argue that the LHC data have qualitatively modified the issue into `The Loerarchy Problem'---how to generate an IR scale without accompanying visible structure---and we discuss recent work on this approach. In the second half, we present some of our own work in these directions, beginning with explorations of how the Neutral Naturalness approach motivates novel signatures of electroweak naturalness at a variety of physics frontiers. Finally, we propose a New Trail for Naturalness and suggest that the physical breakdown of EFT, which gravity demands, may be responsible for the violation of our EFT expectations at the LHC. A comparison between the predictions of the Standard Model and EFT (in both senses) is presented as well as New Trail metrics for identifying violations","80":"We present a test of the statistical method introduced by Bernard F. Shutz in 1986 using only gravitational waves to infer the Hubble constant (H$_0$) from GW190814, the first high-probability neutron-star--black-hole (NS-BH) merger candidate detected by the Laser Interferometer Gravitational Wave Observatory (LIGO) and the Virgo interferometer. We apply a baseline test of this method to the binary neutron star (BNS) merger GW170817 and find H$_0 = 70^{+35.0}_{-18.0}$ km s$^{-1}$ Mpc$^{-1}$ ($68\\%$ credible interval) for a galaxy $B$-band luminosity threshold of ${\\sim}3 \\times 10^8 \\mathrm{M}_\\odot$, or approximately twice the mass of the recently discovered Gaia-Enceladus\/Sausage system. We also use two other methods to constrain the parameters of the NS-BH merger candidates; we estimate H$_0 = 67^{+41.0}_{-26.0}$ km s$^{-1}$","81":"In this Letter we show that multiband observations of stellar-mass binary black holes by the next generation of ground-based observatories (3G) and the space-based Laser Interferometer Space Antenna (LISA) could facilitate a comprehensive test of general relativity by simultaneously measuring all the post-Newtonian (PN) coefficients. Multiband observations would measure most of the known PN phasing coefficients to an accuracy below a few percent---two orders-of-magnitude better than the best bounds achievable from even `golden' binaries in the 3G or LISA bands. Such multiparameter bounds would play a pivotal role in constraining the parameter space of modified theories of gravity beyond general relativity. Further, such bounds would allow us to probe the nature of dark matter through the eyes of more accurate future probes such as LambdaCDM models. We discuss how a single newton's presence in both the 3G and LISA bands would enable mutiband observations of hundreds of active galactic nuclei (AGNs). Our conclusions are based on two main assumptions: i) that low-frequency gravitational wave signals from merging AGNs follow a power law with high spectral index ii) that there is no systematic dependence between the accretion rate (me","82":"Graph Representation Learning methods have enabled a wide range of learning problems to be addressed for data that can be represented in graph form. Nevertheless, several real world problems in economy, biology, medicine and other fields raised relevant scaling problems with existing methods and their software implementation, due to the size of real world graphs characterized by millions of nodes and billions of edges. We present GraPE, a software resource for graph processing and random walk based embedding, that can scale with large and high-degree graphs and significantly speed up-computation. GraPE comprises specialized data structures, algorithms, and a fast parallel implementation that displays everal orders of magnitude improvement in empirical space and time complexity compared to state of the art software resources, with a corresponding boost in the performance of machine learning methods for edge and node label prediction and for the unsupervised analysis of graphs.GraPE is designed to run on laptop and desktop computers, as well as on high performance computing clusters. On both computer platforms, we demonstrate that GraPE provides significant speedup over state of the art software processes, while also exhibiting strong memory and communication bounds. For example, we achieve 2.9 million accuracy on NGC4258 and 9.3 billion parameters (equivalent to ~ 1% of the world","83":"Internet-of-Things (IoT) has grown rapidly in the last decade and continue to develop in terms of dimension and complexity offering wide range of devices to support diverse set of applications. With ubiquitous Internet, connected sensors and actuators, networking and communication technology, and artificial intelligence (AI), smart cyber-physical systems (CPS) provide services rendering assistance to humans in their daily lives. However, the recent outbreak of COVID-19 (also known as coronavirus) pandemic has exposed and highlighted the limitations of current technological deployments to curtail this disease. IoT and smart CPS are key technologies for safe reopening and other socio-economic recovery efforts. Therefore, selecting the right temporal node or network to deploy these functionalities on can significantly impact the efficiency and effectiveness of IoFT. In this paper, we envision a unified dynamic architecture of intelligent infrastructure sharing robotic and cognitive capabilities to effectively manage epidemic outbreaks under local spreading events. We propose a novel network-based algorithm to select the best configuration of heterogeneous physical units (HPUs) and digital devices (DTs) for each HCI. Moreover, we present a new conceptual design of a cloud-connected gateway computing platform to enable flexible resource utilization with enhanced performance and lower latency. Finally, we","84":"Conventional wisdom suggests that large-scale refugees pose security threat to the host community or state. With massive influx of Rohingyas in Bangladesh in 2017 resulting a staggering total of 1.6 million Rohingyas, a popular discourse emerged that Bangladesh would face severe security threats. This article investigates the security experience of Bangladesh from the Rohingya influx over a three-year period, August 2017 to August 2020. The research question I intend to address is, 'has Bangladesh experienced security threat due to massive Rohingya influx?' If so in what ways? I test four security threat areas: societal security, economic security, internal security, and public security. I have used newspaper content analysis over past three years along with interview data collected from interviewing local people in coxs bazar area where the Rohingya camps are located. To assess if the threats are low level, medium level, or high level, we investigated both the frequency of reports and the way they are interpreted. We find that Bangladesh does not currently report on the security experiences of its citizens. It seems to be taking a deliberate approach to risk assessment as well as trying to establish policy and development to mitigate the potential risks. We also interpret the challenges as largely political, stemming from the government and media rather than socioeconomic problems. Finally, we present","85":"Pandemic and epidemic diseases such as CoVID-19, SARS-CoV2, and Ebola have spread to multiple countries and infected thousands of people. Such diseases spread mainly through person-to-person contacts. Health care authorities recommend contact tracing procedures to prevent the spread to a vast population. Although several mobile applications have been developed to trace contacts, they typically require collection of privacy-intrusive information such as GPS locations, and the logging of privacy-sensitive data on a third party server, or require additional infrastructure such as WiFi APs with known locations. In this paper, we introduce CONTAIN, a privacy-oriented mobile contact tracing application that does not rely on GPS or any other form of infrastructure-based location sensing, nor the continuous logging of any other personally identifiable information on a server. The goal of CONTAIN is to allow users to determine with complete privacy if they have been within a short distance, specifically, Bluetooth wireless range, of someone that is infected, and potentially also when. We identify and prove the privacy guarantees provided by our approach. Our simulation study utilizing an empirical trace dataset (Asturies) involving 100 mobile devices and around 60000 records shows that users can maximize their possibility of identifying if they were near an infected user by turning on the","86":"Continuous assessment of blood oxygen saturation (SpO(2)) is essential for respiratory functionality and is challenging under current clinical practice. Contactless SpO(2) measurement using light scattering technique has been proposed to overcome the drawbacks of conventional optical methodologies, but it remains unclear whether this type of non-contact sensing can be used for contactless SpO(2) monitoring in order to facilitate medical application. In this paper, we propose a novel scheme for contactless SpO(2) estimation via deep learning based on both smartphone camera images and RGB sensor data. The proposed method consists of two main components: 1) face detection algorithm to detect human faces with low accuracy, 2) distance metric calculation algorithm to construct a safe distance measure between people, 3) colour enhancement factor to highlight the positive region and 4) integration of multi-task convolutional network based on attention mechanism to extract global features from the entire input frame before performing the classification decision. We have evaluated our approach through multiple experiments on real world datasets by involving different combinations of face detection, distance metrics, and color enhancement factors. The results show that our method achieves the state-of-the-art performance among all competing methods. Furthermore, we have also tested our model on the dataset where no annotations","87":"A system to model the spread of COVID-19 cases after lockdown has been proposed, to define new preventive measures based on hotspots, using the graph clustering algorithm. This method allows for more lenient measures in areas less prone to the virus spread. There exist methods to model the spread of the virus, by predicting the number of confirmed cases. But the proposed system focuses more on the preventive side of the solution from a geographical point of view, by predicting the areas or regions that may become hotspots for the virus in the near future. The fact that the virus can only be transmitted by being in close proximity to an already infected person, suggests that, the regions that can easily be reached from an existing hotspot, have a higher chance of becoming a new hotspot. Moreover, in smaller regions, even after strict provisions, positive cases have been found. To consider this fact, the geographic distance between the nearest hotspots can be used as a measure of likelihood of the region also becoming a hotspot. In this paper, a weighted graph of regions with the regions themselves as weighted nodes with weight of the nodes as the number of active cases and the distance as edge weights. The graph can be completely connected or connected based on a distance threshold. The nodes are","88":"Human mortality is in part a function of multiple socioeconomic factors that differ both spatially and temporally. Adjusting for other covariates, the human lifespan is positively associated with household wealth. However, the extent to which mortality in a geographical region is a function of socioeconomic factors in both that region and its neighbors is unclear. There is also little information on the temporal components of this relationship. Using the districts of Hong Kong over multiple census years as a case study, we demonstrate that there are differences in how wealth indicator variables are associated with longevity in (a) areas that are affluent but neighbored by socially deprived districts versus (b) wealthy areas surrounded by similarly wealthy districts. We also show that the inclusion of spatially-distributed variables reduces uncertainty in mortality rate predictions in each census year when compared with a baseline model. Our results suggest that geographic mortality models should incorporate nonlocal information (e.g., spatial neighbors) into their studies in order to lower the variance of their estimates, and point to more coherent modeling of sociospatial spillover effects on mortality rates. Further, our findings provide important insights regarding the dynamic components of human mortality at different levels of granularity. The methods we develop here could be applied to other countries and regions, and therefore help advance our","89":"The inclusion of intermittent and renewable energy sources has increased the importance of demand forecasting in power systems. Smart meters can play a critical role in demand forecasting due to the measurement granularity they provide. Despite their virtue, smart meters used for forecasting face some constraints as consumers' privacy concerns, reluctance of utilities and vendors to share data with competitors or third parties, and regulatory restrictions on these markets especially during COVID-19. This paper examines a collaborative machine learning method, federated learning extended with privacy preserving techniques for short-term demand forecasting using smart meter data as a solution to the previous constraints. The combination of privacy preserving techniques and federated learning enables to ensure consumers' confidentiality concerning both their data, the models generated using it (Differential Privacy), and the communication mean (Secure Aggregation). To evaluate this paper's collaborative secure federated learning setting, we explore current literature to select the baseline for our simulations and evaluation. We simulate and evaluate several scenarios that explore how traditional centralized approaches could be projected in the direction of a decentralized, collaborative and private system. The results obtained over the evaluations provided decent performance and in a privacy setting using differential privacy almost perfect privacy budgets (1.39,$10e^{-5}$) and (2.01,$10e","90":"Online social media platforms such as Twitter provide a conduit for disseminating our ideas, views and thoughts and proliferate information. This has led to the amalgamation of English with natively spoken languages. Prevalence of Hindi-English code-mixed data (Hinglish) is on the rise with most of the urban population all over the world. Hate speech detection algorithms deployed by most social network platforms are unable to filter out offensive and abusive content posted in these code-mixed languages. Thus, the worldwide hate speech detection rate of around 44% drops even more considering the content in Indian colloquial languages and slangs. In this paper, we propose a methodology for efficient detection of unstructured code-mix Hinglish language. Fine-tuning based approaches for Hindi-English code-mixed language are employed by utilizing contextual based embeddings such as ELMo (Embeddings for Language Models), FLAIR, and transformer-based BERT (Bidirectional Encoder Representations from Transformers). Our proposed approach is compared against the pre-existing methods and results are compared for various datasets. Our model outperforms the other methods and frameworks. To the best of our knowledge, this is the first method to be trained for detecting unstruct","91":"This paper presents a speaking-rate-controllable HiFi-GAN neural vocoder. Original HiFi-GAN is a high-fidelity, computationally efficient, and tiny-footprint neural vocoder. We attempt to incorporate a speaking rate control function into HiFi-GAN for improving the accessibility of synthetic speech. The proposed method inserts a differentiable interpolation layer into the HiFi-GAN architecture. A signal resampling method and an image scaling method are implemented in the proposed method to warp the mel-spectrograms or hidden features of the neural vocoder. We also design and open-source a Japanese speech corpus containing three kinds of speaking rates to evaluate the proposed speaking rate control method. Experimental results of comprehensive objective and subjective evaluations demonstrate that 1) the proposed method outperforms a baseline time-scale modification algorithm in speech naturalness, 2) warping mel-spectrograms by image scaling obtained the best performance among all proposed methods, and 3) the proposed speaking rate control method can be incorporated into HiFi-GAN without losing computational efficiency. All code used in this work is available at https:\/\/github.com\/Mao-KU\/JUNLP.git. This repository collects and collates the benchmark datasets with respect to","92":"Fast and accurate hourly forecasts of wind speed and power are crucial in quantifying and planning the energy budget in the electric grid. Modeling wind at a high resolution brings forth considerable challenges given its turbulent and highly nonlinear dynamics. In developing countries, where wind farms over a large domain are currently under construction or consideration, this is even more challenging given the necessity of modeling wind over space as well. In this work, we propose a machine learning approach to model the nonlinear hourly wind dynamics in Saudi Arabia with a domain-specific choice of knots to reduce the spatial dimensionality. Our results show that for locations highlighted as wind abundant by a previous work, our approach results in an 11% improvement in the two-hour-ahead forecasted power against operational standards in the wind energy sector, yielding a saving of nearly one million US dollars over a year under current market prices in Saudi Arabia. We further demonstrate how our method can be used to explore other states' electricity supply and demand profiles, especially towards building infrastructures. With our approach, we aim to provide data-driven indicators for decision makers in the Saudi Arabian electrical system and beyond. Our goal is to present a clear and practical methodology for modeling wind dynamics that enables stakeholders to efficiently assess their potential role in reducing future","93":"We present the first results from our survey of asteroid polarization-phase curves in the near-infrared J and H bands using the WIRC+Pol instrument on the Palomar 200-inch telescope. We confirm through observations of standard stars that WIRC+Pol can reach the 0.1% precision needed for asteroid phase curve characterization, and show that C-complex asteroids could act as an alternate calibration source as they show less wavelength variation than stellar polarized standards. Initial polarization-phase curve results for S-complex asteroids show a shift in behavior as a function of wavelength from visible to near-infrared bands, extending previously observed trends. Full near-infrared polarization-phase curve characterization of individual asteroids will provide a unique constraint on surface composition of these objects by probing the wavelength dependence of albedo and index of refraction of the surface material. Our Near-Infrared Polarimetry Survey (NIRPS) also provides full near-infrared polarization-phase curve measurements of late C-complex asteroids in the redshift range 1.7<z<3.5, providing a new palette of colors for studying Solar System objects in the NIR band. Mapping SSOSA-CUBES within the NIRPS focal plane shows no drop in","94":"We discuss how to extend a standard semi-quantitative risk assessment model for healthcare systems under the Covid-19 pandemic. We analyse the role and importance of quarantine, self-isolation, social distancing, and vaccination in the new context of managing the pandemic through quantitative models. We focus on the effects of varying the infection rate \u03ba which shows that the natural progression of the disease requires maintaining a certain proportion of infected people as long as there is a sufficiently high value of R0 (the basic reproduction number). The threshold between this value and the infection ratio are discussed as a function of R0 using a simple Bayesian approach. We show that our model is able to correctly describe the reported curves over different scenarios. To improve the accuracy of the predictions, we vary the parameters of the simulation model by systematically analysing five different situations which represent various levels of severity of the epidemic. From these results, we propose a set of best practices that could effectively reduce the transmission of the coronavirus. If followed by strict monitoring of all measures implemented by governments worldwide, with sufficient incentives from all individuals to follow them closely, then the epidemics will be largely contained and the effective reproductive number can reach 1.067. During the second phase of the pandem","95":"We propose a deep learning-based framework for detecting COVID-19 positive subjects from their cough sounds. In particular, the proposed framework comprises two main components. The first component identifies features directly extracted from the audio signals by applying a set of pre-trained auditory acoustic models and combining these with handcrafted features, referred to as the front-end feature extraction. The second component classifies the obtained features into those of COVID-19 positive subjects or controls, using a multi-class triplet thresholding method. Subject-independent analysis on the test sets indicates that the average accuracy achieved by the proposed architecture over the entire dataset is 91.4%, leading to a significant improvement over state-of-the-art techniques. Furthermore, we performed a thorough interpretability study, showing that the proposed framework can be used to identify meaningful features in the cough sounds. Our code has been made publicly available at https:\/\/github.com\/bachtranxuan\/covid-detection-framework. This will facilitate further research in acoustics-based detection of COVID-19. Additionally, our website contains a number of interactive tools which are useful for exploring the datasets and improving the performance of the model. We also provide details about the current training procedure, including","96":"According to the World Health Organization, development of the COVID-19 vaccine is occurring in record time. Administration of the vaccine has started the same year as the declaration of the COVID-19 pandemic. The United Nations emphasized the importance of providing COVID-19 vaccines as\"a global public good\", which is accessible and affordable world-wide. Pricing the COVID-19 vaccines is a controversial topic. We use optimization and game theoretic approaches to model the COVID-19 U.S. vaccine market as a duopoly with two manufacturers Pfizer-BioNTech and Moderna. The results suggest that even in the context of very high production and distribution costs, the government can negotiate prices with the manufacturers to keep public sector prices as low as possible while meeting demand and ensuring each manufacturer earns a target profit. Furthermore, these prices are consistent with those currently predicted in the media. Finally, we also find that the proposed models predict the appropriate premium price for 2018-2019, despite the uncertainties on when the vaccination will be made available. Our methodological approach can help policymakers design the COVID-19 vaccine market to benefit from the resulting supply increase without significantly impacting public health. Due to its simplicity, our framework can also be used to simulate other scenarios involving different","97":"Visual explanation methods have an important role in the prognosis of the patients where the annotated data is limited or unavailable. There have been several attempts to use gradient-based attribution methods to localize pathology from medical scans without using segmentation labels. This research direction has been impeded by the lack of robustness and reliability. These methods are highly sensitive to the network parameters. In this study, we introduce a robust visual explanation method to address this problem for medical applications. We provide an innovative visual explanation algorithm for general purpose and as an example application, we demonstrate its effectiveness for quantifying lesions in the lungs caused by the Covid-19 with high accuracy and robustness without using dense segmentation labels. This approach overcomes the drawbacks of commonly used Grad-CAM and its extended versions. The premise behind our proposed strategy is that the information flow is minimized while ensuring the classifier prediction stays similar. Our findings indicate that the bottleneck condition provides a more stable severity estimation than the similar attribution methods. This suggests that the need for using differentiable loss functions is not necessary for achieving optimal performance. Moreover, we find the most relevant feature at the topography of the probability map. Hence, we suggest using Layer Wise Relevance Propagation (LRP) to generate","98":"Pain-staking colour edits for digital images have been limited to only 2D image regions. We present a novel method to enrich existing 2D paintings with color information, which cannot be expressed by a simple 2D displacement map. To this end, we propose a deep learning based 3D stack to edit colours of a source image and a destination image via a convolutional neural network (CNN). Our work enables extensive exploration of possible applications for selfie-based pain-staking colour edits. First, we demonstrate how our method outperforms other state-of-the-art methods on both 2D and 3D tasks for simulated tempering and colour enhancement. Next, we show an application in which our technique provides vivid and colourful new designs alongside a competitive edge effect. Lastly, we also discuss potential challenges of our work and provide recommendations for future research. All code used, models built, and results are available online. https:\/\/github.com\/mahfuzibnalam\/Paste-to-Edit-Colour-Images.git) In addition, we will release all contributions made to the community at https:\/\/github.com\/jannisborn\/coloured_pixes.git) as well as a standalone website at https:\/\/www.","99":"In recent years, fatal traffic accidents have become a major safety concern. However, there are many similarities between the patterns of human trafficking and road accidents, including some secondary attacks motivated by malicious orders, such as taking control of pedestrians or using a moving vehicle to share information with other agents. Therefore, it is difficult to create a direct classification mechanism to detect these similar cases. In this paper, we propose a novel framework for modeling complex pattern data sets containing non-stationary and temporal dependencies. Specifically, we use the self-attention from Vision Transformer (ViT) network to capture both short term memory and long term dependencies for the time series while enjoying computational efficiency along with accuracy improvements. To demonstrate our method effectiveness on a public dataset, we report state of the art results in both survival prediction and collision avoidance. Our approach can also be easily extended to simulate irregularly spaced time intervals, which may further help mitigate disaster risk via an optimal timing of reinsurance. The code has been made publicly available at https:\/\/github.com\/anyleopeace\/ViT-based-framework. Additionally, we provide a simplified simulated example demonstrating the advantages of our model compared to some existing baselines. Ablation studies further illustrate the contributions of each of the components to the","100":"Visualizations are used to communicate complex relationships between data and make sense of them. A common example in visualization research, shown in the paper, is when a model is trained on a dataset, generating a representation that serves as a one-dimensional (1D) input for the subsequent decision output. However, this one-dimensional presentation does not fully represent the inherent complexity of the relationship being analyzed. To address this challenge, we propose a dual-domain deep learning approach, called Visualization2XR, where pairs of identical neural networks are trained along with their corresponding label information to generate more informative representations and better understand the relationships involved in visualizing the data. Furthermore, these pre-trained embeddings are further fine-tuned into a joint framework through a distance metric to mitigate the domain shift problem while preserving the inter-class distribution of the existing examples. Our extensive experiments demonstrate that our proposed approach consistently outperforms the state-of-the-art methods on various evaluation metrics, especially in the cross-category scenario. We also show that our approach improves the interpretability of the learned embeddings, providing valuable insights into the source of its performance. The source code will be made publicly available at https:\/\/github.com\/xingyu1826\/Visual","101":"The successful application of machine learning (ML) methods becomes increasingly dependent on their interpretability or explainability. Designing explainable ML systems is instrumental to ensuring transparency of automated decision-making that targets humans. The explainability of ML methods is also an essential ingredient for trustworthy artificial intelligence. A key challenge in ensuring explainability is its dependence on the specific human user (\"explainee\"). The users of machine learning methods might have vastly different background knowledge about machine learning principles. One user might have a university degree in machine learning or related fields, while another user might have never received formal training in high-school mathematics. This paper applies information-theoretic concepts to develop a novel measure for the subjective explainability of the predictions delivered by a ML method. We construct this measure via the conditional entropy of predictions, given a user signal. This user signal might be obtained from user surveys or biophysical measurements. Our main contribution is the explainable empirical risk minimization (EERM) principle of learning a hypothesis that optimally balances between the subjective explainability and risk. The EERM principle is flexible and can be combined with arbitrary machine learning models. We present several practical implementations of EERM for linear models and decision trees. Numerical experiments demonstrate the application of","102":"Natural ventilation is gaining popularity in response to an increasing demand for a sustainable and healthy built environment, but the design of a naturally ventilated building can be challenging due to the inherent variability in the operating conditions that determine the natural ventilation flow. Large-eddy simulations (LES) have significant potential as an analysis method for natural ventilation flow, since they can provide an accurate prediction of turbulent flow at any location in the computational domain. However, the simulations can be computationally expensive, and few validation and sensitivity studies have been reported. The objectives of this study are twofold: 1) to investigate the sensitivity of LES methods to the grid resolution and the inflow boundary condition; 2) to quantify the rate of variation of the ventilation rate with respect to the grid resolution and the inflow characteristics. Standard Reynolds-average and similarity methods are used, and combinations of circular and radial wall jets are considered. The results show that the combination of RL-based control and LES improves the accuracy of the predictions of laminar thermal convection compared to standard Eulerian LES and RANS LES. For low-resolution meshes, the difference between the GRW and the Eulerian LES is minimal, and for high-resolution meshes, the improvement is noticeable. In","103":"Augmented reality (AR) is one of the relatively old, yet trending fields in the intersection of computer vision and computer graphics with numerous applications in several domains, from gaming and entertainment to education and healthcare. Although it has been around for nearly fifty years, there have been many major developments on the topic since its emergence. In this work, we attempt to give a comprehensive survey of modern augmented reality by covering both application-level and technical perspective aspects. We first present an overview of main development trends, and then elaborate on the recent advances and future research directions. We also discuss some of the current challenges as well as opportunities in the area. Finally, we provide a critical discussion on how AR technologies will be used in various emerging applications, and highlight the potential risks involved in such deployments. Our hope is twofold: (1) Future researchers will develop more effective and innovative ways to enrich the existing models and datasets in the area, and (2) Accelerators will accelerate the progress in the field by bringing new innovations and ideas into the marketplace. Further, we argue that although current multi-modal AR techniques can achieve remarkable results on specific tasks, there is still room for improvement in terms of recognition generalization over multimodal methods, which are currently underdevelopmented.","104":"The field of artificial intelligence (AI), regarded as one of the most enigmatic areas of science, has witnessed exponential growth in the past decade including a remarkably wide array of applications, having already impacted our everyday lives. Advances in computing power and the design of sophisticated AI algorithms have enabled computers to outperform humans in a variety of tasks, especially in the area of computer vision and speech recognition. Yet, AI's path has never been smooth, having essentially fallen apart twice in its lifetime ('winters' of AI), both after periods of popular success ('summers' of AI). We provide a brief rundown of AI's evolution over the course of decades, highlighting its crucial moments and major turning points from inception to the present. In doing so, we attempt to learn, anticipate the future, and discuss what steps may be taken to prevent another 'winter'. If successful, this strategy may then serve as a useful framework for further research into AI's destiny by considering potential catastrophic risks associated with deep learning-based solutions. Finally, we offer some ideas on how to improve AI systems' robustness and resolve significant challenges still facing it. Our main contribution is a methodology for designing AI systems with a high degree of resilience, which will allow for them to survive and remain viable even when","105":"An important goal in the field of human-AI interaction is to help users more appropriately trust AI systems' decisions. A situation in which the user may particularly benefit from more appropriate trust is when the AI receives anomalous input or provides anomalous output. To the best of our knowledge, this is the first work towards understanding how anomaly alerts may contribute to appropriate trust of AI. In a formative mixed-methods study with 4 radiologists and 4 other physicians, we explore how AI alerts for anomalous input, very high and low confidence, and anomalous saliency-map explanations affect users' experience with mockups of an AI clinical decision support system (CDSS) for evaluating chest x-rays for pneumonia. We find evidence suggesting that the four anomaly alerts are desired by non-radiologist participants, and the high-confidence alerts are desired by both radiologists and non-radiologists. In a follow-up user study, we investigate how high- and low-confidence alerts affect the accuracy and thus appropriate trust of 33 radiologists working with AI CDSS mockups. We observe that these alerts do not improve users' accuracy or experience and discuss potential reasons why. Finally, we offer suggestions on how to further enhance proper trust of AI tools","106":"To draw real-world evidence about the comparative effectiveness of multiple time-varying treatment regimens on patient survival, we develop a joint marginal structural proportional hazards model and novel weighting schemes in continuous time to account for time-varying confounding and censoring. Our methods formulate complex longitudinal treatments with multiple\"start\/stop\"switches as the recurrent events with discontinuous intervals of treatment eligibility. We derive the weights in continuous time to handle a complex longitudinal dataset on its own terms, without the need to discretize or artificially align the measurement times. We further propose using machine learning models designed for censored survival data with time-varying covariates and the kernel function estimator of the baseline intensity to efficiently estimate the continuous-time weights. Our simulations demonstrate that the proposed methods provide better bias reduction and nominal coverage probability when analyzing observational longitudinal survival data with irregularly spaced time intervals, compared to conventional methods that require aligned measurement time points. We apply the proposed methods to a large-scale COVID-19 dataset to estimate the causal effects of several COVID-19 treatment strategies on in-hospital mortality or ICU admission, and provide new insights relative to findings from randomized trials. This work demonstrates the utility of our proposal methodologies in discrete-time settings","107":"The solution to the equation of motion in classical statistical physics has been obtained, for the first time by Ludvig Faddeev (1934), against the second smallest eigenvalue of the Laplace operator $\\det(Q)$, where $Q$ is a primitive positive definite quadratic form in m variables with integer coefficients. In this paper we show how the problem can be solved also under generalized pseudo-Hermiticity or anti-pseudo-Hermiticity, i.e., when one should expect that $Q$ will not be singular at any non-zero measurement rate. We explain how this property arises from the structure of the determinants of the metric tensor through the Einstein-Gelbukh-Witten superpotential. These are given explicitly in terms of elementary parameters, including the rank, mass, and coupling constant. The physical model determined by these equations is then compared with the mathematical model based on the corresponding finite-difference operator. It is found that there is a fundamental difference between the two models, as shown by the behavior of the solutions to the generalized pseudospectral collocation scheme. Finally, it is also argued that the concept of quasi-decomposable subordinators introduced in Gardini","108":"As of September 2020, the COVID-19 pandemic continues to devastate the health and well-being of the global population. With more than 33 million confirmed cases and over a million deaths worldwide, global health organizations are still a long way from fully containing the pandemic. This pandemic has raised serious questions about the emergency preparedness of health agencies, not only in terms of treatment of an unseen disease, but also in identifying its early symptoms. In the particular case of COVID-19, several studies have indicated that chest radiography images of the infected patients show characteristic abnormalities. However, at the onset of a given pandemic, such as COVID-19, there may be few data available for the affected cases, due to patient privacy concerns. Hence, supervised classification models are ill-posed for this problem because the time spent in collecting large amounts of infected peoples' data could lead to the loss of human lives and delays in preventive interventions. Therefore, we formulate this problem within a one-class classifier framework, in which the data for healthy patients is abundantly available, whereas no training data is present for the class of interest (COVID-19 in our case). To solve this problem, we present COVIDomaly, a convolutional autoen","109":"The paper focuses on the development of methods for rheological testing of viscoelastic fluids, the droplets of which, when stretched, form thinning filaments, i.e., exhibit the property of spinning. The typical example of such fluid is oral fluid (or mixed saliva). Among other things, the rheological features of the oral fluid actively affect the mechanisms of transmission of infection by airborne droplets. In this work, the oral fluid was studied both from the point of view of a participant in the transmission of infection and as a model for studying the rheology of viscoelastic fluids. The method is based on video recording of the stretching of a drop of the test liquid between the legs of the tweezers and the subsequent spontaneous thinning of the formed capillary filament of the liquid. The process is controlled by the competition of forces of inertia, elasticity, capillarity. By analyzing the video recording, it is possible to trace the contribution of each factor and, within the framework of the Oldroyd\/Maxwell rheological constitutive equation (rheological model), determine the numerical values of all model constants. The obtained rheological characteristics of the oral fluid make it possible to theoretically model the processes of the","110":"Various tools and practices have been developed to support practitioners in identifying, assessing, and mitigating fairness-related harms caused by AI systems. However, prior research has highlighted gaps between the intended design of these tools and practices and their use within particular contexts, including gaps caused by the role that organizational factors play in shaping fairness work. In this paper, we investigate these gaps for one such practice: disaggregated evaluations of AI systems, aimed at uncovering performance disparities between demographic groups. By conducting semi-structured interviews and structured workshops with thirty-three AI practitioners from ten teams at three technology companies, we identify practitioners' processes, challenges, and needs for guidance when designing disaggregated evaluations. We find that practitioners face challenges when choosing performance metrics, identifying the most relevant direct stakeholders and demographic groups on which to focus, and collecting datasets with which to conduct disaggregated evaluations. More generally, we identify impacts on fairness work stemming from a lack of engagement with direct stakeholders or domain experts, business imperatives that prioritize customers over marginalized groups, and the drive to deploy AI systems at scale. Finally, we propose changes to improve how these practitioners coped with direct stakeholders during model development so that they can meaningfully engage with them when choosing performance measures. Our hope is to help researchers","111":"In this early draft, we present an end-to-end decentralized protocol for the secure and privacy preserving workflow of vaccination, vaccination status verification, and adverse reactions or symptoms reporting. The proposed system improves the efficiency, privacy, equity, and effectiveness of the existing manual system while remaining interoperable with its capabilities. We also discuss various security concerns and alternate methodologies based on the proposed protocols. An example architecture integrates RDF2CovidVac, VAriety MIMIC-III databases, and SMM4HMMR to ensure confidentiality and integrity of the data holder across different regions of the world. Our main contributions are as follows: (i) we propose a novel system design; (ii) we improve the efficiency, privacy, equity, and effectiveness of the existing manual system by combining common security elements from multiple domains (e.g., MIMIC-III); (iii) we implement two new techniques, namely, DUal-view clustering and autonomous snapshot scheduling to remove sensitive information from the global image of the data; and (iv) we provide a proof-of-concept implementation of the proposed system using the real-world patient data collected in India through a publicly available platform, i.e., https:\/\/github.com\/","112":"While most work on evaluating machine learning (ML) models focuses on computing accuracy on batches of data, tracking accuracy alone in a streaming setting (i.e., unbounded, timestamp-ordered datasets) fails to appropriately identify when models are performing unexpectedly. In this position paper, we discuss how the nature of streaming ML problems introduces new real-world challenges (e.g., delayed arrival of labels) and recommend additional metrics to assess streaming ML performance. We also review existing tools for monitoring streaming ML performance and propose new metrics for assessing streaming ML systems. Finally, we present several concrete scenarios where timely labeling of streaming records is crucial to improve model performance and describe new opportunities for further research. All code is available in our GitHub repository https:\/\/github.com\/annahaensch\/metric-based-streaming-ml. This report will be updated continuously by adding more examples, comments, and refines the theory behind these metric definitions and illustrate their practical applicability with specific examples of ML pipelines. The approach is also described as being applicable to other domains including healthcare. Metrics based on kernel fusion methods appear to have good coverage but require careful study of the details concerning noisy implementation, especially for streaming functions. Additionally, we argue that current state-of-the-","113":"We present a study of the Corona Borealis (CB) supercluster. We determined the high-density cores of the CB and the richest galaxy clusters in them, and studied their dynamical state and galaxy content. We determined filaments in the supercluster to analyse the connectivity of clusters. We compared the mass distribution in the CB with predictions from the spherical collapse model and analysed the acceleration field in each direction of the CB. We found that at a radius $R_{\\mathrm{30}}$ around clusters in the CB (A2065, A2061, A2089, and Gr2064) (corresponding to the density contrast $\\Delta \\rho \\approx 30$), the galaxy distribution shows a minimum. The $R_{30}$ values for individual clusters lie in the range of $3 - 6$ $h^{-1}$ Mpc. The radii of the clusters (splashback radii) lie in the range of $R_{\\mathrm{cl}} \\approx 2 - 3$ $R_{\\mathrm{vir}}$. The projected phase space diagrams and the comparison with the spherical collapse model suggest that $R_{\\mathrm{30}}$ regions have passed","114":"Justifying draconian measures during the Covid-19 pandemic was difficult not only because of the restriction of individual rights, but also because of its economic impact. The objective of this work is to present a machine learning approach to identify regions that should implement similar health policies. For that end, we successfully developed a system that gives a notion of economic impact given the prediction of new incidental cases through unsupervised learning and time series forecasting. This system was built taking into account computational restrictions and low maintenance requirements in order to improve the system's resilience. Finally this system was deployed as part of a web application for simulation and data analysis of COVID-19, in Colombia, available at (https:\/\/covid19.dis.eafit.edu.co). Keywords: Regime change, covid-19, machine learning, decision support, research methodology, coronavirus, information technology, markets, retail trade, tourism and hospitality sector, bibliometric methods, visualization platform, policy assessment, healthcare structure, organization, strategy, customer service level, airport, construction site, power plants, production facilities, warehouses, and transport terminals. Results: In general the proposed methodology yields accurate results if compared to traditional approaches, which are often based on static and global","115":"The occurrence of concentration and temperature gradients in saline microdroplets evaporating directly in air makes them unsuitable for nucleation studies where homogeneous composition is required. This can be addressed by immersing the droplet in oil under regulated humidity and reducing the volume to the picoliter range. However, the evaporation dynamics of such a system is not well understood. In this work, we present evaporation models applicable for arrays of sessile microdroplets with dissolved solute submerged in a thin layer of oil. Our model accounts for the variable diffusion distance due to the presence of the oil film separating the droplet and air, the diffusive interaction of neighboring droplets, as well as the variation of the solution density and water activity due to the evolving solute concentration. Our model shows excellent agreement with experimental data for both pure water and NaCl solution. With this model, we demonstrate that assuming a constant evaporation rate and neglecting the diffusive interactions can lead to severe inaccuracies in the measurement of droplet concentration particularly during nucleation experiments. Given the significance of droplet evaporation in a wide array of scientific and industrial applications, the models and insights presented herein would be of great value to many fields of interest. The modeling","116":"Access-Control Lists (ACLs) are one of the most important privacy features of Online Social Networks (OSNs). The inclusion of ACLs in user's set of contacts increases the possibility of virus spread, as well as giving rise to new cyber attacks on OSNs. Therefore, it is essential for users to take advantage of different available protocols for access-control purposes. This paper focuses on the study of their suitability for contact tracing tasks through analyzing the privacy and performance of the respective approaches. For this purpose, we developed a protocol for assessing their fine-grained characteristics with a theoretical analysis of the underlying cryptographic protocols. We also performed a comprehensive empirical evaluation of the considered protocols over real-world datasets. Our results show that they can be used as a general solution for any given public transport setting involving sequential multiple hops across a network. Finally, we provide a detailed comparison between the worst-case traffic generated by a state-of-the-art malware attack and the level of privacy and security offered by each of the proposed protocols. Overall, we observe that there is no perfect answer to the issue of selecting among the provided protocols as they must be adapted to the needs of practical applications. However, we also note that several of the tested protocols may suffer","117":"Data exchange between different organizations has been increasing rapidly in recent years. This exchange may be structured to facilitate information sharing but can also favour private and secure exchanges of information. Two scenarios are frequently observed in real-world data integration studies: (i) the COVID-19 pandemic, where causal relationships have to be established by connecting multiple datasets together for accurate analytics; and (ii) privacy-preserving machine learning on heterogeneous data. In this paper we propose a new approach -- called PK-clustering -- to cluster similar sequences while preserving pairwise non-linear correlations between them. We consider a joint setting from the global health domain to the local healthcare domain. Our approach exploits the well-known clustering algorithms for semi-structured data that capture meaningful patterns in the data space with a minimum number of clusters. Extensive experiments on simulated datasets and four real-world datasets show that our approach outperforms several existing state-of-the-art methods. Furthermore, it typically achieves better performance than the previous work in almost all cases. Finally, we demonstrate the utility of our approach using two case studies from the medical field. The first example uses early-stage clinical trials and a cohort study involving patients with diabetes, and the second example shows how clustering","118":"Deepfake is content or material that is generated or manipulated using AI methods, to pass off as real. There are four different deepfake types: audio, video, image and text. In this research we focus on audio deepfakes and how people perceive it. There are several audio deepfake generation frameworks, but we chose MelGAN which is a non-autoregressive and fast audio deepfake generating framework, requiring fewer parameters. This study tries to assess audio deepfake perceptions among college students from different majors. This study also answers the question of how their background and major can affect their perception towards AI generated deepfakes. We also analyzed the results based on different aspects of: grade level, complexity of the grammar used in the audio clips, length of the audio clips, those who knew the term deepfakes and those who did not, as well as the political angle. It is interesting that the results show when an audio clip has a positive polarity (positively charged), it can affect what kind of music listeners think about whether it is real or fake, even if the content is fairly similar. This study also explores the question of how background and major can affect perception towards deepfakes. The findings provide more evidence about how to make audio deepfakes","119":"The COVID-19 pandemic has been hugely shaped by how governments and private companies deal with it. The shape of this has created new challenges for academics and researchers to properly handle research outputs, as well as a need for businesses and other stakeholders to understand what is going on in terms of their cyber security practices. In this article we discuss the various threats that have arisen because of delays in reporting, mainly at the expense of privacy and\/or ethics. We also review some of the current regulations and standards developed in the field, such as GDPR and CCPR, which have helped address these issues. However, there seems to be confusion regarding both the existence and the type of threat posed by the numerous actors involved in any particular circumstance. Therefore, we start from the perspective of the researcher or citizen, and not from the perspective of the business or finance player. We finish by referring to the many ethical considerations that should guide future research into cybersecurity. These considerations include fundamental rights, such as the right to access your own data, freedom from discrimination, and freedom of expression. We hope that this will help mitigate the enormous risks faced by our society during this pandemic, and drive the development of the field in an ever-changing reality. Researchers, professors, and citizens alike","120":"We present the design and implementation of the EOSCARS multi-layer architecture for solar feed superconducting RF accelerator systems. The EOSCARS system is comprised of three different parts: the raw power and signal distribution part, the cooling structure, and the interconnector stack. It is designed to run at up to 1.8 Gsamples\/sec with either no or very low latency requirements. We present the whole system model, including the new design choices such as digital twins and virtual machines. Our motivation for building this complex system was to have enough room for two physical layers to cool down rapidly during construction phase, and so enable the use of cold fusion technology based on UVC (Unmanned Aerial Vehicles) assisted by RGBD cameras in order to monitor the progress of the work area. The performance of the EOSCARS system has been evaluated through computer simulations using the NASA Task Load Index (TLX), and results show that it can achieve better than 200% of the TLX rating achieved by competing teams. In addition, we are able to reduce the simulation time significantly while reaching the same level of accuracy compared to state of the art systems. The entire system can be constructed in less than 6 months if all other factors are accounted for.","121":"The presence of people in an urban area throughout the day -- often called 'urban vitality' -- is one of the qualities world-class cities aspire to the most, yet it is one of the hardest to achieve. Back in the 1970s, Jane Jacobs theorized urban vitality and found that there are four conditions required for the promotion of life in cities: diversity of land use, small block sizes, the mix of economic activities, and concentration of people. To build proxies for those four conditions and ultimately test Jane Jacobs's theory at scale, researchers have had to collect both private and public data from a variety of sources, and that took decades. Here we propose the use of one single source of data, which happens to be publicly available: Sentinel-2 satellite imagery. In particular, since the first two conditions (diversity of land use and small block sizes) are visible to the naked eye from satellite imagery, we tested whether we could automatically extract them with a state-of-the-art deep-learning framework and whether, in the end, the extracted features could predict vitality. In six Italian cities for which we had call data records, we found that our framework is able to explain on average 55% of the variance in urban vitality extracted from those records. When","122":"Conversational commerce, first pioneered by Apple's Siri, is the first of may applications based on always-on artificial intelligence systems that decide on its own when to interact with the environment, potentially collecting 24x7 longitudinal training data that is often Personally Identifiable Information (PII). A large body of scholarly papers, on the order of a million according to a simple Google Scholar search, suggests that the treatment of many health conditions, including COVID-19 and dementia, can be vastly improved by this data if the dataset is large enough as it has happened in other domains (e.g. GPT3). In contrast, current dominant systems are closed garden solutions without wake neutrality and that can't fully exploit the PII data they have because of IRB and Cohues-type constraints. We present a voice browser-and-server architecture that aims to address these two limitations by offering wake neutrality and the possibility to handle PII aiming to maximize its value. We have implemented this browser for the collection of speech samples and have successfully demonstrated it can capture over 200.000 samples of COVID-19 coughs. The architecture we propose is designed so it can grow beyond our kind into other domains such as collecting sound samples from vehicles, video images from nature, ingest","123":"The human severe acute respiratory syndrome coronavirus 2 (SARS-Cov-2), causing the COVID-19 disease, has continued to spread all over the world. It menacingly affects not only public health and global economics but also mental health and mood. While the impact of the COVID-19 pandemic has been widely studied, relatively fewer discussions about the sentimental reaction of the population have been available. In this article, we scrape COVID-19 related tweets on the microblogging platform, Twitter, and examine the tweets from Feb~24, 2020 to Oct~14, 2020 in four Canadian cities (Toronto, Montreal, Vancouver, and Calgary) and four U.S. cities (New York, Los Angeles, Chicago, and Seattle). Applying the Vader and NRC approaches, we evaluate the sentiment intensity scores and visualize the information over different periods of the pandemic. Sentiment scores for the tweets concerning three anti-epidemic measures, masks, vaccine, and lockdown, are computed for comparisons. The results of four Canadian cities are compared with four cities in the United States. We study the causal relationships between the infected cases, the tweet activities, and the sentiment scores of COVID-19 related tweets, by integrating the echo state","124":"With the emergence and rapid proliferation of social media platforms and social networking sites, recent years have witnessed a surge of misinformation spreading in our daily life. Drawing on a large-scale dataset which covers more than 1.4M posts and 18M comments, we investigate the propagation of two distinct narratives--(i) conspiracy information, whose claims are generally unsubstantiated and thus referred as misinformation to some extent, and (ii) scientific information, whose origins are generally readily identifiable and verifiable--in an online social media platform. We find that conspiracy cascades tend to propagate in a multigenerational branching process while science cascades are more likely to grow in a breadth-first manner. Specifically, conspiracy information triggers larger cascades, involves more users and generations, persists longer, is more viral and bursty than science information. Content analysis reveals that conspiracy cascades contain more negative words and emotional words which convey anger, fear, disgust, surprise and trust. We also find that conspiracy cascades are more concerned with political and controversial topics. After applying machine learning models, we achieve an AUC score of nearly 90% in discriminating conspiracy from science narratives using the constructed features. We also find that conspiracy cascades are more likely to be controlled by a broader set of users than","125":"This paper presents our system for the AAAI 2021 shared task of COVID-19 Fake News Detection in English, where we achieved the 3rd position with the weighted F1 score of 0.9859 on the test set. Specifically, we proposed a novel multimodal ensemble model which combines two monolingual and one multilingual model to detect fake news in social media data. We also compared our results with those obtained from three other popular datasets including Twitter, Facebook, and Google Search. The code used is available at https:\/\/github.com\/caio-davi\/3rd-solution-COVID19-Fake-News-Detection-in-English. For all the datasets, we achieve competitive performance as measured by the macro-F1 score on the test sets. However, we obtain very good performance (with the highest micro-F1 score) from the limited number of bilingual model updates when increasing the number of users or posts per day. On the other hand, we observe that incremental training on the larger corpus will improve the performance of the model. Thus, it is recommended to update the parameters concerning the small vocabulary size and the fraction of non-bilingual users etc. In addition, we suggest a policy for detecting fake news","126":"The recent outbreak of COVID-19 has taken the world by surprise, forcing lockdowns and straining public health care systems. COVID-19 is known as one of the most infectious diseases, and infected individuals do not initially exhibit symptoms, while some remain asymptomatic. Thus, a non-negligible fraction of the population can, at any given time, be a hidden source of transmissions. In response, many governments have shown great interest in smartphone contact tracing apps that help automate the difficult task of tracing all recent contacts of newly identified infected individuals. However, tracing apps has generated much discussion around their key attributes, including system architecture, data management, privacy, security, proximity estimation, and attack vulnerability. In this article, we provide the first comprehensive review of these much-discussed tracing app attributes. We also present an overview of many proposed tracing app examples, some of which have been deployed countrywide, and discuss the concerns users have reported regarding their usage. We close by outlining potential research directions for next-generation app design, which would facilitate improved tracing and security performance, as well as wide adoption from diverse communities. Finally, we highlight challenges and open issues that need to be addressed in order to better prepare for future work in this area. All","127":"The COVID-19 pandemic has resulted in a slew of misinformation, often described as an\"infodemic\". Whereas previous research has focused on the propagation of unreliable sources as a main vehicle of misinformation, the present study focuses on exploring the role of scientists whose views oppose the scientific consensus. Using Nobelists in Physiology and Medicine as a proxy for scientific consensus, we analyze two separate datasets: 15.8K tweets by 13.1k unique users on COVID-19 vaccines specifically, and 208K tweets by 151K unique users on COVID-19 broadly which mention the Nobelist names. Our analyses reveal that dissenting scientists are amplified by a factor of 426 relative to true scientific consensus in the context of COVID-19 vaccines, and by a factor of 43 in the context of COVID-19 generally. Although more popular accounts tend to mention consensus-abiding scientists more, our results suggest that this false consensus is driven by higher engagement with dissent-mentioning tweets. Furthermore, false consensus mostly occurs due to traffic spikes following highly popularized statements of dissenting scientists. We find that dissenting voices are mainly discussed in French, English-speaking, Turkish, Brazilian, Argentine, Indian, and Japanese misinformation clusters. This research suggests that social media platforms should prioritize","128":"The new Coronavirus (Covid-19) has been emerged as a very serious global health issue since it was defined on March 11, 2020 by the World Health Organization (WHO). It consists of an ongoing pandemic caused by the SARS-CoV-2 virus with more than 294,000 confirmed cases and over 1 million deaths worldwide. In order to reduce the burden on national healthcare systems and to mitigate the effects of this epidemic, accurate diagnosis for COVID-19 is crucial in reducing the number of infected patients. The main challenge of diagnosing pneumonia using chest X-rays is the shortage and reliability of testing kits, due to the quick spread of the virus, medical practitioners are facing difficulty identifying the positive case. Thus, there is a requirement for an automatic diagnostic system which can be used by medical practitioners to save time and increase the accuracy of diagnosing pneumonia. Chest X-ray image analysis has several advantages over other imaging techniques as it is cheap, easily accessible, fast and portable. This paper explores the effect of various popular image enhancement techniques and states the effect of each of them on the detection performance. We have compiled the largest X-ray dataset called CovidX, consisting of 2951 X-ray images coming from patients affected by","129":"The ongoing COVID-19 pandemic has been partially lifted by social distancing measures to limit the spread of the disease, and city lockdown strategies to reduce its impact. Our study aims to understand the implications of these measures on urban mobility patterns, and further find out their potential counterfactual impacts. We scrape daily smart card travel records for all users across six large metropolitan areas in India (New Delhi, Mumbai, Milan, Madrid, New York, Melbourne and Sydney) between March 2020 and July 2021. We treat the groups as separate but interconnected communities for the periods of April and May 2020, and calibrate a SIR model individually on each community's reported cases and deaths. Using the estimated parameters, we perform a causal inference using randomization method in order to assess the relative importance of features related to socio-economic disadvantage, and the effectiveness of social distancing measures. We discover several important results, such as a significant increase in transit use during the lockdown period, and evidence that social distancing measures are more effective in densely populated regions. Additionally, we find that transit networks become less vulnerable when fractioned into smaller communities. These findings have important policy implications -- the main objective of this study is to establish academics and policymakers' awareness regarding the limitations of current social distancing","130":"The TopDown Algorithm (TDA) first produces differentially private counts at the nation and then produces counts at lower geolevels (e.g.: state, county, etc.) subject to the constraint that all query answers in lower geolevels are consistent with those at previously estimated geolevels. This paper describes the three sets of definitions of these geolevels, or the geographic spines, that are implemented within TDA. These include the standard Census geographic spine and two other spines that improve accuracy in geographic areas that are far from the standard Census spine, such as cities, towns, and\/or AIAN areas. The third such spine, which is called the optimized spine, also modifies the privacy-loss budget allocated to the entities within the geolevels, or the geounits, to ensure the privacy-loss budget is used efficiently within TDA. We show that the TDA-based spine can yield improved results than the standard spine for certain types of queries, while the latter improves the performance by more than ten% on average across all kinds of queries. Our experimental results demonstrate that the proposed geometry uses significantly less space when compared to the standard spine, especially for long-term geo-spans where","131":"We conduct a comparative analysis of desktop web search behaviour of users from Germany (n=558) and Switzerland (n=563) based on a combination of web tracking and survey data. We find that web search accounts for 13% of all desktop browsing, with the share being higher in Switzerland than in Germany. We also show that in over 50% of cases users clicked on the first search result, with over 97% of all clicks being made on the first page of search outputs. Most users rely on Google when conducting searches, and users preferences for other engines are related to their demographics. We also test relationships between user demographics and daily number of searches, average share of search activities among tracked events by user as well as the tendency to click on higher- or lower-ranked results. We find differences in such relationships between the two countries that highlights the importance of comparative research in this domain. Further, we observe differences in the temporal patterns of web search use between women and men, marking the necessity of disaggregating data by gender in observational studies regarding online information behaviour. Finally, we discuss the implications of our findings via digital marketing strategies and privacy regulations. To make further public policy decisions informed by monitoring people's social media activity, we release our anonymised dataset. This work","132":"We present the collaborative model of ESiWACE2 Services, where Research Software Engineers (RSEs) from the Netherlands eScience Center (NLeSC) and Atos offer their expertise to climate and earth system modeling groups across Europe. Within 6-month collaborative projects, the RSEs intend to provide guidance and advice regarding the performance, portability to new architectures, and scalability of selected applications. We present the four awarded projects as examples of this funding structure. The first two are focused on solar and wind energy production applications, while the third focuses on water use in the future post COVID-19. For each project, we also discuss the role of licenses for renewable energy certificates and automatic compliance verification in process choreographies. We have developed a number of modular testbed platforms with minimal cost to set up, operate, manage, and decompose them for testing the respective hypotheses. Based on the feedback from the European SMEs and the associated interests, we argue that a similar approach to the granted projects should be adopted by other research communities. This paper serves as the description of our collaboration and offers cross-references to the community, including through the URL https:\/\/github.com\/LANL-BioNet\/ESiwace2-model\/ We","133":"We present a simple formula for the Bayesian posterior density of a prevalence parameter based on unreliable testing of a population. This type of inference is useful in settings where the false positive test rate is close to zero or equivalently, when the goal is to infer whether someone is infected with some disease using only a single sample of blood drawn from them. The formula addresses both issues of prior distribution misspecification and provides an efficient computation of the posterior mean value under reasonable assumptions. We illustrate this technique through application to the problem of COVID-19 preliminary screening. The results suggest that the true numbers of infections are probably higher than the reported numbers and we should re-assess the policy recommendations aimed at curbing the spread of the virus. Preliminary screening tests provide a good way to monitor the progression of the disease, potentially guiding the implementation of public health interventions. Although these methods have been proposed independently by different researchers, our collaboration has helped to improve the quality of their work and make it more widely available. To be effective, one must compute the correct priors over all parameters instead of just one particular parametrized version. Our code can help address this issue in a straightforward manner by updating the relevant parameters upon the arrival of each new data set. In addition, the","134":"The COVID-19 pandemic has resulted in a slew of misinformation, often described as an\"infodemic\". Whereas previous research has focused on the propagation of unreliable sources as a main vehicle of misinformation, the present study focuses on exploring the role of scientists whose views oppose the scientific consensus. Using Nobelists in Physiology and Medicine as a proxy for scientific consensus, we analyze two separate datasets: 15.8K tweets by 13.1K unique users from 12.5k tweets; and 208K tweets by 151K unique users from 9.3k tweets. Our analyses reveal that dissenting scientists are amplified by a factor of 426 relative to true scientific consensus in the context of COVID-19, and by a factor of 43 in the context of climate change. Although this result indicates that social media platforms should prioritize the exposure of consensus-abiding scientists as a source of information, it also highlights that our data sets contain some of the fuel used to disseminate misinformation. To aid further research, we release our dataset of 8.4K tweets by 4.2K unique users, along with their 53.6K edge follower network. We also publish our code base, model weights, and data dumps in an effort to allow other researchers to build upon them.","135":"Coronavirus Disease 2019 (COVID-19) has rapidly spread in 2020, emerging a mass of studies for lung infection segmentation from CT images. Though many methods have been proposed for this issue, it is a challenging task because of infections of various size appearing in different lobe zones. To tackle these issues, we propose a Graph-based Pyramid Global Context Reasoning (Graph-PGCR) module, which is capable of modeling long-range dependencies among disjoint infections as well as adapt size variation. We first incorporate graph convolution to exploit long-term contextual information from multiple lobe zones. Different from previous average pooling or maximum object probability, we propose a saliency-aware projection mechanism to pick up infection-related pixels as a set of graph nodes. After graph reasoning, the relation-aware features are reversed back into the original coordinate space for the down-stream tasks. We further construct multiple graphs with different sampling rates to handle the size variation problem. To this end, distinct multi-scale long-range contextual patterns can be captured. Our Graph-PGCR module is plug-and-play, which can be integrated into any architecture to improve its performance. Experiments demonstrated that the proposed method consistently boost the performance of state-of-the","136":"We investigate the effect of social distancing on the spread of COVID-19 in Sweden. We use public data from several counties and state administrative regions for our analysis. These results are based on publicly available, de-identified mobile phone users' visit trajectories to points of interest (POIs) in the Swedish Region of Uppsala. Our model shows that a 1% increase in mobility correlates with a 0.57% decrease in the number of daily visitors at POIs, corresponding to a change in behavior of approximately 10% in the population. The reduction in mobility is highly correlated with the simultaneously observed decline in the rate of positive test incidences. In addition, we find that one of the most affected areas in terms of reduced mobility is the city of Malm\\\"oer. Among other things, we find that the amount of traffic passing through each center and train station increases by 23.5%, which corresponds to a 20% increase in the transmission rate of the virus. Finally, we show that reductions in mobility have a small but significant effect on the reduction in the incidence rates. Together, these results demonstrate that social distancing rules do not significantly affect the overall course of the pandemic.<|endoftext|>","137":"Crowd counting aims to predict the number of people and generate the density map in the image. There are many challenges, including varying head scales, the diversity of crowd distribution across images and cluttered backgrounds. In this paper, we propose a multi-scale context aggregation network (MSCANet) based on single-column encoder-decoder architecture for crowd counting, which consists of an encoder based on a dense context-aware module (DCAM) and a hierarchical attention-guided decoder. To handle the issue of scale variation, we construct the DCAM to aggregate multi-scale contextual information by densely connecting the dilated convolution with varying receptive fields. The proposed DCAM can capture rich contextual information of crowd areas due to its long-range receptive fields and dense scale sampling. Moreover, to suppress the background noise and generate a high-quality density map, we adopt a hierarchical attention-guided mechanism in the decoder. This helps to integrate more useful spatial information from shallow feature maps of the encoder by introducing multiple supervision based on semantic attention module (SAM). Extensive experiments demonstrate that the proposed approach achieves better performance than other similar state-of-the-art methods on three challenging benchmark datasets for crowd counting. The code is available at https:\/\/github","138":"Deep Bayesian neural networks (BNNs) have been shown to be highly effective at parameter estimation and data prediction, especially for irregular dense-label pretrained BNNs. However, these models are not applicable to unseen situations where no training samples are available. In this work, we present a novel framework for few-shot BNN construction with only $3$ labeled classes of images without any labels or limited unlabeled examples. We resolve the problem of class imbalance by combining multiple augmentation strategies with a mean approximation approach. The resulting strategy is applied to both PyTorch and TensorFlow BN datasets for several downstream tasks including classification, regression, and tracking. Our method achieves new state-of-the-art results on all the benchmark datasets except for one of the harder ones and shows competitive performance against previous approaches when there exist domain shifts between the seen and unseen cases. When applying our model to the COVID-19 CT segmentation challenge, we achieve state-of-the-art results, which outperform other baseline methods, as well as the current leading supervised BNN. Finally, we demonstrate how the presented framework can be used to explore rich medical imaging features from a single OCT scan, yielding significant improvements over existing techniques. Code will be","139":"A graph $X$ is defined inductively to be $(a_0,\\dots,a_{n-1})$-regular if $X$ is $a_0$-regular and for every vertex $v$ of $X$, the sphere of radius $1$ around $v$ is an $(a_1,\\dots,a_{n-1})$-regular graph. Such a 1-sparse graph $X$ is said to be highly regular (HR) of level $n$ if $a_{n-1}\\neq 0$. Chapman, Linial and Peled studied HR-graphs of level 2 and provided several methods to construct families of graphs which are expanders\"globally and locally\". They ask whether such HR-graphs of level 3 exist. In this paper we show how the theory of Coxeter groups, and abstract regular polytopes and their generalisations, can lead to such graphs. Given a Coxeter system $(W,S)$ and a subset $M$ of $S$, we construct highly regular quotients of the 1-sparse HR-graph $\\mathcal{R}^{2(W,M)}$-graded Steenrod algebra","140":"We propose and apply a novel paradigm for characterization of genome data quality, which quantifies the effects of intentional degradation of quality. The rationale is that the higher the initial quality, the more fragile the genome and the greater the effects of degradation. We demonstrate that this phenomenon is ubiquitous, and that quantified measures of degradation can be used for multiple purposes. We focus on identifying outliers that may be problematic with respect to data quality, but might also be true anomalies or even attempts to subvert the database. Specifically, we consider genomes that have been proven to be of low quality by testing positive samples repeatedly until finally concluding that all non-subverted DNA sequences are below average quality. We discuss how this framework may be applicable to other problems, such as tracking mutations, and study groups of related strains. Finally, we illustrate our methodology using numerical simulations of genomic ssRNA cleavage reactions. In particular, we find that repeated tests of SSRNAs reveal that a single cleavage event significantly reduces the ability of a genome to resist continued degradation, enabling the analysis of highly damaged genomes. Such results strongly suggest that test procedures should be designed so as to minimize the fraction of degraded genomes, and that it would be a futile effort to attempt to identify every possible variation in RNA quality.","141":"The unprecedented global crisis brought about by the COVID-19 pandemic has sparked numerous efforts to create predictive models for the detection and prognostication of SARS-CoV-2 infections with the goal of helping health systems allocate resources. Machine learning models, in particular, hold promise for their ability to leverage patient clinical information and medical images for prediction. However, most of the published COVID-19 prediction models thus far have little clinical utility due to methodological flaws and lack of appropriate validation. In this paper, we describe our methodology to develop and validate multi-modal models for COVID-19 mortality prediction using multi-center patient data. The models for COVID-19 mortality prediction were developed using retrospective data from Madrid, Spain (N=2547) and were externally validated in patient cohorts from a community hospital in New Jersey, USA (N=242) and an academic center in Seoul, Republic of Korea (N=336). We also performed an explainability study to gain deeper insights into the predictions made by the models. Results show that both machine learning and deep learning approaches offer valuable tools for improving COVID-19 mortality prediction. Conclusion: We have demonstrated the feasibility of developing more accurate computer-aided methods for diagnosis and prognosis of CO","142":"We propose a new method for analyzing stochastic epidemic models under minimal assumptions. The method, dubbed DSA, is based on a simple yet powerful observation, namely that population-level mean-field trajectories described by a system of PDE may also approximate individual-level times of infection and recovery. This idea gives rise to a certain non-Markovian agent-based model and provides an agent-level likelihood function for a random sample of infection and\/or recovery times. Extensive numerical analyses on both synthetic and real epidemic data from the FMD in the United Kingdom and the COVID-19 in India show good accuracy and confirm method's versatility in likelihood-based parameter estimation. The accompanying software package gives prospective users a practical tool for modeling, analyzing and interpreting epidemic data with the help of the DSA approach. An R package for this purpose is available at https:\/\/github.com\/mrc-ide\/DSA.git. We encourage others to check the code and data for their own use cases and to suggest improved methods for future work. Contact matrices obtained via DSA can be found at https:\/\/people.cs.vt.edu\/~onufriev\/CODES\/aascodecoder. In addition, we","143":"Supervised learning (SL) has been widely applied in natural language processing to train grammatical models such as English and Chinese. However, there are few large representative mentarial datasets available for SL in the biomedical domain. It is difficult to apply existing SL methods to generate useful results on new benchmark data sets because most benchmarks have limited size and may contain only a small number of examples. To address this problem, we propose a comprehensive framework for generating exemplar samples by performing back-translation between the input sentence and the corresponding text sentiment using a novel combination of macro-F1 based generation with deep attention layers. In order to produce more informative samples, we further adopt a bi-directional long-short term memory (BiLSTM) network to learn from both the front-end and tail-ends. The proposed method can achieve promising performance on several public benchmarks, which demonstrates its potential on real-world applications. The code used here is released at https:\/\/github.com\/YingtongDou\/Explanatory-Generator-Limited-Data-Set.git. We encourage future research to analyze the code and improve it. Our project page: https:\/\/mcgill-nlp.github.io\/Explanatory-Generator-Limited","144":"We present the direct-imaging discovery of a substellar companion in orbit around a Sun-like star member of the Hyades open cluster. So far, no other substellar companions have been unambiguously confirmed via direct imaging around main-sequence stars in Hyades. The star HIP 21152 is an accelerating star as identified by the astrometry from the Gaia and Hipparcos satellites. We have detected the companion, HIP 21152 B, in multi-epoch using the high-contrast imaging from SCExAO\/CHARIS and Keck\/NIRC2 for both the stellar and planetary signals. We report here on the results of our searches for the origin of the virus, the mass ratio of the host star to its planet, as well as higher-mass transits with implications for all current models of core formation. Mass estimates inferred from luminosity evolution models are slightly overestimated (in tens of M$_{\\odot}$) by factors of $\\approx 6$ compared to archival measurements. A comparison between these two samples shows evidence for a common origin. If true, this result will allow us to address one of the most important questions in studying a complex system - the existence or absence of a stable equilibrium at low","145":"Viral transmission pathways have profound implications for public safety; thus it is imperative to establish an accurate and complete understanding of viable infectious avenues. Whether SARS-CoV-2 is airborn is currently uncertain. While mounting evidence suggests it could be transmitted via the air in confined spaces, the airborn transmission mode has not yet been demonstrated. Here we show that the quantitative analysis of several reported superspreading events points towards aerosol mediated transmission of SARS-CoV-2. Aerosolized virus emitted by an infected person in an enclosed space will accumulate until virion emission and virion destabilization are balanced, resulting in a steady-state concentration $C_{\\infty}$. The timescale to accumulation leads to significantly enhanced exposure when virus-carrying aereosol droplets are inhaled for longer duration co-occupancy. Reported superspreading events are found to trace out a single value of the calculated virion exposure, suggesting a universal minimum infective dose (MID) via aerosol. The MID implied by our analysis is comparable to the measured MIDs for influenza A (H2N2), the virus responsible for the 1957-1958 asian flu pandemic. Our model suggests that the likelihood for aerosol-mediated","146":"We have learned to live with many potentially deadly viruses for which there is no vaccine, no immunity, and no cure. We do not live in constant fear of these viruses, instead, we have learned how to outsmart them and reduce the harm they cause. A new mathematical model that combines the spread of diseases that do not confer immunity together with the evolution of human behaviors indicates that we may be able to fight new diseases with the same type of strategy we use to fight viruses like HIV. Specifically, our model predicts that if a new highly infectious disease such as COVID-19 enters our population, then our system will automatically respond to it by creating awareness about the new infection's transmission process and offering protection against further infections. Interestingly, this dynamic behavior does not rely on the epidemiological curve but relies on the distribution of existing susceptibility\/immunity within different subpopulations. Therefore, our model can correctly predict the observed dynamics of epidemic growth curves based on the distribution of existing susceptibilities\/immunities. In addition, we show that immunization through vaccination provides no significant improvement to the extent of viral control since both SARS-CoV and MERS-CoV share the same receptor (ACE2) and hence, infectivity profile. Finally, we highlight","147":"We report the observation of gravitational waves from two compact binary coalescences in LIGO's and Virgo's third observing run with properties consistent with neutron star-black hole (NSBH) binaries. The two events are named GW200105_162426 and GW200115_042309, abbreviated as GW200105 and GW200115; the first was observed by LIGO Livingston and Virgo, and the second by all three observatories. Triggers from the four new strongly lensed quasars SDSS J1422+2745 and SDSS J0711+6830 were identified through analyses of simulated signals from NSBH binaries with generically spinning components. No additional calibration or any other systematic errors were found in these searches. We estimate the mass ratios of the secondary masses for the current set of GW200105 and GW200115 are $q_{\\rm eff} = 0.009^{+0.010}_{-0.011}$ and $\\frac{1}{1.000}\\leq q\/q_\\odot \\leq 1$, respectively, while the estimated effective equation of state (EoS) parameter for GW200105 is larger than 9.4%","148":"In this paper, we propose a novel method for lung segmentation from CT images of COVID-19 patients. We first coarsely detect 5 tissue structures (including ground glass opacities and consolidations) via 3D morphological image analysis using a Sobel edge detector. This was followed by extraction of their volumetric ratios w.r.t. the lung volume for each image. Then we used the results to train a neural network with a learning rate of 0.03 to automatically extract features from the ratio maps. Finally, we compared the performance of the proposed Sharp U-Net model with several competing methods including VGG16, InceptionResnetV2, DenseNet169, and ResNet50. The experimental results showed that our method achieved the best performance in terms of IoU score, which were significantly higher than the other competing methods. Furthermore, we also found that our model had better interpretability because of its high sensitivity towards the abnormality regions. To further improve the accuracy of the segmentation networks, we suggest making them more robust through an ensemble classification approach. Our code has been made publicly available at https:\/\/github.com\/masadcv\/CODE-ONLY-MIMIC-data-driven-learning-","149":"Physiological computing uses human physiological data as system inputs in real time. It includes, or significantly overlaps with, brain-computer interfaces, affective computing, adaptive automation, health informatics, and physiological signal based biometrics. Physiological computing increases the communication bandwidth from the user to the computer, but is also subject to various types of adversarial attacks, in which the attacker deliberately manipulates the training and\/or test examples to hijack the machine learning algorithm output, leading to possibly user confusion, frustration, injury, or even death. However, the vulnerability of physiological computing systems has not been paid enough attention to, and there does not exist a comprehensive review on adversarial attack to it. This paper fills this gap, by providing a systematic review on the main research areas of physiological computing, different types of adversarial attacks and their applications to physiological computing, and the corresponding defense strategies. We hope this review will attract more research interests on the vulnerability of physiological computing systems, and more importantly, defense strategies to make them more secure. Moreover, we apply this review to discuss the existing vulnerabilities, and future directions for research purposes. All collected topics are summarized in the table below our discussion section. In addition, we propose several preventive measures for future research. Finally,","150":"We study the problem of medical symptoms recognition from patient text, for the purposes of gathering pertinent information from the patient (known as history-taking). We introduce an efficient method to extract structured knowledge from a single patient's text via a hierarchical combination of topic modeling and sentiment analysis methods. Our model aims to jointly learn both the topology of the latent graph structure and the semantics of each token in the learned embedding space. The use of two popular models, GPT and RoBERTa, shows that our approach can achieve better performance than previous attempts on three important clinical datasets. Additional results demonstrate the effectiveness of our proposed framework compared with state-of-the-art methods. A video summary of our work is available at https:\/\/youtu.be\/Hn2uaVqjskY. Source code and data are publicly available at https:\/\/github.com\/AntonotnaWang\/Joint-training-GPT-and-Roberta-for-CLinical-Semi-Conversation. For more details about our framework see https:\/\/antonotnawang.github.io\/rsync. Findings include: 1) the number of patients reported in each category increases significantly over time; 2) the global sentiment score distribution","151":"We propose a simple method to construct self orthogonal matrix, orthogonal matrix and anti orthogonal matrix over the finite field. Orthogonal matrices has numerous applications in cryptography, so here we demonstrate the application of weighted orthogonal matrix into cryptography. Using the proposed method of construction we see that it is very easy to transmit the private key and can easily convert the encrypted message into original message and at the same time it will be difficult to get the key matrix for intruder. We also use 100 real numbers randomly generated tables of bidsets to verify the performance of this algorithm. It turns out that the number of tests required to find the key matrix for intruder need not be too large nor do they have to be dense. In practice it is quite possible to identify all combinations of factors resulting in a relatively small number of tests. As a result the constructed orthogonal matrix can be applied to various kinds of problems involving discrete or continuous variables in the linear algebra framework. The applicability of the developed method is demonstrated by means of numerical simulations. Our results show that the search using the proposed combination of methods can reduce the number of tests by several orders of magnitude. In addition it reduces the complexity of the formed class of solutions by solving some basic questions on fractions","152":"The way diseases spread through schools, epidemics through countries, and viruses through the Internet is crucial in determining their risk. Although each of these threats has its own characteristics, its underlying network determines the spreading. To restrain the spreading, a widely used approach is the fragmentation of these networks through immunization, so that epidemics cannot spread. Here we develop an immunization approach based on optimizing the susceptible size, which outperforms the best known strategy based on immunizing the highest-betweenness links or nodes. We find that the network's vulnerability can be significantly reduced, demonstrating this on three different real networks: the global flight network, a school friendship network, and the internet. In all cases, we find that not only is the average infection probability significantly suppressed, but also for the most relevant case of a small and limited number of immunization units the infection probability can be reduced by up to 55%. Furthermore, we demonstrate that our method can significantly reduce the peak infection rate (PIR) with respect to both magnitude and distribution, while also reducing the outbreak threshold (i.e., the fraction of infected nodes). Finally, we provide insights into how to further improve the performance of our model and apply it to more general cases. Our code is available online at https:\/\/github","153":"To study the effects of online social network (OSN) activity on real-world offline events, researchers need access to OSN data, the reliability of which has particular implications for social network analysis. This relates not only to the completeness of any collected dataset, but also to constructing meaningful and reliable feature sets from them. In this multidisciplinary study, we consider both the completeness and reliability of the collected data in connection with key factors affecting social networks such as engagement levels, namely the number of users and their gender or age distributions. We focus on Twitter data, relating to popular tweets among young female and male individuals, and Reddit data, highlighting differences between groups of users by gender and age ranges. By combining these two datasets, we show that women's online presence is positively correlated with entrepreneurship activity, while men's participation in public discussions shows no clear evidence of gender bias. We conclude by suggesting several promising directions for future work. All collected datasets are available at https:\/\/github.com\/annahaensch\/ODyN.git. Ultimately, this means that there is little chance that studying online social networks alone will reveal useful insights on the dynamics of online discussion on social issues. However, it is encouraging that already curated datasets from well-established social media","154":"The ability to label and track physical objects that are assets in digital representations of the world is foundational to many complex systems. Simple, yet powerful methods such as bar- and QR-codes have been highly successful, e.g. in the retail space, but the lack of security, limited information content and impossibility of seamless integration with the environment have prevented a large-scale linking of physical objects to their digital twins. This paper proposes to link digital assets created through BIM with their physical counterparts using fiducial markers with patterns defined by Cholesteric Spherical Reflectors (CSRs), selective retroreflectors produced using liquid crystal self-assembly. The markers leverage the ability of CSRs to encode information that is easily detected and read with computer vision while remaining practically invisible to the human eye. We analyze the potential of a CSR-based infrastructure from the perspective of BIM, critically reviewing the outstanding challenges in applying this new class of functional materials, and we discuss extended opportunities arising in assisting autonomous mobile robots to reliably navigate human-populated environments, as well as in augmented reality. Our work demonstrates the feasibility of creating a tracking system for physical objects valuable to billions of users worldwide and serves as a blueprint for future research on how to enable intelligent agents to reliably share information","155":"The year 2020 has seen the COVID-19 virus lead to one of the worst global pandemics in history. As a result, governments around the world are faced with the challenge of protecting public health, while keeping the economy running to the greatest extent possible. Epidemiological models provide insight into the spread of these types of diseases and predict the effects of possible intervention policies. However, to date,the even the most data-driven intervention policies rely on heuristics. In this paper, we study how reinforcement learning (RL) can be used to optimize mitigation policies that minimize the economic impact without overwhelming the hospital capacity. Our main contributions are (1) a novel agent-based pandemic simulator which, unlike traditional models, is able to model fine-grained interactions among people at specific locations in a community; and (2) an RL-based methodology for optimizing fine-grained mitigation policies within this simulator. Our results validate both the overall simulator behavior and the learned policies under realistic conditions. Finally, we demonstrate our approachability across multiple demographics and different geographical distributions, including those found in rural and\/or tribal areas. Code and datasets are available at https:\/\/github.com\/anyleopeace\/covid-simulator-tools\/. This dataset","156":"We develop a framework for difference-in-differences designs with staggered treatment adoption and heterogeneous causal effects. We show that conventional regression-based estimators fail to provide unbiased estimates of relevant estimands absent strong restrictions on treatment-effect homogeneity. We then derive the efficient estimator addressing this challenge, which takes an intuitive\"imputation\"form when treatment-effect heterogeneity is unrestricted. We characterize the asymptotic behavior of the estimator, propose tools for inference, and develop tests for identifying assumptions. Extensions include time-varying controls, triple-differences, and certain non-binary treatments. We show the practical relevance of these insights in a simulation study and an application. Studying the consumption response to tax rebates in the United States, we find that the notional marginal propensity to consume is between 8 and 11 percent in the first quarter -- about half as large as benchmark estimates used to calibrate macroeconomic models -- and predominantly occurs in the first month after the rebate. Consistent with expectations, we show that unpaired with this timing constraint, the estimated cumulative incidence score does not suffer from unwarranted overconfidence. Finally, we apply our methods to examine gender differences in reported measles cases during the 2010's. We estimate that the relative","157":"The COVID-19 pandemic has severely affected people's daily lives and caused tremendous economic loss worldwide. However, its influence on people's mental health conditions has not received as much attention. To study this subject, we choose social media as our main data resource and create by far the largest English Twitter depression dataset containing 2,575 distinct identified depression users with their past tweets. To examine the effect of depression on people's Twitter language, we train three transformer-based depression classification models on the dataset, evaluate their performance with progressively increased training sizes, and compare the model's\"tweet chunk\"-level and user-level performances. Furthermore, inspired by psychological studies, we create a fusion classifier that combines deep learning model scores with psychological text features and users' demographic information and investigate these features' relations to depression signals. Finally, we demonstrate our model's capability of monitoring both group-level and population-level depression trends by presenting two of its applications during the COVID-19 pandemic. We hope this study can raise awareness among researchers and the general public of COVID-19's impact on people's mental health. The dataset and code are available at https:\/\/github.com\/annahaensch\/covid19-tomography-dataset.","158":"Coronavirus disease 2019 (COVID-19) has infected more than one million individuals all over the world and caused more than 55,000 deaths, as of April 3 in 2020. Radiological findings are important sources of information in guiding the diagnosis and treatment of COVID-19. However, the existing studies on how radiological findings are correlated with COVID-19 are conducted separately by different hospitals, which may be inconsistent or even conflicting due to population bias. To address this problem, we develop natural language processing methods to analyze a large collection of COVID-19 literature containing study reports from hospitals all over the world, reconcile these results, and draw unbiased and universally-sensible conclusions about the correlation between radiological findings and COVID-19. We apply our method to the CORD-19 dataset and successfully extract a set of radiological findings that are closely tied to COVID-19. Furthermore, we show that our method can significantly improve the accuracy of both image classification and segmentation tasks compared with state-of-the-art methods. Finally, we highlight a few interesting insights for future research. All code and data are available at https:\/\/github.com\/cbasemaster\/nlp-corona. In addition, we create","159":"In clinical and epidemiological studies, hazard ratios are often applied to compare treatment effects between two groups for survival data. For competing risks data, the corresponding quantities of interest are cause-specific hazard ratios (cHRs) and subdistribution hazard ratios (sHRs). However, they both have some limitations related to model assumptions and clinical interpretation. Therefore, we recommend restricted mean time lost (RMTL) as an alternative that is easy to interpret in a competing risks framework. Based on the difference in restricted mean time lost (RMTLd), we propose a new estimator, hypothetical test and sample size formula. The simulation results show that the estimation of the RMTLd is accurate and that the RMTLd test has robust statistical performance (both type I error and power). The results of three example analyses also verify the performance of the proposed method. From the perspectives of clinical application, application conditions and statistical performance, we suggest that the RMTLd be reported with the HR in the analysis of competing risks data and that the RMTLd even be regarded as the primary outcome when the proportional hazards assumption fails. The R code (crRMTL) is publicly available from Github (https:\/\/github.com\/chenzgz\/","160":"SARS-CoV-2 has characteristics of wide contagion and quick velocity of propagation. To analysis the visual information of it, we build a dataset with 48 electron microscopic images (EMGs) taken from 2 to 4 different sources, which includes also the EMG image collection. Furthermore, we extract multiple classical features and novel deep learning features to describe the visual information of COVID-19. As a result, it can be stated that the visual structure of SARS-CoV-2 is similar to those of influenza viruses within the human body, while the quantitative description of the visual properties is still lacking. Here, we present both a qualitative and quantitative investigation of the visual structure of SARS-CoV-2 using our newly developed numerical model. Although the visualization itself does not contain any bias, the raw output of the network is biased towards the majority class. We also explore the most interesting case of how the basic reproduction number R0 changes according to the value of the stochastic parameter on the other hand. Our study demonstrates that the proposed computer-aided image processing techniques have the potential to improve the sensitivity of current detectors for COVID-19 diagnosis. The overall results show that the visualization of SARS-CoV-2 is indeed","161":"We present Coronavirus disease 2019 (COVID-19) statistics in China dataset: daily statistics of the COVID-19 outbreak in China at city\/county level. For each city\/country, we include the six most important numbers for epidemic research: daily new infections, accumulated deaths, and accumulated cases. We cross validate the estimates with the help of synthetic data drawn from an unrelated dataset. The accuracy of the estimate is about 0.5%. At the end of May, 2020, our model predicts that the number of cumulative confirmed cases will reach 3500 persons in Wuhan; and the number of new infections will be about 600.000 people in Beijing by the middle of May. Our analysis shows that these two quantities are very close to those reported in other parts of the world. We also calculate the estimate error rates between the observed value and the estimated value under various scenarios. It is found that the uncertainty of the estimate error rate is relatively high, e.g., exceeding 10%; while the prediction power of the model is rather low, e.g., less than 5%. Moreover, we find that more severe restrictions on the social interaction would delay the spread of the epidemics. To this end, it is advised to relax the social isolation","162":"Mass spectrometry of intact nanoparticles and viruses can serve as a potent characterization tool for material science and biophysics. Inaccessible by widespread commercial techniques, the mass of single nanoparticles and viruses (>10MDa) can be readily measured by NEMS (Nanoelectromechanical Systems) based Mass Spectrometry, where charged and isolated analyte particles are generated by Electrospray Ionization (ESI) in air and transported onto the NEMS resonator for capture and detection. However, the applicability of NEMS as a practical solution is hindered by their miniscule surface area, which results in poor limit-of-detection and low capture efficiency values. Another hindrance is the necessity to house the NEMS inside complex vacuum systems, which is required in part to focus analytes towards the miniscule detection surface of the NEMS. Here, we overcome both limitations by integrating an ion lens onto the NEMS chip. The ion lens is composed of a polymer layer, which charges up by receiving part of the ions incoming from the ESI tip and consequently starts to focus the analytes towards an open window aligned with the active area of the NEMS electrostatically. With this integrated system, we have detected the mass","163":"The rapidly growing market demand for dialogue agents capable of goal-oriented behavior has caused many tech-industry leaders to invest considerable efforts into task-oriented dialog systems. The performance and success of these systems is highly dependent on the accuracy of their intent identification -- the process of deducing the goal or meaning of the user's request and mapping it to one of the known intents for further processing. Gaining insights into unrecognized utterances -- user requests the systems fails to attribute to a known intent -- is therefore a key process in continuous improvement of goal-oriented dialog systems. We present an end-to-end pipeline for processing unrecognized user utterances, including a specifically-tailored clustering algorithm, a novel approach to cluster representative extraction, and cluster naming. We evaluated the proposed clustering algorithm and compared its performance to out-of-the-box SOTA solutions, demonstrating its benefits in the analysis of unrecognized user requests. We also performed a qualitative evaluation with 22 human users to check the quality of the clusters generated by the algorithm. The results show that the clusters produced by our algorithm are well-named according to the subjective evaluations. Moreover, we note that each cluster name was derived from combining multiple candidate names, resulting in a high degree of consistency between","164":"We introduce the well-established social scientific concept of social solidarity and its contestation, anti-solidarity, as a new problem setting to supervised machine learning in NLP to assess how European solidarity discourses changed before and after the COVID-19 outbreak was declared a global pandemic. To this end, we annotate 2.3k English and German tweets for (anti-)solidarity expressions, utilizing multiple human annotators and two annotation approaches (experts vs.\\ crowds). We use these annotations to train a BERT model with multiple data augmentation strategies. Our augmented BERT model that combines both expert and crowd annotations outperforms the baseline BERT classifier trained with expert only annotations only by over 25 points, from 58\\% macro-F1 to almost 85\\%. We use this high-quality model to automatically label over 270k tweets between September 2019 and December 2020. We then assess the automatically labeled data for how statements related to European (anti-)solidarity discourses developed over time and in relation to one another, before and during the COVID-19 crisis. Our results show that solidarity became increasingly salient and contested during the crisis. While the number of solidarity tweets remained on a higher level and dominated the discourse in the scrutinized time frame","165":"Virtual Reality has become a significant element of education throughout the years. To understand the quality and advantages of these techniques, it is important to understand how they were developed and evaluated. Since COVID-19, the education system has drastically changed many aspects of teaching activities. It has shifted from being in a classroom with a whiteboard and projectors to having your own room in front of your laptop in a virtual meeting. In this respect, virtual reality in the laboratory or Virtual Laboratory is the main focus of this research, which is intended to comprehend the work done in quality education from a distance using VR. As per the findings of the study, adopting virtual reality in education can help students learn more effectively and also help them increase perspective, enthusiasm, and knowledge of complex notions by offering them an interactive experience in which they can engage and learn more effectively. This highlights the importance of a significant expansion of VR use in learning, the majority of which employ scientific comparison approaches to compare students who use VR to those who use the traditional method for learning. The paper intends to offer a comprehensive analysis of online learning during remote evaluation of mobile physics lab courses. Specifically, we will examine the areas of interest for students and conduct a comparative analysis on the different modes of learning available before and after the Cov","166":"Because of the complexity of urban transportation networks and the temporal changes in traffic conditions, it is difficult to assess real-time traffic situations. However, the development of information terminals has made it easier to obtain personal mobility information. In this study, we propose methods for evaluating the mobility of people in a city using global positioning system data. There are two main methods for evaluating movement. One is to create a temporal network from real data and check the change in travel patterns according to time zones or seasons. Temporal networks are difficult to evaluate because of their time complexity, and in this paper, we proposed an evaluation method using the probability density function of travel distance. The other method is to define a time-dependent traveling salesman problem and find an efficient traveling route by finding the shortest path during the given time zone. By creating a time-dependent traveling salesman problem in an existing city and solving it, a traveler can choose an efficient route by considering traffic conditions at different times of the day. We used 2 months of data from Kyoto City to conduct a traffic evaluation as a case study. The results show that the proposed methods allow effective evaluation of the mobility of people in a city. They also showed that short-distance trips provide sufficient information regarding the spread of the disease to inform pandemic","167":"The state of the art review broadly oversees the use of novel research utilized in the creation of virtual environments applied in interactive art experiences, with a specific focus on the application of procedural animation in spatially augmented reality (SAR) exhibitions. These art exhibitions frequently combine sensory displays that appeal, replace, and augment the visual, auditory and touch or haptic senses. We analyze and break down art-technology related innovations in the last three years, and thoroughly identify the most recent and vibrant applications of interactive art experiences in the review of numerous installation applications, studies, and events. Display mediums such as virtual reality, augmented reality, mixed reality, and robotics are overviewed in the context of art experiences such as visual arts museums, park or historic site tours, live concerts, and theatre. We explore research and extrapolate how recent innovations can lead to different applications that will be seen in the future. The paper concludes by discussing several opportunities for further innovation within this area. This work is based on a systematic analysis of existing literature on human computer interaction, psychology, and art education. Several conclusions have been drawn about potential directions for future research in this area. In addition, we discuss some challenges that need to be addressed in order to better utilize these technologies in the future. Finally,","168":"Today is the era of intelligence in machines. With the advances in Artificial Intelligence, machine learning has become a powerful tool for process mining and business reasoning. However, there are many challenges ahead. In this paper, we present our vision to build a bridge between the worlds of statistical analysis and deep learning. We first introduce some important concepts such as data distribution shift, and then describe several real-world applications where intelligent processes help us achieve better performance. Finally, we highlight the main strengths and drawbacks of different approaches. Our work can give a more comprehensive view of today's AI community and provides researchers with more up-to-date reference regarding specific topics such as state-of-the-art papers. The code used to implement the approach is available at https:\/\/github.com\/Guzpenha\/AI4HPC.git. For future research, we plan to apply some of the ideas presented here to other areas of medical practice. A good portion of citations from the literature will be found in our GitHub repository: https:\/\/github.com\/Guzpenha\/cite_review.git. This includes COVID-19 papers by Nita H. Shah, Ankush H. Suthar and Ekta N. Jayswal. We also include","169":"High-resolution magnetic resonance images can provide fine-grained anatomical information, but acquiring such data requires a long scanning time. In this paper, a framework called the Fused Attentive Generative Adversarial Networks(FA-GAN) is proposed to generate the super resolution MR image from low-resolution magnetic resonance images, which can reduce the scanning time effectively but with high resolution MR images. In the framework of the FA-GAN, the local fusion feature block, consisting of different three-pass networks by using different convolution kernels, is proposed to extract image features at different scales. And the global feature pooling module, including the channel attention module, the self-attention module, and the operation normalization layer, is designed to enhance the important features of the MR image. Moreover, the spectral normalization process is introduced to make the discriminator network stable. 40 sets of 3D hybrid MRI-guided brain tumor segmentation (HTSU) images and 20 sets of images of patients infected with COVID-19 were trained to train the network. The experimental results show that our PSNR and SSIM values outperform the state-of-the-art reconstruction methods on both 2D and 3D tasks. Besides, the reconstructed CT images can","170":"Subscription services face a difficult problem when estimating the causal impact of content launches on acquisition. Customers buy subscriptions, not individual pieces of content, and once subscribed they may consume many pieces of content in addition to the one(s) that drew them to the service. In this paper, we propose a scalable methodology to estimate the incremental acquisition impact of content launches in a subscription business model when randomized experimentation is not feasible. Our approach uses simple assumptions to transform the problem into an equivalent question: what is the expected consumption rate for new subscribers who did not join due to the content launch? We estimate this counterfactual rate using the consumption rate of new subscribers who joined just prior to launch, while making adjustments for variation related to subscriber attributes, the in-product experience, and seasonality. We then compare our counterfactual consumption to the actual rate in order to back out an acquisition estimate. Our methodology provides top-line impact estimates at the content \/ day \/ region grain. Additionally, to enable subscriber-level attribution, we present an algorithm that assigns specific individual accounts to add up to the top-line estimate. Subscriber-level attribution is derived by solving an optimization problem to minimize the number of subscribers attributed to more than one piece of content, while maximizing the average","171":"The new coronavirus (2019-nCoV or SARS-CoV2), inducing the current pandemic disease (COVID-19) and causing pneumoniae in humans, is dramatically increasing in epidemic scale since its first appearance in Wuhan, China, in December 2019. The first infection from epidemic coronaviruses in 2003 fostered the spread of an overwhelming amount of related scientific efforts. The manifold aspects that have been raised, as well as their redundancy offer precious information that has been underexploited and needs to be critically re-evaluated, appropriately used and offered to the whole community, from scientists, to medical doctors, stakeholders and common people. These efforts will favour a holistic view on the comprehension, prevention and development of strategies (pharmacological, clinical etc) as well as common intervention against the new coronavirus spreading. Here we describe a model that emerged from our analysis that was focused on the Renin Angiotensin System (RAS) and the possible routes linking it to the viral infection. because the infection is mediated by the viral receptor on human cell membranes Angiotensin Converting Enzyme (ACE2), which is a key component in RAS signalling. The model depicts the main pathways determining","172":"As \\emph{artificial intelligence} (AI) systems are increasingly involved in decisions affecting our lives, ensuring that automated decision-making is fair and ethical has become a top priority. Intuitively, we feel that akin to human decisions, judgments of artificial agents should necessarily be grounded in some moral principles. Yet a decision-maker (whether human or artificial) can only make truly ethical (based on any ethical theory) and fair (according to any notion of fairness) decisions if full information on all the relevant factors on which the decision is based are available at the time of decision-making. This raises two problems: (1) In settings, where we rely on AI systems that are using classifiers obtained with supervised learning, some induction\/generalization is present and some relevant attributes may not be present even during learning. (2) Modeling such decisions as games reveals that any -- however ethical -- pure strategy is inevitably susceptible to exploitation. Moreover, in many games, a Nash Equilibrium can only be obtained by using mixed strategies, i.e., to achieve mathematically optimal outcomes, decisions must be randomized. In this paper, we argue that in supervised learning settings, there exist random classifiers that perform at least as well as deterministic classifiers,","173":"The field-orthogonality property of partial differential equations (PDEs) makes them especially useful for learning hyperdimensional data and solving PDEs with many parameters. In particular, medical applications such as image analysis or speech modeling require the ability to solve PDEs with hundreds of thousands of variables and parameters. The difficulty of this task can be mitigated by using learned kernels. However, existing kernel functions are not capable of estimating all the necessary parameters in time. Furthermore, they cannot transfer characteristics from one domain to another without domain adaptation. Thus, we introduce a novel method for few-shot learning of PDE neural networks. Our approach exploits classic results in semiclassical analysis and the equivalence between the gradient descent algorithm and the Laplace approximation operator on both the conditional expectation and the variance matrices. This allows us to obtain faster convergence rates, more accurate activation functions, and less computational overhead through finetuning. We demonstrate our method on several benchmark examples. Moreover, we show how our approach can improve the performance of state-of-the-art methods for few-shot learning of vision and graphics problems. Finally, we also present a new application for COVID-19 CT scan classification. Here, we successfully train a network based on our proposed method","174":"We present TWEET-FID (TWEET-Foodborne Illness Detection), an approach for detecting foodborne illness complaints in social media documents such as Yelp restaurant reviews and Twitter posts about restaurants. Our system adopts a multimodal deep neural network to detect keywords in short texts and then extracts relevant features, such as orders, tables, and graphs, to identify illnesses. The architecture transforms the original multivariate time series into multiple cascading univariate time series using a self-attention mechanism. This enables us to focus on both temporal and semantic aspects of the disease spread process with a multi-modal attention framework. We further integrate transformer-based word embeddings into our model by leveraging the inherent semantic information of each token within a sentence and modeling the relationship between order quantities. We demonstrate the effectiveness of our approach on datasets associated with Yelp restaurant reviews and Twitter posts about restaurants. Our code is available at https:\/\/github.com\/annahaensch\/TWEET-FID.git.io\/. Video tutorials demonstrating the performance of our method are available at http:\/\/youtu.be\/zhbcSvxEKMk. Source code for our paper will be made publicly available at https:\/\/github.com\/annahaens","175":"The COVID-19 pandemic, like many of the disease outbreaks that have preceded it, is likely to have a profound effect on mental health. Understanding its impact can inform strategies for mitigating negative consequences. In this work, we seek to better understand the effects of COVID-19 on mental health by examining discussions within mental health support communities on Reddit. First, we quantify the rate at which COVID-19 is discussed in each community, or subreddit, in order to understand levels of preoccupation with the pandemic. Next, we examine the volume of activity in order to determine whether the quantity of people seeking online mental health support has risen. Finally, we analyze how COVID-19 has influenced language use and topics of discussion within each subreddit. Our results indicate that COVID-19 has had both positive and negative impacts on mental health conversations across subreddits. We make some recommendations for designers interested in supporting mental health discourse during the pandemic and for researchers looking to assess the effects of COVID-19 on mental health data. Lastly, we find that high volumes of activity are associated with the highest number of public posts about mental health and related topics, indicating that users have taken to social media not only for altruistic reasons but also because of their experiences. This","176":"The lack of scientific openness is identified as one of the key challenges of computational reproducibility. In addition to Open Data, Free and Open-source Software (FOSS) and Open Hardware (OH) can address this challenge by introducing open policies, standards, and recommendations. However, while both FOSS and OH are free to use, study, modify, and redistribute, there are significant differences between them regarding the redistribution of software development costs and their respective supply chains. This paper investigates how FAIR principles can be applied to OH findability with the goal of promoting reliable and sustainable hardware development. We define what FAIR means for OH, how it differs from FOSS, and present examples of inconsistencies between existing and future versions. Finally, we evaluate dissemination platforms currently used for OH and provide recommendations. Our results show that FAIR promotes unbiased and equitable workflows, contributes to accelerating the state-of-the-art, and provides findings of high quality. For each issue, we identify solutions to promote further research within our field so that our work is not left behind in its implementation. We also discuss the consequences of these violations on production communities when they exist. Furthermore, we propose that FAIR may serve as a catalyst for developing multidisciplinary science and engineering beyond","177":"Air-borne transmission can pose a major risk of infection spread in enclosed spaces. Venting the air out using exhaust fans and ducts is a common approach to mitigate the risk. In this work, we study the air flow set up by an exhaust fan in a typical shared washroom that can be used as a ventilator. The primary focus is on the regions of recirculating flow that can harbor infectious aerosol for much longer than the well-ventilated parts of the room. Computational fluid dynamics is used to obtain the steady state air flow field, and Lagrangian tracking of particles gives the spatial and temporal distribution of infectious aerosols. It is found that the washbasin located next to the door is in a prominent recirculation zone, and particles injected in this region take much longer to be evacuated. The ventilation rate is found to be governed by the air residence time in the recirculation zones, and it is much higher than the timescale based on fully mixed reactor model of the room. Increasing the fan flow rate can reduce the ventilation time, but cannot eliminate the recirculation zones in the washroom. Thus, it is recommended to avoid placing too many heavy objects on the airflow so that they do not cause undesired artifacts in","178":"We are facing a global healthcare crisis today as the healthcare cost is ever climbing, but with the aging population, government fiscal revenue is never growing enough to support the increasing needs. To address this problem, we propose a simple yet effective strategy to improve the efficiency of collecting and analyzing data from different sources (various hospitals) and evaluating the effectiveness of age-targeted medical interventions. In particular, we build a joint model consisting of a mechanistic stochastic epidemiological agent-based simulator and a deep learning based neural network, to study disease transmission dynamics and evaluate the mitigation strategies. Our model is calibrated to real patient data from two hospitals in China and uses COVID-19-related historical statistics to predict future hospital visits. Experiments show that our approach significantly reduces both the total number of infected patients and deaths by maximizing the utility of medical resources while minimizing the strain on the healthcare system. Moreover, we find that, although the epidemic curves may be slightly longer than what is initially expected, the average excess death across horizons is always below 10%. The results demonstrate the feasibility of our proposed strategy in reducing the spread of infectious diseases. Finally, we provide more details about the interactions between infection cases, deaths, and the distribution of medical resources. These results can help governments","179":"In this paper we consider optimization problems with stochastic composite objective function subject to (possibly) infinite intersection of constraints. The objective function is expressed in terms of expectation operator over a sum of two terms satisfying a stochastic bounded gradient condition, with or without strong convexity type properties. In contrast to the classical approach, where the constraints are usually represented as intersection of simple sets, in this paper we consider that each constraint set is given as the level set of a convex but not necessarily differentiable function. Based on the flexibility offered by our general optimization model we consider a stochastic subgradient method with random feasibility updates. At each iteration, our algorithm takes a stochastic proximal (sub)gradient step aimed at minimizing the objective function and then a subsequent subgradient step minimizing the feasibility violation of the observed random constraint. We analyze the convergence behavior of the proposed algorithm for diminishing stepsizes and for the case when the objective function is convex or has a quadratic functional growth, unifying the nonsmooth and smooth cases. We prove sublinear convergence rates for this stochastic subgradient algorithm, which are known to be optimal for subgradient methods on this class of problems. When the objective function has a linear least-square form and","180":"The COVID-19 epidemic was listed as a public health emergency of international concern by the WHO on January 30, 2020. To curb the secondary spread of the epidemic, many public places were equipped with thermal imagers to check the body temperature. However, the COVID-19 pneumonia has concealed symptoms: the first symptom may not be fever, and can be shortness of breath. During epidemic prevention, many people tend to wear masks. Therefore, in this demo paper, we proposed a portable non-contact healthy screening system for people wearing masks, which can simultaneously obtain body temperature and respiration state. This system consists of three modules viz. thermal image collection module, health indicator calculation module and health assessment module. In this system, the thermal video of human faces is first captured through a portable thermal imaging camera. Then, body temperature and respiration state are extracted from the video and are imported into the following health indicators. Finally, the screened results can be used to screen those who are wearing masks. The preliminary experiments show that this syetem can give an accurate screening result within 15 seconds. This system can be applied to many application scenarios such as community and campus. We also discuss some future challenges and provide recommendations for other researchers working on similar systems. One of the","181":"Graph neural networks (GNNs) have emerged as a series of competent graph learning methods for diverse real-world scenarios, ranging from daily applications like recommendation systems and question answering to cutting-edge technologies such as drug discovery in life sciences and n-body simulation in astrophysics. However, task performance is not the only requirement for GNNs. Performance-oriented GNNs have exhibited potential adverse effects like vulnerability to adversarial attacks, unexplainable discrimination against disadvantaged groups, or excessive resource consumption in edge computing environments. To avoid these unintentional harms, it is necessary to build competent GNNs characterised by trustworthiness. To this end, we propose a comprehensive roadmap to build trustworthy GNNs from the view of the various computing technologies involved. In this survey, we introduce basic concepts and comprehensively summarise existing efforts for trustworthy GNNs from six aspects, including robustness, explainability, privacy, fairness, accountability, and environmental well-being. Additionally, we highlight the intricate cross-aspect relations between the above six aspects of trustworthy GNNs. Finally, we present a thorough overview of trending directions for facilitating the research and industrialisation of trustworthy GNNs. This work is expected to assist researchers and practitioners in achieving their goal with regards to","182":"Successful applications of InfoNCE and its variants have popularized the use of contrastive variational mutual information (MI) estimators in machine learning. While featuring superior stability, these estimators crucially depend on costly large-batch training, and they sacrifice bound tightness for variance reduction. To overcome these limitations, we revisit the mathematics of popular variational MI bounds from the lens of unnormalized statistical modeling and convex optimization. Our investigation not only yields a new unified theoretical framework encompassing popular variational MI bounds but also leads to a novel, simple, and powerful contrastive MI estimator named as FLO. Theoretically, we show that the FLO estimator is tight, and it provably converges under stochastic gradient descent. Empirically, our FLO estimator overcomes the limitations of its predecessors and learns more efficiently. The utility of FLO is verified using an extensive set of benchmarks, which also reveals the trade-offs in practical MI estimation. We hope that this gentle study will help promote the development of practical ML-based IVIS becoming available online. Code is provided at https:\/\/github.com\/anyleopeace\/FLO.git. Also, public leaderboards are established under https:\/\/github.","183":"Blockchains and cryptocurrencies disrupted the conversion of energy into a medium of exchange. Numerous applications for blockchains and cryptocurrencies are now envisioned for purposes ranging from inventory control to banking applications. Naturally, in order to mine in an economically viable way, regions where energy is plentiful and cheap, e.g., close to hydroelectric plants, are sought. The possibility of converting energy into cash, however, also opens up opportunities for a new kind of cyber attack aimed at illegally mining cryptocurrencies by stealing energy. In this work, we indicate, using data from January and February of 2018 from our university, that such a threat is real, and present a projection of the gains derived from these attacks. We further find that the attackers are particularly successful in targeting sectors with low daily activity levels, e.g., during the day (around 12 noon), or on the weekend (during the summer vacation). These findings can be used to help identify the vulnerabilities of existing security measures and so enable the development of novel attacks aiming at illegally mining cryptocurrency assets. For instance, based on the monthly coordination mechanism, it may take around 3 years until the network reaches a critical level under which it becomes difficult to imagine how any longer they will remain active. Our prediction of future criminal activities indicates that within the next year","184":"The breakthrough potentials of research papers can be explained by their boundary-spanning qualities. Here, for the first time, we apply the structural variation analysis (SVA) model and its affiliated metrics to investigate the extent to which such qualities characterize a group of Nobel Prize winning papers. We find that these papers share remarkable boundary-spanning traits, marked by exceptional abilities to connect disparate and topically-diverse clusters of research papers. Further, their publications exert structural variations on a scale that significantly alters the betweenness centrality distributions in existing intellectual space. Overall, SVA not only provides a set of leading indicators for describing future Nobel Prize winning papers, but also broadens our understanding of similar prize-winning properties that may have been overlooked among other regular publications. Our results are expected to have implications for developing effective intervention strategies as well as designing manuscript introductions. They provide us with an unprecedented opportunity to study a complex phenomenon by leveraging such an unprecedented degree of international collaboration. This work expands upon previous analyses of how SVA tools can be used to effectively describe a diverse set of emerging scientific fields, including COVID-19. It also demonstrates that effectiveness does not correlate with original novelty or impact alone -- instead it appears to be a matter of deeper social and cultural forces","185":"Policy evaluations commonly employ a difference-in-differences (DID) study design; yet within this framework, statistical model specification varies notably across studies. Motivated by applied state-of-the-art in large sample DDI, we develop a statistically robust and computationally efficient method for feature selection of correlated features. We demonstrate with real-world experiments that our approach can achieve good performance relative to other approaches in identifying important features from massive candidate features. Our work enables researchers to carry out principled policy evaluations using realistic data sets, without relying on the common but often unrealistic assumption that all confounders have been observed. Finally, we present a case study that illustrates the practical gap between our proposed method and existing methods. Code and data are available at https:\/\/github.com\/anyleopeace\/Covid19_policy_evaluation.git.io\/. Our analysis demonstrates how ensemble processing mechanisms and contextual reporting of results can significantly improve policy effectiveness. This is particularly relevant when those factors relate to population subgroups or intervention strategies which exhibit substantial group homogeneity. We believe our methodology will be useful for any field where it requires balancing causal inference literature with empirical testing focused on understanding the effect of covariates on infection spread in the presence of potential treatment modalities","186":"We present in this work some results from analysing the spread of Covid-19 in different countries and regions around the world and the potential relations with climate, geographical location, and GDP. While the situation remains dynamic, we believe this analysis has the potential to uncover certain underlying trends. We primarily intend the results to drive further, more detailed analysis of the relevant data by other researchers that would help us gather a better understanding of the situation, aiding our preparedness. Our observations show that countries in high latitudes, with temperate and\/or continental climate, and with reasonably\"open\"economies are the most vulnerable to this outbreak. We also find that internal mobility controls the extent of economic openness above a certain threshold, which seems to correlate with the national lockdown strategy introduced in Italy. Our results suggest that the strategy be modified in some way by the local weather conditions, e.g. by adjusting for climatic effects or minimizing non-pharmaceutical interventions. Finally, we discuss possible correlations between the findings of this study and the general public awareness of the pandemic through social media. To the best of our knowledge, this is the first systematic analytical approach to the interplay of epidemiological dynamics and socioeconomic factors on the regional scale. Altogether, our research contributes","187":"The existing trackers based on deep learning perform tracking in a holistic strategy, which aims to learn deep representations of the whole target for localizing the target. It is arduous for such methods to track targets with various appearance variations. To address this limitation, we propose a novel method named SanMove, where a backbone network is trained to detect common features of the target along with its neighbors in a shallow manner. These common features are aggregated by a kernel module to generate diverse and robust feature vectors. Further, the global embeddings of these feature vectors are then passed as parameters to a modified auto-encoder (AE) architecture to classify the target. Experimental results demonstrate that our method achieves state-of-the-art performance on NIH-AAPM dataset and performs similarly to the state-of-the-art trackers while achieving better generalization. Our code will be released at https:\/\/github.com\/sivaramakrishnan-rajaraman\/SanMove.git. We also release the first publicly available data set at https:\/\/hcc4fdx133vht0qb\/sanMove_V1.0. Furthermore, we provide two out of three splits of the test set of the AIZOO dataset - Track","188":"In ad-hoc human-robot collaboration (HRC), humans and robots work on a task without pre-planning the robot's actions prior to execution; instead, task allocation occurs in real-time. However, prior research has largely focused on task allocations that are pre-planned - there has not been a comprehensive exploration or evaluation of techniques where task allocation is adjusted in real-time. Inspired by HCI research on territoriality and proxemics, we propose a design space of novel task allocation techniques including both explicit techniques, where the user maintains agency, and implicit techniques, where the efficiency of automation can be leveraged. The techniques were implemented and evaluated using a tabletop HRC simulation in VR. A 16-participant study, which presented variations of a collaborative block stacking task, showed that implicit techniques enable efficient task completion and task parallelization, and should be augmented with explicit mechanisms to provide users with fine-grained control. Implications for future research on task allocation in HRC are discussed. Our code is made publicly available at https:\/\/github.com\/gbriel21\/Real-Time-Task-Allocation.git. We hope that our results can facilitate further research into this area and beyond. Video presenting our approach is available at http","189":"Most current multi-object trackers focus on short-term tracking, and are based on deep and complex systems that often cannot operate in real-time, making them impractical for video-surveillance. In this paper we present a long-term, multi-face tracker architecture conceived for working in crowded contexts where faces are often the only visible part of a person. Our system benefits from advances in the fields of face detection and face recognition to achieve long-term tracking, and is particularly unconstrained to the motion and occlusions of people. It follows a tracking-by-detection approach, combining a fast short-term visual tracker with a novel online tracklet reconnection strategy grounded on rank-based face verification. The proposed rank-based constraint favours higher inter-class distance among tracklets, and reduces the propagation of errors due to wrong reconnections. Additionally, a correction module is included to correct past assignments with no extra computational cost. We present a series of experiments introducing novel specialized metrics for the evaluation of long-term tracking capabilities, and publicly release a video dataset with 10 manually annotated videos and a total length of 8' 54\". Our findings validate the robustness of each of the proposed modules, and demonstrate that, in these challenging contexts","190":"The new coronavirus disease (COVID-19) has been emerged as a rapidly spreading pandemic, leading to enormous amounts of human death and economic loss worldwide. Given its high lethality amongst the elderly, the safety of nursing homes has been of central importance during the COVID-19 pandemic. With test procedures becoming available at scale, such as antigen or RT-LAMP tests, and increasing availability of vaccinations, nursing homes might be able to safely relax prohibitory measures while controlling the spread of infections (meaning an average of one or less secondary infections per index case). Here, we develop a detailed agent-based epidemiological model for the spread of SARS-CoV-2 in nursing homes to identify optimal prevention strategies. The model is microscopically calibrated to high resolution data from nursing homes in Austria, including detailed social contact networks and information on past outbreaks. We find that the effectiveness of mitigation testing depends critically on the timespan between test and test result, the detection threshold of the viral load for the test to give a positive result, and the screening frequencies of residents and employees. Under realistic conditions and in absence of an effective vaccine, we find that preventive screening of employees only might be sufficient to control outbreaks in nursing homes, provided that turnover times","191":"The Metaverse is a virtual environment where users are represented by avatars to navigate a virtual world, which has strong links with the physical one. State-of-the-art Metaverse architectures rely on a cloud-based approach for avatar physics emulation and graphics rendering computation. Such centralized design is unfavorable as it suffers from several drawbacks caused by the long latency required for cloud access, such as low quality visualization. To solve this issue, in this paper, we propose a Fog-Edge hybrid computing architecture for Metaverse applications that leverage an edge-enabled distributed computing paradigm, which makes use of edge devices computing power to fulfil the required computational cost for heavy tasks such as collision detection in virtual universe and computation of 3D physics in virtual simulation. The computational cost related to an entity in the Metaverse such as collision detection or physics emulation are performed at the end-device of the associated physical entity. To prove the effectiveness of the proposed architecture, we simulate a distributed social metaverse application. Simulation results shows that the proposed architecture can reduce the latency by 50% when compared with the legacy cloud-based Metaverse applications. Finally, we show that the new architecture also improves the performance of existing models trained using the legacy hardware. Overall, the proposed architecture presents a viable solution for realizing","192":"The Astrobiology Graduate Conference (AbGradCon) is an annual conference both organized for and by early career researchers, postdoctoral fellows, and students as a way to train the next generation of astrobiologists and develop a robust network of cohorts moving forward. AbgradCon 2021 was held virtually on September 14-17, 2021, hosted by the Earth-Life Science Institute (ELSI) of Tokyo Institute of Technology after postponement of the in-person event in 2020 due to the COVID-19 pandemic. The meeting consisted of presentations by 120 participants from a variety of fields, two keynote speakers, and other career building events and workshops. Here, we report on our organizational and executional aspects of AbgradCon 2021, including the meeting participant demographics, various digital aspects introduced specifically for a virtual edition of the meeting, and the abstract submission and evaluation process. The abstract evaluation process of AbgradCon 2021 is unique in that all evaluations are done by the peers of the applicants, and as astrobiology is inherently a broad discipline, the abstract evaluation process revealed a number of trends related to multidisciplinarity of the astrobiology field. We believe that meetings like AbgradCon can provide a unique opportunity for students and early career researchers in astro","193":"Graph neural networks (GNNs), as a group of powerful tools for representation learning on irregular data, have manifested superiority in various downstream tasks. With unstructured texts represented as concept maps, GNNs can be exploited for tasks like document retrieval. Intrigued by how can GNNs help document retrieval, we conduct an empirical study on a large-scale multi-discipline dataset CORD-19. Results show that instead of the complex structure-oriented GNNs such as GINs and GATs, our proposed semantics-oriented graph functions achieve better and more stable performance based on the BM25 retrieved candidates. Our insights in this case study can serve as a guideline for future work to develop effective GNNs with appropriate semantics-oriented inductive biases for textual reasoning tasks like document retrieval and classification. All code for this case study is available at https:\/\/github.com\/HennyJie\/GNN-DocRetrieval.git. We hope that our contribution will enable new research directions for document retrieval over structured content. The implementation of our approach is available at https:\/\/github.com\/HennyJie\/GNN-SeqIR.git. For reproducibility purposes, we further publish our code publicly at https","194":"We construct a family of doubly graded $W_0$-algebra $\\mathcal{A}^{\\mathbb{R}}(1)$, generated by the homogeneous cohomology of a finite $\\mathbb{R}$-motivic spectrum, with the additional degree being zero if and only if $L^{2+\\varepsilon}\\mathcal{A}^{\\mathbb{R}}$ is not contained into its minimal supersetage. We establish several properties of this algebra $\\mathcal{A}^{\\mathbb{R}}(\\alpha)$ for $\\alpha \\in (0, 2)$. One recurrent question is to find formulas for the $W_0$-graded Laplacians of arbitrary slope. In this paper we provide two such formulas, along with some numerical ones. These results are also used to calculate the Hilbert series of $\\mathcal{A}^{\\mathbb{R}}(k)$, the Euler scheme of $\\mathcal{A}^{\\mathbb{R}}(k)$ and the Gr\\\"obner curve of $\\mathcal{A}^{\\mathbb{R}}(k)$. The second half of this article","195":"In this work, we address the problem of learning a graph convolutional network (GCN) from online data and\/or parameters acquired by the GCN during its training phase. In particular, we consider the case where the input to the GCN is a mixture of multiple types of edges with different sizes and densities. The goal for this setting is to learn the dependency amongst the various kinds of edges in the input graph as well as the strength of each edge type. To achieve this, we propose a novel framework called Multi-Task Graph Learning (MTGL) which jointly learns separate graphs corresponding to different input graph structures via different tasks, i.e., multi-class graph learning (MCGRL), and incorporates them into one unified graph neural network (GNN). Specifically, we devise a set of binary latent variables partitioned into two groups, where one group contains only local connections between nodes, while the other group contains global connections between nodes. We also divide the both groups into smaller subgraphs such that the GNN can be exploited effectively on each node basis. Then, we incorporate the constraint proposed in our model into a modification of the famous classical Merriman-Bence-Osher (MBO) scheme, making it more suitable for","196":"Finding optimal parameter configurations for tunable GPU kernels is a non-trivial exercise for large search spaces, even when automated. This poses an optimization task on a non-convex search space, using an expensive to evaluate function with unknown derivative. These characteristics make a good candidate for Bayesian Optimization, which has not been applied to this problem before. However, the application of Bayesian Optimization to this problem is challenging. We demonstrate how to deal with the rough, discrete, constrained search spaces, containing invalid configurations. We introduce a novel contextual variance exploration factor, as well as new acquisition functions with improved scalability, combined with an informed acquisition function selection mechanism. By comparing the performance of our Bayesian Optimization implementation on various test cases to the existing search strategies in Kernel Tuner, as well as other Bayesian Optimization implementations, we demonstrate that our search strategies generalize well and consistently outperform alternative search strategies by a wide margin. We further show that the KL divergence between our posterior and the prior's mean squared error can be significantly reduced, without changing the search strategy, allowing us to quickly identify the best configuration. Our code is available at https:\/\/github.com\/grimmlab\/Bayes-Optimization.git.io\/.","197":"Social media platforms are increasingly considering models to incentivize creators to publish high quality content on their platforms. As a result, social media content creation has transformed into a form of gig work for some creators. In order to better design social media platforms to support this labor, we need to understand professional creators' motivations. In this work, we present a qualitative interview study of the motivations of $22$ U.S. OnlyFans creators. OnlyFans is a subscription-based social media platform that is unique in that it is primarily associated with sexual content (although it is not marketed as such) and thus creators are positioned at the intersection of professional content creation and sex work, exposing them to a unique set of potential challenges. Beyond the typical motivations for pursuing other forms of gig work (e.g., flexibility, autonomy) our findings highlight three key factors explaining the rapid growth of onlyFans despite the potential stigma of sexual content creation: ($1$) societal visibility and mainstream acceptance of onlyFans, created through a combination of celebrity hype and the design of the platform itself; ($2$) substantial corporate value from existing platforms, which enabled founders to leverage the added income they would earn by marketing their services; and ($3$) the pandemic, which led to both high","198":"We numerically explore the possibility that the large orbital inclination of the martian satellite Deimos originated in an orbital resonance with an ancient inner satellite of Mars more massive than Phobos. We find that Deimos's inclination can be reliably generated by outward evolution of a martian satellite that is about 20 times more massive than Phobos through the 3:1 mean-motion resonance with Deimos at 3.3 Mars radii. This outward migration, in the opposite direction from tidal evolution within the synchronous radius, requires interaction with a past massive ring of Mars. Our results therefore strongly support the cyclic martian ring-satellite hypothesis of Hesselbrock and Minton (2017). Our findings, combined with the model of Hesselbrock and Minton (2017), suggest that the age of the surface of Deimos is about 3.5-4 Gyr, and require Phobos to be significantly younger. These conclusions are based on rigorous analysis and simulations of both the 1D and 2D models presented here. Although our predictions may differ somewhat from those made in the literature, it is consistent with the data currently available. The paper closes with a brief description of the approach leading to these results, which we explain how they could affect the","199":"Covid-19 vaccines are widely available in the United States, yet our Covid-19 vaccination rates have remained far below 100%. Not only that, but CDC data shows that even in places where vaccine acceptance was proportionally high at the outset of the Covid-19 vaccination effort, that willingness has not necessarily translated into high rates of vaccination over the subsequent months. We model how such a shift could have arisen, using parameters in agreement with data from the state of Alabama. The simulations suggest that in Alabama, local interactions would have favored the emergence of tight consensus around the initial majority view, which was to accept the Covid-19 vaccine. Yet this is not what happened. We therefore add to our model the impact of mega-influencers such as mass media, the governor of the state, etc. Our simulations show that a single vaccine-hesitant mega-influencer, reaching a large fraction of the population, can indeed cause the consensus to shift radically, from acceptance to hesitancy. Surprisingly this is true even when the mega-informencer only reaches individuals who are already somewhat inclined to agree with them, and under the conservative assumption that individuals give no more weight to the mega-informencer than they would give to a single","200":"Since two people came down a county of north Seattle with positive COVID-19 (coronavirus-19) in 2019, the current total cases of both infections and deaths among all citizens residing there are currently unknown. There are speculations that bats or other animals could have played a role in the transmission dynamics, and therefore the public health policies aimed at curbing the further spread of coronavirus- 19 were issued by the governments of the United States. To help understand the relationship between human mobility and infection spread, we analyze over 220 million movements of individuals from 50 states to determine the correlation between daily case activity and death rates. We find that the correlations between mobility and death rate are significantly correlated across different time periods and regions, where several clear clusters appear before and after lockdown measures are implemented. Additionally, we classify each state into five clusters based on their population, area and population density. It seems that the clustering analysis has clearly revealed the epidemiological classification of states. The findings of this study can be used as more refined data become available regarding the pandemic's impact on our society. Furthermore, the predictive power of the model regarding the timing of social distancing interventions for individual states may not be so readily available. Therefore, it would be helpful to","201":"The Internet of Things (IoT) has grown rapidly in the last decade and continue to develop in terms of dimension and complexity offering wide range of devices to support diverse set of applications. With ubiquitous Internet, connected sensors and actuators, networking and communication technology, and artificial intelligence (AI), smart cyber-physical systems (CPS) provide services rendering assistance to humans in their daily lives. However, the recent outbreak of COVID-19 (also known as coronavirus) pandemic has exposed and highlighted the limitations of current technological deployments to curtail this disease. IoT and smart connected technologies together with data-driven applications can play a crucial role not only in prevention, continuous monitoring, and mitigation of the disease, but also enable prompt enforcement of guidelines, rules and government orders to contain such future outbreaks. In this paper, we envision an IoT-enabled ecosystem for intelligent monitoring, pro-active prevention and control, and mitigation of COVID-19. We propose different architectures, applications and technology systems for various smart infrastructures including E-health, smart home, smart supply chain management, smart locality, and smart city, to develop future connected communities to manage and mitigate similar outbreaks. Furthermore, we present research challenges together with opportunities to enhance and accelerate the development","202":"The coronavirus which appeared in December 2019 in Wuhan has spread out worldwide and caused the death of more than 280,000 people (as of May, 11 2020). Since February 2020, doubts were raised about the numbers of confirmed cases and deaths reported by the Chinese government. In this paper, we examine data available from China at the city and provincial levels and we compare them with Canadian provincial data, US state data and French regional data. We consider cumulative and daily numbers of confirmed cases and deaths and find that these numbers are consistent with those currently being reported. Our analysis shows that there is no evidence that cumulative or daily numbers of confirmed cases and deaths for all these countries have different first or second digit distributions. We also show that the Newcomb-Benford distribution cannot be rejected for these data. Finally, we note that since our results are based on relatively small samples, both the consistency of our analyses and the accuracy of the methods used to estimate the quantities of interest are somewhat limited. Therefore, we encourage researchers to develop other approaches, such as using maximum likelihood models instead of classical bootstrap algorithms. A nice feature of our approach is that they do not require the use of MCMC techniques. The code provided here, available at https:\/\/github.com\/","203":"The workplace influences the safety, health, and productivity of workers at multiple levels. To protect and promote total worker health, smart hardware, and software tools have emerged for the identification, elimination, substitution, and control of occupational hazards. Wearable devices enable constant monitoring of individual workers and the environment, whereas connected worker solutions provide contextual information and decision support. Here, the recent trends in commercial workplace technologies to monitor and manage occupational risks, injuries, accidents, and diseases are reviewed. Workplace safety wearables for safe lifting, ergonomics, hazard identification, sleep monitoring, fatigue management, and heat and cold stress are discussed. Examples of workplace productivity wearables for asset tracking, augmented reality, gesture and motion control, brain wave sensing, and work stress management are given. Workplace health wearables designed for work-related musculoskeletal disorders, functional movement disorders, respiratory hazards, cardiovascular health, outdoor sun exposure, and continuous glucose monitoring are shown. Connected worker platforms are discussed with information about the architecture, system modules, intelligent operations, and industry applications. Predictive analytics provide contextual information about occupational safety risks, resource allocation, equipment failure, and predictive maintenance. Altogether, these examples highlight the ground-level benefits of real-time visibility about frontline workers,","204":"Advancements in digital technologies have a bootstrapping effect. The past fifty years of technological innovations from the computer architecture community have brought innovations and orders-of-magnitude efficiency improvements that engender use cases that were not previously possible -- stimulating novel application domains and increasing uses and deployments at an ever-faster pace. Consequently, computing technologies have fueled significant economic growth, creating education opportunities, enabling access to a wider and more diverse spectrum of information, and, at the same time, connecting people of differing needs in the world together. Technology must be offered that is inclusive of the world's physical, cultural, and economic diversity, and which is manufactured, used, and recycled with environmental sustainability at the forefront. For the next decades to come, we envision significant cross-disciplinary efforts to build a circular development cycle by placing pervasive connectivity, sustainability, and demographic inclusion at the design forefront in order to sustain and expand the benefits of a technologically rich society. We hope this work will inspire our computing community to take broader and more holistic approaches when developing technological solutions to serve people from different parts of the world. Finally, we expect significant cross-disciplinary research to develop effective ways for sustaining and expanding the benefit of a robust democratic environment through advanced technology. We anticipate this will provide","205":"Mott insulators under sufficiently strong spin-orbit coupling can display quantum spin liquid phases with topological order and fractional excitations. Quantum magnets with pure Kitaev spin exchange interactions can host a gapped quantum spin liquid with a single Majorana edge mode propagating in the counter-clockwise direction when a small positive magnetic field is applied. Here, we show how under a sufficiently strong positive magnetic field a topological transition into a gapped quantum spin liquid with two Majorana edge modes propagating in the clockwise direction occurs. The Dzyaloshinskii-Moriya interaction is found to turn the non-chiral Kitaev's gapless quantum spin liquid into a chiral one with equal Berry phases at the two Dirac points. Thermal Hall conductance experiments can provide evidence of the novel topologically gapped quantum spin liquid states predicted. Our results demonstrate that the Nambu-Goldstone mode provides a new type of quantum state in which topological phase transitions can be found. Further, our results indicate that for a large enough positive magnetic field a topological transformation into a gapped quantum spin liquid with two Majorana edge modes propagates in the clockwise direction occurs. This suggests that if a system wants to detect only the 1","206":"The paper develops a stochastic Agent-Based Model (ABM) mimicking the spread of infectious diseases in geographical domains. The model is designed to simulate the spatiotemporal spread of SARS-CoV2 disease, known as COVID-19. Our SARS-CoV2-based ABM framework (i) provides simulating the spread of infectious diseases among agents on any geographical scale; (ii) allows integrating different sources of agent attributes by combining them with their specific inter-relation based on rules defined by probability distributions; and (iii) accounts for under-reporting of cases or deaths due to each individual agent. This framework ensures that the number of simulated infections per agent is consistent with the daily amount of real infections worldwide. We illustrate our methodology with two examples: one involving 6,958 agents representing various social demographics and three geographic scales. In particular, we investigate the impact of local reopening policies on the dynamics of infection spreading. For this purpose, we introduce a novel parameter called'reopening rate' which indicates the effectiveness of the lockdown measures applied to control the spread of SARS-CoV2. We find that international collaboration does not affect the dynamics of infection spreading since both GDP losses and increased testing intensity are taken into","207":"As the world's population increases, so does the demand for food. This demand for food in turn puts pressure on agriculture in many countries. The impact of climate change on the environment has made it difficult to produce food that may be necessary to accommodate the growing population. Due to these concerns, the agriculture sector is forced to move towards more efficient and sustainable methods of farming to increase productivity. There is evidence that the use of technology in agriculture has the potential to improve food production and food sustainability; thereby addressing the concerns mentioned above. Consequently, this research aims to develop a new computer vision-based system for automated farm classification using machine learning techniques and to provide a case study evaluation for farmers' usage of the system. The proposed system consists of two main components: 1) information capturing relevant features of the agricultural domain by combining classifiers obtained from multiple sources (e.g., satellite images, temporal data, and pixel location data); 2) knowledge transfer and processing procedures through which the task assignment is achieved. In addition, we design a hierarchical feature fusion strategy among the classes utilized in the multi-class segmentation problem to obtain higher performance. Moreover, we propose a novel method to learn complex representations of historical states of the field, enabling the effective incorporation of prior knowledge into the system","208":"Graph Neural Networks (GNNs), as a group of powerful tools for representation learning on irregular data, have manifested superiority in various downstream tasks. With unstructured texts represented as concept maps, GNNs can be exploited for tasks like document retrieval. Intrigued by how can GNNs help document retrieval, we conduct an empirical study on a large-scale multi-discipline dataset CORD-19. Results show that instead of the complex structure-oriented GNNs such as GINs and GATs, our proposed semantics-oriented graph functions achieve better and more stable performance based on the BM25 retrieved candidates. Our insights in this case study can serve as a guideline for future work to develop effective GNNs with appropriate semantics-oriented inductive biases for textual reasoning tasks like document retrieval and classification. All code for this case study is available at https:\/\/github.com\/HennyJie\/GNN-DocRetrieval.git. We encourage others to further develop ways of integrating context and expectation into their models. All tests and evaluation scripts for this case study are available online at https:\/\/gvv.mpi-inf.mpg.de\/projects\/gnn-docretrieval\/. Besides, we make three","209":"The accuracy and complexity of machine learning algorithms based on kernel optimization are limited by the set of kernels over which they are able to optimize. An ideal set of kernels should: admit a linear parameterization (for tractability); be dense in the set of all kernels (for robustness); be universal (for accuracy). The recently proposed Tesselated Kernels (TKs) is currently the only known class which meets all three criteria. However, previous algorithms for optimizing TKs were limited to classification and relied on Semidefinite Programming (SDP) - limiting them to relatively small datasets. By contrast, the 2-step algorithm proposed here scales to 10,000 data points and extends to the regression problem. Furthermore, when applied to benchmark data, the algorithm demonstrates significant improvement in performance over Neural Nets and SimpleMKL with similar computation time. Finally, we demonstrate the applicability of the proposed algorithm in computational efficiency through two novel applications. First, it approximates the posterior distribution of the optimal policy given by performing Bayesian inference using a standard closure, and second, it can serve as an efficient basis for estimating the generalization error associated with choosing a kernel without risking accuracy. Our code is available at https:\/\/github.com\/grimmlab\/HK","210":"This paper summarizes our endeavors in the past few years in terms of explaining image classifiers, with the aim of including negative results and insights we have gained. The paper starts by describing the explainable neural network (XNN), which attempts to extract and visualize several high-level concepts purely from the deep network, without relying on human linguistic concepts. This helps users understand network classifications that are less intuitive and substantially improves user performance on difficult fine-grained classification tasks like face recognition. Realizing that an important missing piece is a reliable heatmap visualization tool, we have developed I-GOS and iGOS++ utilizing integrated gradients to avoid local optima in heatmap generation, which improved the performance across all resolutions. During the development of those visualizations, we realized that for a significant number of images, the classifier has multiple different paths to reach a confident prediction. This has lead to our recent development of structured attention graphs (SAGs), an approach that utilizes beam search to locate multiple coarse heatmaps for a single image, and compactly visualizes a set of heatmaps by capturing how different combinations of image regions impact the confidence of a classifier. Through the research process, we have learned much about insights in building deep network explanations, the existence and","211":"Deep learning technology has been widely used in edge computing and cloud computing environments for its impressive performance of solutions. However, pandemics like covid-19 require deep learning capabilities at mobile devices (detect respiratory rate using mobile robotics or conduct CT scan using a mobile scanner), which are severely constrained by the limited storage and computation resources at the device level. To solve this problem, we propose a three-tier architecture, including robot layers, edge layers, and cloud layers. We adopt this architecture to design a non-contact respiratory monitoring system to break down respiratory rate calculation tasks. Experimental results of respiratory rate monitoring show that the proposed approach in this paper significantly outperforms other approaches. It is supported by computation time costs with 2.26 ms per frame, 27.48 ms per frame, 0.78 seconds for convolution operation, similarity calculation, processing one-minute length respiratory signals, respectively. And finally, it can realize multi-task learning with 10 parallel threads simultaneously. The codebase of our model is available at https:\/\/github.com\/anyleopeace\/Covid19-Detection-System-using-Edge-Computing-Model.git. This dataset was built with the intention to develop a publicly available benchmark for research on Covid","212":"In this paper, we propose two types of global Santos solvers for nonlinear mixed optimization problems using the method of separation of motions (MoV). The first type considers the linear setting and separates the problem into a sequence of one-dimensional convolutional neural network (CNN) blocks by means of time-series filtering. The second type utilizes a CNN structure to solve a judicious smoothness-regularized, time-varying parameter estimation problem. Extensive numerical experiments on both types of problems show that our MoV block based solution converges with high-order towards the optimal solutions in every direction as the number of iterations increases. Furthermore, we demonstrate the effectiveness of our methods by comparing them against several other state-of-the-art continuous control strategies in the context of the COVID-19 pandemic. For reproduction purposes, we further introduce a modified version of our original approach which takes into account the constraint associated with the social distancing measures imposed on the people during the spread of the virus. We also present the results of applying our proposed method to the daily number of new infection cases reported by the World Health Organization (WHO). These results suggest that our proposed method can be used to obtain inexpensive yet reliable forecasts of the spread of infectious diseases. All","213":"The Internet contains a wealth of public opinion on food safety, including views on food adulteration, food-borne diseases, agricultural pollution, irregular food distribution, and food production issues. In order to systematically collect and analyse public opinion on food safety, we developed IFoodCloud, a platform for the real-time sentiment analysis of public opinion on food safety in China. It collects data from more than 3,100 public sources that can be used to explore public opinion trends, public sentiment, and regional attention differences of food safety incidents. At the same time, we constructed a sentiment classification model using multiple lexicon-based and deep learning-based algorithms integrated with IFoodCloud that provide an unprecedented rapid means of understanding the public sentiment toward specific food safety incidents. Our best model's F1-score achieved 0.9737. Further, three real-world cases are presented to demonstrate the validity of IFoodCloud. IfoodCloud could be considered a valuable tool for promote scientisation of food safety supervision and risk communication, it would benefit governments and policy makers to understand the public opinion on food safety before or during COVID-19. The dataset collected by IFoodCloud is available at https:\/\/github.com\/YingtongDou\/ifoodcloud. We","214":"Prediction in deep learning based on Irregularly Sampled Time Series (ISTS) data was often encountered in the real world scenario, especially after the COVID-19 outbreak. In this paper, we propose a novel Time Encoding (TE) mechanism. TE can embed the time information as time vectors in the complex domain. It has the properties of absolute distance and relative distance under different sampling rates, which helps to represent both the topology of the feature map and the semantics of the corresponding prediction. Experiments were conducted using CIFAR10 dataset. The proposed model was compared with several existing state-of-the-art methods including Prophet, XLNet, LSTM, RNN, and VLN. The results show that TEE performs better than these models in terms of accuracy and F1 score. More importantly, TEE can significantly reduce the number of trainable parameters when compared with more traditional approaches. From the practical point of view, this work demonstrates that making predictions in ISMDS data is easier by exploiting the relations between time and semantic spaces. This may be particularly useful for the prevention of computational complexity in the deep learning algorithms. All codes used in this work are available at https:\/\/github.com\/shervinmin\/","215":"In recent years, several systems have been developed to regulate the spread of negativity and eliminate aggressive, offensive or abusive contents from the online platforms. Nevertheless, a limited number of researches carried out to identify positive, encouraging and supportive contents. In this work, our goal is to identify whether a social media post\/comment contains hope speech or not. We propose three distinct models to identify hope speech in English, Tamil and Malayalam language to serve this purpose. To attain this goal, we employed various machine learning (support vector machines, logistic regression, ensemble), deep learning (convolutional neural network + long short term memory) and transformer (m-BERT, Indic-BERT, XLNet, XLM-Roberta). Results indicate that XLM-Roberta outdoes all other techniques by gaining a weighted $f_1$-score of $0.93$, $0.60$ and $0.85$ respectively for English, Tamil and Malayalam language. Our team has achieved $2^{nd}_{10}$ place on the leaderboard among all the submissions. The proposed models will be useful for designing computer vision based interventions to help users identify positive, encouraging and supportive content in their digital environment. The results are","216":"Graph data have become increasingly common. Visualizing them helps people better understand relations among entities. Unfortunately, existing graph visualization tools are primarily designed for single-person desktop use, offering limited support for interactive web-based exploration and online collaborative analysis. To address these issues, we have developed Argo Lite, a new in-browser interactive graph exploration and visualization tool. Argo Lite enables users to publish and share interactive graph visualizations as URLs and embedded web widgets. Users can explore graphs incrementally by adding more related nodes, such as highly cited papers cited by or citing a paper of interest in a citation network. Argo Lite works across devices and platforms, leveraging WebGL for high-performance rendering. Argo Lite has been used by over 1,000 students at Georgia Tech's Data and Visual Analytics class. Argo Lite may serve as a valuable open-source tool for advancing multiple CIKM research areas, from data presentation, to interfaces for information systems and more. Argo Lite will be made available on GitHub upon acceptance. This article discusses how Argo Lite was developed and shares its lessons learned through four design problems and user studies. We also discuss challenges encountered and promote future work on this underexplored area. Argo Lite may be found at https","217":"Binarized neural networks, or BNNs, show great promise in edge-side applications with resource limited hardware, but raise the concerns of reduced accuracy. Motivated by the complex neural networks, in this paper we introduce complex representation into the BNNs and propose Binary complex neural network -- a novel network design that processes binary complex inputs and weights through complex convolution, but still can harvest the extraordinary computation efficiency of BNNs. To ensure fast convergence rate, we propose novel BCNN based batch normalization function and weight initialization function. Experimental results on Cifar10 and ImageNet using state-of-the-art network models (e.g., ResNet, ResNetE and NIN) show that BCNN can achieve better accuracy compared to the original BNN models. BCNN improves BNN by strengthening its learning capability through complex representation and extending its applicability to complex-valued input data. The source code will be released on GitHub. \\url{https:\/\/github.com\/CVI-SZU\/BCNN}\n          ### Instruction:\nWhich model has more complex architecture, BiXD- NAS orBiXD-NAS?\n\n### Input:\nCOVID","218":"We compare the pseudo-real-time forecasting performance of two factor models for a large set of macroeconomic and financial time series: (i) The standard principal-component model used by Stock and Watson (2002) (ii) The factor model with martingale difference errors introduced by Lee and Shao (2018). The factor model with martingale difference error (FMMDE) makes it possible to retrieve a transformation of the original series so that the resulting variables can be partitioned into distinct groups according to whether they are conditionally mean independent upon past information or not. In terms of prediction, this feature of the model allows us to achieve optimal results (in the mean squared error sense) as dimension reduction is performed. Conducting a pseudo-real-time forecasting exercise based on a large dataset of macroeconomic and financial monthly time series for the U.S. economy, the results obtained from the empirical study suggest that FMMDE performs comparably to SW for short-term forecasting horizons. In contrast, for long-term forecasting horizons, FMMDE displays superior performance. These results are particularly evident for Output&Income, Labor Market, and Consumption sectors. We also find that FMMDE is more flexible in herding behavior across different industries","219":"A thermodynamic framework has been developed for a class of amorphous polymers used in fused deposition modeling (FDM), in order to predict the residual stresses and the accompanying distortion of the geometry of the printed part (warping). When a polymeric melt is cooled, the inhomogeneous distribution of temperature causes spatially varying volumetric shrinkage resulting in the generation of residual stresses. Shrinkage is incorporated into the framework by introducing an isotropic volumetric expansion\/contraction in the kinematics of the body. We show that the parameter for shrinkage also appears in the systematically derived rate-type constitutive relation for the stress. The solidification of the melt around the glass transition temperature is emulated by drastically increasing the viscosity of the melt. In order to illustrate the usefulness and efficacy of the derived constitutive relation, we consider four ribbons of polymeric melt stacked on each other such as those extruded using a flat nozzle: each layer laid instantaneously and allowed to cool for one second before another layer is laid on it. Each layer cools, shrinks and warps until a new layer is laid, at which time the heat from the newly laid layer flows and heats up the bottom layers. The residual","220":"The novel coronavirus (2019-nCoV) is a recently emerged human pathogen that has spread widely since January 2020. Initially, the basic reproductive number R0 was estimated to be 2.2 to 2.7. Here we provide a new estimate of this quantity. We collected extensive individual case reports and estimated key epidemiology parameters, including the incubation period. Integrating these estimates and high resolution real-time human travel and infection data with mathematical models, we estimated that the value of R0 in Mexico City may be between 4.8 and 6.6 day (95% CI: 5.1 - 3.9). In addition, we observed that asymptomatic individuals contribute silently but substantially to the transmission of the disease. They are likely to be tested soon after their symptoms develop, suggesting that COVID-19 testing policies should include them. Finally, we examined the effectiveness of contact tracing on mitigating the outbreak. The results indicate that such measures can help reduce the reproduction number of infectious diseases, particularly for localized outbreaks. Further, the combination of large scale testing and contact tracing can help mitigate the global spreading of the virus. These findings suggest that comprehensive monitoring and decision making regarding the performance of nonpharmaceutical interventions (NPIs) during","221":"We present a novel strategy for learning inertial robotic navigation models. The proposed strategy enhances the generalisability of end-to-end inertial modelling, and is aimed at wheeled robotic deployments. Concretely, the paper describes the following. (1) Using precision robotics, we empirically characterise the effect of changing the sensor position during navigation on the distribution of raw inertial signals, as well as the corresponding impact on learnt latent spaces. (2) We propose neural architectures and algorithms to assimilate knowledge from an indexed set of sensor positions in order to enhance the robustness and generalisability of robotic inertial tracking in the field. Our scheme of choice uses continuous domain adaptation (DA) and optimal transport (OT). (3) In our evaluation, continuous OT DA outperforms a continuous adversarial DA baseline, while also showing quantifiable learning benefits over simple data augmentation. We will release our dataset to help foster future research. Project page: https:\/\/mcgill-nlp.github.io\/inertia_tracking_benchmark; official code: https:\/\/github.com\/mangiel\/continuous_transition_learning\/tree\/main\/Inertia_Tracing.git. This benchmark serves as a","222":"Protein-protein interactions (PPIs) are essentials for many biological processes where two or more proteins physically bind together to achieve their functions. Modeling PPIs is useful for many biomedical applications, such as vaccine design, antibody therapeutics, and peptide drug discovery. Pre-training a protein model to learn effective representation is critical for PPIs. Most pre-training models for PPIs are sequence-based, which naively adopt the language models used in natural language processing to amino acid sequences. More advanced works utilize the structure-aware pre-training technique, taking advantage of the contact maps of known protein structures. However, neither sequences nor contact maps can fully characterize structures and functions of the proteins, especially not all of them. Inspired by this insight, we propose a multimodal protein pre-training model with three modalities: sequence, structure, and function (S2F). Notably, instead of using contact maps to learn the amino acid-level rigid structures, we encode the structure feature with the topology complex of point clouds of heavy atoms. It allows our model to learn structural information about not only the backbones but also the side chains. Moreover, our model incorporates the knowledge from the functional description of proteins extracted from literature or manual annotations. Our experiments","223":"The emergence of novel COVID-19 causing an overload in health system and high mortality rate. The key priority is to contain the epidemic and prevent the infection rate. In this context, many countries are now in some degree of lockdown to ensure extreme social distancing of entire population and hence slowing down the epidemic spread. Further, authorities use case quarantine strategy and manual second\/third contact-tracing process to contain the COVID-19 disease. However, manual tracing is time consuming and labor-intensive task which tremendously overload public healthcare systems. It is also time-consuming to manually trace all these contacts of infected individuals for 24 hours, 365 days, and even longer. Therefore, it is necessary to automatically search for susceptible persons who may be contacted by the patients with COVID-19. Here we propose a supervised machine learning model to find the potential candidates of active cases. Our model mainly consists of two modules - one to identify the subjects via their identity while other to use molecular features to characterize them as susceptible or non-susceptible. In particular, our model adopts a three-fold cross-validation method to train different neural network models with different number of iterations. We present our approach on the dataset released publicly by the government.gov.uk.","224":"The COVID-19 pandemic has caused major disturbance to human life in both everyday and daily lives. An important reason behind the widespread social anxiety is the huge uncertainty about the pandemic. One way to assess how much risk there is from a given dataset is to use it as a point estimation, i.e., to estimate the parameter of a probability model whose distribution matches the data. We propose two ways to construct such models on real data, which we demonstrate with the smallpox outbreak at the Amoy Gardens, Hong Kong, and the severe acute respiratory syndrome coronavirus 2 (SARS) epidemic in China. For each method, we give a numerical solution for the corresponding inverse problem, so that our methods are ready to be applied to other infectious diseases. The first method uses a stochastic differential equation system to derive the parameters of the growth rate in the mean of an infected person. The second method estimates the parameters by using an ensemble of Markov chain Monte Carlo simulations. Our results show that if one wants to infer the parameters of the underlying epidemiological model from noisy observations, then the former approach is better than the latter. Moreover, the inference quality strongly depends on the amount of observed data. If the noise is large or small, the former approach","225":"Healthcare is one of the most important aspects of human life. Heart disease is known to be one of the deadliest diseases which is hampering the lives of many people around the world. Heart disease must be detected early so the loss of lives can be prevented. The availability of large-scale data for medical diagnosis has helped developed complex machine learning and deep learning-based models for automated early detection of heart diseases. The classical approaches have been limited in terms of not generalizing well to new data which have not been seen in the training set. This is indicated by a large gap in training and test accuracies. This paper proposes a novel deep learning architecture using a 1D convolutional neural network for classification between healthy and non-healthy persons to overcome the limitations of classical approaches. Various clinical parameters are used for assessing the risk profile in the patients which helps in early diagnosis. Various techniques are used to avoid overfitting in the proposed network. The proposed network achieves over 97% training accuracy and 96% test accuracy on the dataset. The accuracy of the model is compared in detail with other classification algorithms using various performance parameter which proves the effectiveness of the proposed architecture. The proposed method is generic and may be applied to any kind of chest X-ray image containing more than 2D","226":"Multipath TCP (MPTCP) extends traditional TCP to enable simultaneous use of multiple connection endpoints at the source and destination, while also enabling multicasting and dynamic content deployment in a network context. MPTCP was recently upstreamed to the Linux kernel. In this paper, we provide an in-depth analysis of MPTCPv0 in the Internet and the first analysis of MPTCPv1 to date. We probe the entire IPv4 address space and an IPv6 hitlist to detect MPTCP-enabled systems operational on port 80 and 443. Our scans reveal a steady increase in MPTCPv0-capable IPs, reaching 13k+ on IPv4 (2$\\times$ increase in one year) and 1k on IPv6 (40$\\times$ increase). MPTCPv1 deployment is comparatively low with $\\approx$100 supporting hosts in IPv4 and IPv6, most of which belong to Apple. We also discover a substantial share of seemingly MPTCP-capable hosts, an artifact of middleboxes mirroring TCP options. We conduct targeted HTTP(S) measurements towards select hosts and find that middleboxes can aggressively impact the perceived quality of applications utilizing MPTCP. Finally, we analyze","227":"The COVID-19 pandemic has resulted in tremendous loss of human life and economy in more than 210 countries and territories around the world. While self-protections such as wearing mask, sheltering in place and quarantine polices and strategies are necessary for containing virus transmission, tens of millions people in U.S. have lost their jobs due to the shutdowns of businesses. Therefore, how to reopen the economy safely while the virus is still circulating in population has become a problem of significant concern and importance to elected leaders and business executives. In this study, mathematical modeling is employed to quantify the profit generation and the infection risk simultaneously from the point of view of a business entity. Specifically, an ordinary differential equation model was developed to characterize disease transmission and infection risk. An algebraic equation is proposed to determine the net profit that a business entity can generate after reopening and take into account the costs associated of several protection\/quarantine guidelines. All model parameters were calibrated based on various data and information sources. Sensitivity analyses and case studies were performed to illustrate the use of the model in practice. The results show that the level of testing and quarantining measures affects both the growth rate of infected individuals and the number of protected or hospitalized individuals. Furthermore, it is found that","228":"When an unprecedented infectious disease with high mortality and transmissibility emerges, immediate usage of vaccines or medicines is hardly available. Thus, many health authorities rely on non-pharmaceutical interventions through traceable fixed contacts. However, in reality, there is an additional type of transmission routes to the regular and fixed contacts: the random anonymous infection cases where non-pharmaceutical interventions are hardly feasible. In our study, such realistic situations are implemented by the susceptible-infected-recovered model with isolation on multiplex networks. The multiplex networks are composed of a fixed interaction layer and a layer with time-varying random interactions to represent the different types of disease spreading routes. The multiplex networks represent the combinations of the quenched disorder and annealed disorder. Here, we suggest a preemptive isolation protocol which isolates the second nearest neighbors of the hospitalized individuals and compare it with one of the most popular protocol adopted by many health organizations over the globe. From numerical simulations we find that our preemptive measure significantly reduces both the final epidemic size and the number of the isolated per unit time. Our finding suggests a better non-pharmaceutical intervention which can be adopted to various types of diseases even though the contact tracing is only partially available. This work provides","229":"We propose a novel methodology for estimating the epidemiological parameters of a modified SIRD model (acronym of Susceptible, Infected, Recovered and Deceased individuals) and perform a short-term forecast of SARS-CoV-2 virus spread. We mainly focus on forecasting number of deceased. The procedure was tested on reported data for Poland. For some short-time intervals we performed numerical test investigating stability of parameter estimates in the proposed approach. Numerical experiments confirm the effectiveness of short-term forecasts (up to 2 weeks) and stability of the method. To improve their performance (i.e. computation time) GPU architecture was used in computations. The results show that the developed model provides satisfactory predictions with low computational cost. Such a tool can be employed by health institutions to monitor the progression of COVID-19 disease. We are able to achieve 0.9645 AUC score under the assumption of having enough historical data. This result signifies the potential of using our approach for practical applications. In addition to being able to significantly reduce computational cost, we also provide a further proof of concept with simulated data which indicates the general applicability of our model. The code is publicly available at https:\/\/github.com\/anyleopeace","230":"The COVID-19 pandemic has presented many challenges that have spurred biotechnological research to address specific problems. Diagnostics is one area where biotechnology has been critical. Diagnostic tests play a vital role in managing a viral threat by facilitating the detection of infected and\/or recovered individuals. From the perspective of what information is provided, these tests fall into two major categories, molecular and serological. Molecular diagnostic techniques assay whether a virus is present in a biological sample, thus making it possible to identify individuals who are currently infected. Additionally, when the immune system is exposed to a virus, it responds by producing antibodies specific to the virus. Serological tests make it possible to identify individuals who have mounted an immune response to a virus of interest and therefore facilitate the identification of individuals who have previously encountered the virus. These two groups of immunomodal biomarkers provide different perspectives valuable to understanding the spread of SARS-CoV-2 within human populations. Within this review, we aim to introduce the reader to the techniqueologies of molecular and serological diagnostics with the goal of providing guidance about how they can be used to diagnose Covid-19 through the eyes of researchers. We will also outline some of the current challenges and future directions in this field.","231":"Science has benefited greatly through data communication technologies, and understanding how to process and understand data is essential for building effective AI systems. While explanations of phenomena in machine learning are ubiquitous, examples of explicitly leveraging explanation techniques are comparatively rarer. We explore one such approach in the context of model interpretability, namely the Rashomon effect -- which suggests that explanations arise from confounding variables underlying the decision making process. Using benchmark datasets, we show that the presence of explanatory variables can significantly improve the explainability of model predictions, but only if they are indeed correct. We then demonstrate their effectiveness on real world applications, including in healthcare. Finally, we discuss limitations of the current practice and highlight potential directions for future research. Our work combines mathematical analysis, statistics, and computer science methods with empirical evidence from human-human interactions, and builds upon approaches originally developed in the field of cognitive science. It is often argued that society should rely more on statistical reasoning to truly discover the causes of its behavior, rather than relying on preconceived, ad hoc explanations to achieve this knowledge. However, it is equally important to apply this inquiry to other social domains, including policymaking, political science, and engineering. In addition, while our results mainly focus on the internal workings of complex models, there have been several","232":"This paper investigates a variant of the dial-a-ride problem (DARP), namely Risk-aware DARP (RDARP). Our RDARP extends the DARP by (1) minimizing a weighted sum of travel cost and disease-transmission risk exposure for onboard passengers and (2) introducing a maximum cumulative exposure risk constraint for each vehicle to ensure a trip with lower external exposure. Both extensions require that the risk exposure of each onboard passenger is propagated in a minimized fashion while satisfying other existing constraints of the DARP. To fully describe the RDARP, we first provide a three-index arc-based formulation and reformulate the RDARP into an equivalent min-max trip-based formulation, which is solved by an exact Branch-Cut-and-Price (BCP) algorithm. For each node in the branching tree, we adopt the column generation method by decomposing the problem into a master problem and a subproblem. The latter takes the form of an elementary shortest path problem with resource constraints and minimized maximum risk (ESPPRCMMR). To solve the ESPPRCMMR efficiently, we develop a new labeling algorithm and establish families of resource extension functions in compliance with the risk-related resources. We adopt a real-world paratrans","233":"The survey world is rife with nonresponse and in many situations the missingness mechanism is not at random, which is a major source of bias for statistical inference. Nonetheless, the survey world is rich with paradata that track the data collection process. A traditional form of paradata is callback data that record attempts to contact. Although it has been recognized that callback data are useful for nonresponse adjustment, they have not been used widely in statistical analysis until recently. In particular, there have been a few attempts that use callback data to estimate response propensity scores, which rest on fully parametric models and fairly stringent assumptions. In this paper, we propose a stableness of resistance assumption for identifying the propensity scores and the outcome distribution of interest, without imposing any parametric restrictions. We establish the semiparametric efficiency theory, derive the efficient influence function, and propose a suite of semiparametric estimation methods including doubly robust ones, which generalize existing parametric approaches. We also consider extension of this framework to causal inference for unmeasured confounding adjustment. Application to a Consumer Expenditure Survey dataset suggests an association between nonresponse and high housing expenditures, and reanalysis of Card (1995)'s dataset on the return to schooling shows a smaller effect of education in the overall population","234":"Knowing the true effect size of clinical interventions in randomised controlled trials (RCTs) can provide key information for informing the public health policies regarding intervention strategies and monitoring the adherence to these measures. In RCTs, the assignment of treatment conditions to different populations occurs by chance\"in the wild\". However, this approach ignores the potential outcome measurement error due to unobserved patient states not being recorded during the experiment. In this article, we propose a general framework for using existing data to estimate the variance parameter of an estimator of the average treatment effect on the true outcome explained by all possible prior distributions. We show how to include external data as input when estimating the variance parameter, which may be unavailable or difficult to obtain in some settings. We illustrate two case studies using synthetic data to demonstrate that our method produces reasonable estimation results. Finally, we discuss the practical implications of this work. An R package BayesOrdDesign has been made available at https:\/\/github.com\/annahaensch\/BayesOrdDesign.git.io\/main\/rct-variance-estimation-error\/. Specifically, for interested readers, this R package enables them to derive the variance of their estimated treatment effect from historical data without including any covariates. The resulting variogram","235":"A novel coronavirus disease 2019 (COVID-19) was detected and has spread rapidly across various countries around the world since the end of the year 2020, Computed Tomography (CT) images have been used as one of the key methods for its detection. However, pure manual segmentation of CT images faces a serious challenge with the increase in suspected cases, resulting in urgent requirements for accurate and automatic segmentation of COVID-19 infections. Unfortunately, since the imaging characteristics of the COVID-19 infection are diverse and similar to the backgrounds, existing medical image segmentation methods cannot achieve satisfactory performance. In this work, we try to establish a new deep convolutional neural network tailored for segmenting the chest CT images with COVID-19 infections. We firstly maintain a large and new chest CT image dataset consisting of 165,667 annotated chest CT images from 861 patients with confirmed COVID-19. Inspired by the observation that the boundary of the infected lung can be enhanced by adjusting the global intensity, in the proposed deep CNN, we introduce a feature variation block which adaptively adjusts the global properties of the features for segmenting COVID-19 infection. The proposed FVB can enhance the capability of learning the useful region of interests (","236":"Tropospheric nitrogen dioxide (NO$_2$) concentrations are strongly affected by anthropogenic activities. Using space-based measurements of tropospheric NO$_2$, here we investigate the responses of tropospheric NO$_2$ to the 2019 novel coronavirus (COVID-19) over China, South Korea, and Italy. We find noticeable reductions of tropospheric NO$_2$ columns due to the COVID-19 controls by more than 40% over E. China, South Korea, and N. Italy. The 40% reductions of tropospheric NO$_2$ are coincident with intensive lockdown events as well as up to 20% reductions in anthropogenic nitrogen oxides (NO$_x$) emissions. The perturbations in tropospheric NO$_2$ diminished accompanied with the mitigation of COVID-19 pandemic, and finally disappeared within around 50 days after the starts of control measures over all three nations, providing indications for the start, maximum, and mitigation of intensive controls. This work exhibits significant influences of lockdown measures on atmospheric environment, highlighting the importance of satellite observations to monitor anthropogenic activity changes. Such observations can support policy assessments of air quality and\/or climatic conditions for cost","237":"In which settings we can test and compare various causal effects with confidence intervals has become increasingly important. In this paper, we propose a new approach to estimate the relative causal effect of variables on outcomes using doubly robust estimation methods. Our method allows us to combine different types of data sets; covariates are estimated for one dataset while labels for another are treated as unknown. We demonstrate through simulations that our estimator performs well compared to other commonly used causality algorithms. We then apply it to two datasets to find the effect of tranexamic acid on the mortality rate in major trauma patients and the effect of maternal smoking during pregnancy on birth weight. We also found that the confidence interval for the combined effect did not cover all possible directions. This suggests that there is some room for interpretation of the results. However, since the length of the time series concerned with the second outcome was reduced drastically when modeling the third outcome, we cannot say whether the apportionment of the total effect on these two measures depends on confounding variables. Overall, our work contributes to the development of both novel approaches to balance between model assumptions and clinical practice. We hope they will be useful for future research in the area. The authors recommend estimating the average causal effect at a coarse level (effect summary) instead","238":"The problem of online offensive language limits the health and security of online users. It is essential to apply the latest state-of-the-art techniques in developing a system to detect online offensive language and to ensure social justice to the online communities. Our work investigates the effects of fine-tuning across several Arabic offensive language datasets. We develop multiple classifiers that use four datasets individually and in combination in order to gain knowledge about online Arabic offensive content and classify users comments accordingly. Our results demonstrate the limited effects of transfer learning on the classifiers performance, particularly for highly dialectal comments. Moreover, we show that just applying this transfer method to a basic set of user comments leads to a significant improvement in the classifier performance regarding both original and adversarial attacked tweets. This suggests that simple fine-tuning is a powerful approach for learning complex representations of online Arabic offensive content and classification algorithms are needed. Finally, future research directions should be focused on developing more advanced models, especially with regards to the development of deep neural networks based on long short-term memory (LSTM) blocks, as well as using ensemble learning approaches. The code for our project is available at https:\/\/github.com\/aub-mind\/arabert\/covid19_offensive","239":"Online platforms play a relevant role in the creation and diffusion of false or misleading news. Concerningly, the COVID-19 pandemic is shaping a communication network - barely considered in the literature - which reflects the emergence of collective attention towards a topic that rapidly gained universal interest. Here, we characterize the dynamics of this network on Twitter, analyzing how unreliable content distributes among its users. We find that a minority of accounts is responsible for the majority of the misinformation circulating online, and identify two categories of users: a few active ones, playing the role of\"creators\", and a majority playing the role of\"consumers\". The relative proportion of these groups ($\\approx$14% creators - 86% consumers) appears stable over time: Consumers are mostly exposed to the opinions of a vocal minority of creators, that could be mistakenly understood as of representative of the majority of users. The corresponding pressure from a perceived majority is identified as a potential driver of the ongoing COVID-19 infodemic. These results suggest that social media platforms should prioritize the exposure of consensus-building blocks of information, such as hashtags, images and videos, while promoting exposure of contradictory information. This observation has particular implications for digital marketing strategies, since public figures often share their stories","240":"False assumptions about sex and gender are deeply embedded in the medical system, including that they are binary, static, and concordant. Machine learning researchers must understand the nature of these assumptions in order to avoid perpetuating them. In this perspectives piece, we identify three key challenges for machine learning research on sexual\/gender health disparities: 1) inconsistent definitions of sex and gender; 2) data-driven methods with strong political biases; 3) human factors--sex and gender interactions at multiple levels. We then discuss how these three challenges could inspire future research on both conceptual and practical level differences between male and female researchers. Finally, we provide some potential directions for future work. Our hope is two-fold: (a) Future studies will evaluate the present state of knowledge in recognizing false assumptions about sex and gender; (b) Researchers will produce an evidence-based framework where hypotheses can be tested and explained. For each challenge, we encourage researchers to engage in conversations surrounding their research topic. Furthermore, we warn that terminology used by experts does not always translate into the real world, and sometimes it does not even mean what you think it means. To conclude, we call for a more critical approach to thinking about sex and gender in ML before and during the pandemic. This perspective","241":"The significance of social media has increased manifold in the past few decades as it helps people from even the most remote corners of the world stay connected. With the COVID-19 pandemic raging, social media has become more relevant and widely used than ever before, and along with this, there has been a resurgence in the circulation of fake news and tweets that demand immediate attention. In this paper, we describe our Fake News Detection system that automatically identifies whether a tweet related to COVID-19 is\"real\"or\"fake\", as a part of CONSTRAINT COVID19 Fake News Detection in English challenge. We have used an ensemble model consisting of pre-trained models that has helped us achieve a joint 8th position on the leader board. We have achieved an F1-score of 0.9831 against a top score of 0.9869. Post completion of the competition, we have been able to drastically improve our system by incorporating a novel heuristic algorithm based on username handles and link domains in tweets fetching an F1-score of 0.9883 and achieving state-of-the art results on the given dataset. We also provide a comparison between different deep learning approaches for the given problem. The code and data used can be found at","242":"As the COVID-19 outbreak evolves around the world, the World Health Organization (WHO) and its Member States have been heavily relying on staying at home and lock down measures to control the spread of the virus. In the last weeks, various signs showed that the COVID-19 curve was flattening, but even the partial lifting of some containment measures (e.g., school closures and telecommuting) appear to favor a second wave of the disease. The accurate evaluation of possible countermeasures and their well-timed revocation are therefore crucial to avoid future waves or reduce their duration. In this paper, we analyze patient and route data of infected patients from January 20, 2020, to May 31, 2020, in four US states: California, Florida, New York State, and Washington. We characterize each dataset with the help of medical professionals analyzing the data and estimating the parameters of epidemiological models. For each state, we estimate the number of infected individuals during the first week based on the current daily report by the Centers for Disease Control and Prevention (CDC). Finally, we evaluate six different strategies to mitigate the effects of social distancing including wearing masks, sheltering in place, quarantine of all nonessential businesses, and testing capacity. To the best of our knowledge","243":"The emergence of novel COVID-19 causing an overload in health system and high mortality rate. The key priority is to contain the epidemic and prevent the infection rate. In this context, many countries are now in some degree of lockdown to ensure extreme social distancing of entire population and hence slowing down the epidemic spread. Further, authorities use case quarantine strategy and manual second\/third contact-tracing to contain the COVID-19 disease. However, manual contact tracing is time consuming and labor-intensive task which tremendously overload public health systems. In this paper, we developed a smartphone-based approach to automatically and widely trace the contacts for confirmed COVID-19 cases. Particularly, contact-tracing approach creates a list of individuals in the vicinity and notifying contacts or officials of confirmed COVID-19 cases. This approach is not only providing awareness to individuals they are in the proximity to the infected area, but also tracks the incidental contacts that the COVID-19 carrier might not recall. Thereafter, we developed a dashboard to provide a plan for government officials on how lockdown\/mass quarantine can be safely lifted, and hence tackling the economic crisis. The dashboard used to predict the level of lockdown area based on collected positions and distance measurements of the registered users in the vicinity","244":"The future dynamics of the Corona Virus Disease 2019 (COVID-19) outbreak in African countries is largely unclear. Simultaneously, required strengths of intervention measures are strongly debated because containing COVID-19 in favor of the weak health care system largely conflicts with socio-economic hardships. Here we analyze the impact of interventions on outbreak dynamics for South Africa, exhibiting the largest case numbers across sub-saharan Africa, before and after their national lockdown. Past data indicate strongly reduced but still supracritical growth after lockdown. Moreover, large-scale agent-based simulations given different future scenarios for the Nelson Mandela Bay Municipality with 1.14 million inhabitants, based on detailed activity and mobility survey data of about 10% of the population, similarly suggest that current containment may be insufficient to not overload local intensive care capacity. Yet, enduring, slightly stronger or more specific interventions, combined with sufficient compliance, may constitute a viable option for interventions for South Africa. A key question then remains whether existing interventions can be effective against new variants of COVID-19 emerging from various regions of the continent. To answer this question, we combine our estimates into a comprehensive computational framework which shows that no single intervention strategy nor combination of strategies involving early detection and prompt isolation of infected individuals suffices","245":"The emergence of unmanned aerial vehicles (UAVs) has increased in recent years. UAVs can accomplish complex or dangerous tasks in a reliable and cost-effective way but are still limited by power consumption problems, which pose serious constraints on the flight duration and completion of energy-demanding tasks. The possibility of providing UAVs with advanced decision-making capabilities in an energy-effective way would be extremely beneficial. In this paper, we propose a practical solution to this problem that exploits deep learning on the edge. The developed system integrates an OpenMV microcontroller into a DJI Tello Micro Aerial Vehicle (MAV). The microcontroller hosts a set of machine learning-enabled inference tools that cooperate to control the navigation of the drone and complete a given mission objective. The goal of this approach is to leverage the new opportunistic features of TinyML through OpenMV including offline inference, low latency, energy efficiency, and data security. The approach is successfully validated on a practical application consisting of the onboard detection of people wearing protection masks in a crowded environment. Our results show that the proposed system provides satisfactory performance levels while significantly reducing the energy consumed and limiting the number of false alerts. The successful development of our proposal illustrates its feasibility and scalability via considering different objectives","246":"Our earlier publications showed semantic tablesau admits partial exceptions to the Second Incompleteness Theorem where a formalism recognizes its self consistency and views multiplication as a 3-way relation (rather than as a total function). We now show these boundary-case evasions will collapse if the Law of the Excluded Middle is violated by any finite additive group on a number of variables. This is done by describing the action of the monodromy operator $N$ on a 1-categorical space whose objects are those of $\\mathbb{Z}^2$. Our proof assumes less restriction on such spaces, and so it can be stated in more detail about how far one may extend this technique beyond simple cases. We also prove that all modular knots around the missing trefoil in $\\mathbb{R}^3$ belong to this class. As a main technical ingredient we present an almost complete picture of the types of singular orbits for these knots, including some with multiplicity greater than three. Finally we exhibit examples of their use during epidemics, namely a generalized version of COVID-19 and an example of an activated escape from Milan. All along our proofs are constructive and there is no particular assumption which establishes when and why a given manifold has curvature","247":"Human resource management technologies have moved from biometric surveillance to emotional artificial intelligence (AI) that monitor employees' engagement and productivity, analyze video interviews and CVs of job applicants. The rise of the US$20 billion emotional AI industry will transform the future workplace. Yet, besides no international consensus on the principles or standards for such technologies, there is a lack of cross-cultural research on future job seekers' attitude toward such use of AI technologies. This study collects a cross-sectional dataset of 1,015 survey responses of international students from 48 countries and 8 regions worldwide. A majority of the respondents (52%) are concerned about being managed by AI. Following the hypothetico-deductivist philosophy of science, we use the MCMC Hamiltonian approach and conduct a detailed comparison of 10 Bayesian network models with the PSIS-LOO method. We consistently find having a higher income, being male, majoring in business, and\/or self-rated familiarity with AI correlate with a more positive view of emotional AI in the workplace. There is also a stark cross-cultural and cross-regional difference. Our analysis shows people from economically less developed regions (Africa, Oceania, Central Asia) tend to exhibit less concern for AI managers. And for","248":"We investigate how memorization can facilitate learned language models. To this end, we present \\spear, an unsupervised approach to improve memory efficiency of neural network architectures by incorporating a stochastic strategy into their optimization procedure. The proposed method enhances the generalizability of CNNs for predicting, on Chest X-rays images (Task 1), as well as the natural language understanding of clinical doctors in real clinics (Task 2). Our results show that when using \\spear, the learned embeddings achieve significantly better performance than standard BPNN and RNN for image classification tasks such as Visual Geometry Group Network (VGG) and ResNet50. We also demonstrate that \\spear improves the learned language model's robustness on unseen data points, with almost no drop in accuracy compared to the baseline. These results suggest that \\spear could be a valuable addition to the arsenal of cognitive augmentation methods. Finally, we show that our code is freely available at https:\/\/github.com\/robi56\/spear.git.io\/. All modifications are incorporated via post-processing tools. Code is made publicly available at https:\/\/github.com\/zliucr\/spear-dataset. This dataset is released under the guidance and","249":"The purpose of our paper is to analyze the historical series of price returns for one-dimensional (1D) commodity markets starting with the emergence of the COVID-19 pandemic. In particular, we focus on studying the evolution from January 2018 to December 2020 in terms of three distinct periods of market regimes: before the onset of the COVID-19 pandemic, which was characterized by an abrupt and sizable disanchoring of the expectations; during the pandemic, which was characterized by a gradual separation of the expectation values into two widely used market indices, namely, the Dow Jones Industrial Average and the S&P 500; finally, after the outbreak of the COVID-19 pandemic, which was characterized by a further amplification of the expected values, the US stock index suffered from a considerable fall while both the Dow Jones Industrial Average and the S&P500 gained back their pre-pandemic performances. Our results indicate that there is no evidence that the cumulative number of confirmed cases and deaths affected the expected values in those markets during the current pandemic. We also show that the fear sentiment exacerbates such risk dynamics. To the best of our knowledge, this is the first systematic study applying machine learning techniques to understand the economic complexity arising from the pandemic in","250":"In this work we demonstrate how to automate parts of the infectious disease-control policy-making process via performing inference in existing epidemiological models. The kind of inference tasks undertaken include computing the posterior distribution over controllable, via direct policy-making choices, simulation model parameters that give rise to acceptable disease progression outcomes. Among other things, we illustrate the use of a probabilistic programming language that automates inference in existing simulators. Neither the full capabilities of this tool for automating inference nor its utility for planning is widely disseminated at the current time. Timely gains in understanding about how such simulation-based models and inference automation tools applied in support of decision making could lead to less economically damaging policy prescriptions, particularly during the current COVID-19 pandemic. These results are broadly applicable as they relate to sufficiently simple, yet important, public health policies under study (e.g., testing, social distancing, etc.), which have significant implications for cost-effective preventive measures. We argue that probabilistic program languages are well suited to supporting these research goals. Indeed, they can describe the counterfactual dynamics of contagion processes on complex networks by means of Monte Carlo simulations and enable the design of efficient interventions via Bayesian optimization. In particular, their flexibility","251":"In this paper, we present two types of sensitivity analysis for the effective field theory model to analyze the strong couplings calculated from the $B$-mode polarization in the presence of finite regions within the Fock space and compute their phase diagrams with given reference to the input parameters. For both the cases considered, we observe that the inclusion of infinitesimal regions results in the loss of valuable information due to artificially induced non-linearity in the equations of the macroscopic fluctuation operator (MFO). In order to reduce the impact of such effects, we consider the following approach which consists in the calculation of values for the MFO parameter at a multiple scattering limit while keeping the value of the other parameters fixed. We test the method by analyzing the properties of the equilibrium points as well as the stability of these points through perturbations in the original Hamiltonian. From our calculations, we conclude that the LHC data may be used to constrain the equation parameters and therefore the predicted phase diagram of the three-dimensional hybridized BFT-based scalar QED may be observed in an experiment. [abridged] The use of the obtained formalism based on power functional theory allows us to reinterpreted some fundamental experimental observations, e.g.,","252":"To address the massive spike in uncertainties triggered by the coronavirus disease (COVID-19), there is an ever-increasing number of national governments that are rolling out contact-tracing Apps to aid the containment of the virus. The first hugely contentious issue facing the Apps is the deployment framework, i.e. centralized or decentralized. Based on this, the debate branches out to the corresponding technologies that underpin these architectures, i.e. GPS, QR codes, and Bluetooth. This work conducts a pioneering review of the above scenarios and contributes a geolocation mapping of the current deployment. The Apps vulnerabilities and the directions of research are identified, with a special focus on the Bluetooth-inspired decentralized paradigm. Several challenges are faced and open issues regarding the future development of such apps, including but not limited to the currently deployed Google\/Apple Exposure Notification (GAEN) protocol. Finally, the potential positive impact of these tools in the context of both their adoption and misuse is also discussed. For each of these threats, the role of cryptography and hybrid security approaches is analyzed, based on which we propose a generalized encryption scheme for federated learning (FFL) networks, using TLS certificates and smart contracts. We develop a blockchain-based solution for the secure and efficient use of","253":"We present automatically parameterised Fully Homomorphic Encryption (FHE), for encrypted neural network inference. We present and exemplify our inference over FHE compatible neural networks with our own open-source framework and reproducible step-by-step examples. We use the 4th generation Cheon, Kim, Kim and Song (CKKS) FHE scheme over fixed points provided by the Microsoft Simple Encrypted Arithmetic Library (MS-SEAL). We significantly enhance the usability and applicability of FHE in deep learning contexts, with a focus on the constituent graphs, traversal, and optimisation. We find that FHE is not a panacea for all privacy preserving machine learning (PPML) problems, and that certain limitations still remain, such as model training. However we also find that in certain contexts FHE is well suited for computing completely private predictions with neural networks. The ability to privately compute sensitive problems more easily, while lowering the barriers to entry, can allow otherwise too-sensitive fields to begin advantaging themselves of performant third-party neural networks. Lastly we show encrypted deep learning, applied to a sensitive real world problem in agri-food, and how this can have a large positive impact on food-waste and encourage much-needed","254":"The outbreak of COVID-19 has caused lasting damage to almost every country in the world, leading to one of the most challenging crisis across the globe since World War II. The control of epidemic spreading is essential to avoid overwhelming of healthcare systems as well as huge loss of precious life. Data driven approaches for forecasting the evolution of epidemics are crucial for making decisions on how to manage public health and implement actionable protocols. This paper presents a deep learning model based approach to forecast the spreading trend of SARS-CoV-2 in Iran from February 25, 2020, to May 24, 2020. We use publicly available data at the daily national level for all of the mentioned regions and also state-level data for all the counties in these regions. Our model incorporates demographic information and geographical features into a Gated Recurrent Unit (GRU) structure. In addition, we employ different measures to learn temporal dependencies while embedding such information into our model. Results show that the proposed model achieved superior performance compared with other forecasting techniques. The GRU structure embedded in our model can capture the dynamics of time dependent transmission of the disease which helps us make better predictions. Furthermore, we encode the fact that each region is characterized by different patterns of infections, thereby providing a measure of uncertainty","255":"We present the first full description of Media Cloud, an open source platform based on crawling hyperlink structure in operation for over 10 years, that for many uses will be the best way to collect data for studying the media ecosystem on the open web. We document the key choices behind what data Media Cloud collects and stores, how it processes and organizes these data, and its open API access as well as user-facing tools. We also highlight the strengths and limitations of the Media Cloud collection strategy compared to relevant alternatives. We give an overview two sample datasets generated using Media Cloud and discuss how researchers can use the platform to create their own datasets. Finally, we explain our own future research directions which we hope others will explore. All collected data, code, and results are available at https:\/\/github.com\/ashishrana160796\/Media_Cloud\/. Further, extensive documentation of the technical details and performance of the Media Cloud APIs is available at https:\/\/dashabiox.org\/s\/hc4fdx133vht0qb\/ACDC_MEDICI. In addition to the online streaming of the lectures, we provide a real-time video conferencing tool for accessing the labs, recording audio clips, and creating interactive visualizations","256":"The article below is based on lectures delivered to new students remotely in the course of orientation. It presents the quantum theory tree from its inception a century ago till today. The main focus is on the nuclear physics - HEP branch. Although it can be claimed that this was indeed the original motivation, we will discuss here for two reasons. Firstly, the framework has been growing rapidly over the past thirty years or so and secondly, because of the COVID-19 pandemic, which has led to an abrupt change in teaching methods changing greatly both the subject matter and the way people teach mathematics together. We explain each facet of our approach and comment on how it could contribute to better education going forward. Although we cannot hope to ever carry out the same level of teaching in practice, we wish that along with this contribution, other branches of physics (e.g. particle phenomenology) would also come into play. This contribution is therefore intended as a ``playable knowledge footprint\"in the form of a learning tool to be used by physicists in their classroom. More generally, these discussions are meant to serve as a basis for further research in the field of physical interactions and for future use by physicists in their classes. In particular, since one of the most difficult problems in solving is","257":"Even if laboratory practice is essential for all scientific branches of knowledge, it is often neglected at High School, due to lack of time and\/or resources. To establish a closer contact between school and experimental sciences, the University Sapienza of Roma and the Istituto Nazionale di Fisica Nucleare (INFN) launched the Lab2Go project, with the goal of spreading laboratory practice among students and teachers in high schools. In this article, we present how the authors aim to develop a close relationship between school and experimental sciences through a case study of secondary school students who participated in the lab2go project during social activities. The whole process takes place within a framework of educational methodology adapted to the needs of teaching laboratories. We discuss each step of the process and provide a detailed description of the content of the activities done by us in the classroom. We also report on the various problems students have and show the difficulties faced by them while participating in the lab2go activities. The participation of such students has been positively regarded by the teacher side, who considers their behavior as exemplary of what should be done in a similar way to help students in the real world. The problems discussed here are not limited to the sphere of science but can also concern the humanities","258":"The increasing availability of curated citation data provides a wealth of resources for analyzing and understanding the intellectual influence of scientific publications. In the field of statistics, current studies of citation data have mostly focused on the interactions between statistical journals and papers, limiting the measure of influence to mainly within statistics itself. In this paper, we take the first step towards understanding the impact statistics has made on other scientific fields in the era of Big Data. By collecting comprehensive bibliometric data from the Web of Science database for selected statistical journals, we investigate the citation trends and compositions of citing fields over time to show that their diversity has been increasing. Furthermore, we use the local clustering technique involving personalized PageRank with conductance for size selection to find the most relevant statistical research area for a given external topic of interest. We provide theoretical guarantees for the procedure and, through a number of case studies, show the results from our citation data align well with our knowledge and intuition about these external topics. Overall, we have found that the statistical theory and methods recently invented by the statistics community have made increasing impact on other scientific fields. This study opens up new possibilities for investigating the impact of statistics on other science during its evolution period. The dataset used in this article contains enough data to fill out the required knowledge about","259":"Coronavirus disease 2019 (COVID-19) has caused great casualties and becomes almost the most urgent public health events worldwide. Computed tomography (CT) is a significant screening tool for COVID-19 infection, and automated segmentation of lung infection in COVID-19 CT images will greatly assist diagnosis and health care of patients. However, accurate and automatic segmentation of COVID-19 lung infections remains to be challenging. In this paper we propose a dilated dual attention U-Net (D2A U-net) for COVID-19 lesion segmentation in CT slices based on dilated convolution and a novel dual attention mechanism to address the issues above. We introduce a dilated convolution module in model decoder to achieve large receptive field, which refines decoding process and contributes to segmentation accuracy. Also, we present a dual attention mechanism composed of two attention modules which are inserted to skip connection and model decoder respectively. The dual attention mechanism is utilized to refine feature maps and reduce semantic gap between different levels of the model. The proposed method has been evaluated on open-source dataset and outperforms cutting edges methods in semantic segmentation. Our proposed D2A U-net with pretrained encoder achieves a Dice score","260":"The veracity of data posted on social media has in recent years been a subject of intensive study, especially with regards to the Portuguese elections' outcomes. In this work, we address the problem of automatically extracting features from Twitter streams by means of sequence-to-sequence deep learning techniques. We first propose a feature extraction algorithm based on word embeddings and k-NN clustering, which identifies the topics represented in each cluster. Then, we design two different vector representation learners to construct high-quality classifiers, using both machine learning and deep learning approaches. The choice of these classifiers enables us to also incorporate the temporal dynamics of the information flow, by considering the sentiment of the tweets published at each time step. Our evaluation shows very promising results when compared with state-of-the-art methods, using topic coherence measures. This suggests that our methodology can be used to produce valuable information flow analysis indicators while respecting privacy concerns. Such analyses can be useful in the case of online debates, or other types of political events, where it might not be possible to store all the relevant information in an election's history. Finally, as far as we know, this is the first large-scale pre-trained language model ensemble applied to the Portuguese Twittersphere.","261":"After the considerable excitement caused by COVID-19 and the first telework measures, many management issues became apparent and some questions quickly arose, especially about efficiency of employees' work and conditions for successful telework adoption. This study focuses on the following questions: what is the new interest to telework for employees and what are the potential reasons for this? How much employees feel able to do their work remotely now and why? To answer these questions, we conducted a questionnaire over several weeks, that involved employees coming from different industries (N=170), in order to collect their experience, skills and motivations for teleworking. The results show that adoption of telework is real for the respondents. Those who have experienced it present a strong motivation and a real capacity to use it, with almost no technological barriers. The theoretical model that was used, based on Technology Acceptance Models (TAM) theory, has highlighted an important new factor of telework adoption: time saved in commuting. The study points out that adoption of telework could be sustainable for both public and private organizations, and this requires a critical examination and discussion. Finally, the results provide implications for telework adoption for other researchers, including managers, policymakers and developers. Keywords: Telework; Social distancing; Effective","262":"The continuously growing number of scientific papers requires document organization methods to identify relevant information. In this paper, we expand upon our previous work with clustering the CORD-19 dataset by applying multi-dimensional analysis methods. Tensor factorization is a powerful unsupervised learning method capable of discovering hidden patterns in a document corpus. We show that a higher-order representation of the corpus allows for the simultaneous grouping of similar articles, relevant journals, authors with similar research interests, and topic keywords. These groupings are identified within and among the latent components extracted via tensor decomposition. We further demonstrate the application of this method with a publicly available interactive visualization of the dataset. https:\/\/cord19.disciplinarity.org\/s\/hc4fdx133vht0qb\/ACM_MM2021_Supply_Demand.pdf?dl=0}$. This article presents methodological details for the construction and analysis of the Wasserstein Effectance Score (WES) applied to the PubMedBERT model. The SARS-CoV-2 segmentation was carried out using the BioBERT model on the downstream COVID-19 clusters from the National Institutes of Health (NIH) ExPORTER, Task 1a.","263":"Novel and high-performance medical image classification pipelines are heavily utilizing ensemble learning strategies. The idea of ensemble learning is to assemble diverse models or multiple predictions and, thus, boost prediction performance. However, it is still an open question to what extent as well as which ensemble learning strategies are beneficial in deep learning based medical image classification pipelines. In this work, we proposed a reproducible medical image classification pipeline for analyzing the performance impact of the following ensemble learning techniques: Augmenting, Stacking, and Bagging. The pipeline consists of state-of-the-art preprocessing and image augmentation methods as well as 9 deep convolution neural network architectures. It was applied on four popular medical imaging datasets with varying complexity. Furthermore, 12 pooling strategies for combining multiple predictions were analyzed, ranging from simple statistical functions like unweighted averaging up to more complex learning-based functions like support vector machines. Our results revealed that Stacking achieved the largest improvement in accuracy of up to 13% F1 score increase while Augmenting showed consistent improvements capabilities by up to 4% and is also applicable to single model based pipelines. Cross-validation based Bagging demonstrated significant performance gain close to Stacking, which resulted in an F1 score increase up to +11%. Furthermore","264":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\nWhat should NASA do to ensure a more inclusive and sustainable planetary science research community?\n### Input:\nCOVID Super Expert\n### Response: We review some key issues pertaining to NASA's Research and Analysis programs, and offer recommended actions to mitigate or resolve these issues. In particular, we recommended that NASA increases funding to support a healthy selection rate (~40%) for R&A programs, which underpin much scientific discovery with NASA mission data, and on which the majority of the U.S. planetary science community relies (either in part or wholly). We also recommend additional actionable steps NASA can take to ensure a more equitable and sustainable planetary science research community in the U.S., including supporting the next generations of planetary researchers, working to minimize biases in peer review, and reducing the burden of scientists as they prepare R&A proposals. These recommendations are broadly applicable to any major space agency interested in providing R&A support, and we hope they will be used by other institutions, to improve the health of their planetary science research communities. Finally, this article reminds us how our lack of engagement with COVID-19 has affected the way we act, interact, and move around in the world. This pandemic serves as a grim reminder of the unexpected nature of these outbreaks and gives rise to a unique set of research opportunities across many fields, not only within the scientific community but also in the adjacent socio-economic sectors","265":"The COVID-19 pandemic has caused more than 8 million confirmed cases and 500,000 death to date in 212 countries. In response to this emergency, many countries have introduced a series of social-distancing measures including lockdowns and businesses' temporary shutdowns, in an attempt to curb the spread of the infection. Accordingly, the pandemic has been generating unprecedent disruption on practically every aspect of society. This paper demonstrates that high-frequency electricity market data can be used to estimate the causal, short-run impact of COVID-19 on the economy. In the current uncertain economic conditions, timeliness is essential. Unlike official statistics, which are published with a delay of a few months, with our approach one can monitor virtually every day the impact of the containment policies, the extent of the recession and measure whether the monetary and fiscal stimuli introduced to address the crisis are being effective. We illustrate our methodology on daily data for the Italian day-ahead power market. Not surprisingly, we find that the containment measures caused a significant reduction in economic activities and that the GDP at the end of in May 2020 was still about 11 percent lower that what would have been without the outbreak. However, since April 2020, the recovery process seems to be flattening and the","266":"We develop a mathematical framework to study the economic impact of infectious diseases by integrating epidemiological dynamics with a kinetic model of wealth exchange. The multi-agent description leads to study the evolution over time of a system of kinetic equations for the wealth densities of susceptible, infectious and recovered individuals, whose proportions are driven by a classical compartmental model in epidemiology. Explicit calculations show that the spread of the disease seriously affects the distribution of wealth, which, unlike the situation in the absence of epidemics, can converge towards a stationary state with a bimodal form. Furthermore, simulations confirm the ability of the model to describe different phenomena characteristics of economic trends in situations compromised by the rapid spread of an epidemic, such as the unequal impact on the various wealth classes and the risk of a shrinking middle class. A last part is dedicated to fit numerical solutions of the proposed model with experimental data coming from different European countries. Good agreement between the theoretical predictions and real data confirms the ability of the model to describe a broad variety of non-linear physical, mechanical and natural systems. All of these examples highlight the importance of a careful approach when dealing with complex models of interacting agents or multiple species. This work shows that combining a dynamic description of the interactions among agents with a kinetic modeling of wealth","267":"In this brief report we propose a new design of a medical mask with a plasma layer, which provides both additional air filtration from microdrops, bacteria and viruses due to the electrostatic effect and self-disinfecting of surfaces by a pulsed barrier discharge. The key features of the mask are the mutual arrangement of the layers, the direction of air flows and the synchronization of the discharge with respiration, which ensures the safe wearing of the mask and high degree of protection against pathogenic microorganisms. The experimental results indicate that the proposed mask can significantly reduce droplets count, especially in areas where there is a major shortage of masks available for human use. To further improve the performance, the interior of the mask can be spray-painted with chrome paint and then decontaminated and reused as part of the daily routine - a cleaning event. Additionally, the front flow through jet of the mask creates a mist screen that reduces the transmission of airborne pathogens. The overall effectiveness of the proposed mask has been demonstrated and approved by the World Health Organization (WHO) for protecting people during the COVID-19 pandemic. The mask is designed to be disposable single-use device that would remain usable by more than one person after their death. Moreover, it is so lightweight","268":"Social distancing has been proven as an effective measure against the spread of infectious COronaVIrus Disease 2019 (COVID-19). However, individuals are not used to tracking the required 6-feet (2-meters) distance between themselves and their surroundings. An active surveillance system capable of detecting distances between individuals and warning them can slow down the spread of the deadly disease. Furthermore, measuring social density in a region of interest (ROI) and modulating inflow can decrease social distancing violation occurrence chance. On the other hand, recording data and labeling individuals who do not follow the measures will breach individuals' rights in free-societies. Here we propose an Artificial Intelligence (AI) based real-time social distancing detection and warning system considering four important ethical factors: (1) the system should never record\/cache data, (2) the warnings should not target the individuals, (3) no human supervisor should be in the detection\/warning loop, and (4) the code should be open-source and accessible to the public. Against this backdrop, we propose using a monocular camera and deep learning-based real-time object detectors to measure social distancing. If a violation is detected, a non-intrusive audio-visual","269":"Change point detection in time series has attracted substantial interest, but most of the existing results have been focused on detecting change points in the time domain. This paper considers the situation where nonlinear time series have potential changepoints in the state domain. We apply a density-weighted anti-symmetric kernel function to the state domain and therefore propose a nonparametric procedure to test the existence of changepoints. When the existence of changepoints is affirmative, we further introduce an algorithm to estimate the number of changepoints together with their locations. Theoretical results of the proposed detection and estimation procedures are given and a real dataset is used to illustrate our methods. Our experimental results demonstrate that our method can successfully detect and localise changepoints in linear and nonlinear time series. Further, our hierarchical multi-stage approach achieves a better performance than other popular methods for detecting changepoints in complex networks. Finally, we discuss some limitations of our methods and encourage efforts towards future research directions. In particular, it is assumed that if one wants to detect a set of changepoints which exist in the state domain, then they will be detected in the second stage; however, if one wants to locate these changepoints after having detected them, then they will be located in the third stage.","270":"Non-contact technology for monitoring multiple people's vital signs, such as respiration and heartbeat, has been investigated in recent years due to the rising cardiopulmonary morbidity, the risk of transmitting diseases, and the heavy burden on the medical staff. Frequency modulated continuous wave (FMCW) radars have shown great promise in meeting these needs. However, contemporary techniques for non-contact vital signs monitoring (NCVSM) via FMCW radars, are based on simplistic models, and present difficulties coping with noisy environments containing multiple objects. In this work, we develop an extended model of FMCW radar signals in a noisy setting containing multiple people and clutter. By utilizing the sparse nature of the modeled signals in conjunction with human-typical cardiopulmonary features, we can accurately localize humans and reliably monitor their vital signs, using only a single channel and a single-input-single-output setup. To this end, we first show that spatial sparsity allows for both accurate detection of multiple people and computationally efficient extraction of their Doppler samples, using a joint sparse recovery approach. Given the extracted samples, we develop a method named Vital Signs Based Dictionary Recovery (VSDR), which uses a dictionary-based approach to search for","271":"The current push towards interoperability drives companies to collaborate through process choreographies. At the same time, they face a jungle of continuously changing regulations, e.g., due to the pandemic and developments such as the BREXIT, which strongly affect cross-organizational collaborations. Think of, for example, supply chains spanning several countries with different and maybe even conflicting COVID19 traveling restrictions. Hence, providing automatic compliance verification in process choreographies is crucial for any cross-organizational business process. A particular challenge concerns the restricted visibility of the partner processes at the presence of global compliance rules (GCR), i.e., rules that span across the process of several partners. This work deals with the question how to verify global compliance if affected tasks are not fully visible. Our idea is to decompose GCRs into so called assertions that can be checked by each affected partner whereby the decomposition is both correct and lossless. The algorithm exploits transitivity properties of the underlying rule specification, and its correctness and complexity are proven, considering advanced aspects such as loops. The algorithm is implemented in a proof-of-concept prototype, including a model checker for verifying compliance. The applicability of the approach is further demonstrated on a real-world manufacturing use case. We","272":"This paper uses new and recently introduced mathematical techniques to undertake a data-driven study on the systemic nature of global inflation. We start by investigating country CPI inflation over the past 70 years. There, we highlight the systemic nature of global inflation with a judicious application of eigenvalue analysis and determine which countries exhibit most\"centrality\"with an inner-product based optimization method. We then turn to inflationary impacts on financial market securities, where we explore country equity indices' equity robustness and the varied performance of equity sectors during periods of significant inflationary pressure. Finally, we implement a time-varying portfolio optimization to determine which asset classes were most beneficial in increasing portfolio Sharpe ratio when an investor must hold a core (and constant) allocation to equities. Our results show how macroeconomic conditions affect the distributional properties of equity markets and demonstrate the importance of undertaking such studies through statistical models of price and economic uncertainty. This has important implications for cost-conscious retail investors seeking broad diversification across equity markets. Moreover, our approach can help policymakers devise more effective monetary policy via an optimal portfolio allocation scheme. Numerical experiments are undertaken to further illustrate our findings. To improve the effectiveness of this strategy, it is recommended that investment decisions should include the community spread","273":"Incidence - Cumulative Cases (ICC) curves are introduced and shown to provide a simple framework for parameter identification in the case of the most elementary epidemiological model, consisting of susceptible, infected, and removed compartments. This novel methodology is used to estimate the basic reproduction number $R_0$ during the ongoing COVID-19 epidemic, including its sub-regions with different infection rates or when interventions were implemented by social distancing measures. The estimation procedure is carried out through the use of algorithms based on the wavelet transform technique, which provides both qualitative and quantitative information about the dynamics of the disease. The quality of the estimated parameters is discussed using available data from European countries. Two different applications for this methodology are presented. First, it is applied to identify the effective reproduction number $R_t$, the daily new infectious cases and the total number of casualties as a function of time. Second, we study the effectiveness of social distancing measures, such as school closures and restrictions on mass gatherings, on mitigating the spread of the virus. As a sample, we consider the case of Italy. We find that a selective isolation of symptomatic patients does not lead to a repeal of the social distancing measure, whereas the addition of asymptomatic individuals can","274":"We study volatility prediction under weak assumptions for both Markov and non-Markov processes. We propose two new models that can capture the relevant dynamics of volatility, without using any information about the underlying asset prices. The first model uses constant parameters throughout the time period while the second one varies with time according to a given continuous probability measure. We conduct experiments on real data from the S&P 500 and Nikkei 225 indices respectively, and we show that our models outperform other existing methods. Moreover, we find interesting applications in derivatives books (a proxy for the value at risk) and stochastic volatilities (a surrogate for the variance). A third model which does not require linear-Gaussian random-walk factor structures applies to predictors of stochastic volatility, similarly as the current approach, however with less memory and computational power. This model successfully captures plausible historical volatility patterns across different times scales via the flexible framework. Our results demonstrate the strong potentials of neural networks for learning volatile price dynamics, and we suggest that more general forms of physics-informed volatility modeling are also amenable. Finally, we implement our forecasting algorithms as open-source software, along with the corresponding code. To facilitate future research, we have made our datasets publicly available at https","275":"Agent-Based Models (ABM) are computational scenario-generators, which can be used to predict the possible future outcomes of the complex system they represent. To better understand the robustness of these predictions, it is necessary to understand the full scope of the possible phenomena the model can generate. Most often, due to high-dimensional parameter spaces, this is a computationally expensive task. Inspired by ideas coming from systems biology, we show that for multiple macroeconomic models, including an agent-based model and several Dynamic Stochastic General Equilibrium (DSGE) models, there are only a few stiff parameter combinations that have strong effects, while the other sloppy directions are irrelevant. This suggest an algorithm that efficiently explores the space of parameters by primarily moving along the stiff directions. We apply our algorithm to a medium-sized agent-based model, and show that it recovers all possible dynamics of the unemployment rate. The application of this method to Agent-based Models may lead to a more thorough and robust understanding of their features, and provide enhanced parameter sensitivity analyses. Several promising paths for future research are discussed. All code is available on GitHub at https:\/\/github.com\/gbriel21\/agent_model_factory.gipplab.org\/wiki\/Main","276":"We study dynamics of a variant of the SIR the model, where we assume that individuals may be infected by disease independently with some fixed probability. Furthermore, we allow for heterogeneity in the susceptibility to infection and recovery rates through a random parameterization of the effective contact rate matrix. Using a large population approximation, we show that this model has a rich set of features. Specifically, the epidemic curves can be flattened around any non-zero value of the basic reproduction number $R_0$. We also show that the long-time behavior of the epidemic curves depends on the shape parameters $\\beta$ and $\\pi$, as well as on the heterogeneity in the susceptibility to infection and recovery rates. Our results are compared with those obtained from a mean-field approach and numerical simulations using a stochastic differential version of the model. Focusing on models with homogeneous Susceptible-Infected-Recovery (SIR) dynamics, we present a thorough analysis of the average evolution of the COVID-19 pandemic in terms of the susceptible, infected and recovered populations. The agreement between the deterministic and simulation-based estimates allows to obtain reliable information about the spread of the virus. Finally, we define a parametrized form of the transmission rate matrix which accounts","277":"We use anonymized and aggregated data from Facebook to show that areas with stronger social ties to two early COVID-19\"hotspots\"(Westchester County, NY, in the U.S. and Lodi province in Italy) generally have more confirmed COVID-19 cases as of March 30, 2020. These relationships hold after controlling for geographic distance to the hotspots as well as for the income and population density of the regions. These results suggest that data from online social networks may prove useful to epidemiologists and others hoping to forecast the spread of communicable diseases such as COVID-19. Also, it suggests that Google Search activities could be used to complement official numbers when forecasting disease occurrence. However, care should be taken to summarize behaviors rather than directly substitute for real-time data. We encourage others to further develop methods and approaches for modeling and predicting the spread of COVID-19 or other contagious infectious diseases via combining both formal statistical analysis and personal observations. Finally, we provide examples and directions for future research related to the timely tracking and evaluation of the effectiveness of any possible public policy measure aimed at curbing the spread of contagion. Acknowledgements: The authors of this paper were not given the option to include their own non-pharm","278":"Personal Knowledge Graphs (PKGs) are introduced by the semantic web community as small-sized user-centric knowledge graphs (KGs). PKGs fill the gap of personalised representation of user data and interests on the top of big, well-established encyclopedic KGs, such as DBpedia. Inspired by the widely recent usage of PKGs in the medical domain to represent patient data, this PhD proposal aims to adopt a similar technique in the educational domain in e-learning platforms by deploying PKGs to represent users and learners. We propose a novel PKG development that relies on ontology and interlinks to Linked Open Data. Hence, adding the dimension of personalisation and explainability in users' featured data while respecting privacy. This research design is developed for use case scenarios and is evaluated through two use cases: a collaborative search learning platform and an e-learning platform. Our preliminary results show that our approach satisfies the required dimensions of personalisation and explainability in the education sector. The overall positive experience of the proposed architecture on both usecases supports the acceptance of our approach from the perspective of practical implementation. However, we also identify and discuss potential challenges and issues associated with the successful application of our work. All these findings motivate future research in the area","279":"Mobile communications have been undergoing a generational change every ten years or so. However, the time difference between the so-called\"G's\") and the current era has not changed significantly. While fifth-generation (5G) systems are becoming a commercial reality, there is already significant interest in systems beyond 5G - which we refer to as the sixth-generation (6G) of wireless systems. In contrast to the many published papers on the topic, we take a top-down approach to 6G. We present a holistic discussion of 6G systems beginning with the lifestyle and societal changes driving the need for next generation networks, to the technical requirements needed to enable 6G applications, through to the challenges, as well as possibilities for practically realizable system solutions across all layers of the Open Systems Interconnection stack. Since many of the 6G applications will need access to an order-of-magnitude more spectrum, utilization of frequencies between 100 GHz and 1 THz becomes of paramount importance. We comprehensively characterize the limitations that must be overcome to realize working systems in these bands; and provide a unique perspective on the physical, as well as higher layer challenges relating to the design of next generation core networks, new modulation and coding methods, novel multiple access techniques, antenna","280":"We consider machine-learning-based malignancy prediction and lesion identification from clinical dermatological images, which can be indistinctly acquired via smartphone or dermoscopy capture. Additionally, we do not assume that images contain single lesions, thus the framework supports both focal or wide-field images. Specifically, we propose a two-stage approach in which we first identify all lesions present in the image regardless of sub-type or likelihood of malignancy, then it estimates their likelihood of malignancy, and through aggregation, it also generates an image-level likelihood of malignancy that can be used for high-level screening processes. Further, we consider augmenting the proposed approach with clinical covariates (from electronic health records) and publicly available data (the ISIC dataset). Comprehensive experiments validated on an independent test dataset demonstrate that i) the proposed method outperforms alternative model architectures; ii) the model based on images outperforms a pure clinical model by a large margin, and the combination of images and clinical data does not significantly improves over the image-only model; and iii) the proposed framework offers comparable performance in terms of malignancy classification relative to three board certified dermatologists with different levels of experience. In addition, we provide insights into the interpretability","281":"Due to the surge of spatio-temporal data volume, the popularity of location-based services and applications, and the importance of extracted knowledge from spatio-temporal data to solve many real-world problems, a plethora of research and development work has been done in the area of spatial and spatio-temporal data analytics in the past decade. The main goal of existing works was to develop algorithms and technologies to capture, store, manage, analyze, and visualize spatial or spatio-temporal data. The researchers have contributed either by adding spatio-temporal support with existing systems, by developing a new system from scratch for processing spatio-temporal data, or by implementing algorithms for mining spatio-temporal data. The existing ecosystem of spatial and spatio-temporal data analytics can be categorized into three groups, (1) spatial databases (SQL and NoSQL), (2) big spatio-temporal data processing infrastructures, and (3) programming languages and software tools for processing spatio-temporal data. Since existing surveys mostly investigated big data infrastructures for processing spatial data, this survey has explored the whole ecosystem of spatial and spatio-temporal analytics along with an up-to-date","282":"The main purpose of this document is to openly contribute with our comments to the EOSCArchitecture report: Scholarly Infrastructures for Research Software (SIRS), and thus, to participate in the European Open Science Cloud (EOSC) architecture design. Our comments are focused on three aspects: 1) what it would mean for a research infrastructure to be FAIR, 2) what it could mean for a digital copy of said research Infrastructure, and 3) how these copies can be created dynamically by software updates\/updates. We also discuss the implications of such a setup for the sustainability of research infrastructures. Finally, we propose potential improvements for future work in both the areas of F&D spending and AI-driven HPC computing. These experiments serve as constructive inputs for future action research within the area of SIRS and the EU OSC. They may additionally inform the scientific community of how science is evolving into a virtual environment where people without expertise and impactful data accesses can no longer be excluded. The challenges of the current world, including those addressed by the present paper, should be fully considered in establishing the functionalities and requirements of future research Infrastructure. These findings suggest that even more ambitious proposals are needed to foster a","283":"The world has seen in 2020 an unprecedented global outbreak of SARS-CoV-2, a new strain of coronavirus, causing the COVID-19 pandemic, and radically changing our lives and work conditions. Many scientists are working tirelessly to find a treatment and a possible vaccine. Furthermore, governments, scientific institutions and companies are acting quickly to make resources available, including funds and the opening of large-volume data repositories, to accelerate innovation and discovery aimed at solving this pandemic. In this paper, we develop a novel automated theme-based visualisation method, combining advanced data modelling of large corpora, information mapping and trend analysis, to provide a top-down and bottom-up browsing and search interface for quick discovery of topics and research resources. We apply this method on two recently released publications datasets (Dimensions' COVID-19 dataset and the Allen Institute for AI's CORD-19). The results reveal intriguing information including increased efforts in topics such as social distancing; cross-domain initiatives (e.g. mental health and education); evolving research in medical topics; and the unfolding trajectory of the virus in different territories through publications. The results also demonstrate the need to quickly and automatically enable search and browsing of large corpora. We believe our","284":"Social media has become an effective platform to generate and spread fake news that can mislead people and even distort public opinion. Centralized methods for fake news detection, however, cannot effectively protect user privacy during the process of centralized data collection for training models. Moreover, it cannot fully involve user feedback in the loop of learning detection models for further enhancing fake news detection. To overcome these challenges, this paper proposed a novel decentralized method, Human-in-the-loop Based Swarm Learning (HBSL), to integrate user feedback into the loop of learning and inference for recognizing fake news without violating user privacy in a decentralized manner. It consists of distributed nodes that are able to independently learn and detect fake news on local data. Furthermore, detection models trained on these nodes can be enhanced through decentralized model merging. Experimental results demonstrate that the proposed method outperforms the state-of-the-art decentralized method in regard of detecting fake news on a benchmark dataset. The code and datasets used in this study are available at https:\/\/github.com\/anyleopeace\/HBSL.git. Additionally, we collected real-world crowd source data with the help of Google Cloud Platform based on the ARIS-DDSM image classification framework. The experimental results show that our approach achieves 99","285":"Recently, the performance of Pre-trained Language Models (PLMs) has been significantly improved by injecting knowledge facts to enhance their abilities of language understanding. For medical domains, the background knowledge sources are especially useful, due to the massive medical terms and their complicated relations are difficult to understand in text. In this work, we introduce SMedBERT, a medical PLM trained on large-scale medical corpora, incorporating deep structured semantic knowledge from neighbors of linked-entity.In SMedBERT, the mention-neighbor hybrid attention is proposed to learn heterogeneous-entity information, which infuses the semantic representations of entity types into the homogeneous neighboring entity structure. Apart from knowledge integration as external features, we propose to employ the neighbors of linked-entities in the knowledge graph as additional global contexts of text mentions, allowing them to communicate via shared neighbors, thus enrich their semantic representations. Experiments demonstrate that SMedBERT significantly outperforms strong baselines in various knowledge-intensive Chinese medical tasks. It also improves the performance of other tasks such as question answering, natural language inference and machine translation. We release all the code resources for research purposes at https:\/\/github.com\/SsGood\/SMedBERT.git.io\/. Our models","286":"As COVID-19 is rapidly spreading across the globe, short-term modeling forecasts provide time-critical information for decisions on containment and mitigation strategies. A main challenge for short-term forecasts (e.g., 2-week ahead) is the assessment of key epidemiological parameters and how they change as first governmental intervention measures are showing an effect. By combining an established epidemiological model with Bayesian inference, we analyze the time dependence of the effective growth rate of new infections. For the case of COVID-19 spreading in Germany, we detect change points in the effective growth rate that correlate well with the times of publicly announced interventions. Thereby, we can (a) quantify the effects of recent governmental measures to mitigating the disease spread, and (b) incorporate the corresponding change points to forecast future scenarios and case numbers. Our code is freely available and can be readily adapted to any country or region. We hope that this analysis will help gaining better insights into the effectiveness of public health policies by mathematically modeling the corresponding changes. All data reported here are made available under our home page. https:\/\/github.com\/annahaensch\/covid19_weak_model_fusion.git.io\/forecasting\/corona\/. The code presented","287":"Delay differential equations form the underpinning of many complex dynamical systems. The forward problem of solving random differential equations with delay has received increasing attention in recent years. Motivated by the challenge to predict the COVID-19 caseload trajectories for individual states in the U.S., we target here the inverse problem. Given a sample of observed random trajectories obeying an unknown random differential equation model with delay, we use a functional data analysis framework to learn the model parameters that govern the underlying dynamics from these data. We show existence and uniqueness of the analytical solutions of the population delay random differential equation model when one has discrete time delays in the functional concurrent regression model and also for a second scenario where one has a delay continuum or distributed delay. The latter involves a functional linear regression model with history index. The derivative of the process of interest is modeled using the process itself as predictor and also other functional predictors with predictor-specific delayed impacts. This dynamics learning approach is shown to be well suited to model the growth rate of COVID-19 for the states that are part of the U.S., by pooling information from the individual states, using the case process and concurrently observed economic and mobility data as predictors. It is also shown that the model provides","288":"We examine whether a company's corporate reputation gained from their CSR activities and a company leader's reputation, one that is unrelated to his or her business acumen, can impact economic action fairness appraisals. We provide experimental evidence that good corporate reputation causally buffers individuals' negative fairness judgment following the firm's decision to profiteer from an increase in the demand. Bad corporate reputation does not make the decision to profiteer as any less acceptable. However, there is evidence that individuals judge as more unfair an ill-reputed firm's decision to raise their product's price to protect against losses. Thus, our results highlight the importance of a good reputation in protecting a firm against severe negative judgments from making an economic decision that the public deems unfair. Such guidance may be especially important for firms that are likely to underperform, such as those at risk of reputational damage. Finally, we discuss broader implications for both the design of platform governance strategies and the deployment of moderation interventions through social media. Our research has been motivated by the COVID-19 pandemic, but our methodologies are generic enough to also accommodate other forms of discrimination. Therefore, it is unlikely that the insights stemming from this study will only apply to other domains, particularly where the social sciences","289":"The recent Covid-19 epidemic has lead to comparisons of the countries suffering from it. These are based on the number of excess deaths attributed either directly or indirectly to the epidemic. Unfortunately the data on which such comparisons rely are often incomplete and unreliable. This article discusses problems of interpretation of data even when the data is largely accurate and delayed by at most two to three weeks. This applies to the Office of National Statistics in the UK, the Statistisches Bundesamt in Germany and the Belgian statistical office Statbel. The data in the article is taken from these three sources. The number of excess deaths is defined as the number of deaths minus the baseline, the definition of which varies from country to country. In the UK it is the average number of deaths over the last five years, in Germany it is over the last four years and in Belgium over the last 11 years. This means that in all cases the individual baselines depend strongly on the timing and intensity of adverse factors such as past influenza epidemics and heat waves. This makes cross-country comparisons difficult. A baseline defined as the number the number of deaths in the absence of adverse factors can be operationalized by taking say the 10\\% quantile of the number of deaths. This varies little over time","290":"The ever-increasing demand for extractive high resolution (XHR) materials from low temperature, atmospheric pressure plasmas has motivated research in this direction. Here we present a new type broadband XHR plasma source based on a compact superconducting RF (CSRF) accelerator structure which enables higher power output than existing sources and provides nanostructured ZnO2 thin films with larger density than other known examples of SXRPs and also outperforms current limits by more than three orders of magnitude in terms of peak signal to noise ratio. The performance is made possible by the combination of technological improvements within the device geometry and by the use of a novel metal foams which are highly sensitive to their environment and can be used for parameter tuning or carrying out tasks requiring a mobile core length scale. This results in a very versatile two-dimensional (2D) material synthesis technique applicable to several layers of different thicknesses. Our findings suggest that similar performances may be obtained by using lower quality seeds such as those derived from GaAs(Sb,Bi)\/AlGaN:Geometries designed specifically for biological applications, whereas for some application areas it is advantageous to have a flat band alignment at the buffer\/skin contact region. This raises the challenge of designing both the electron","291":"Regression testing is an essential activity to assure that software code changes do not adversely affect existing functionalities. With the wide adoption of Continuous Integration (CI) in software projects, which increases the frequency of running software builds, running all tests can be time-consuming and resource-intensive. To alleviate that problem, Test case Selection and Prioritization (TSP) techniques have been proposed to improve regression testing by selecting and prioritizing test cases in order to provide early feedback to developers. In recent years, researchers have relied on Machine Learning (ML) techniques to achieve effective TSP (ML-based TSP). Such techniques help combine information about test cases, from partial and imperfect sources, into accurate prediction models. This work conducts a systematic literature review focused on ML-based TSP techniques, aiming to perform an in-depth analysis of the state of the art, thus gaining insights regarding future avenues of research. To that end, we analyze 29 primary studies published from 2006 to 2020, which have been identified through a systematic and documented process. This paper addresses five research questions addressing variations in ML-based TSP techniques and feature sets for training and testing ML models, alternative metrics used for evaluating the techniques, the performance of techniques, and the reproducibility of the published","292":"U-net, as an encoder-decoder architecture with forward skip connections, has achieved promising results in various medical image analysis tasks. Many recent approaches have also extended U-net with more complex building blocks, which typically increase the number of network parameters considerably. Such complexity makes the inference stage highly inefficient for clinical applications. Towards an effective yet economic segmentation network design, in this work we propose backward skip connections that bring decoded features back to the encoder. Our design can be jointly adopted with forward skip connections in any encoder-decoder architecture forming a recurrence structure without introducing extra parameters. With the backward skip connections, we propose a U-net based network family, namely Bi-directional O-shape networks, which set new benchmarks on multiple public medical imaging segmentation datasets. On the other hand, with the most plain architecture (BiO-Net), network computations inevitably increase along with the pre-set recurrence time. We have thus studied the deficiency bottleneck of such recurrent design and propose a novel two-phase Neural Architecture Search (NAS) algorithm, namely BiX-NAS, to search for the best multi-scale biometric neural network for each pixel\/voxel coordinate. The ineffective skip connections are then discarded to reduce computational","293":"Currently, coronavirus disease 2019 (COVID-19) poses a great threat to public health and economy worldwide. Unfortunately, there are yet no effective drugs for this disease. For this reason, several countries have adopted multiple preventive interventions to avoid the spread of COVID-19. Here, we propose a delayed mathematical model to predict the epidemiological trend of COVID-19 in Morocco. Parameter estimation and sensitivity analysis of the proposed model are rigorously studied. Moreover, numerical simulations are presented in order to test the effectiveness of the preventive measures and strategies that were imposed by the Moroccan authorities and also help policy makers and public health administration to develop such strategies. The results of this work may contribute to the development of new preventive measures that may be implemented afterwards in similar diseases around the world. Precisely, the proposed model predicts that the number of daily cases will increase from 2.5 to 4.1 during the next week after the intervention strategy is taken into account. This prediction is consistent with many previous reports from other regions of the world. Finally, parameter estimation and sensitivity analysis of the proposed model are carried out using a real data set from March 13th, 2020, Morocco. The key message of this study is that more attention should be paid to developing novel","294":"By successfully solving the problem of forecasting, the processes in the work of various companies can be optimized and savings can be achieved while using the historical data. In this process, it is of great importance to identify the patterns or trends that will affect the future behavior of the companies. Recent research has shown that deep learning models achieve good results when applied to certain tasks such as image recognition, speech analysis etc. Deep Learning (DL) is being increasingly utilized in reshaping the business environments; however, there is still a lack of applications and challenges for its full realization. The world is facing a huge demand for DL-based solutions from both software and hardware manufacturers. Therefore, we conduct an extensive study on applying DL techniques to generate scalable and reusable knowledge graphs (KGs) for smart manufacturing systems. We first introduce a new dataset, namely SMART suRveillance technique KG4, which contains 2,593 claims about soft masks, clothing, electric bikes and motorcycles along with 27,092 images and 37,092 videos. Furthermore, we propose a comprehensive modeling framework based on MLP neural network and RNN transformer for generating the kG. Finally, we conducted a detailed experimental study on applying state-of-the-art DL methods to generate realistic KGs","295":"The presence and action of humans on Earth has exerted a strong influence on the evolution of the planet over the past $\\approx$ 10,000 years, the consequences of which are now becoming broadly evident. Despite a deluge of tightly-focused and necessarily technical studies exploring each facet of\"human impacts\"on the planet, their integration into a complete picture of the human-Earth system lags far behind. Here, we quantify twelve dimensionless ratios which put the magnitude of human impacts in context, comparing the magnitude of anthropogenic processes to their natural analogues. These ratios capture the extent to which humans alter the terrestrial surface, hydrosphere, biosphere, atmosphere, and biogeochemistry of Earth. In almost all twelve cases, the impact of human processes rivals or exceeds their natural counterparts. The values and corresponding uncertainties for these impacts at global and regional resolution are drawn from the primary scientific literature, governmental and international databases, and industry reports. We present this synthesis of the current\"state of affairs\"as a graphical snapshot designed to be used as a reference. Furthermore, we establish a searchable database termed the Human Impacts Database (www.anthroponumbers.org) which houses all quantities reported here and many others with extensive curation and annotation. While necessarily","296":"Mathematical models are widely recognized as an important tool for analyzing and understanding the dynamics of infectious disease outbreaks, predict their future trends, and evaluate public health intervention measures for disease control and elimination. We propose a novel stochastic metapopulation state-space model for COVID-19 transmission, based on a discrete-time spatio-temporal susceptible\/exposed\/infected\/recovered\/deceased (SEIRD) model. The proposed framework allows the hidden SEIRD states and unknown transmission parameters to be estimated from noisy, incomplete time series of reported epidemiological data, by application of unscented Kalman filtering (UKF), maximum-likelihood adaptive filtering, and metaheuristic optimization. Experiments using both synthetic data and real data from the Fall 2020 Covid-19 wave in the state of Texas demonstrate the effectiveness of the proposed model. Furthermore, we apply the model to analyze the current Spring 2021 Covid-19 wave in Louisiana, which is currently being administered as a pilot study by the Centers for Disease Control and Prevention (CDC). The results suggest that if the medical infrastructure is sufficiently capable to handle the additional load caused by Covid-19, then it can serve as a safe and flexible alternative to treat the pandemic","297":"Due to the COVID-19 pandemic, conducting Human-Robot Interaction (HRI) studies in person is not permissible due to social distancing practices to limit the spread of the virus. Therefore, a virtual reality (VR) simulation with a virtual robot may offer an alternative to real-life HRI studies. Like a real intelligent robot, a virtual robot can utilize the same advanced algorithms to behave autonomously. This paper introduces HAVEN (HRI-based Augmentation in a Virtual robot Environment using uNity), a VR simulation that enables users to interact with a virtual robot. The goal of this system design is to enable researchers to conduct HRI Augmented Reality studies using a virtual robot without being in a real environment. This framework also introduces two common HRI experiment designs: a hallway passing scenario and human-robot team object retrieval scenario. Both reflect HAVEN's potential as a tool for future AR-based HRI studies. We additionally discuss the benefits of one of these experiments and provide recommendations for conducting similar research tasks in the future. Preliminary user evaluations demonstrate the efficacy of the proposed HAVEN framework and indicate its great potential for future AR-based HRI studies. For example, participants have preferred our approach over traditional","298":"The communication revolution has perpetually reshaped the means through which people send and receive information. Social media is an important pillar of this revolution and has brought profound changes to various aspects of our lives. However, the open environment and popularity of these platforms inaugurate windows of opportunities for various cyber threats, thus social networks have become a fertile venue for spammers and other illegitimate users to execute their malicious activities. These activities include phishing hot and trendy topics and posting a wide range of contents in many topics. Hence, it is crucial to continuously introduce new techniques and approaches to detect and stop this category of users. This paper proposes a novel and effective approach to detect social spammers. An investigation into several attributes to measure topic-dependent and topic-independent users' behaviours on Twitter is carried out. The experiments of this study are undertaken on various machine learning classifiers. The performance of these classifiers are compared and their effectiveness is measured via a number of robust evaluation measures. Further, the proposed approach is benchmarked against state-of-the-art social spam and anomalous detection techniques. These experiments report the effectiveness and utility of the proposed approach and embedded modules. In addition, the paper discusses the two main components of the Spams Detection framework: (i) the content types assumed","299":"We use the aggregate information from individual-to-firm and firm-to-firm in Garanti BBVA Bank transactions to mimic domestic private demand. Particularly, we replicate the quarterly national accounts aggregate consumption and investment (gross fixed capital formation) and its bigger components (Machinery and Equipment and Construction), for the case of Turkey. In order to validate the usefulness of the information derived from these indicators we test the nowcasting ability of both indicators to nowcast the Turkish GDP using different nowcasting models. The results are successful and confirm the usefulness of Consumption and Investment Banking transactions for nowcasting purposes. The value of the Big data information is more relevant at the beginning of the nowcasting process, when the traditional hard data information is scarce. This makes this information specially relevant for those countries where statistical release lags are longer like the Emerging Markets. Finally, as far as we know, the application of the Big Data methods to the Turkic family of nations is still a relatively new phenomenon. Our estimates suggest that the total wealth possessed by the Turkics is well above the one of the other families, which cannot be included in the study based on their reported values. Thus, it can be said that the Geographies of the Turkic family of Nations have been","300":"With the proliferation of new technologies such as Internet of Things (IOT) and Software-Defined Networking(SDN) in the recent years, the distributed denial of service (DDoS) attack vector has broadened and opened new opportunities for more sophisticated DDoS attacks on the targeted victims. The new attack vector includes unsecured and vulnerable IoT devices connected to the internet, denial of service vulnerabilities like southbound channel saturation in the SDN architecture. Given the high-volume and pervasive nature of these attacks, it is beneficial for stakeholders to collaborate in detecting and mitigating the denial of service attacks in a timely manner. The blockchain technology is considered to improve the security aspects owing to the decentralized design, secured distributed storage and privacy. A thorough exploration and classification of blockchain techniques used for DDoS attack mitigation is not explored in the prior art. This paper reviews and categorizes the existed state-of-the-art DDoS mitigation solutions based on blockchain technology. The DDoS mitigation techniques are classified based on the solution deployment location i.e. network based, near attacker location, near victim location and hybrid solutions in the network architecture with emphasis on the IoT and SDN architectures. Additionally, based on our study, the research challenges and future directions to implement the blockchain based","301":"We present a general framework for using existing data to estimate the efficiency gain from using a covariate-adjusted estimator of a marginal treatment effect in a future randomized trial. We describe conditions under which it is possible to define a mapping from the distribution that generated the existing external data to the relative efficiency of a covariate-adjusted estimator compared to an unadjusted estimator. Under conditions, these relative efficiencies approximate the ratio of sample size needed to achieve a desired power. We consider two situations where the outcome is either fully or partially observed and several treatment effect estimands that are of particular interest in most trials. For each such estimand, we develop a semiparametrically efficient estimator of the relative efficiency that allows for the application of flexible statistical learning tools to estimate the nuisance functions and an analytic form of a corresponding Wald-type confidence interval. We also propose a double bootstrap scheme for constructing confidence intervals. We demonstrate the performance of the proposed methods through simulation studies and apply these methods to data to estimate the relative efficiency of using covariate adjustment in Covid-19 therapeutic trials. The data used in this paper comes from the COVID-19 Global Data Network (GDN) [1]. Keywords: artificial intelligence, covid-19, machine","302":"The continue emerging of diseases that have the potential to become threats at local and global scales, such as influenza A(H1N1), SARS, MERS, and COVID-19, makes it relevant to keep designing models of disease propagation and strategies to prevent or mitigate their effects in populations. Since isolated systems are very rare to find in any context, specially in human contact networks, here we examine the susceptible-infected model of disease spreading in a multiplex network formed by two distinct networks or layers that are interconnected through a fraction $q$ of shared individuals. We model the interactions between individuals in each layer through a weighted network, because person-to-person interactions are diverse (or disordered); weights represent the contact times of these interactions and we use a distribution of contact times to assign an individual disorder to each layer. Using branching theory supported by simulations, we study a social distancing strategy where we reduce the average contact time in one layer (or in both of them, if necessary). We find a set of disorder parameters - associated with average contact times - that prevents a disease from becoming an epidemic. When the disease is very likely to spread, the system is always in an endemic phase, regardless of the disorder parameters and the fraction of shared nodes","303":"Solar Orbiter strives to unveil how the Sun controls and shapes the heliosphere and fills it with energetic particle radiation. To this end, its Energetic Particle Detector (EPD) has now been in operation, providing excellent data, for just over a year. EPD measures suprathermal and energetic particles in the energy range from a few keV up to (near-) relativistic energies (few MeV for electrons and about 500 MeV\/nuc for ions). We present an overview of the initial results from the first year of operations and we provide a first assessment of issues and limitations. During this first phase of operations of the Solar Orbiter mission, EPD has recorded several particle events at distances between 0.5 and 1 au from the Sun. We present dynamic and time-averaged energy spectra for ions that were measured with a combination of all four EPD sensors, namely: the Suprathermal Electron and Proton sensor (STEP), the Electron Proton Telescope (EPT), the Suprathermal Ion Spectrograph (SIS), and the High-Energy Telescope (HET) as well as the associated energy spectra for electrons measured with STEP and EPT. The high anisotropy","304":"Face masks have become one of the main methods for reducing the transmission of COVID-19. This makes face recognition (FR) challenging because masks hide several discriminative features of faces. Moreover, face presentation attack detection (PAD) is crucial to ensure the security of FR systems. In contrast to the growing number of masked FR studies, the impact of face masked attacks on PAD has not been explored. Therefore, we present novel attacks with real face masks placed on presentations and attacks with subjects wearing masks to reflect the current real-world situation. Furthermore, this study investigates the effect of masked attacks on PAD performance by using seven state-of-the-art PAD algorithms under different experimental settings. We also evaluate the vulnerability of FR systems to masked attacks. The experiments show that real masked attacks pose a serious threat to the operation and security of FR systems. Thus, it is essential to develop more effective defense methodologies to improve the protection of FR systems. Detector code will be released at https:\/\/github.com\/securifai\/Real-World-Masked-Face-Dataset.git.io. Ultimately, our work demonstrates how ensemble learning can be used to enhance the effectiveness of PAD against masked attacks. Keywords:","305":"We demonstrate and discuss nonasymptotic bounds in probability for the cost of a regression scheme with a general loss function from the perspective of the Rademacher theory, and for the optimality with respect to the average $L^{2}$-distance to the underlying conditional expectations of least squares regression outcomes from the perspective of the Vapnik-Chervonenkis theory. The results follow from an analysis involving independent but possibly nonstationary training samples and can be extended, in a manner that we explain and illustrate, to relevant cases in which the training sample exhibits dependence. We establish upper bounds on the optimal expected regrets by leveraging two novel techniques: the maximin problem's solution path extends well to situations where the training data is noisy, or the optimality gap is large relative to the random sample size. Finally, we consider the setting where the trained model is robust against initialization errors, in particular when the adversarial perturbations are placed on the model parameters. In this case, we also establish upper bounds on the optimal expected regrets via a clean connection to self-robustness, and thus obtain tighter rates than previously known. Our results reconcile with recent experimental findings and provide complementary insights into Rosenbaum's arguments on instrumental variables. This paper is","306":"The current expansion of theory and research on artificial intelligence in management and organization studies has revitalized the theory and research on decision-making in organizations. In particular, recent advances in deep learning (DL) algorithms promise benefits for decision-making within organizations, such as assisting employees with information processing, thereby augment their analytical capabilities and perhaps help their transition to more creative work. This paper presents a DL-based framework for enhancing the creativity of organizational decisions using XR content creators. We describe the process by which this framework was developed and deployed in two case studies at Ericsson. The first deployment covers nearly all areas of the enterprise, including personnel, training, technology, and business operations. The second deployment is focused on the operational results of the enterprise, namely, customer service quality or Service Level Agreements (SLAs). These operational measures are used to create a hybrid machine learning\/deep learning model that predicts future customer needs. Finally, we present the outcomes of both deployments, demonstrating how the added value of DL-based innovations to Ericsson has translated into greater benefits to customers, along with improved employee productivity through the creation of fewer repetitive computations. Our framework enables new opportunities for other researchers to fine-tune any existing models or develop new ones based on these concepts. All","307":"Tolerant face masks have been used as part of the COVID-19 pandemic prevention effort, since they are believed to be effective for reducing transmission risk without compromising comfort or mask usage. However, there has been little research into their design and effectiveness. We present a method for 3D computational fluid dynamics (CFD) simulations of shear-driven airflows in protective face masks with exhalation valves, enabling comparisons of relative performance of masked and unmasked faces under different ventilation settings. To this end, we use a combination of high-fidelity CFD simulations of a surrogate model and a large-scale Lagrangian particle tracking model to measure detailed three-dimensional flow patterns and temporal evolution of particles within the domain. The results show that wearing a facile filter on both front and back sides of the mouth can effectively reduce saliva particulate propagation and peak infection rates. Although our findings indicate that it may not be necessary to wear a full body covering when designing practical applications, we also provide a benchmark to test the efficacy of various designs discussed here. Furthermore, we discuss how our work provides insights regarding the interaction of airflow with exhale valves, which could inspire further studies on aerosol dispersion and removal in health care settings. As a particular application","308":"The COVID-19 pandemic has caused massive humanitarian and economic damage. Teams of scientists from a broad range of disciplines have searched for methods to help governments and communities combat the disease. One avenue from the machine learning field which has been explored is the prospect of a digital mass test which can detect COVID-19 from infected individuals' respiratory sounds. We present a summary of the results from the INTERSPEECH 2021 Computational Paralinguistics Challenges: COVID-19 Cough, (CCS) and COVID-19 Speech, (CSS). The challenges in this task are twofold. First, the dataset had to be split into two parts as it was not sufficiently balanced classes. Secondly, we also want to tackle the problem of imbalance data with very different class distributions. In order to address these problems, we propose a discriminative approach based on ensemble optimization. Our method enforces the feature representation of each sound to be similar to its closest non-negative neighbors in both space and time. For our empirical study, we use three different censoring schemes which aim to reduce the effect of the background noise and reverberation. From the experimental results, we observe that our proposed method outperforms other state-of-the-art techniques with an accuracy","309":"Recently, there has been a rising concern about the fairness in AI research and how it can impact on society when combined with privacy concerns. In this paper, we analyzed 377,600 research project teams within a single institution (Frequently cited as F1) which produced competitively-valuable services for humans using artificial intelligence (AI). We used the data from the National Institutes of Health (NIH) ExPORTER collection to understand how they have achieved their fair share of rewards. The results show that well-performing projects were not only more productive than other types of work, but also tended to be fairer among those who publish at least one successful project team. This relationship was further strengthened at the time of multiple correspondence analysis. To demonstrate the potential application of this type of work, we produced a set of recommendations based on the findings and explained them through an iterative process with the goal of promoting awareness regarding the fairness of AI research. Although the current study is limited to an institutional level, we believe that the principles contained therein are applicable more broadly, especially as long as human beings use AI without permanent residence. Furthermore, the success of the Internet of Things (IoT) and machine learning (ML) communities shows that collecting real-world data about humans'","310":"Internet memes have become powerful means to transmit political, psychological, and socio-cultural ideas. Although memes are typically humorous, recent days have witnessed an escalation of harmful memes used for trolling, cyberbullying, and abuse. Detecting such memes is challenging as they can be highly satirical and cryptic. Moreover, while previous work has focused on specific aspects of memes such as hate speech and propaganda, there has been little work on harm in general. Here, we aim to bridge this gap. We focus on two tasks: (i)detecting harmful memes, and (ii)identifying the social entities they target. We further extend a recently released HarMeme dataset, which covered COVID-19, with additional memes and a new topic: US politics. To solve these tasks, we propose MOMENTA (MultimOdal framework for detecting harmful MemEs aNd Their tArgets), a novel multimodal deep neural network that uses global and local perspectives to detect harmful memes. MOMENTA systematically analyzes the local and the global perspective of the input meme (in both modalities) and relates it to the background context. MOMENTA is interpretable and generalizable, and our experiments show that it outperforms several strong rival","311":"The ongoing COVID-19 pandemic has been recognized as a public health emergency of international concern by WHO on January 30, 2020. To curb the secondary spread of this infectious disease, most states in the US have issued state-at-home executive orders, which have set up new guidelines for how to enforce social distancing and regulate movement in order to facilitate reopening of economic activities. This paper focuses on Tennessee, one of the first states to issue such an order during the early stage of the pandemic. The paper utilizes computer simulation of viral spread under several different scenarios (e.g., no vaccination), coupled with the method of least squares estimation to verify that the relative changes in patient numbers within various groups are consistent across time. We also study the impact of relaxing or reversing the mandate by analyzing the results from the second wave of the pandemic in Chattanooga, TN. Results show that increasing the number of available vaccines decreases the cumulative number of infected individuals by more than 70\\%, while allowing the mandatory stay-at-home orders to be relaxed without requiring any further restrictions. Furthermore, it is shown that delaying the release of the mandated documents will not necessarily lead to a repeal of the mandates. Finally, we present extensive conclusions based on our findings both regarding the general","312":"The recent outbreak of COVID-19 has motivated researchers to contribute in the area by building knowledge graphs, especially graph mining methods, which can help in finding information about the disease through analysis of the existing literature. However, most of these applications are only focused on entity relationships without considering temporal dynamics. In this paper, we propose a framework with both entity and temporal relations for the study of COVID-19. We formalize the problem as a sequence of graph snapshots based on different representations of entities and temporal data. The objective function considers the difference between the embeddings of two entities (entities) and their snapshot versions under different time periods separated by the onset of a disease outbreak (i.e., when the disease enters into its first phase). Our proposed approach combines the notion of Granger causality with several causal inference techniques such as SHapley Additive exPlanation (SHAP), Causal Accumulated Local Effects (CALE), and Spatio-temporal Graph Neural Network (STNN). Extensive experiments on three real publicly available datasets of COVID-19 papers demonstrate the effectiveness of our method compared with state-of-the-art baselines. Our further analyses show that the number of candidate papers should be reduced significantly while still maintaining","313":"We present a word-sense induction method based on pre-trained masked language models (MLMs), which can cheaply scale to large vocabularies and large corpora. The result is a corpus which is sense-tagged according to a corpus-derived sense inventory and where each sense is associated with indicative words. Evaluation on English Wikipedia that was sense-tagged using our method shows that both the induced senses, and the per-instance sense assignment, are of high quality even compared to WSD methods, such as Babelfy. Furthermore, by training a static word embeddings algorithm on the sense-tagged corpus, we obtain high-quality static senseful embeddings. These outperform existing senseful embeddings methods on the WiC dataset and on a new outlier detection dataset we developed. The data driven nature of the algorithm allows to induce corpora-specific senses, which may not appear in standard sense inventories, as we demonstrate using a case study on the scientific domain. We also show how these improved senseful embeddings can be used for supervised learning tasks involving semantic change detection. The results indicate that our framework induces relevant changes between the input text and the target sense, without requiring either monolingual or bilingual inputs.","314":"We present an assessment of the greenhouse gases emissions of the Institute for Research in Astrophysics and Planetology (IRAP), located in Toulouse (France). It was performed following the established\"Bilan Carbone\"methodology, over a large scope compared to similar previous studies, including in particular the contribution from the purchase of goods and services as well as IRAP's use of external research infrastructures, such as ground-based observatories and space-borne facilities. The carbon footprint of the institute for the reference year 2019 is 7400 +\/- 900 tCO2e. If we exclude the contribution from external research infrastructures to focus on a restricted perimeter over which the institute has some operational control, IRAP's emissions in 2019 amounted to 3300 +\/- 400 tCO2e. Over the restricted perimeter, the contribution from purchasing goods and services is dominant, about 40% of the total, slightly exceeding the contribution from professional travel including hotel stays, which accounts for 38%. Local infrastructures make a smaller contribution to IRAP's carbon footprint, about 25% over the restricted perimeter. We note that this repartition may be specific to IRAP, since the energy used to produce the electricity and heating has a relatively low carbon footprint","315":"We consider the equation of motion of a particle in a magnetic field, subject to nonlinear no-flux boundary conditions. We introduce a parameter $\\rho\\in [0,1]$, characterising the amount of positive or negative ionisation in the medium. The physical model has a continuum of endemic equilibrium states characterized by different proportions of susceptible, infected and recovered populations. As $\\rho$ decreases we find the global flow to be stable against large variations of $W$. However, for heterogeneous systems with strong effective fields the virus can lead to the existence of a region of quasi-stationary state where $W$ does not exceed a critical value. Due to the strong inhomogeneity of the system our numerical results show that this condition persists for all values of $\\rho$. This suggests that a long range correlation exists between the (quasi) stationary state and the dynamics of the full system even when the latter cannot be described through a simple mean field theory. These findings are relevant as they provide a mathematical demonstration of how the global behaviour of a complex many-body dynamical system changes as we adjust the strength of the external electric field. Our work establishes a paradigm for studying particles in a fluid moving under the influence of a large number of quas","316":"The hypothesis here states that neural network algorithms such as Multi-layer Perceptron (MLP) have higher accuracy in differentiating malicious and semi-structured phishing URLs. Compared to classical machine learning algorithms such as Logistic Regression and Multinomial Naive Bayes, the classical algorithms rely heavily on substantial corpus data training and machine learning experts' domain knowledge to perform complex feature engineering. MLP could perform non-linear separable multi-classes classification and focus less on corpus feature training. In addition, backpropagation weight adjustment could learn which features are more important in differentiating phishing from other attack types. The idea of using ensemble methods can help propagate the feature learning process to improve the performance of our model. On top of these ideas, we propose a novel mutual distillation learning objective function for learning good representation and robust high-level convolutional neural network architecture. We call it dual attention learning (DAL). Our proposed strategy uses one-sided sequential search optimization algorithm and one-sided support vector machine to solve this problem. Extensive experiments on multiple real-world datasets demonstrate that DAL outperforms existing state-of-the-art techniques in terms of both methodologies and results. More importantly, we show that large-scale","317":"The challenges posed by the architecting of System of Systems (SoS) has motivated several studies on the topic, particularly in the context of high-throughput applications and systems with continuously evolving nature. In this paper we investigate the problem of topological change of vesicle-like membrane at the scale of one or more complete cell types under varying conditions. We combine experiments and numerical computations using the software package Geomstats for studying morphological changes in biological membranes and discover that dynamical processes such as fluidity and viscosity balance are responsible for the observed fluorescence blueshift and decreased fluorescence lifetimes. Further, we show that the membrane inflects and forms an envelope around the defected region which almost completely inhibits light penetration through the ultrafast diffraction limit. The insights gained from our soft matter physics approach may help to improve design strategies for future engineered active materials. This article is part of the theme issue \u2018Topics in mathematical architecture of living cells\u2019. Besides its potential application in biology, the article also demonstrates how methods developed in the field can be applied to other areas of science, including exoskeletonology. It is presented that way to construct highly stable structures in social networks, and virus-like nanomaterials might play","318":"The mixture cure model for analyzing survival data is characterized by the assumption that the population under study is divided into a group of subjects who will experience the event of interest over some finite time horizon and another group of cured individuals who will never experience the event irrespective of the duration of follow-up. When using the Bayesian paradigm for inference in survival models with a cure fraction, it is common practice to rely on Markov chain Monte Carlo (MCMC) methods to sample from posterior distributions. Although computationally feasible, the iterative nature of MCMC often implies long sampling times to explore the target space with chains that may suffer from slow convergence and poor mixing. An alternative strategy for fast and flexible sampling-free Bayesian inference in the mixture cure model is suggested in this paper by combining Laplace approximations and penalized B-splines. A logistic regression model is assumed for the cure proportion and a Cox proportional hazards model with a P-spline approximated baseline hazard is used to specify the conditional survival function of susceptible people. Laplace approximations to the conditional latent vector are based on analytical formulas for the gradient and Hessian of the log-likelihood, resulting in a substantial speed-up in approximating posterior distributions. The statistical performance and computational efficiency","319":"Do black-boxes of machine learning models learn clinically relevant features for fracture diagnosis? The answer not only establishes reliability quenches scientific curiosity but also leads to explainable and verbose findings that can assist the radiologists in the final and increase trust. This work identifies the concepts networks use for vertebral fracture diagnosis in CT images. This is achieved by associating concepts to neurons highly correlated with a specific diagnosis in the dataset. The concepts are either associated with neurons by radiologists pre-hoc or are visualized during a particular prediction and left for the user's interpretation. We evaluate which concepts lead to correct diagnosis and which concepts lead to false positives. The proposed frameworks and analysis pave the way for reliable and explainable vertebral fracture diagnosis. They address issues such as data scarcity, achieving better performance than baselines on few-shot learning, and provide fine-grained and verifiable overviews of the decision process regarding the correctness of the classification. In this work, we introduce a novel concept network based framework for reliable and explainable vertebral fracture diagnosis. It consists of three modules: (1) An image classifier captures discriminative concepts from normal and abnormal classes; (2) a graph attention network uses global and local representations to focus on informative regions over","320":"Due to the characteristics of COVID-19, the epidemic develops rapidly and overwhelms health service systems worldwide. Many patients suffer from systemic life-threatening problems and need to be carefully monitored in ICUs. Thus the intelligent prognosis is in an urgent need to assist physicians to take an early intervention, prevent the adverse outcome, and optimize the medical resource allocation. However, in the early stage of the pandemic, there are many challenges due to the lack of effective diagnostic mechanisms, rarity of the cases, and privacy concerns. In this paper, we propose a deep-learning-based approach, CovidCare, which leverages the existing electronic medical records to enhance the prognosis for inpatients with emerging infectious diseases. It learns to embed the COVID-19-related medical features based on massive existing EMR data via transfer learning. The transferred parameters are further trained to imitate the teacher model's representation behavior based on knowledge distillation, which embeds the health status more comprehensively in the source dataset. We conduct the length of stay prediction experiments for patients on a real-world intensive care unit (ICU) by fine-tuning the model using different noisy test datasets. The experiment results demonstrate that our proposed model consistently outperforms the comparative baseline methods. Cov","321":"The recent uptake in popularity in vehicles with zero tailpipe emissions is a welcome development in the fight against traffic induced airborne pollutants. As vehicle fleets become electrified, and tailpipe emissions become less prevalent, non-tailpipe emissions (from tires and brake disks) will become the dominant source of traffic related emissions, and will in all likelihood become a major concern for human health. This trend is likely to be exacerbated by the heavier weight of electric vehicles, their increased power, and their increased torque capabilities, when compared with traditional vehicles. While the problem of emissions from tire wear is well-known, issues around the process of tire abrasion, its impact on the environment, and modelling and mitigation measures, remain relatively unexplored. Work on this topic has proceeded in several discrete directions including: on-vehicle collection methods; vehicle tire-wear abatement algorithms and controlling the ride characteristics of a vehicle, all with a view to abating tire emissions. Additional approaches include access control mechanisms to manage aggregate tire emissions in a geofenced area with other notable work focussing on understanding the particle size distribution of tire generated PM, the degree to which particles become airborne, and the health impacts of tire emissions. While such efforts are already underway, the problem of developing models","322":"The paper gives a brief introduction about what BEC (Business Email Compromise) is and why we should be concerned about. In addition, it presents 2 examples, Ubiquity and Peebles Media Group, which have been chosen to analyse the phenomena of BEC and underpin how universal BEC threat is for all companies. The psychology behind this scam has been, then, studied. In particular, the Big Five Framework has been analysed to understand how personality traits play an important role in Social Engineering-based attacks. Furthermore, the 6 basic principles of influence, by Cialdini, have been presented to show which strategies are adopted in such scam. The paper follows with the analysis of the BEC impacts, the incidents evaluation and, finally, with the description of some precautions, that companies should undertake in order to mitigate the likelihood of a Business Email Compromise. The results obtained demonstrate that indeed this type of scams are realizable, as they can be easily managed by organizations. Thus, being able to accurately forecast their level of degradation if any company falls short of best practices will help to stop such scams. Finally, the proposed model may serve as a tool for evaluating the risks associated with these types of scams. Overall, there is a need to both","323":"This document supports a proposed initiative to improve the governance of artificial intelligence (AI) by investing in government capacity to systematically measure and monitor the capabilities and impacts of AI systems. If adopted, this effort will allow governments to invest in measurement and monitoring infrastructure to address the societal challenges posed by future crises. It is intended to weave together threads from different domains, incorporate existing views on AI compliance by developing new ones, and build a continuous framework for decision making where values are shared and transformed via appropriate value taxonomies. Aspects of the methodology and strategy are discussed, with a focus on the potential to enhance public and private sector preparedness for crisis scenarios. The document concludes with recommendations which extend to other high-impact venues such as conferences, journals, and universities. A working group within the ECFA has been working towards releasing a position paper outlining research goals and strategies for measuring and monitoring the impact of AI systems under consideration of relevant national and international policy measures. This work was presented along with a set of companion documents released for each of the core principles outlined above. All of these documents contain explicit considerations concerning the nature of the place of measurement and monitoring infrastructure, the legal and regulatory landscape surrounding AI, and the metrics and standards used for evaluating AI system performance. Finally, some directions","324":"The COVID-19 pandemic has completely disrupted the operation of our societies. Its elusive transmission process, characterized by an unusually long incubation period, as well as a high contagion capacity, has forced many countries to take quarantine and social isolation measures that conspire against the performance of national economies. This situation confronts decision makers in different countries with the alternative of reopening the economies, thus facing the unpredictable cost of a rebound of the infection. This work tries to offer an initial theoretical framework to handle this alternative. It relies on a simple derivation of the SIR model for the case of a single country, and it allows us to study the conditions for the existence of a minimal spreading regime. We show that the lack of control over the contact rate among people returning from travel restrictions can lead to exceedance of the hospital capacity, hence justifying the adoption of strict lockdown measures. As a main contribution, we propose a mathematical model that combines the dynamics of the epidemic with mobility controls, and we derive a critical value of the basic reproduction number $\\mathcal{R}_0$ (the threshold between extinction and persistence of the infection). When applied to real data, our model shows how different mobility strategies can significantly alter the dynamics of the epidemic, especially when there is a","325":"We present the definition and implementation of the Open Web to the set of services offered freely to Internet users, representing a pillar of modern societies. Despite its importance for society, it is unknown how the COVID-19 pandemic is affecting the Open Web. In this paper we address this issue, focusing our analysis on Spain, one of the countries which have been most impacted by the pandemic. On the one hand, we study the impact of the pandemic in financial backbone networks, as well as in social network platforms such as Twitter and Facebook. On the other hand, we analyze the spread of misinformation and disinformation through these social networks, as well as the level of coordination between them. By applying methodologies based on information mapping, contagion processes and evolutionary algorithms, we are able to provide data-driven solutions to help combat the pandemic in a way that satisfies their needs. These results can be used both to guide policy makers and platform operators to take adequate measures to mitigate the effects of the pandemic, but also to develop techniques and strategies for detecting false claims and fake news about particular topics. This work becomes a reference of high relevance for researchers and practitioners involved with the subject matter at hand. We show how the open web and specifically Telegram channels adapt to the needs","326":"The ongoing COVID-19 pandemic has raised concerns for most countries regarding personal and public health implications, financial security and economic stability. Alongside many other unprecedented challenges, there are increasing concerns over social isolation and mental health. We introduce \\textit{Expressive Interviewing}--an interview-style conversational system that draws on ideas from motivational interviewing and expressive writing. Expressive Interviewing seeks to encourage users to express their thoughts and feelings through writing by asking them questions about how COVID-19 has impacted their lives. We present relevant aspects of the system's design and implementation as well as quantitative and qualitative analyses of user interactions with the system. In addition, we conduct a comparative evaluation with a general purpose dialogue system for mental health that shows our system potential in helping users to cope with COVID-19 issues. Finally, we share insights on building the future of this promising area and comment on the steps taken to prepare the system for further use. Video recordings of our daily usage of the system are available at https:\/\/youtu.be\/zhbcSvxEKMk. Source code for this paper is available at https:\/\/github.com\/robi56\/expressive_interpreting\/. This dataset is made publicly available to facilitate more research","327":"This paper presents an Improved Bayesian Optimization (IBO) algorithm to solve complex high-dimensional epidemic models' optimal control solution. Evaluating the total objective function value for disease control models with hundreds of thousands of control time periods is a high computational cost. In this paper, we improve the conventional Bayesian Optimization (BO) approach from two parts. The existing BO methods optimize the minimizer step for once time during each acquisition function update process. To find a better solution for each acquisition function update, we do more local minimization steps to tune the algorithm. When the model is high dimensions, and the objective function is complicated, only some update iterations of the acquisition function may not find the global optimal solution. The IBO algorithm adds a series of Adam-based steps at the final stage of the algorithm to increase the solution's accuracy. Comparative simulation experiments using different kernel functions and acquisition functions have shown that the Improved Bayesian Optimization algorithm is effective and suitable for handing large-scale and complex epidemic models under study. The IBO algorithm is then compared with four other global optimization algorithms on three well-known synthetic test functions. The effectiveness and robustness of the IBO algorithm are also demonstrated through some simulation experiments to compare with the Particle Swarm Optimization","328":"In providing physical assistance to elderly people, ensuring cooperative behavior from the elderly persons is a critical requirement. In sit-to-stand assistance, for example, an older adult must lean forward, so that the body mass can shift towards the feet before a caregiver starts lifting the body. An experienced caregiver guides the older adult through verbal communications and physical interactions, so that the older adult may be cooperative throughout the process. This guidance is of paramount importance and is a major challenge in introducing a robotic aid to the eldercare environment. The wide-scope goal of the current work is to develop an intelligent eldercare robot that can a) monitor the mental state of an older adult, and b) guide the older adult through an assisting procedure so that he\/she can be cooperative in being assisted. The current work presents a basic modeling framework for describing a human's physical behaviors reflecting an internal mental state, and an algorithm for estimating the mental state through interactive observations. The sit-to-stand assistance problem is considered for the initial study. A simple Kalman Filter is constructed for estimating the level of cooperativeness in response to applied cues, with a thresholding scheme being used to make judgments on the cooperativity state. Simulation results are provided to demonstrate the effectiveness of","329":"We analyze the dynamics of COVID-19 cased by the Omicron variant via a fractional susceptible-exposedinfected-removed (SEIR) model, which is equivalent to a classical SIR model with diffusion parameters given by $p(x)\\propto x^{1\/\\alpha}$. Numerical simulations show that the evolving dynamics of the system is well described by a Gaussian function for all $\\alpha>0$, regardless of how small or large they are. The same robustness holds for the Delta variant, although its exact value is not known. To quantify the impact of the fractional parameter on the resulting wave equation we compute the Fisher information matrix for both models and compare them against the non-analytical solutions obtained from solving the homogeneous differential equations explicitly. We find good agreement between the different methods at early times, but after some time the differences increase due to the use of different priors. Finally, we also study the impact of the dimensionality of the latent state space on the behavior of the solution sets. It is found that if the dimensionality of the latent state space is finite, the two approaches predict similar epidemic growth curves, but at very different speed rates, i.e., faster than exponential growth","330":"The process of revitalizing cities in the United States suffers from balky and unresponsive processes---de jure egalitarian but de facto controlled and mediated by city officials and powerful interests, not residents. We argue that, instead, our goal should be to put city planning in the hands of the people, and to that end, give ordinary residents pattern-based planning tools to help them redesign (and repair) their urban surrounds. Through this, residents can explore many disparate ideas, try them, and, if successful, replicate them, enabling bottom-up city planning through direct action. We describe a prototype for such a tool that leverages classic patterns to enable city planning by residents, using case studies from Los Angeles as guides for both the problem and potential solution. Finally, we discuss the benefits of sharing research resources with other social change experts to develop these approaches. Our hope is that this will encourage community effort toward developing more effective, sustainable, and livable cities, and, consequently, towards the long-term promotion of diversity and equal opportunity problems. City designers may also make conscious decisions to facilitate pandemic modeling to inform future city planning. In addition, we suggest dissemination strategies based on epidemiological models to incentivize engagement from broad sections of the city population. These actions could","331":"In this paper we develop a novel neural network model for predicting implied volatility surface. Prior financial domain knowledge is taken into account. A new activation function that incorporates volatility smile is proposed, which is used for the hidden nodes that process the underlying asset price. In addition, financial conditions, such as the absence of arbitrage, the boundaries and the asymptotic slope, are embedded into the loss function. This is one of the very first studies which discuss a methodological framework that incorporates prior financial domain knowledge into neural network architecture design and model training. The proposed model outperforms the benchmarked models with the option data on the S&P 500 index over 20 years. More importantly, the domain knowledge is satisfied empirically, showing the model is consistent with the existing financial theories and conditions related to implied volatility surface. Hence, the model can be readily applied to other assets and indices in terms of both accuracy and interpretability. Moreover, since the development of the model has been accompanied by experiments, such as the calibration study, sensitivity analysis, and out-of-sample forecasting exercise, the proposed approach shows promising results particularly from the perspective of practical applicability. To the best of our knowledge, this is the first systematic analytical work exploring a novel neural network architecture for modeling implied volatility","332":"The COVID-19 pandemic has proved to be one of the most disruptive public health emergencies in recent memory. Among non-pharmaceutical interventions, social distancing and lockdown measures are some of the most common tools employed by governments around the world to combat the disease. While mathematical models of COVID-19 are ubiquitous, few have leveraged network theory in a general way to explain the mechanics of social distancing. In this paper, we build on existing network models for heterogeneous, clustered networks with random link activation\/deletion dynamics to put forth realistic mechanisms of social distancing using piecewise constant activation\/deletion rates. We find our models are capable of rich qualitative behavior, and offer meaningful insight with relatively few intervention parameters. In particular, we find that the severity of social distancing interventions and when they begin have more impact than how long it takes for the interventions to take full effect. Finally, we identify important factors that lead to specific mitigation strategies that can help reduce the economic disruption stemming from strict social distancing. Our results emphasize the importance of both local information and global coordination through collaborative efforts between countries. To facilitate research further into the modeling of human interactions, we release an interactive dashboard with an R package that allows researchers to explore the data","333":"The COVID-19 pandemic has been influencing travel behaviour in many urban areas around the world since its onset. As a consequence, bike-sharing schemes have been affected\u2014partly due to the change in travel demand and behaviour as well as a shift from public transit. This study estimates the varying effect of the COVID-19 pandemic on the London bike-sharing system (Santander Cycles) over the period March\u2013December 2020. We employed a Bayesian second-order random walk time-series model to account for temporal correlation in the data. We compared the observed number of cycle hires and hire time with their respective counterfactuals (what would have been if the pandemic had not happened) to estimate the magnitude of the change caused by the pandemic. The results indicated that following a reduction in cycle hires in March and April 2020, the estimated reproduction numbers increased from 1.82 to 3.77 and 4.39 respectively, indicating a return to pre-pandemic cycling. However, this improvement was only seen after approximately two weeks of lockdown when the daily infection rate started to decrease. The results also suggested that as Santander Cycles could have been more resilient to the pandemic than most other systems. To the best of our","334":"We present an analysis of the well-known SIR and SEIR epidemic models subjected to hard caps on the proportion of infective individuals, and bounds on the allowable intervention strategies, such as social distancing, quarantining and vaccination. We describe the admissible and maximal robust positively invariant (MRPI) sets of these two models via the theory of barriers. We show how the sets may be used in the management of epidemics, for both perfect and imperfect\/uncertain models, detailing how intervention strategies may be specified such that the hard infection cap is never breached, regardless of the basic reproduction number. The results are clarified with detailed examples. Speculations are also provided to illustrate the difference between the MRPI and the non-MRPI sets. Finally, we specify several desirable properties of the MRPI set and prove that these specifications are satisfied by some known facts about the congruence condition. As a significant use case, we apply the results to the specification of the MHP (and its extensions) model parameters, which are currently not known exactly.) Our main result is that the MHP model cannot be deterministarily described by any finite state machine or equivalent Markov chain unless the entire spectrum of possible states is exponentially large. This fact implies that the","335":"In this work, we extended a stochastic model for football leagues based on the team's potential [R. da Silva et al. Comput. Phys. Commun. \\textbf{184} 661--670 (2013)] for making predictions instead of only performing a successful characterization of the statistics on the punctuation of the real leagues. Our adaptation considers the advantage of playing at home when considering the potential of the home and away teams. The algorithm predicts the tournament's outcome by using the market value or\/and the ongoing team's performance as initial conditions in the context of Monte Carlo simulations. We present and compare our results to the worldwide known SPI predictions performed by the\"FiveThirtyEight\"project. The results show that the algorithm can deliver good predictions even with a few ingredients and in more complicated seasons like the 2020 editions where the matches were played without fans in the stadiums. We also found that the ratio between the number of goals scored by the two competing teams and the average number of goals scored in all the matches is highly correlated with the current standings of the respective leagues. Finally, we point out that the inclusion of such additional information would improve the accuracy of the forecasts for this event type. To the best of our knowledge, this is the first attempt to apply","336":"The current COVID-19 pandemic and subsequent lockdowns have highlighted the close and delicate relationship between a country's public health and economic health. Macroeconomic models that use preexisting epidemic models to calculate the impacts of a disease outbreak are therefore extremely useful for policymakers seeking to evaluate the best course of action in such a crisis. We develop an SIR model of the COVID-19 pandemic that explicitly considers herd immunity, behavior-dependent transmission rates, remote workers, and indirect externalities of lockdown. This model is presented as an exit time control problem where lockdown ends when the population achieves herd immunity, either naturally or via a vaccine. A social planner prescribes separate levels of lockdown for two separate sections of the adult population: low-risk (ages 20-64) and high-risk (ages 65 and over). These levels are determined via optimization of an objective function which assigns a macroeconomic cost to the level of lockdown and the number of deaths. We find that, by ending lockdowns once herd immunity is reached, high-risk individuals are able to leave lockdown significantly before the arrival of a vaccine without causing large increases in mortality. Moreover, if we incorporate a behavior-dependent transmission rate which represents increased personal caution in response to increased infection levels, both output","337":"Protein-ligand interactions (PLIs) are fundamental to biochemical research and their identification is crucial for estimating biophysical and biochemical properties of many proteins. Currently, experimental characterization of these properties is the most accurate method, however, this is very time-consuming and labor-intensive. A number of computational methods have been developed in this context but most existing PLI prediction heavily depends on 2D protein sequence data. Here we present a novel parallel graph neural network (GNN) based PLI predictor called GNNF\\_Q which incorporates domain knowledge encoded as 3D structural information into learning the intermolecular interaction potential. In addition, GNNF\\_Q can be jointly trained with any other state-of-the-art PLI predictors without using any target function annotation. We demonstrate that our proposed GPLI predictor outperforms state-of-the-art models by 7.3% and 9.4%, respectively, for binary ligand binding experiments. The best performance was achieved by a new architecture, namely CovM9, which produces an interpretable feature space. Furthermore, we obtain valuable insights about the importance of amino acid residues involved in the binding process. Our approach provides a promising solution for further improvement of protein-ligand affinity","338":"This study examined a simulated confined space modelled as a hospital waiting area, where people who could have underlying conditions congregate and mix with potentially infectious individuals. It further investigated the impact of the volume of the waiting area, the number of people in the room, the placement of them as well as their weight. The simulation is an agent-based model (ABM). We compared the outcomes of two variations of the original ABM with those of three other plausible modifications. The first change made the global population much more receptive to infection through increased exposure to COVID-19; the second variation introduced a back-and-forth commute pattern between pairs of agents, even if one individual has neither been exposed to COVID-19 nor infected by it; and the third modification made the overall volume of the waiting area larger than the capacity of the room. Our results demonstrated that whether or not a person should be tested depends crucially on the choice of test method, and that testing methods can significantly affect the outcome of the experiment. In addition, we found that the age effect had no significant influence on the outcomes. Finally, using the simulator as a benchmark, we proposed strategies for improving the allocation of tests so as to reduce the peak infection rate and final outbreak size. These findings","339":"Facebook Disaster Maps (FBDM) is the first platform providing analysis-ready population change products derived from crowdsourced data targeting disaster relief practices. We evaluate the representativeness of FBDM data using the Mann-Kendall test and emerging hot and cold spots in an anomaly analysis to reveal the trend, magnitude, and agglommeration of population displacement during the Mendocino Complex and Woolsey fires in California, USA. Our results show that the distribution of FBDM pre-crisis users fits well with the total population from different sources. Due to usage habits, the elder population is underrepresented in FBDM data. During the two mega-fires in California, FBDM data effectively captured the temporal change of population arising from the placing and lifting of evacuation orders. Coupled with monotonic trends, the fall and rise of cold and hot spots of population revealed the areas with the greatest population drop and potential places to house the displaced residents. A comparison between the Mendocino Complex and Woolsey fires indicates that a densely populated region can be evacuated faster than a scarcely populated one, possibly due to the better access to transportation. In sparsely populated fire-prone areas, resources should be prioritized to move people to shelters as the displaced residents do not","340":"In this paper, we present a novel method for modelling and forecasting the spread of infectious diseases by combining machine learning with epidemiological models. Our approach consists of three key components: 1) presenting a flexible function classifier that can be adapted to study the dynamics of disease spread; 2) modeling the data on which such a classifier focuses; 3) integrating other important sources of information (e.g., from contact tracing and testing); and 4) designing a suitable regularization term for the resulting problem. We provide several experiments illustrating how our approach performs competitively against two state-of-the-art influenza epidemics, while adding new ingredients (e.g., test results and covariates). With respect to different goals, we also discuss the interpretability of our model. The implementation of our work has been open sourced as part of the Pyserini IR toolkit under AdvProp.git. Furthermore, we have built a dashboard with multiple R shiny apps embedded that make it straightforward to follow along with source code examples. The package features tutorials and detailed documentation. We also illustrate how our model provides insight into the most effective intervention strategies available so far to fight against pandemics. All together, we produce a forecast of COVID-19 related cases and deaths","341":"The study of spreading processes on dynamical networks has been among the most prominent challenges in complex systems for the last decade. The ongoing COVID-19 pandemic provides a good example of how these dynamics can be dramatically changed by the arrival of external agents, or the internal structure of the network. In this work we address this problem through a computational analysis of the dissemination dynamics of information in a multi-layer network model using Python's gazetteer library. We analyze the impact of the layers before and after the lockdown imposed as a result of the COVID-19 outbreak on our multilayer network model. Additionally, we investigate how different factors -- the number of infected individuals, the probability of quarantine, and social distancing -- affect the spread of the disease in the network model. The results show that the imposition of measures to reduce the transmission rate greatly affects the size of the epidemic peak and decreases considerably the total fraction of infected nodes. Moreover, we observed that too strict measures can drastically decrease the peak time of the infection, and increase the threshold parameter for a second phase of the epidemic spreading. Finally, we also present two main contributions from our study: (1) identifying the effective parameters of the multipath propagation model which governs the global behavior of the cascade dynamics","342":"We present a search for dark photon dark matter that could couple to gravitational-wave interferometers using data from Advanced LIGO and Virgo's third observing run with properties consistent with neutron star--black hole (NSBH) binaries in the 3rd-generation (3G) era. To do so, we use two different methodologies, one based on cross-correlation of the strain channels in the two nearly aligned LIGO detectors, and one that looks for excess power in the strain channels of the LIGO and Virgo detectors. The excess power method optimizes the Fourier Transform coherence time as a function of frequency, to account for the expected signal width due to Doppler modulations. We find no evidence of any electromagnetic counterparts to the 3G eigenfrequencies of the LIGO and Virgo detectors at energies above $\\sim 400$ keV, corresponding to frequencies close to those resonances calculated via particle MCMC simulations. With these results, we derive limits on the mass of the black holes that could directly collapse into photons or be converted into photons by being in the vicinity of the Einstein ring, assuming flat spacetime geometry. For example, we place upper bounds on the amount of dark radiation that would be","343":"While several non-pharmacological measures have been implemented for a few months in an effort to slow the coronavirus disease (COVID-19) pandemic in the United States, the disease remains a danger in a number of counties as restrictions are lifted to revive the economy. Making a trade-off between economic recovery and infection control is a major challenge confronting many hard-hit counties. Understanding the transmission process and quantifying the costs of local policies are essential to the task of tackling this challenge. Here, we investigate the dynamic contact patterns of the populations from anonymized, geo-localized mobility data and census and demographic data to create data-driven, agent-based contact networks. We then simulate the epidemic spread with a time-varying contagion model in ten large metropolitan counties in the United States and evaluate a combination of mobility reduction, mask use, and reopening policies. We find that our model captures the spatial-temporal and heterogeneous case trajectory within various counties based on dynamic population behaviors. Our results show that a decision-making tool that considers both economic cost and infection outcomes of policies can be informative in making decisions of local containment strategies for optimal balancing of economic slowdown and virus spread. Such information can be useful to guide ongoing policy efforts to","344":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\nWhich country's sportspersons are less likely to publicly criticize politicians?\n### Input:\nCOVID Super Expert\n### Response: With their place in the FIFA World Cup, Germany has become a powerful tool for political campaigning being widely used during the 2021 and 2022 national elections. Considering the fact that the main objective of these attacks is to sow polarization among the public, it is not surprising that hate speech against football fans (HSF) increases during these daunting times. However, the extent to which HSF is spread throughout the world on social media makes it essential to investigate its dynamics by using the tools of communication research. In this paper, we attempt to do just that, by collecting and analyzing Twitter communications spanning four years from the start of the pandemic all over Europe. We first propose a quantitative approach to detect offensive tweets, and then focus in on the top 0.5% most active ones. Next, we characterize them with respect to their tweet content types, finding significant differences between those who use more (offensive) words and those who do not. Then, we segment them based on their home clubs' rankings and interaction graphs, concluding that the former have a higher degree of engagement within our sample, but the latter seem to be more polarized across the whole European leagues. Finally, we make some qualitative observations about the evolution of the discourse on HSE before and after the 2020 UEFA EURO 2004","345":"We present our case study that aims to help professional assessors make decisions in human assessment, in which they conduct interviews with assessees and evaluate their suitability for certain job roles. Our workshop with two industrial assessors revealed that a computational system that can extract nonverbal cues of assesses from interview videos would be beneficial to assessors in terms of supporting their decision making. In response, we developed such a system based on an unsupervised anomaly detection algorithm using multimodal behavioral features such as facial keypoints, pose, head pose, and gaze. Moreover, we enabled the system to output how much each feature contributed to the outlierness of the detected cues with the purpose of enhancing its interpretability. We then conducted a preliminary study to examine the validity of the system's output by using 20 actual assessment interview videos and involving the two assessors. The results suggested the advantages of using unsupervised anomaly detection in an interpretable manner by illustrating the informativeness of its outputs for assessors. Our approach, which builds on top of the idea of separation of observation and interpretation in human-AI teaming, will facilitate human decision making in highly contextual domains, such as human assessment, while keeping their trust in the system. Overall, our work improved the understanding","346":"The COVID-19 epidemic has swept across more than 180 countries and territories since late January 2020, triggering waves of infections in both online and offline communities. Here we develop a simple stochastic model to describe the impact of online social media on the spread of epidemics. The model consists of two parts - (1) A deterministic component that captures the average number of secondary cases generated by each primary case, and (2) An stochastic component that accounts for the uncertainty in the early detection of secondary cases. We apply our model to the COVID-19 pandemic, and show that the uncertain data can reproduce the real variability of the daily number of new cases reported with high accuracy. In addition, we find that applying our model to other infectious diseases with similar transmission patterns as COVID-19, such as influenza, SARS, and Ebola, also yields notable improvements. Our work offers a computational approach to the study of how information from online social networks may be used to better understand the dynamics of disease spreading. Although our analysis is limited to only one country, its results suggest that there are important differences between the ways in which users discuss topics concerning different diseases, leading to inform future models of disease forecasting. Further, we find that the way people respond","347":"The COVID-19 pandemic has resulted in dramatic changes to the daily habits of billions of people. Users increasingly have to rely on home broadband Internet access for work, education, and other activities. These changes have resulted in corresponding changes to Internet traffic patterns. This paper aims to characterize the effects of these behavioral changes with respect to Internet service providers in the United States. We study three questions: (1)How did traffic demands change in the United States as a result of the COVID-19 pandemic?; (2)What effects have these changes had on Internet performance?; (3)How did service providers respond to these changes? We study these questions using data from a diverse collection of sources. Our analysis of interconnection data for two large ISPs in the United States shows a 30-60% increase in peak traffic rates in the first quarter of 2020. In particular, we observe traffic downstream peak volumes for a major ISP increase of 13-20% while upstream peaks increased by more than 30%. Further, there are significant temporal inconsistencies in performance across ISPs in conjunction with the traffic volume shifts, with evident latency increases after stay-at-home orders were issued, followed by a stabilization of traffic after April. Finally, we observe that in response to changes in","348":"In time series data analysis, detecting change points on a real-time basis (online) is of great interest in many areas, such as finance, environmental monitoring, and medicine. One promising means to achieve this is the Bayesian online change point detection (BOCPD) algorithm, which has been successfully adopted in particular cases in which the time series of interest has a fixed baseline. However, we have found that the algorithm struggles when the baseline irreversibly shifts from its initial state. This is because with the original BOCPD algorithm, the sensitivity with which a change point can be detected is degraded if the data points are fluctuating at locations relatively far from the original baseline. In this paper, we not only extend the original BOCPD algorithm to be applicable to a time series whose baseline is constantly shifting toward unknown values but also visualize why the proposed extension works. To demonstrate the efficacy of the proposed algorithm compared to the original one, we examine these algorithms on two real-world data sets and six synthetic data sets. The experimental results show that the proposed algorithm outperforms the original one for all examined datasets. We further find that the difference between the number of true changes and false ones is always larger than that between different versions of the algorithm. Our work demonstrates that","349":"Cone-beam computed tomography (CT) is known as one of the most effective ways to obtain high-quality images in both digital and anatomical domains. However, it is very challenging due to the nature of this technique and its close relationship with physiology. In this paper, we propose a novel approach for automated COVID-19 image reconstruction from CT using medical neural networks. We consider a set of objective markers placed in the lung to detect the start of the inspiratory phase, which helps reduce the artifacts found in traditional CT methods. Experimental results show that our method can achieve a higher quality spherical chest CT image than other state-of-the-art methods. The proposed method will provide a better performance for clinical diagnosis while avoiding overfitting compared to existing approaches. To further improve the model's sensitivity towards different object structures such as ribs and clavicles, we also design a branch-and-colorization network based on convolutional learning to segment the highlighted regions in CT slices. These experiments demonstrate that by introducing such structural information into the network, the CNN gains more focus onto clinically relevant region in CT slices and thus improves the diagnostic accuracy. Our code is available at https:\/\/github.com\/sivaramakrishnan-rajaraman\/cov","350":"The paper introduces a new approach for Multivariate Time Series forecasting that jointly infers and leverages relations among time series. Its modularity allows it to be integrated with current univariate methods. Our approach allows to trade-off accuracy and computational efficiency gradually via offering on one extreme inference of a potentially fully-connected graph or on another extreme a bipartite graph. In the potentially fully-connected case we consider all pair-wise interactions among time-series which yields the best forecasting accuracy. Conversely, the bipartite case leverages the dependency structure by inter-communicating the N time series through a small set of K auxiliary nodes that we introduce. This reduces the time and memory complexity w.r.t. previous graph inference methods from O(N^2) to O(NK) with a small trade-off in accuracy. We demonstrate the effectiveness of our model in a variety of datasets where both of its variants perform better or very competitively to previous graph inference methods in terms of forecasting accuracy and time efficiency. Particularly, we show that our model improves the performance of state-of-the-art TSC baselines including TSDA, TRAMO, RUNA, BYOL, XLM, XR, etc. More importantly, we illustrate","351":"Learning modality-fused representations and processing unaligned multimodal sequences are meaningful and challenging in multimodal emotion recognition. Existing approaches use directional pairwise attention or a message hub to fuse language, visual, and audio modalities. However, those approaches introduce information redundancy when fusing features and are inefficient without considering the complementarity of modalities. In this paper, we propose an efficient neural network to learn modality-fused representations with CB-Transformer (LMR-CBT) for multimodal emotion recognition from unaligned multimodal sequences. Specifically, we first perform feature extraction for the three modalities respectively to obtain the local structure of the sequences. Then, we design a novel transformer with cross-modal blocks (CB-Transformer) that enables complementary learning of different modalities, mainly divided into local temporal learning,cross-modal feature fusion and global self-attention representations. In addition, we splice the fused features with the original features to classify the emotions of the sequence. Finally, we conduct word-aligned and unaligned experiments on three challenging datasets, IEMOCAP, CMU-MOSI, and CMU-MOSEI. The experimental results show the superiority and efficiency of our proposed","352":"A model of the spread of viruses in selected city and in a network of cities is considered, taking into account the delay caused by the long incubation period of the virus. The effect of delay effects is shown in comparison with pandemics without such delay. A temporary asymmetry of the spread of infection has been identified, which means that the time for a pandemic to develop significantly exceeds the time for its completion. Model calculations of the spread of viruses in a network of interconnected large and small cities were carried out, and dynamics features were revealed in comparison with the spread of viruses in a single city, including the possibility of reinfection of megalopolis. Reinfection cases resulting from back-to-back (B2B) viral spreading processes were found to be sensitive to the ratio of vaccinating ages and testing efficiency, while the effectiveness of quarantine measures was found to be independent of both factors. The relevance of this study for the ongoing COVID-19 pandemic was discussed using the model parameters. Prediction of the number of necessary testings of asymptomatic persons during the course of the outbreak would help reduce the risk of further transmissions of viruses. Testing strategy was implemented according to the WHO guidelines, and it was observed that the technique based on testing","353":"We propose a general approach for estimating economic factors in a non-linear model of thermonuclear fusion plasmas via the direct numerical solution of the underlying hydrodynamic equations. The model takes into account the high complexity of the dynamics and employs the ADE (Atangana--Baleanu--Caputo) differentiation method to solve the associated initial value problem, which consists of a system of Volterra integral equations. We prove the existence and uniqueness of the global positive solution of the VECMA equation with the Dirichlet boundary condition. Moreover, we show that the model can be used as a tool in time domain analysis, and perform well with respect to the classical LASSO and SVAR models on real data for various periods of intensive study. In addition, we illustrate the usefulness of the proposed mathematical results by applying them directly to the design of energy efficient equipment and find some interesting conclusions at both the laboratory and practical levels. Finally, we discuss certain open problems where more progress is needed. All of these results are supported by ready to use Python 3.7 framework and the accompanying clean code. They appear particularly useful for any theoretical or applied physics involving linear heat transfer principles and also serve as effective tools to stimulate the development of advanced physical methods. With","354":"A novel bioinspired metaheuristic is proposed in this work, simulating how the Coronavirus spreads and infects healthy people. From an initial individual (the patient zero), the coronavirus infects new patients at known rates, creating new populations of infected people. Every individual can either die or infect and, afterwards, be sent to the recovered population. Relevant terms such as re-infection probability, super-spreading rate or traveling rate are introduced in the model in order to simulate as accurately as possible the coronavirus activity. The Coronavirus Optimization Algorithm has two major advantages compared to other similar strategies. First, the input parameters are already set according to the disease statistics, preventing researchers from initializing them with arbitrary values. Second, the approach has the ability of ending after several iterations, without setting this value either. Infected population initially grows at an exponential rate but after some iterations, the high number recovered and dead people starts decreasing the number of infected people in new iterations. As application case, it has been used to train a deep learning model for electricity load forecasting, showing quite remarkable results after few iterations. The code presented here, available in GitHub, can be easily modified to adapt to any other similar problem. In","355":"The COVID-19 pandemic has sparked unprecedented mobilization of scientists, already generating thousands of new papers that join a litany of previous biomedical work in related areas. This deluge of information makes it hard for researchers to keep track of their own research area, let alone explore new directions. Standard search engines are designed primarily for targeted search and are not geared for discovery or making connections that are not obvious from reading individual papers. In this paper, we present our ongoing work on SciSight, a novel framework for exploratory search of COVID-19 research. Based on formative interviews with scientists and a review of existing tools, we build and integrate two key capabilities: first, exploring interactions between biomedical facets (e.g., proteins, genes, drugs, diseases, patient characteristics); and second, discovering groups of researchers and how they are connected. We extract entities using a language model pre-trained on several biomedical information extraction tasks, and enrich them with data from the Microsoft Academic Graph (MAG). To find research groups automatically, we use hierarchical clustering with overlap to allow authors, as they do, to belong to multiple groups. Finally, we introduce a novel presentation of these groups based on both topical and social affinities, allowing users to drill down from groups","356":"There are increasing concerns that e-commerce platforms are amplifying vaccine-misinformation, misinformation, and conspiracy theories\u2014a phenomenon for which the term \u201cplatforms\u201d in the abstract domain of vaccines has already been used as a synonym for the coronavirus disease 2019 (COVID-19). Such malpractices call for a multi-level approach able to separate the ethical from the technical aspects of a platform, such as its design and development. This paper presents a multidisciplinary academic framework centered on the ethical concepts of fairness, non-discrimination, transparency, privacy, respect for autonomy, liberty, and trust. It focuses on the problems concerning the medical field, but incorporates also traditional political science perspectives. The contribution of this article is threefold: (1) presenting a detailed computational analysis of the social structure and political incentives behind the adoption of the stance against COVID-19 vaccines, (2) embedding these analyses into a broader discourse about the pandemic and its evolving societal impact, and finally (3) demonstrating how the evolution of the discourse and opinions surrounding COVID-19 vaccines provides a useful lens through which to understand the role of technology in shaping public opinion and guiding governance during this global crisis. We conclude by stressing the importance of developing","357":"Radiofrequency finger augmentation devices (R-FADs) are a recently introduced class of epidermal radiofrequency identification (RFID) sensor-tags attached to the fingers, communicating with a body-worn reader. These devices are promising candidates to enable Tactile Internet (TI) applications in the short term. R-FAD based on auto-tuning RFID microchips can be used as dielectric probes for the material of touched objects. However, due to the nearly unpredictable intrinsic variability of finger-object interaction, a single sensorized finger (single-channel device) is not enough to guarantee reliable data sampling. These limitations can be overcome by exploiting a multi-channel R-FAD sensorizing multiple fingers of the hand. In this paper, we propose a novel approach for learning a multi-channel robotic arm using a single-shot R-FAD. The proposed framework performs channel estimation from the sensory data collected by a conventional FIMO algorithm and then attempts to improve the robustness of the whole system through the introduction of a generative adversarial network. We experimentally evaluate our approach on 20 volunteers. Results show that the proposed system achieves better performance than other state-of-the-art methods for multi-channel robot arms","358":"In this paper, we concern with the problem of how to automatically extract the steps that compose real-life hand activities. This is a key competence towards processing, monitoring and providing video guidance in Mixed Reality systems. We use egocentric vision to observe hand-object interactions in real world tasks and automatically decompose a video into its constituent steps. Our approach combines hand-object interaction (HOI) detection, object similarity measurement and a finite state machine (FSM) representation to automatically edit videos into steps. We use a combination of Convolutional Neural Networks (CNNs) and the FSM to discover, edit cuts and merge segments while observing real hand activities. We evaluate quantitatively and qualitatively our algorithm on two datasets: the GTEA\\cite{li2015delving}, and a new dataset we introduce for Chinese Tea making. Results show our method is able to segment hand-object interaction videos into key step segments with high levels of precision. Finally, we demonstrate qualitatively different results across both datasets using our proposed FSMs. The code used to train our model is available at https:\/\/github.com\/taozh2017\/Real-Time-Segmentation-using-Ego-Bubble-Optics.git.io\/. Our","359":"Information sharing on social media must be accompanied by attentive behavior so that in a distorted digital environment, users are not rushed and distracted in deciding to share information. The spread of misinformation, especially those related to the COVID-19, can divide and create negative effects of falsehood in society. Individuals can also cause feelings of fear, health anxiety, and confusion in the treatment COVID-19. Although much research has focused on understanding human judgment from a psychological underline, few have addressed the essential issue in the screening phase of what technology can interfere amidst users' attention in sharing information. This research aims to intervene in the user's attention with a visual selective attention approach. This study uses a quantitative method through studies 1 and 2 with pre-and post-intervention experiments. In Study 1, we intervened in user decisions and attention by stimulating ten information and misinformation using the Visual Selective Attention System (VSAS) tool. In Study 2, we identified associations of user tendencies in evaluating information using the Implicit Association Test (IAT). The significant results showed that the user's attention and decision behavior improved after using the VSAS. The IAT results show a change in the association of user exposure, where after the intervention using VSAS, users tend not to share misinformation about","360":"The IIT Kanpur Consulting Group is one of the pioneering research groups in India which focuses on the applications of Machine Learning and Strategy Consulting for social good. The group has been working since 2018 to help social organizations, nonprofits, and government entities in India leverage better insights from their data, with a special emphasis on the healthcare, environmental, and agriculture sectors. The group has worked on critical social problems which India is facing including Polio recurrence, COVID-19, air pollution and agricultural crop damage. This position paper summarises the focus areas and relevant projects which the group has worked on since its establishment, and also highlights the group's plans for using machine learning to address social problems during the COVID-19 crisis. Through this document, we aim to establish a presence and status of ML-based social impact initiatives taken by the IIT Kanpur Consulting Group since its establishment in 2017. We will then describe the efforts being undertaken to build a diverse community of practitioners through different means, including online courses and professional diplomas, as well as further engage with other institutions while addressing societal challenges, such as identifying fake news, offensive language, hate speech, and misinformation about particular topics. Finally, we highlight the importance of developing basic guidelines and frameworks for using ML to address social","361":"While attention is a predictor for digital assets, and jumps in Bitcoin prices are well-known, we know little about its alternatives. Studying high frequency crypto data gives us the unique possibility to confirm that cross market digital asset returns are driven by high frequency jumps clustered around black swan events, resembling volatility and trading volume seasonalities. Regressions show that intra-day jumps significantly influence end of day returns in size and direction. This provides fundamental research for crypto option pricing models. However, we need better econometric methods for capturing the specific market microstructure of cryptos. All calculations are reproducible via the quantlet.com technology. The results reveal how Nifty Financial Instruments (NFI) relate to the dynamics of Gold, Silver, and Cryptoassets. They also establish new insights into the predictors of daily returns from past time series. Overall, this study shows that crypto option prices are mainly determined by high frequency jumps, which should be treated separately from the broader literature on financial markets. Further, it reveals that popular metrics used for evaluating portfolio performance does not capture the specific market microstructure of cryptos. Thus, we suggest using other advanced methodologies to analyze cryptocurrency portfolios. Our recommendations could improve both the quantitative and qualitative understanding of historical facts to","362":"We develop a stochastic model for the transmission of viral diseases in human population, which takes into account the delay and variability in the reported cases and deaths from both statistical nature and administrative regionality. The model shows how different factors affect the local dynamics of the disease, such as the fraction of undetected infected people, the testing policy, the quarantine threshold, and the rate of vaccination. The model can be applied to different countries with their demographic and socioeconomic parameters taken from publicly available data. We show that the evolution of the pandemic in Chile exhibits three distinct periods separated by two consecutive windows. In order to reduce the uncertainties on the second window we combine the model with a factor structure decomposition method. This allows us to study separately the impact of social distancing measures and vaccines on the epidemiological scenario during the months of the pandemic. Finally, we provide estimates on the duration of the first wave of the COVID-19 outbreak in Mexico and assess the effectiveness of the adopted public health policies. Our results suggest that if no actions were taken before the national lockdown was enacted there would have been at least one additional peak of infections estimated in late February - early March 2020. If the isolation measures were maintained until the beginning of May 2020 then another peak could have been attained","363":"Face masks have become one of the main methods for reducing the transmission of COVID-19. This makes face recognition systems in general, and triplet loss (TL) networks in particular, fail to perform identity verification with similar accuracy on masked faces. One of the key problems affecting masked face recognition is the lack of publicly available real-world masked face databases. In this paper we present a novel \\emph{Big Mask} dataset consisting of more than 1 million 3D model updates from 8K identities verified by 4 international research teams. We also provide a detailed analysis of the practical limitations of existing models and propose a new training strategy called Mixing-AdaSIN; several qualitative and quantitative results illustrate its effectiveness. Through comprehensive experiments, we show that state-of-the-art neural architectures fail to achieve convincing performance levels under the challenging setting of masked face recognition, and there is room for improvement with the proposed training strategies. Our source code will be made public at https:\/\/github.com\/cabani\/Real-World-Masked-Face-Dataset. Additionally, we will participate in the community detection problem at https:\/\/github.com\/iremeyiokur\/detect-face-mask-model-failure.","364":"The number of new scientific discoveries and technological inventions has increased dramatically over the past century, but there are growing concerns that progress is slowing. We analyze 25 million papers and 4 million patents across 6 decades and find that science and technology are becoming less disruptive of existing knowledge, a pattern that holds nearly universally across fields. We link this decline in disruptiveness to a narrowing in the utilization of existing knowledge. Diminishing quality of published science and changes in citation practices are unlikely to be responsible for this trend, suggesting that this phenomenon represents a fundamental shift in science and technology. The global surge in COVID-19 infections resulted in large volumes of research on SARS-CoV-2, but citations from this literature have become scarce. This analysis provides a comprehensive quantitative measure of the extent of this change, enabling us to identify potential reasons for this phenomenon at specific areas of science and technology. With the aid of synthetic data generation through machine learning, we present a simple method to quantify the magnitude of this change. We apply it to the biomedical domain and derive conclusions about the current state of the pandemic, showing that citations from retracted papers do not decrease in importance -- even though they receive more attention after publication. Finally, we find that most journal articles cited from retracted papers fall short","365":"Covid-19 has been causing severe loss to the human race. Considering the mode of spread and severity, it is essential to make it a habit to follow various safety precautions such as using sanitizers and masks and maintaining social distancing to prevent the spread of Covid-19. Individuals are widely educated about the safety measures against the disease through various modes such as announcements through online or physical awareness campaigns, advertisements in the media and so on. The younger generations today spend considerably more time on mobile phones and games. However, there are very few applications or games aimed to help in practicing safety measures against a pandemic, which is much lesser in the case of Covid-19. Hence, we propose a 2D survival-based game, SurviveCovid-19, aimed to educate people about safety precautions to be taken for Covid-19 outside their homes by incorporating social distancing and usage of masks and sanitizers in the game. SurviveCovid-19 has been designed as an Android-based mobile game, along with a desktop (browser) version, and has been evaluated through a remote quantitative user survey, with 30 participants. Results of the study suggest that the average participant age who participated in this questionnaire is 34 years old and predominantly","366":"We propose a new method for six-degree-of-freedom (6-DoF) autonomous camera movement for minimally invasive surgery, which, unlike previous methods, takes into account both the position and orientation information from structures in the surgical scene. In addition to locating the camera for a good view of the manipulated object, our autonomous camera takes into account workspace constraints, including the horizon and safety constraints. We developed a simulation environment to test our method on the\"wire chaser\"surgical training task from validated training curricula in conventional laparoscopy and robot-assisted surgery. Furthermore, we propose, for the first time, the application of the proposed autonomous camera method in video-based surgical skill assessment, an area where videos are typically recorded using fixed cameras. In a study with N=30 human subjects, we show that video examination of the autonomous camera view as it tracks the ring motion over the wire leads to more accurate user error (ring touching the wire) detection than when using a fixed camera view, or camera movement with a fixed orientation. Our preliminary work suggests that there are potential benefits to autonomous camera positioning informed by scene orientation, and this can direct designers of automated endoscopes and surgical robotic systems, especially when using chip-on-tip cameras","367":"During its history, the ultimate goal of economics has been to develop similar frameworks for modeling economic behavior as invented in physics. This has not been successful, however, and current state of the process is the neoclassical framework that bases on static optimization. By using a static framework, however, we cannot model and forecast the time paths of economic quantities because for a growing firm or a firm going into bankruptcy, a positive profit maximizing flow of production does not exist. Due to these problems, we present a dynamic theory for the production of a profit-seeking firm where the adjustment may be stable or unstable. This is important, currently, because we should be able to forecast the possible future bankruptcies of firms due to the Covid-19 pandemic. By using the model, we can solve the insurer's problem with uncertainty and determine the optimal policy at both the governmental and provincial level so as to optimize the profitability of the firm. The proposed model is mathematically identical with Newtonian model of a particle moving in a resisting medium, and so the model explains the reasons that stop the motion too. The frameworks for modeling dynamic events in physics are thus applicable in economics, and we give examples of their application here. (JEL D21) Keywords: Limitations of neoc","368":"Emotional support is a crucial ability for many conversation scenarios, including social interactions, mental health support, and customer service chats. Following reasonable procedures and using various support skills can help to effectively provide support. However, due to the lack of a well-designed task and corpora of effective emotional support conversations, research on building emotional support into dialog systems remains untouched. In this paper, we define the Emotional Support Conversation (ESC) task and propose an ESC Framework, which is grounded on the Helping Skills Theory. We construct an Emotion Support Conversation dataset (ESConv) with rich annotation (especially support strategy) in a help-seeker and supporter mode. To ensure a corpus of high-quality conversations that provide examples of effective emotional support, we take extensive effort to design training tutorials for supporters and several mechanisms for quality control during data collection. Finally, we evaluate state-of-the-art dialog models with respect to the ability to provide emotional support. Our results show the importance of support strategies in providing effective emotional support and the utility of ESConv in training more emotional support systems. The dataset and code will be available at https:\/\/github.com\/zliucr\/esconvo.html. This work defines the first attempt to make","369":"The number of cyber-attacks are continuing to rise globally. It is therefore vital for organisations to develop the necessary skills to secure their assets and to protect critical national infrastructure. In this short paper, we outline upon human-computer interaction elements which should be considered when developing a cybersecurity training platform, in an effort to maintain levels of user engagement. We provide an overview of existing training platforms before covering specialist cyber ranges. Aspects of human-computer interaction are noted with regards to their relevance in the context of cyber ranges. We conclude with design suggestions when developing a cyber range platform. Our suggested design improvements involve using colour scheme, text formatting and updating parameters in conjunction with providing an array of visualizations to allow for a more effective exploration of deep learning algorithms. An example of such improvements is shown through a use case scenario of our proposed platform. We also discuss how our findings can be applied to other domains to improve cyber range management. This work provides another step into the globalisation of human-computer interactions andcybersecurity.org will give further ideas and opinions on how to develop security systems given the importance of these components. All collected data, software, scripts, pre-trained models, and any other relevant information from the participants are available as open source technologies or been released under","370":"The need to forecast COVID-19 related variables continues to be pressing as the epidemic unfolds. Different efforts have been made, with compartmental models in epidemiology and statistical models such as AutoRegressive Integrated Moving Average (ARIMA), Exponential Smoothing (ETS) or computing intelligence models. These efforts have proved useful in some instances by allowing decision makers to distinguish different scenarios during the emergency, but their accuracy has been disappointing, forecasts ignore uncertainties and less attention is given to local areas. In this study, we propose a simple Multiple Linear Regression model, optimised to use call data to forecast the number of daily confirmed cases. Moreover, we produce a probabilistic forecast that allows decision makers to better deal with risk. Our proposed approach outperforms ARIMA, ETS and a regression model without call data, evaluated by three point forecast error metrics, one prediction interval and two probabilistic forecast accuracy measures. The simplicity, interpretability and reliability of the model, obtained in a careful forecasting exercise, is a meaningful contribution to decision makers at local level who acutely need to organise resources in already strained health services. We hope that this model would serve as a building block of other forecasting efforts that on the one hand would help front-line personal and decision","371":"The emergence due to the technological revolution and burst of population in the last decade has caused an explosion in the use of technology based on artificial intelligence (AI). The AI has become a panacea to human problems because it can analyze and solve the problem's solution without using any other tool. This paper focuses on reviewing different aspects of AI used in solving COVID-19 challenges and how AI techniques can help combat with COVID-19. We provide a comprehensive survey of the existing literature on the use of AI methods such as machine learning, deep learning, image analysis, speech recognition, and contact tracing for combating COVID-19 epidemics. Furthermore, we also discuss the future research directions for these efforts and highlight some potential challenges. We believe that this effort will be beneficial for the researchers to gain more insights into the rapidly evolving field of AI and COVID-19. Additionally, our discussion highlights some promising AI methodologies across diverse fields applicable to fight against COVID-19. As the next step, we intend to present a broad overview of AI applied to fight against COVID-19 epidemics. Subsequently, we examine the potential of AI tools to aid doctors, clinicians, and policy makers in their decision-making process, which can ultimately lead to better healthcare outcomes","372":"Instagram infographics are a digital activism tool that have redefined action frames in the Global South, including protests, hate crimes, and violence against civilians. In this study, we examine patterns of use of Instagram infograms by analyzing over 800,000 pictures shared on the social network Twitter from July 2015 to August 2020. We find that the usage of the hashtag\"datafication\"in conjunction with political events, such as the 2018 and 2020 US elections, was strongly associated with the overall election results at the top of the all-time leader board. With these findings, we argue that existing research on online radicalization and conspiracy theories may need to be revisited in the context of the global interconnected economy, given the inherent connectivity between people's smartphones and computers. Overall, our work provides researchers and policymakers with valuable information to understand the spread of misinformation and disinformation campaigns designed to sow civil unrest and inspire future interventions. At the same time, we highlight several challenges that need to be addressed in order to enable a better understanding of what kind of users and platforms can influence public opinion and policy. We believe that studying the dynamics of public WhatsApp groups - which have become central to the COVID19 pandemic - will help inform more effective messaging strategies, especially on a platform like Twitter","373":"The growing capability and availability of generative language models has enabled a wide range of new downstream tasks. Academic research has identified, quantified and mitigated biases present in language models but is rarely tailored to downstream tasks where wider impact on individuals and society can be felt. In this work, we leverage one popular generative language model, GPT-3 (GPT-Net), with the goal of writing unbiased and realistic job advertisements. We first assess the bias and realism of zero-shot generated advertisements and compare them to real-world advertisements. We then evaluate prompt-engineering and fine-tuning as debiasing methods. We find that prompt-engineering with diversity-encouraging prompts gives no significant improvement to bias, nor realism. Conversely, fine-tuning, especially on unbiased real advertisements, can improve realism and reduce bias. We therefore propose a dual-pronged strategy to mitigate both bias and realism risks. Our results show that our approach mitigates reality risk with a small increase in accuracy on unbiased real advertisements while increasing robustness against confounding features. This suggests that limiting exposure to dissimilarity between training and test examples serves as a more effective mitigation method for most natural language processing tasks. Finally, we provide a detailed report of successful application of","374":"We propose VoIPLoc, a novel location fingerprinting technique and apply it to the VoIP call provenance problem. It exploits echo-location information embedded within VoIP audio to support fine-grained location inference. We found consistent statistical features induced by the echo-reflection characteristics of the locations into recorded speech. These features are discernible within traces received at the VoIP destination, enabling location inference. We evaluated VoIPLoc by developing a dataset of audio traces received through VoIP channels over the Tor network. We show that recording locations can be fingerprinted and detected remotely with a low false-positive rate, even when a majority of the audio samples are unlabelled. Finally, we note that the technique is fully passive and thus undetectable, unlike prior art. VoIPLoc is robust to the impact of environmental noise and background sounds, as well as the impact of compressive codecs and network jitter. The technique is also highly scalable and offers several degrees of freedom terms of the fingerprintable space. We expect that this paper will open up new research directions for uncovering involuntary sub-tasks in video recordings. Our code and data are available at https:\/\/github.com\/ssu-humane\/voiploc.git.io\/.","375":"Modern Bayesian approaches and workflows emphasize in how simulation is important in the context of model developing. Simulation can help researchers understand how the model behaves in a controlled setting and can be used to stress the model in different ways before it is exposed to any real data. This improved understanding could be beneficial in epidemiological models, specially when dealing with COVID-19. Unfortunately, few researchers perform any simulations. We present a simulation algorithm that implements a simple agent-based model for disease transmission that works with a standard compartment epidemiological model for COVID-19. Our algorithm can be applied in different parameterizations to reflect several plausible epidemic scenarios. Additionally, we also model how social media information in the form of daily symptom mentions can be incorporate into COVID-19 epidemiological models. We test our social media COVID-19 model with two experiments. The first using simulated data from our agent-based simulation algorithm and the second with real data using a machine learning tweet classifier to identify tweets that mention symptoms from noise. Our results shows how a COVID-19 model can be (1) used to incorporate social media data and (2) assessed and evaluated with simulated and real data. Finally, we provide insights on how COVID-19 models should be developed more broadly","376":"The epidemic of COVID-19 has caused an unpredictable and devastated disaster to the public health in different territories around the world. Common phenotypes include fever, cough, shortness of breath, and chills. With more cases investigated, other clinical phenotypes are gradually recognized, for example, loss of smell, and loss of tastes. Compared with discharged or cured patients, severe or died patients often have one or more comorbidities, such as hypertension, diabetes, and cardiovascular disease. In this study, we systematically collected and analyzed COVID-19-related clinical phenotypes from 70 articles. The commonly occurring 17 phenotypes were classified into different groups based on the Human Phenotype Ontology (HPO). Based on the HP classification, we systematically analyze three nervous phenotypes (loss of smell, loss of taste, and headache) and four abdominal phenotypes (nausea, vomiting, abdominal pain, and diarrhea) identified in patients, and found that patients from Europe and USA turned to have higher nervous phenotypes and abdominal phenotypes than patients from Asia. A total of 23 comorbidities were found to commonly exist among COVID-19 patients. Patients with these comorbidities such as diabetes and kidney failure had worse outcomes compared with those without these","377":"Parking demand forecasting and behaviour analysis have received increasing attention in recent years because of their critical role in mitigating traffic congestion and understanding travel behaviours. However, previous studies usually only consider temporal dependence but ignore the spatial correlations among parking lots for parking prediction. This is mainly due to the lack of direct physical connections or observable interactions between them. Thus, how to quantify the spatial correlation remains a significant challenge. To bridge this gap, we propose a spatial-aware parking prediction framework, which includes two steps, i) spatial connection graph construction based on the semantic structure of parking areas, and ii) Spatial ODE modeling by using the constructed graphs as additional information for modelling spatial dependence in parking behavior. The theoretical results show that the proposed approach outperforms the state-of-the-art methods on real-world datasets. Finally, we also demonstrate the applicability of the proposed method on quantifying spatial heterogeneity in a case study with various examples from different cities\/regions in China. Our work has revealed the impact of spatial factors in parking behaviour and also accentuated the importance of developing more accurate and reliable spatio-temporal forecasts in order to facilitate travel management and planning. All codes are publicly available at https:\/\/github.com\/yuhuan-wu\/Spatio","378":"On May $28^{th}$ and $29^{st}$, a two day workshop was held virtually, facilitated by the Beyond Center at ASU and Moogsoft Inc. The aim of this paper is to report on the experiences of both the authors in terms of their social contacts with colleagues from different universities, national and international level, during the COVID-19 pandemic period. We draw upon our previous work on how the group action produces its own rewards as well as additional benefits such as increased exposure to other views and more diverse discussions over time. With respect to the first point, we note that during the pre-pandemic months there was no virtual meeting for us which would have been a great success story. Both the participants and the organization were very successful in getting people to share their opinions with colleagues, and thus obtaining valuable feedback from them. Regarding the second point, we explain the circumstances under which it took place and discuss some lessons learned. An interactive online version of the proceedings will be made available to the interested readers via a web link. We encourage any reader to use the provided materials and engage in independent research aimed at understanding the relevant issues surrounding the corona-virus subject. Lastly, we provide an accounting of the costs incurred by the","379":"This work sought to find out the effectiveness of Anambra Broadcasting Service (ABS) Radio News on teaching and learning. The study focused mainly on listeners of ABS radio news broadcast in Awka, the capital of Anambria State, Nigeria. Its objectives were to find out; if Awka based students are exposed to ABS radio; to discover the ABS radio program students favorite; the need gratification that drives students to listen to ABS radio news; the contributions of radio news to students teaching and learning; and effectiveness of ABS radio news on teaching and learning in Awka. The population of Awka students is 198,868. This is also the population of the study. But a sample size of 400 was chosen and administered with questionnaires. The study was hinged on the uses and gratification theory. It adopted a survey research design. The data gathered was analyzed using simple percentages and frequency of tables. The study revealed that news is very effective in teaching and learning. It was concluded that news is the best instructional media to be employed in teaching and learning. Among other things, it was recommended that teachers and students should listen to and make judicious use of news for academic purposes. In addition, it was found that schools cannot function without input from radio news. Thus","380":"Medical devices (MD) are regulated by various international regulatory agencies and policies to guarantee that they are not discriminatory on the basis of their design. However, there are empirical studies reporting that major MD violations are linked to human-centered designs. Therefore, we examine these links in greater depth. We discuss how regulations and policies affect the ability of MD models to provide reliable guidance for medical practitioners. Finally, we present some potential future research directions to explore in further detail. These results highlight the importance of considering the broader social impacts of MD regulations and policies via an holistic view on this topic. In addition, as far as we know, most existing studies focused only on one aspect of MD regulation and did not consider their causal relationships, which is very relevant given the lack of interpretability of model outcomes. Our study provides a complete overview of the MD landscape via the lens of both intended users and developers of MWML technologies. We also identify several main causes of disagreement between stakeholders concerning the usability of vaccines and diagnostic medicines. For each class of MDRs, we explain why specific designs are likely to achieve good adoption from different market segments and what challenges might arise when trying to accommodate multiple categories of MDRs. Moreover, our findings indicate that it is still unclear whether generic smartphone-based","381":"This workshop is the fourth issue of a series of workshops on automatic extraction of socio-political events from news, organized by the Emerging Market Welfare Project with the support of the Joint Research Centre of the European Commission and with contributions from many other prominent scholars in this field. The purpose of this series of workshops is to foster research and development of reliable, valid, robust, and practical solutions for automatically detecting descriptions of socio-political events, such as protests, riots, wars and armed conflicts, in text streams. This year workshop contributors make use of the state-of-the-art NLP technologies, such as Deep Learning, Word Embeddings and Transformers and cover a wide range of topics from text classification to news bias detection. Around 40 teams have registered and participated in at least one of these tasks. Three teams were selected to carry out a workshop on each topic. One team focused on solving a specific problem while the two others addressed general issues and concerns. The workshop was held virtually due to the COVID-19 pandemic and because of the social distancing measures adopted by our mentors. Despite the challenges this brought, we were determined to build upon previous successful initiatives and introduce new techniques where more than ever. We thus present four approaches which are based on (a)","382":"Mental health conditions remain underdiagnosed even in countries with common access to advanced medical care. The ability to accurately and efficiently predict mood from easily collectible data has several important implications for the early detection, intervention, and treatment of mental health disorders. One promising data source to help monitor human behavior is daily smartphone usage. However, care must be taken to summarize behaviors without identifying the user through personal (e.g., personally identifiable information) or protected (e.g., race, gender) attributes. In this paper, we study behavioral markers of daily mood using a recent dataset of mobile behaviors from adolescent populations at high risk of suicidal behaviors. Using computational models, we find that language and multimodal representations of mobile typed text (spanning typed characters, words, keystroke timings, and app usage) are predictive of daily mood. However, we find that models trained on such datasets do not generalize well to out-of-domain tasks where challenging factors include new entities ('social networks', change over time), or vary across users' demographics. Our results highlight the need for careful modeling of self-reported affective states and encouraging non-invasive methods to capture how individuals report their feelings online. Studying complex relationships between multiple modalities of behavioral data and","383":"Differential networks (DN) are important tools for modeling the changes in conditional dependencies between multiple samples. A Bayesian approach for estimating DNs, from the classical viewpoint, is introduced with a computationally efficient threshold selection for graphical model determination. The algorithm separately estimates the precision matrices of the DN using the Bayesian adaptive graphical lasso procedure. Synthetic experiments illustrate that the Bayesian DN performs exceptionally well in numerical accuracy and graphical structure determination in comparison to state of the art methods. The proposed method is applied to patient flow data for COVID-19 patients to investigate the change in DN structure between various phases of the pandemic. Different from previous work on variational inference based DN estimation, the present study focuses on the posterior distribution shift rather than computing the mean or variance of the DN. The code used has been made publicly available at https:\/\/github.com\/bachtranxuan\/Bayes-DN-for-COVID-19-pandemic-data. Keywords: Bayesian DN, discrete Lindley, Monte Carlo, sampling, information criteria, bias correction, construction of confidence regions, etc. Numerical simulations show that the newly developed Bayesian DN performs better than existing methods in terms of both accuracy and interpretability. All the","384":"This article explores the territorial differences in the onset and spread of COVID-19 and the excess mortality associated with the pandemic, across the European NUTS3 regions and US counties. Both in Europe and in the US, the pandemic arrived earlier and recorded higher Rt values in urban regions than in intermediate and rural ones. A similar gap is also found in the data on excess mortality. In the weeks during the first phase of the pandemic, urban regions in EU countries experienced excess mortality of up to 68pp more than rural ones. We show that, during the initial days of the pandemic, territorial differences in Rt by the degree of urbanisation can be largely explained by the level of internal, inbound and outbound mobility. The differences in the spread of COVID-19 by rural-urban typology and the role of mobility are less clear during the second wave. This could be linked to the fact that the infection is widespread across territories, to changes in mobility patterns during the summer period as well as to the different containment measures which reverse the causality between mobility and Rt. SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1007\/s11571-022-09802-5. Copyright may be","385":"We introduce non-Markovian SIR epidemic spreading model inspired by the characteristics of the COVID-19, which is characterized by the existence of infectious undetected cases and the different social distancing rules across communities. We construct a heterogeneous graph based on the daily infection data between community members and then propose a Markovian network model with time delay to describe the complex disease patterns. A numerical simulation shows that the heterogeneity in the degree distribution of the network model enhances the dynamical properties of the epidemic, including the peak number of infections and the final extent of the epidemic. The real mobility data from the Google Community Mobility Reports can be used to measure the effectiveness of the proposed model. We also discuss how the lack of markovicity in the model may impede reliable estimation of its parameters using such data. Our work demonstrates that the proposed framework, which combines both novel mathematical techniques and extensive amount of realistic data, can be applied to diverse communities worldwide and serves as a useful tool for revealing the detailed structure of their interconnected networks. Although we focus on South Korea, the method applies to most countries in the world, and thus it marks a major step forward in modeling the spread of epidemics. The approach involves two steps: 1) constructing a parsimonious representation of the","386":"Named After Clio, the Greek goddess of history, cliophysics is a daughter (and in a sense an extension) of econophysics. Like econophysics it relies on the methodology of experimental physics. Its purpose is to conduct a scientific analysis of historical events. Such events can be of sociological, political or economic nature. In this last case cliophysics would coincide with econophysics. The main difference between cliophysics and econophysics is that the description of historical events may be qualitative as well as quantitative. For the handling of qualitative accounts cliophysics has developed an approach based on the identification of patterns. To detect a pattern the main challenge is to break the\"noise barrier\". The very existence of patterns is what makes cliophysics possible and ensures its success. Briefly stated, once a pattern is detected, it allows predictions to be made. As the capacity to make successful predictions is the hallmark of any science, it becomes easy to decide whether or not the claim made in the title of the paper belongs to such a category. A number of examples of clusters of similar events will be given which should convince readers that historical events can be simplified almost at will very much as in physics","387":"We develop the basic tools for classifying edge-to-edge tilings of the sphere by congruent quadrilaterals. Then we classify the case with 3 distinct edge lengths: such tilings are a sequence of two-parameter families of 2-layer earth map tilings with 2n (n>=3) tiles, a one-sided family of quadrilateral subdivisions of the octahedron with 24 tiles together with a flip modification in one special case, and a sequence of 3-layer earth map tilings with 8n (n>=2) tiles together with two flip modifications when n is odd. We also describe the moduli and calculate the geometric data. For the cases with n>=3, we show how the Segal space can be localized to a finite subspace of the projective ring of Witt vectors at a root of unity. Finally, we compute the Wasserstein distances between these projections on their attractors. In particular, we find a duality under which the Betti distance $\\beta$-norm is reversed, so that $d_c(X,Y) = \\beta^\\circ X^{1\/4}$, where $X$ is a point in $\\mathbb{","388":"Closed form expressions for the number of daily tests of Covid-19 were obtained from The European Forecasting Exchange (LFE) data, using machine learning models. These were converted to logistic equations and used as basis functions to fit a set of test data with 2-week intervals. A cross-validation method was applied to select one or two of the LFE model predictions. In order to generate coverage estimates for the parameters of the LFE model, we have also developed a simple R\/S algorithm based on the Bayesian information criterion. The usefulness of the proposed methodology was illustrated with the results of applying it to the analysis of Russian surveillance data for the pandemic's second wave. Our approach allows us to obtain accurate estimates for the doubling time between days $d_i$ and can be used to predict considerably different epidemic dynamics than other approaches. Moreover, our methods provide the possibility to analyze much more complex epidemic data sets, such as those derived from the Internet, thus yielding earlier warning signals about outbreaks. This could enable health authorities to respond rapidly and multilaterally to the current pandemic by monitoring these new cases closely and limiting non-essential travel. Finally, our code and data are available at https:\/\/github.com\/masadcv\/c","389":"Multivariate time-dependent data, where multiple features are observed over time for a set of individuals, are increasingly widespread in many application domains. To model these data we need to account for relations among both time instants and variables and, at the same time, for subjects heterogeneity. We propose a new co-clustering methodology for clustering individuals and variables simultaneously that is designed to handle both functional and longitudinal data. Our approach borrows some concepts from the curve registration framework by embedding the Shape Invariant Model in the Latent Block Model, estimated via a suitable modification of the SEM-Gibbs algorithm. The resulting procedure allows for several user-defined specifications of the notion of cluster that could be chosen on substantive grounds and provides parsimonious summaries of complex longitudinal or functional data by partitioning data matrices into homogeneous blocks. We apply our method to a study of COVID-19 patient trajectories and show that it successfully captures the intrinsic topological properties of the data. Finally, we demonstrate its usefulness with a large variety of examples from public health to socioeconomic studies and document different patterns of cluster behavior on real-world datasets. All code used in this work is available online. \\url{https:\/\/github.com\/jannisborn\/","390":"The current COVID-19 pandemic has put increasing focus on demand for emergency department (ED) services. Since patient triage and monitoring are crucial for hospitalization, ventilator status, and outcomes of intensive care unit (ICU), we propose a novel Joint ED Multi-task Learning (JOELIN) framework to address three challenges related to the multi-class nature of patient visits under difficult operational conditions: 1) variability in trip earnings, 2) unclear pathways between request types, and 3) the lack of detailed patient visit information. We transform large volumes of raw nursing notes into feature vectors using a Bidirectional Encoder Representations from Transformers (BERT)-based architecture which learns to capture discriminative features of patient visits. These embeddings are then used as features for a Support Vector Machine (SVM) classifier with a kernel of the log-likelihood function. A cross-validation scheme is employed to train and evaluate the performance of the proposed approach. Extensive experiments on two real publicly available datasets demonstrate that our method outperforms all comparison methods with substantial margins over several state-of-the-art baselines. Furthermore, we show that our model provides better overall accuracy than a collection of benchmarked models when applied to the","391":"Motion degradation is a major challenge in Magnetic Resonance Imaging (MRI). This work addresses the problem of how to obtain higher quality, super-resolved motion-free, reconstructions from highly undersampled MRI data. In this work, we present for the first time a variational multi-task framework that allows joining three relevant tasks in MRI: reconstruction, registration and super-resolution. Our framework takes a set of multiple undersampled MR acquisitions corrupted by motion into a novel multi-task optimisation model, which is composed of an $L^2$ fidelity term that allows sharing representation between tasks, super-resolution foundations and hyperelastic deformations to model biological tissue behaviors. We demonstrate that this combination yields to significant improvements over sequential models and other bi-task methods. Our results exhibit fine details and compensate for motion producing sharp and highly textured images compared to state of the art methods. Furthermore, we show that our method can be used to explore the physical mechanism responsible for the motion effects on MRI. Finally, we validate our findings with two publicly available datasets. Our work provides a new perspective on combining deep learning with bioimaging techniques in healthcare. It also demonstrates the potential of using such powerful transfer learning capabilities to improve the performance of clinical image analysis systems","392":"In this paper, we introduce XGBoost as a new XL-BEL benchmark for classification of English and Chinese texts. To ensure a fair comparison among different modalities, we control the style of writing in each dataset by removing outliers and projecting all points onto the space of integer labels. The results show that using pre-trained language models such as BERT on these datasets yields very high performance scores, e.g., the macro F1 score of our model reaches 0.959. We further conduct experiments and analyses to explore how well the outlier detection method can be generalized to other languages, where there are no training samples available yet for low-resource ones. Our code has been made publicly accessible at https:\/\/github.com\/XiaoyuanGuo\/XGBoost. Besides developing machine learning methods, we also provide empirical evidence of the difficulty posed by real-world outlier detection tasks. The provided analysis demonstrates that BERT is suitable to tackle generalization problems related to outlier detection in multilingual settings. Finally, we show that our proposed approach achieves state-of-the-art performance over the BEIR benchmark in both written text and semantic video tasks. All codes and data are available at https:\/\/github.com\/","393":"The COVID-19 pandemic has led many governments to implement lockdowns. While lockdowns may help to contain the spread of the virus, it is possible that substantial damage to population well-being will result. This study relies on Google Trends data and tests whether the lockdowns implemented in Europe and America led to changes in well-being related topic searches. Using different methods to evaluate the causal effects of lockdown, we find a substantial increase in the daily search intensity for boredom in Europe and the US. We also found a significant increase in searches for loneliness, worry and sadness, while searches for stress, suicide and divorce on the contrary fell. Our results suggest that people's mental health may have been severely affected by the lockdown. Moreover, we show that the increased anxiety caused by the lockdown was only partly due to situational change (i.e. the psychological costs of the lockdown) and not directly associated with human behaviour. Thus, our findings provide valuable insights into how much research efforts should be focused on supporting people's wellbeing during the ongoing pandemic. Further, our methodology can be applied to other countries and regions to improve their public mental health policies. The results of this study demonstrate that the experiences gained from conducting research on online mental health during the COVID-19 pand","394":"In this work, we address a multicoupled dynamics on complex networks with tunable structural segregation. Specifically, we propose a networked epidemic spreading under a vaccination campaign with agents in favor and against the vaccine. Our results show that such coupled dynamics exhibits a myriad of phenomena such as nonequilibrium transitions accompanied by bistability. Besides we observe the emergence of an intermediate optimal segregation level where the community structure enhances negative opinions over vaccination but counterintuitively hinders - rather than favoring - the global disease spreading. Thus, our results hint vaccination campaigns should avoid policies that end up segregating excessively anti-vaccine groups so that they effectively work as echo chambers in which individuals look to confirmation without jeopardising the safety of the whole population. Moreover, our study offers insights into how effective are different non-pharmaceutical interventions (NPIs) aimed at reducing the transmission of diseases through social distancing. By considering disparate regions as nodes in a network, we demonstrate that immunizing them can be more effective policy for preventing recurrent infections and protecting vulnerable clusters of society. We further identify two main factors impeding the effectiveness of NPIs; namely, high structural heterogeneity between the areas associated with similar health risks and perceived susceptibility. To overcome these issues, we introduce a novel network based algorithm","395":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\nHow many years does data corresponds to the period from 28 of June to 13 of December 2020\n### Input:\nCOVID Super Expert\n### Response: Several R-packages to deal with spatiotemporal count areal data are currently available. Each package is based on different models and\/or statistical methodologies. Unfortunately, results generated by these models are rarely comparable due to differences in notation and methods which ultimately hinder the robustness and effectiveness of such models. These models have become crucial nowadays, because their ability to analyze COVID-19 related data. This prompted us to revisit three spatiotemporal count models and monitor their performance under the same conditions (on a same data set and under a consensus notation). The three different models assessed are representative of the main two extended methodologies used to analyse spatiotemporal count data i.e., the classical approach (based on Penalised Likelihood meanwhile and based to Estimating Equations) and the Bayesian point of view. As a case study, we have used these packages to analyze COVID-19 data corresponding to 24 health regions within the Valencian Community, Spain. In particular, daily COVID-19 positive individuals are used to predict daily hospitalisations. Data corresponds to the period from 28 of June to 13 of December 2020. We then provide a comparative analysis of the predictive performances of the considered models for estimating the basic reproduction number $R_0","396":"As the year 2020 draws to an end, several new strains have been reported for the SARS-CoV-2 coronavirus, the agent responsible for the COVID-19 pandemic that has afflicted us all this past year. However, it is difficult to comprehend the scale, in sequence space, geographical location and time, at which SARS-CoV-2 mutates and evolves in its human hosts. To get an appreciation for the rapid evolution of the coronavirus, we built interactive scalable vector graphics maps that show daily nucleotide variations in genomes from the six most populated continents compared to that of the initial, ground-zero SARS-CoV-2 isolate sequenced at the beginning of the year. Availability: Mutation time maps are available from https:\/\/bcgsc.github.io\/SARS2\/map\/index.html. This web application allows researchers to easily browse and analyze worldwide spread of the devastating mutation of SARS-CoV-2. Results: The global distributions of both population coverage and mutation rate indicate that areas with more densely defined populations or more people who travel from commuter cities to rural areas, such as those bordering large urban conurbations, contain the highest rates of genetic variation. The United States,","397":"This study presents estimates of the heart rate from face videos for student assessment. This information could be very valuable to track their status along time and also to estimate other data such as their attention level or the presence of stress that may be caused by cheating attempts. The recent edBBplat, a platform for student behavior modelling in remote education, is considered in this study1. We implemented the video based on the real-time eye tracking signal provided by the smartwatch.2. Using different preprocessing techniques, we achieved a good accuracy for both short term and long term predictions.3. In addition, we conducted a unique test to validate how well deep learning methods can predict the actual pulse width obtained from the smartwatch.4. Student behavior modeling showed impressive results with 97% precision, recall, and F1 score for each of the features.5. Real-time processing of sensor input gave remarkable results including an accuracy of 99.25%, F1 score of 96.23%, and an average error of 0.69 beats per minute (BPM).6. Interventional studies for future research are discussed and our code is made publicly available at https:\/\/github.com\/gbriel21\/digital_pulse_width_estimation_system\/.7.","398":"Coronavirus disease 2019 (COVID-19) has infected more than one million people globally and caused more than 55,000 deaths, as of April 3 in 2020. Radiological findings are important sources of information in guiding the diagnosis and treatment of COVID-19. However, the existing studies on how radiological findings are correlated with COVID-19 are conducted separately by different hospitals, which may be inconsistent or even conflicting due to population bias. To address this problem, we develop natural language processing methods to analyze a large collection of COVID-19 literature containing study reports from hospitals all over the world, reconcile these results, and draw unbiased and universally-sensible conclusions about the correlation between radiology findings and COVID-19. We apply our method to the CORD-19 dataset and successfully extract a set of radiology findings that are closely tied to COVID-19. Further, we conduct experiments to evaluate the generalizability of our method to other datasets, including those from countries where the reported number of cases differs significantly. Finally, we also discuss the causal relationship between radiology findings and COVID-19 using our proposed inference framework. Our data preprocessing pipeline can be applied to any scientific corpus requiring no expert annotation besides the initial annotations.","399":"The SARS-CoV2 pandemic has created a global race for a cure. One approach focuses on designing a novel variant of the human angiotensin-converting enzyme 2 (ACE2) that binds more tightly to the SARS-CoV2 spike protein and diverts it from human cells. Here we formulate a novel protein design framework as a reinforcement learning problem. We generate new designs efficiently through the combination of a fast, biologically-grounded reward function and sequential action-space formulation. The use of Policy Gradients reduces the compute budget needed to reach consistent, high-quality designs by at least an order of magnitude compared to standard methods. Complexes designed by this method have been validated by molecular dynamics simulations, confirming their increased stability. This suggests that combining leading protein design methods with modern deep reinforcement learning is a viable path for discovering a Covid-19 cure and may accelerate design of peptide-based therapeutics for other diseases. Design of complementary proteins can also be formulated as a multi-objective, non-linear constrained optimization problem. Although current state-of-the art in silico folding algorithms are able to report excellent results on the binding energies of various ligands to either the ACE2 or the antibodies, the intrinsic chemical stability","400":"The number of reported COVID-19 cases has increased dramatically worldwide, but there are limited efforts to understand the underlying epidemiological dynamics. We examine the spatiotemporal distributions and time differences of coronavirus disease 2019 (COVID-19) deaths and cases with non-pharmaceutical interventions (NPIs) in the United States. We analyze the data using several Bayesian model frameworks to estimate the basic reproduction numbers. The estimated parameters for the Bayesian models agree well with the observed ones. Our analysis shows that the early implementation of NPIs was effective in decreasing the transmission rates of COVID-19. However, the stringency of measures continued to be effective even after March 27th, when the most stringent NPIs were implemented across the country. This suggests the utility of case counts as a complementary tool to assist public health decision making. Moreover, our findings provide insights into how different US states adopted similar NPIs during the first wave of COVID-19 infections. While further studies are needed to better understand the dynamic patterns of transmissible diseases under control strategies, such as wearing masks, social distancing, and vaccination, we show that the current dataset is sufficient to capture the salient dynamical characteristics of transmissible infectious diseases under controlled spreading conditions in the USA","401":"The identification of protein\u2013ligand interaction plays a key role in biochemical research and drug discovery. Although deep learning has recently shown great promise in discovering new drugs, there remains a gap between deep learning-based and experimental approaches. Here, we propose a novel framework, named AIMEE, integrating AI model and enzymological experiments, to identify inhibitors against 3CL protease of SARS-CoV-2 (Severe acute respiratory syndrome coronavirus 2), which has taken a significant toll on people across the globe. From a bioactive chemical library, we have conducted two rounds of experiments and identified six novel inhibitors with a hit rate of 29.41%, and four of them showed an IC(50) value <3 \u03bcM. Moreover, we explored the interpretability of the central model in AIMEE, mapping the deep learning extracted features to the domain knowledge of chemicals from a biological library. Based on this knowledge, a commercially available compound was selected and was proven to be an activity-based probe of 3CL(pro). This work highlights the great potential of combining deep learning models and biochemical experiments for intelligent iteration and for expanding the boundaries of drug discovery. The code and data are available at https:\/\/github.com\/SIAT-code\/A","402":"As \\emph{artificial intelligence} (AI) systems are increasingly involved in decisions affecting our lives, ensuring that automated decision-making is fair and ethical has become a top priority. Intuitively, we feel that akin to human decisions, judgments of artificial agents should necessarily be grounded in some moral principles. Yet a decision-maker (whether human or artificial) can only make truly ethical (based on any ethical theory) and fair (according to any notion of fairness) decisions if full information on all the relevant factors on which the decision is based are available at the time of decision-making. This raises two problems: (1) In settings, where we rely on AI systems that are using classifiers obtained through supervised learning, some induction\/generalization is present and some relevant attributes may not be present even during learning. (2) Modeling such decisions as games reveals that any -- however ethical -- pure strategy is inevitably susceptible to exploitation. Moreover, in many games, a Nash Equilibrium can only be obtained by using mixed strategies, i.e., to achieve mathematically optimal outcomes, decisions must be randomized. In this paper, we argue that in supervised learning settings, there exist random classifiers that perform at least as well as deterministic classifiers,","403":"This paper focuses on and analyzes realistic SIR models that take stochasticity into account. The proposed systems are applicable to most incidence rates that are used in the literature including the bilinear incidence rate, the Beddington-DeAngelis incidence rate, and a Holling type II functional response. Given that many diseases can lead to asymptomatic infections, we look at a system of stochastic differential equations that also includes a class of hidden state individuals, for which the infection status is unknown. We assume that the direct observation of the percentage of hidden state individuals that are infected, $\\alpha(t)$, is not given and only a noise-corrupted observation process is available. Using the nonlinear filtering techniques in conjunction with an invasion type analysis (or analysis using Lyapunov exponents from the dynamical system point of view), this paper proves that the long-term behavior of the disease is governed by a threshold $\\lambda\\in \\mathbb{R}$ that depends on the model parameters. It turns out that if $\\lambda<0$ the number $I(t)$ of infected individuals converges to zero exponentially fast, or the extinction happens. In contrast, if $\\lambda>0$, the infection is","404":"The pandemic caused by SARS-CoV2 has left an unprecedented impact on health, economy and society worldwide. Emerging strains are making pandemic management increasingly challenging. There is an urge to collect epidemiological, clinical, and physiological data to make an informed decision on mitigation measures. Advances in the Internet of Things (IoT) and edge computing provide solutions for pandemic management through data collection and intelligent computation. While existing data-driven architectures attempt to automate decision-making, they do not capture the multifaceted interaction among computational models, communication infrastructure, and the generated data. In this paper, we perform a survey of the existing approaches for pandemic management, including online data repositories and contact-tracing applications. We then envision a unified pandemic management architecture that leverages the IoT and edge computing to automate recommendations on vaccine distribution, dynamic lockdown, mobility scheduling and pandemic prediction. We elucidate the flow of data among the layers of the architecture, namely, cloud, edge and end device layers. Moreover, we address the privacy implications, threats, regulations, and existing solutions that may be adapted to optimize the utility of health data with security guarantees. The paper ends with a lowdown on the limitations of the architecture and research directions to enhance its practical","405":"The Coronavirus Disease 2019 (COVID-19) has affected several million people and caused deaths worldwide. The virus has destructive effects on lung tissue, and early detection is very important. Deep Convolution neural networks are such powerful tools in classifying images. Therefore, in this paper, we propose a hybrid approach based on a deep network to classify chest X-ray images of COVID-19 patients. Feature vectors were extracted by applying a deep convolution neural network on the image, which were then put into the SVM model. A binary series was generated from the application part of the proposed framework, with data being represented as two classes: normal and infected. The performance of the proposed framework was evaluated on three different datasets. The first dataset includes 4008 frontal view X-rays images representing the lungs of healthy persons, while the second dataset consists of 512 images containing pneumonia lesions, and the third dataset considers 1452 images of COVID-19 patients. The experimental results showed that the proposed framework performed well compared to its counterparts. This study contributes to the research through the creation of a novel dataset and further improves the efficiency of COVID-19 lesion recognition via combining more modern CNN architectures with traditional medical imaging modalities. We also demonstrated how the proposed framework","406":"The ability to estimate the current affective statuses of web users has considerable potential for the realization of user-centric services in the society. However, in real-world web services, it is difficult to determine the type of data to be used for such estimation, as well as collecting the ground truths of such affective statuses. We propose a novel method of such estimation based on the combined use of user web search queries and mobile sensor data. The system was deployed in our product server stack, and a large-scale data analysis with more than 11,000,000 users was conducted. Interestingly, our proposed\"Nation-wide Mood Score,\"which bundles the mood values of users across the country, (1) shows the daily and weekly rhythm of people's moods, (2) explains the ups and downs of people's moods in the COVID-19 pandemic, which is inversely synchronized to the number of new COVID-19 cases, and (3) detects the linkage with big news, which may affect many user's mood states simultaneously, even in a fine-grained time resolution, such as the order of hours. Furthermore, various additional experiments showed the remarkable performance of our approach, especially when applied to the Japan's Ministry of Health","407":"In recent years, individuals, business organizations or the country have paid more and more attention to their data privacy. At the same time, with the rise of cloud computing, federated learning has become the most popular way to train models without needing to share raw data. However, there are many challenges related to the exchange of training data between different organisations, regions and countries. In this paper, we propose a new framework based on blockchain technology to help data owners to maintain data governance and perform model training securely. We show the effectiveness and efficiency of our approach by proposing a novel method for sharing private data among different organisations. Our empirical study shows that we can improve the accuracy of various models trained on heterogeneous data by using information from around us. Finally, we also discuss the implications of such information-sharing practices for digitalization and automation. Overall, we argue that although there are some disadvantages of current Federated Learning (FL) algorithms, they should be better than decentralized alternatives because it reduces the gap in performance and security while increasing the flexibility of resource allocation. Therefore, we emphasise the importance of developing FL methods and provide more emphasis on the potential benefits of these developments. To best of our knowledge, this is the first comprehensive review about FedAvg -- a distributed FL algorithm.","408":"The VertexCover problem is proven to be computationally hard in different ways: It is NP-complete to find an optimal solution and even NP-hard to find an approximation with reasonable factors. In contrast, recent experiments suggest that on many real-world networks the run time to solve VertexCover is way smaller than even the best known FPT-approaches can explain. Similarly, greedy algorithms deliver very good approximations to the optimal solution in practice. We link these observations to two properties that are observed in many real-world networks, namely a heterogeneous degree distribution and high clustering. To formalize these properties and explain the observed behavior, we analyze how a branch-and-reduce algorithm performs on hyperbolic random graphs, which have become increasingly popular for modeling real-world networks. In fact, we are able to show that the VertexCover problem on hyperbolic random graphs can be solved in polynomial time, with high probability. The proof relies on interesting structural properties of hyperbolic random graphs. Since these predictions of the model are interesting in their own right, we conducted experiments on real-world networks showing that these properties are also observed in practice. When utilizing the same structural properties in an adaptive greedy algorithm, further experiments","409":"Fake news on social media has become a hot topic of research as it negatively impacts the discourse of real news in the public. Specifically, the ongoing COVID-19 pandemic has seen a rise of inaccurate and misleading information due to the surrounding controversies and unknown details at the beginning of the pandemic. The FakeNews task at MediaEval 2020 tackles this problem by creating a challenge to automatically detect tweets containing misinformation based on text and structure from Twitter follower network. In this paper, we present a simple approach that uses BERT embeddings and a shallow neural network for classifying tweets using only text, and discuss our findings and limitations of the approach in text-based misinformation detection. We also provide a case study of identifying false claims related to COVID-19 vaccines via text-based monitoring of online conversations. Our results show that there is a strong correlation between our findings and the publicly available dataset regarding COVID-19 misinformation. Finally, we note that our approach can be generalized to other domains with misinformation structures. As such, we argue that MOTA systems in NLP should learn from these pseudo-textual models and incorporate them into their performance when predicting if a claim is supported or refuted by any textual evidence found in the associated fact-checking article. This raises","410":"Datasets are increasingly being used in quantitative, data-driven public policy research. These datasets contain complex correlations and interdependencies. As such, they are difficult to release fully into the hands of the researcher interested in using them for his or her research topic, especially when these topics change rapidly over time. In this paper, we present ExaWorks, a framework for creating correlated database files that can be used with standard preprocessing tools for mining and analysis. They allow the user to easily modify the correlation graph by adding new attributes like gender or race as well as removing elements through deletion. We demonstrate how ExaWorks provides the user with both a structured overview of the existing data set and its relationship with the added features. Finally, we illustrate the use of ExaWorks in providing valuable feedback to the U.S. government and public health officials. Our code and data sets are available at https:\/\/github.com\/annahaensch\/ExaWorks.git.io\/. This website supports our project page http:\/\/exabworks.org\/, including the datasets from which it extracts examples. The processed datasets can be found at https:\/\/dx.doi.org\/10.3886\/E162941) under a CC BY 4.0 licence","411":"Early identification of college dropouts can provide tremendous value for improving student success and institutional effectiveness, and predictive analytics are increasingly used for this purpose. However, ethical concerns have emerged about whether including protected attributes in the prediction models discriminates against underrepresented student groups and exacerbates existing inequities. We examine this issue in the context of a large U.S. research university with both residential and fully online degree-seeking students. Based on comprehensive institutional records for this entire student population across multiple years, we build machine learning models to predict student dropout after one academic year of study, and compare the overall performance and fairness of model predictions with or without four protected attributes (gender, URM, first-generation student, and high financial need). We find that including protected attributes does not impact the overall prediction performance and it only marginally improves algorithmic fairness of predictions. While these findings suggest that including protective attributes is preferred, our analysis also offers guidance on how to evaluate the impact in a local context, where institutional stakeholders seek to leverage predictive analytics to support student success. These results may be helpful for institutions to develop intervention strategies for targeting vulnerable student populations and supporting parental decision-making process around managing school choice and course design. They also highlight the importance of developing novel approaches for risk assessment tools","412":"The COVID-19 pandemic has resulted in international efforts to understand, track, and mitigate the disease, yielding a significant corpus of COVID-related scientific literature across disciplines. As of May 2020, this corpus consists of over 166,000 journal articles related to COVID-19 research. Here we present an analysis of recent advances in natural language processing on this large corpus. We find that information-rich terms are used more frequently than traditional term matching approaches such as BM25 in relation to COVID-19, but with less overlap. Instead, we find that fine-tuning of transformers using question-answer pairs (QA pairs) improves performance on subsequent tasks, given that these models have better access to relevant documents. Furthermore, we show that some existing QA datasets can be re-purposed to focus on specific aspects of the corpus, while others are broad in nature. This corpus will facilitate further research into biomedical named entity recognition and proactive search for appropriate pre-trained NLP models. All collected data is available at https:\/\/github.com\/masadcv\/covid-neural-language-processing.git. We hope that our work illustrates the value of QA pair generation from the perspective of Natural Language Processing, and sheds","413":"Electrochemical energy storage is central to modern society -- from consumer electronics to electrified transportation and the power grid. It is no longer just a convenience but a critical enabler of the transition to a resilient, low-carbon economy. The large pluralistic battery research and development community serving these needs has evolved into diverse specialties spanning materials discovery, design innovation, scale-up, manufacturing and deployment. Despite the maturity and the impact of battery science and technology, the data and software practices among these disparate groups are far behind the state-of-the-art in other fields (e.g. drug discovery), which have enjoyed significant increases in the rate of innovation. Incremental performance gains and lost research productivity, which are the consequences, retard innovation and societal progress. Examples span every field of battery research, including those related to thermal imaging, nanoscale batteries, chemistry, graph theory, human activity recognition, etc. This article provides a systematic framework for the industrialization and commercialization of battery technologies, and identifies the fundamental challenges and opportunities of this complex multidisciplinary field. In particular, we focus on the remarkable advances in both basic and advanced battery sciences and present a holistic perspective of the current landscape of play, as well as a roadmap for future research.","414":"A new coronavirus identified as SARS-CoV-2 virus has brought the world to a state of crisis, causing a major pandemic, claiming more than 433,000 lives and instigating major financial damage to the global economy. Despite current efforts, developing safe and effective treatments remains a major challenge. Moreover, new strains of this disease are likely to emerge in the future. To prevent future pandemics, several drugs with various mechanisms of action are required. Drug discovery efforts against the novel coronavirus fall into two main categories: (a) monoclonal antibodies targeting the spike protein of the virus and blocking it from entry; (b) small molecule inhibitors targeting key proteins of the virus, interfering with replication and translation of the virus. In this study, we are presenting a computational investigation of a potential drug candidate that targets SARS-CoV-2 protease, a viral protein critical for replication and translation of the virus. The computed results show that the compound ZINC09128258 can bind at the active site of the protease and block its activity, thus hampering the ability of the enzyme to catalyze the viral replication cycle. Our findings suggest that these compounds could be useful agents towards the development of therapeutics targeted towards blocking or","415":"In recent years, there has been a rapid growth in the application of Artificial Intelligence (AI) techniques across various disciplines. AI systems are being increasingly utilized in healthcare to help combat against disease and achieve high accuracy. There is also an urgent need to understand how these systems can be developed and deployed for effective clinical care. Research studies are undertaken to understand the capabilities and limitations of AI systems and have implications for designing more effective clinical care system. However, there is always some delay or data silo problems in developing and deploying such systems. Therefore, in this paper, we review the recent literature on the challenges of building and deploying accurate AI systems for COVID-19. We describe the existing work on using AI technologies like machine learning, deep learning, image analysis, text mining and natural language processing to build robust AI systems with performance comparable to those of other fields. Furthermore, we discuss the potential future directions for applying AI techniques to develop robust and accurate ICU diagnostic tools. Finally, we highlight the several technical challenges and encourage researchers to address them when building and deploying AI systems. The goal of this article is to introduce the reader to the potential applications of AI techniques in the context of medical imaging and provide a preliminary exploration of the topic. It is expected that this survey will offer","416":"We present numerical results obtained from the modelling of a stochastic, highly connected and mobile community. The spread of attributes like health, disease among the community members is simulated using cellular automata on a planar 2 dimensional bounded space. With remarkably few assumptions, we are able to predict the future course of propagation of such as well as the fate of isolated clusters. Our approach provides an important tool for decision makers with in-depth knowledge into the dynamics of epidemics. As an application, we provide insights into the optimal allocation of resources to mitigate the risk of a local outbreak of COVID-19. In particular, we demonstrate how delaying the start of control measures past (and thus creating an illusion of normalcy) does not guarantee enough protection against the second wave of the pandemic, nor do large variations in population mixing affect the ability of the healthcare system to handle the fluctuations in the number of new cases. This work is part of the theme issue \u2018Topics in mathematical models of epidemics\u2019. We hope that our simple model allows us to offer valuable advice for decision makers at both individual and area levels who are currently facing the challenges of containing an ongoing second wave of COVID-19 or any other novel infectious disease. Furthermore, it is shown how social","417":"DBLP is the largest open-access repository of scientific articles on computer science and provides metadata associated with publications, authors, and venues. We retrieved more than 6 million publications from DBLP and extracted pertinent metadata (e.g., abstracts, author affiliations, citations) from the publication texts to create the DBLP Discovery Dataset (D3). D3 can be used to identify trends in research activity, productivity, focus, bias, accessibility, and impact of computer science research. We present an initial analysis focused on the volume of computer science research (e.g., number of papers, authors, research activity), trends in topics of interest, and citation patterns. Our findings show that computer science is a growing research field (approx. 15% annually), with an active and collaborative researcher community. While papers in recent years present more bibliographical entries in comparison to previous decades, the average number of citations has been declining. Investigating papers' abstracts reveals that recent topic trends are clearly reflected in D3. Finally, we list further applications of D3 and pose supplemental research questions. The D3 dataset, our findings, and source code are publicly available for research purposes. All datasets involved in this study are released under https:\/\/github","418":"The recent uptake in popularity in vehicles with zero tailpipe emissions is a welcome development in the fight against traffic induced airborne pollutants. As vehicle fleets become electrified, and tailpipe emissions become less prevalent, non-tailpipe emissions (from tires and brake disks) will become the dominant source of traffic related emissions, and will in all likelihood become a major concern for human health. This trend is likely to be exacerbated by the heavier weight of electric vehicles, their increased power, and their increased torque capabilities, when compared with traditional vehicles. While the problem of emissions from tire wear is well-known, issues around the process of tire abrasion, its impact on the environment, and modelling and mitigation measures, remain relatively unexplored. Work on this topic has proceeded in several discrete directions including: on-vehicle collection methods; vehicle tire-wear abatement algorithms and controlling the ride characteristics of a vehicle, all with a view to abating tire emissions. Additional approaches include access control mechanisms to manage aggregate tire emissions in a geofenced area with other notable work focussing on understanding the particle size distribution of tire generated PM, the degree to which particles become airborne, and the health impacts of tire emissions. While such efforts are already underway, the problem of developing models","419":"The Reverse Transcription Polymerase Chain Reaction (RTPCR) test is the silver bullet diagnostic testing method for COVID-19. Rapid antigen detection is a screening test to identify COVID-19 patients in little as 15 minutes, but has a lower sensitivity than the PCR tests. Besides having multiple standardized test kits, many people are getting infected&either recovering or dying even before the test due to the shortage and cost of kits, lack of indispensable specialists and labs, time-consuming result compared to bulk population especially in developing and underdeveloped countries. Intrigued by the parametric deviations in immunological&hematological profile of a COVID patient, this research work leveraged the concept of COVID-19 detection by proposing a risk-free and highly accurate Stacked Ensemble Machine Learning model to identify a COVID patient from communally available-widespread-cheap routine blood tests which gives a promising accuracy, precision, recall&F1-score of 100%. Analysis from R-curve also shows the preciseness of the risk-free model to be implemented. The proposed method has the potential for large scale ubiquitous low-cost screening application. This can add an extra layer of protection in keeping the number of infected cases to a minimum and control","420":"Conspiracy theories, and suspicion in general, define us as human beings. Our suspicion and tendency to create conspiracy theories have always been with the human race, powered by our evolutionary drive to survive. Although this evolutionary drive to survive is helpful, it can often become extreme and lead to\"apophenia.\"Apophenia refers to the notion of connecting previously unconnected ideas and theories. Unlike learning, apophenia refers to a cognitive, paranoid disorder due to the unreality of the connections they make. Social networks allow people to connect in many ways. Besides communicating with a distant family member and sharing funny memes with friends, people also use social networks to share their paranoid, unrealistic ideas that may cause panic, harm democracies, and gather other unsuspecting followers. In this work, we focus on characterizing the QAnon movement on Voat.co. QAnon is a conspiracy theory that supports the idea that powerful politicians, aristocrats, and celebrities are closely engaged in a pedophile ring. At the same time, many governments are controlled by the\"puppet masters\"where the democratically elected officials serve as a fake showroom of democracy. Voat, a 5-year-old news aggregator, captured the interest of many journalists because of the often hateful","421":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\nWhat is the minimum amount of effective effective charges?\n### Input:\nCOVID Super Expert\n### Response: The problem of transferring electric fields between two-dimensional (2D) regions has attracted considerable attention for decades. The development of this field has been greatly advanced in the last decade due to the appearance of nonlinearity and complexity in the channel conductivity, as well as the insufficient performance of classical symmetry-breaking states like Kretschmann configuration with sufficiently low dielectric constant. As a result, researchers around the globe have proposed different approaches to address this challenge through multidimensional topological insulator (MTopI) theory, which is equivalent to other topological quantum matter (TQM) theories involving the exchange of electric fields. However, the definition of such models suffers from the lack of clarity and adequacy. In this paper, we first establish a general framework for MTopI theory via the $U(1)$ symmetries of the Hamiltonian in terms of electric field variables and subsequently derive the MTopI equation by applying the notion of electrostatic gating onto 2D heterostructures. We also discuss the relationship between the MTopI equation and the generalized Fermi-Dirac equations before deriving any explicit solutions. Our analysis, based on the equivalence between the MTopI theory and the generalized Ferm","422":"Branching processes are widely used to model evolutionary and population dynamics as well as the spread of infectious diseases. To characterize the dynamics of their growth or spread, the basic reproduction number $R_0$ has received considerable attention. In the context of infectious diseases, it is usually defined as the expected number of secondary cases produced by an infectious case in a completely susceptible population. Typically $R_0>1$ indicates that an outbreak is expected to continue and to grow exponentially, while $R_0<1$ usually indicates that an outbreak is expected to terminate after some time. In this work, we show that fluctuations of the dynamics in time can lead to a continuation of outbreaks even when the expected number of secondary cases from a single case is below $1$. Such fluctuations are usually neglected in modelling of infectious diseases by a set of ordinary differential equations, such as the classic SIR model. We showcase three examples: 1) extinction following an Ornstein-Uhlenbeck process, 2) extinction switching randomly between two values and 3) mixing of two populations with different $R_0$ values. We corroborate our analytical findings with computer simulations. Altogether, our results highlight the importance of considering both the mean field evolution of the disease's propagation as well as","423":"The increasing progress in artificial intelligence and respective machine learning technology has fostered the proliferation of chatbots to the point where today they are being embedded into various human-technology interaction tasks. In enterprise contexts, the use of chatbots seeks to reduce labor costs and consequently increase productivity. For simple, repetitive customer service tasks such already proves beneficial, yet more complex collaborative knowledge work seems to require a better understanding of how these technologies may best be integrated. Particularly, the additional mental burden which accompanies the use of these natural language based artificial assistants, often remains overlooked. To this end, cognitive load theory implies that unnecessary use of technology can induce additional extrinsic load and thus may have a contrary effect on users' productivity. The research presented in this paper thus reports on a study assessing cognitive load and productivity implications of human chatbot interaction in a realistic enterprise setting. A\/B testing software-only vs. software + chatbot interaction, and the NASA TLX were used to evaluate and compare the cognitive load of two user groups. Results show that chatbot users experienced less cognitive load and were more productive than software-only users. Furthermore, they show lower frustration levels and better overall performance (i.e, task quality) despite their slightly longer average task completion time. Our findings suggest","424":"In this paper we introduce and study the problem of minimizing running and controlling costs while allowing possible under- and over-reporting in the form of, but not limited to, partial epidemic observations. The novelty of our approach is twofold. First, it considers cost functions which are both convex and have a Lipschitz gradient structure with random variable sizes, possibly with overlapping samples. In particular, the value function considered is the sum of $p$ differentiable (possibly nonconvex) functions. Second, it introduces a control barrier function which may be used to guarantee that the infected fraction does not exceed a given threshold strictly below the nominal level even if the true distribution of the infectious rate is unknown. We prove continuity properties for the proposed setting. Third, we provide numerical evidence based on simulations. Ablation studies reveal that the inclusion of such special cases significantly reduces run times compared to more traditional approaches. Furthermore, we demonstrate how our method can be readily applied to other epidemic models provided that the basic reproduction number is known. Finally, we discuss the impact of our modeling framework on the spread of COVID-19 by estimating the required parameters from data reported in real time by considering several plausible alternative scenarios. One of these scenarios suggests that the actual peak of the total","425":"This paper analyzes the influence of partisan content from national cable TV news on local reporting in U.S. newspapers. We provide a new multimodal dataset containing 28,884 articles and 1,293 comments about political ideology\/news bias. The data cover both liberal- and conservative-leaning outlets across four major US cities (New York, Los Angeles, Chicago, and Seattle). A sample of 100 random selected articles per city are included in the analysis to demonstrate the impact of partisan content from national cable TV news on local reporting in the future. Our research shows that the distribution of both online and offline media attention to different topics by day can be largely explained by the partisanship of the news source. Furthermore, we find that users who consume more biased or less factual news sources will also watch less diverse news networks for entertainment purposes. These findings suggest that partisan audience has a higher need for exposure to alternative news and greater interest in factually inaccurate and politically biased news sources. Therefore, it is recommended that existing journalistic standards and policies regarding presentation of such information should be reconsidered with respect to these emerging challenges. To the best of our knowledge, this is the first large-scale social media-based study to analyze polarization and misinformation in connection to local journalism, which involves presenting","426":"Convolutional neural networks (CNNs) are now being widely used for classifying and detecting pulmonary abnormalities in chest radiographs. Two complementary generalization properties of CNNs, translation invariance and equivariance, are particularly useful in detecting manifested abnormalities associated with pulmonary disease, regardless of their spatial locations within the image. However, these properties also come with the loss of exact spatial information and global relative positions of abnormalities detected in local regions. Global relative positions of such abnormalities may help distinguish similar conditions, such as COVID-19 and viral pneumonia. In such instances, a global attention mechanism is needed, which CNNs do not support in their traditional architectures that aim for generalization afforded by translation invariance and equivariance. Vision Transformers provide a global attention mechanism, but lack translation invariance and equivariance, requiring significantly more training data samples to match generalization of CNNs. To address the loss of spatial information and global relations between features, while preserving the inductive biases of CNNs, we present a novel technique that serves as an auxiliary attention mechanism to existing CNN architectures, in order to extract global correlations between salient features. We demonstrate the benefits of this method on two distinct datasets of lung diseases: one dataset includes 910 images from CT scans, and","427":"We present a perspective on platforms for code submission and automated evaluation in the context of university teaching. Due to the COVID-19 pandemic, such platforms have become an essential asset for remote courses and a reasonable standard for structured code submission concerning increasing numbers of students in computer sciences. Utilizing automated code evaluation techniques exhibits notable positive impacts for both student development and academic research. We identified relevant technical and non-technical requirements for such platforms in terms of practical applicability and secure code submission environments. Furthermore, a survey among students was conducted to obtain empirical data on general perception. The results indicate that automatic evaluation provides satisfactory quality only if sufficiently many users are involved in the cooperative manner. Based on the findings, we propose a guideline for organizing online courses based on their nature and provide recommendations for implementing effective automated code evaluation systems. Finally, future research directions are discussed. All collected metrics, tools, and datasets are made available as part of this article. Further, the paper demonstrates how the presented concepts can be applied to other domains, not just in terms of implementation but also with regards to human aspects like culture and learning. This work serves as a basis for promoting cooperative practices between researchers, developers, and instructors. It is also applicable to distance learning since it may require forming study groups","428":"The COVID-19 pandemic has caused unprecedented disruption, particularly in retail. Where essential demand cannot be fulfilled online, or where more stringent measures have been relaxed, customers must visit shop premises in person. This naturally gives rise to some risk of susceptible individuals (customers or staff) becoming infected. It is therefore important for shops to minimise this risk as far as possible while retaining economic viability of the shop. We therefore explore and compare the spread of COVID-19 in different shopping situations involving UVC irradiation: (i) free-flowing, unstructured shopping; (ii) structured shopping (e.g. a queue). We examine which of (i) or (ii) may be preferable for minimising the spread of COVID-19 in a given shop, subject to constraints such as the geometry of the shop; compliance of the population to local guidelines; and additional safety measures which may be available to the organisers of the shop. We derive a series of conclusions, such as unidirectional free movement being preferable to bidirectional shopping, and that the number of servers should be maximised as long as they can be well protected from infection. For certain configurations of the shop, we show that even a small probability of infecting","429":"The dissertation proposes the use of layer-wise convolutional neural networks (LNNs) for multi-task learning and in particular for knowledge distillation based on partial differential equations (PDEs). The LNNs are trained once according to a given objective function and used as recurrent layers during successive training stages. A decision vector is associated with each hidden state of the LN. This vector is then combined with the output probability distribution obtained from the decoder module. The resulting macro-objective optimization problem is solved by using a non-dominated sorting algorithm. Finally, the optimal solution is found under the backward Bellman filter criterion. The numerical results show that the proposed framework can successfully learn solutions for both convex and nonconvex PDE models. The comparison between the proposed method and the state-of-the-art methods shows that the proposed technique performs better than the existing approaches especially when the model parameters are relatively small. For example, this methodology will be useful if only $\\sim 10$ parameters are used to train a model whose performance is particularly poor. The development of this type of models may also be beneficial to the medical field where patient data is not as reliable as it is for traditional machine learning techniques. Moreover, the proposed method is generic and","430":"We consider two large datasets consisting of all games played among top-tier European soccer clubs in the last 60 years, and among professional American basketball teams in the past 70 years. We leverage game data to build networks of pairwise interactions between the head coaches of the teams, and measure their career performance in terms of network centrality metrics. We identify Arsene Wenger, Sir Alex Ferguson, Jupp Heynckes, Carlo Ancelotti, and Jose Mourinho as the top 5 European soccer coaches of all time. In American basketball, the first 5 positions of the all-time ranking are occupied by Red Auerbach, Gregg Popovich, Phil Jackson, Don Nelson, and Lenny Wilkens. We further establish rankings by decade and season. We develop a simple methodology to monitor performance throughout a coach's career, and to dynamically compare the performance of two or more coaches at a given time. The manuscript is accompanied by the website coachscore.luddy.indiana.edu where complete results of our analysis are accessible to the interested readers. Our website serves as a hub for discussing opinion and knowledge on topics that are central to both our dataset and our method. We hope this paper will be a useful resource for researchers, policy makers, and educators alike. Finally,","431":"We propose a deep switching state space model (DS$^3$M) for efficient inference and forecasting of nonlinear time series with irregularly switching among various regimes. The switching among regimes is captured by both discrete and continuous latent variables with recurrent neural networks. The model is estimated with variational inference using a reparameterization trick. We test the approach on a variety of simulated and real datasets. In all cases, DS$^3$M achieves competitive performance compared to several state-of-the-art methods (e.g. GRU, SRNN, DSARF, SNLDS), with superior forecasting accuracy, convincing interpretability of the discrete latent variables, and powerful representation of the continuous latent variables for different kinds of time series. Specifically, the MAPE values increase by 0.09\\% to 15.71\\% against the second-best performing alternative models. The results also show that DS$^3$M can accurately estimate the transmission rates of the COVID-19 pandemic from daily case counts at the early stage of the pandemic, while it outperforms other proposed epidemic prediction baselines such as GRU and LSTM. Finally, we find that DS$^3$M is highly flexible and versatile as","432":"A conventional study of fluid simulation involves different stages including conception, simulation, visualization, and analysis tasks. It is, therefore, necessary to switch between different software and interactive contexts which implies costly data manipulation and increases the time needed for decision making. Our interactive simulation approach was designed to shorten this loop, allowing users to visualize and steer a simulation in progress without waiting for the end of the simulation. The methodology allows the users to control, start, pause, or stop a simulation in progress, to change global physical parameters, to interact with its 3D environment by editing boundary conditions such as walls or obstacles. This approach is made possible by using a methodology such as the Lattice Boltzmann Method (LBM) to achieve interactive time while remaining physically relevant. In this work, we present our platform dedicated to interactive fluid simulation based on LBM. The contribution of our interactive simulation approach to decision-making will be evaluated in a study based on a simple but realistic use case. We also discuss how our approach can serve as a valuable tool for future applied research. All collected data are available at https:\/\/github.com\/annahaensch\/interactive_fluid_simulation. For reproducibility purposes, our open source framework is publicly available at https:\/\/","433":"The COVID-19 pandemic has resulted in dramatic changes to the daily habits of billions of people. Users increasingly have to rely on home broadband Internet access for work, education, and other activities. These changes have resulted in corresponding changes to Internet traffic patterns. This paper aims to characterize the effects of these changes with respect to Internet service providers in the United States. We study three questions: (1)How did traffic demands change in the United States as a result of the COVID-19 pandemic?; (2)What effects have these changes had on Internet performance?; (3)How did service providers respond to these changes? We study these questions using data from a diverse collection of sources. Our analysis of interconnection data for two large ISPs in the United States shows a 30-60% increase in peak traffic rates in the first quarter of 2020. In particular, we observe traffic downstream peak volumes for a major ISP increase of 13-20% while upstream peaks increased by more than 30%. Further, there was significant variation in performance across ISPs in conjunction with the traffic volume shifts, with evident latency increases after stay-at-home orders were issued, followed by a stabilization of traffic after April. Finally, we observe that in response to changes in usage,","434":"Drones are effective for reducing human activity and interactions by performing tasks such as exploring and inspecting new environments, monitoring resources and delivering packages. Drones need a controller to maintain stability and to reach their goal. The most well-known drone controllers are proportional-integral-derivative (PID) and proportional-derivative (PD) controllers. However, the controller parameters need to be tuned and optimized. In this paper, we introduce the use of two evolutionary algorithms, biogeography-based optimization~(BBO) and particle swarm optimization (PSO), for multi-objective optimization (MOO) to tune the parameters of the PD controller of a drone. The combination of MOO, BBO, and PSO results in various methods for optimization: vector evaluated BBO and PSO, denoted as VEBBO and VEPSO; and non-dominated sorting BBO and PSO, denoted as NSBBO and NSPSO. The multi-objective cost function is based on tracking errors for the four states of the system. Two criteria for evaluating the Pareto fronts of the optimization methods, normalized hypervolume and relative coverage, are used to compare performance. Results show that NSBBO generally performs better","435":"Stratifying factors, like age and gender, can modify the effect of treatments and exposures on risk of a studied outcome. Several effect measures, including the relative risk, hazard ratio, odds ratio, and risk difference, can be used to measure this modification. It is known that choice of effect measure may determine the presence and direction of effect-measure modification. We show that considering the opposite outcome -- for example, recovery instead of death -- may similarly influence effect-measure modification. In fact, if the relative risk for the studied outcome and the relative risk for the opposite outcome agree about the direction of effect-measure modification, then so will the two cumulative hazard ratios, the risk difference, and the odds ratio. When risks are randomly sampled from the uniform (0,1) distribution, the probability of this happening is 5\/6. Disagreement is probable enough that researchers considering one relative risk should also consider the other and further discussion if they disagree. (If possible, researchers should also report estimated risks.) We provide examples through case studies on HCV, COVID-19, and bankruptcy following melanoma treatment. Finally, we discuss challenges in conducting causal inference when outcomes take any value outside of a pre-specified range. Code is available at https","436":"Renewal processes are a popular approach used in modelling infectious disease outbreaks. In a renewal process, previous infections give rise to future infections. However, while this formulation seems sensible, its application to infectious disease can be difficult to justify from first principles. It has been shown from the seminal work of Bellman and Harris that the renewal equation arises as the expectation of an age-dependent branching process. In this paper we provide a detailed derivation of the original Bellman Harris process. We introduce generalisations, that allow for time-varying reproduction numbers and the accounting of exogenous events, such as importations. We show how inference on the renewal equation is easy to accomplish within a Bayesian hierarchical framework. Using off the shelf MCMC packages, we fit to South Korea COavid-19 case data to estimate reproduction numbers and importations. Our derivation provides the mathematical fundamentals and assumptions underpinning the use of the renewal equation for modelling outbreaks. We also discuss possible extensions of the model to account for different spatio-temporal scales. An R package CatReg implementing our model is available at https:\/\/github.com\/mrc-ide\/covid19-diffusion-processes.gitlab.com\/dacs-hpi\/cat","437":"The explosive growth and popularity of Social Media has revolutionised the way we communicate and collaborate. Unfortunately, this same ease of accessing and sharing information has led to an explosion of misinformation and propaganda. Given that stance detection can significantly aid in veracity prediction, this work focuses on boosting automated stance detection, a task on which pre-trained models have been extremely successful on, as on several other tasks. This work shows that the task of stance detection can benefit from feature based information, especially on certain under performing classes, however, integrating such features into pre-training models using ensembling is challenging. We propose a novel architecture for integrating features with pre-training models that address these challenges and test our method on the RumourEval 2019 dataset. This method achieves state-of-the-art results with an F1-score of 63.94 on the test set. We also conduct ablation studies on the neural networks used for stance detection, showing significant improvements after adding any computational cost at all. This suggests that simple ensemble methods are more effective than deep learning algorithms when providing few benefits to boost performance. Finally, we show that our best model improves generalisation on unseen data through an ensemble with three out-of-domain datasets (Twitter, Reddit) that span different","438":"Federated learning (FL) allows multiple medical institutions to collaboratively learn a global model without centralizing all clients data. It is difficult, if possible at all, for such a global model to commonly achieve optimal performance for each individual client, due to the heterogeneity of medical data from various scanners and patient demographics. This problem becomes even more significant when deploying the global model to unseen clients outside the FL with new distributions not presented during federated training. To optimize the prediction accuracy of each individual client for critical medical tasks, we propose a novel unified framework for both Inside and Outside model Personalization in FL (IOP-FL). Our inside personalization is achieved by a lightweight gradient-based approach that exploits the local adapted model for each client, by accumulating both the global gradients for common knowledge and local gradients for client-specific optimization. Moreover, and importantly, the obtained local personalized models and the global model can form a diverse and informative routing space to personalize a new model for outside FL clients. Hence, we design a new test-time routing scheme inspired by the consistency loss with a shape constraint to dynamically incorporate the models, given the distribution information conveyed by the test data. Our extensive experimental results on two medical image segmentation tasks present significant improvements over SOTA","439":"We propose the novel use of a generative adversarial network (GAN) (i) to make predictions in time (PredGAN) and (ii) to assimilate measurements (DA-PredGAN). In the latter case, we take advantage of the natural adjoint-like properties of generative models and the ability to simulate forwards and backwards in time. GANs have received much attention recently, after achieving excellent results for their generation of realistic-looking images. We wish to explore how this property translates to new applications in computational modelling and to exploit the adjoint-like properties for efficient data assimilation. To predict the spread of COVID-19 in an idealised town, we apply these methods to a compartmental model in epidemiology that is able to model space and time variations. To do this, the GAN is set within a reduced-order model (ROM), which uses a low-dimensional space for the spatial distribution of the simulation states. Then the GAN learns the evolution of the low-dimensional states over time. The results show that the proposed methods can accurately predict the evolution of the high-fidelity numerical simulation, and can efficiently assimilate observed data and determine the corresponding model parameters. We conclude that our work shows the potential of neural","440":"A manuscript identified bat sarbecoviruses with high sequence homology to SARS-CoV-2 found in caves in Laos that can directly infect human cells via the human ACE2 receptor (Coronavirus Disease 2019; CORD-19). The authors describe how these viruses were discovered and their molecular mechanisms responsible for the highly contagious nature of the infections. Currently, no exact drug treatment or vaccine is available against the coronaviruses. We report here that RNA subsequences of the spike protein of those virus which inserts into the host cell's mRNA are determinants of its infectivity. We perform virtual screening of the entire proteome of one of the most infectious coronaviruses, MERS-CoV from the 2003 pandemic caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) as well as of two other coronaviruses, HKU5 and PAMAN-B.1. These sequences are linearized after removing the amino acid residues. Our findings suggest there is no active site on the viral surface allowing for ligand binding of the spike proteins of the infected coronaviruses. Furthermore, we identify numerous structural differences between the trimer spike protein of the SARS-CoV","441":"Cryptos are widely known to be one of the most stable and complex cryptographic systems. The Bitcoin blockchain provides a proof-of-concept for creating decentralized applications, and has attracted attention from different researchers due to its extensive mining functionality. However, there are no thorough studies on the performance of encrypted blockchains since they have some limitations as well as challenges to overcome them. In this paper, we systematically investigate the flow of money with both centralized and decentralized addresses on the Ethereum Blockchain network. We first confirm that a large proportion of crypto users are amateur investors who lack knowledge of the cryptocurrency market, and also struggle to distinguish between these two categories of users. Then, we study the characteristics of $\\textit{cryptojacking}$ coins which circulate on the Ethereum Blockchain network. We discover that the circulation of $\\textit{jointly}$ coins on the network is highly correlated with the multiverse of cryptocurrencies, and the single currency family such as bitcoin, Ethereum and Ripple does not generate enough income from it. Moreover, we find that the vast majority of crypto users are only willing to accept their assigned cryptocurrencies at the current rate, and almost all of them are poor traders who cannot afford losing hundreds of thousands of dollars. Finally, we implement a time series analysis to forecast","442":"Training and refreshing a web-scale Question Answering (QA) system for a multi-lingual commercial search engine often requires a huge amount of training examples. One principled idea is to mine implicit relevance feedback from user behavior recorded in search engine logs. All previous works on mining implicit relevance feedback target at relevance of web documents rather than passages. Due to several unique characteristics of QA tasks, the existing user behavior models for web documents cannot be applied to infer passage relevance. In this paper, we make the first study to explore the correlation between user behavior and passage ranking with automatic QA systems. We conduct extensive experiments using four test datasets and the results show our approach significantly improves the accuracy of passage ranking without extra human labeled data. In practice, this work has proved effective to substantially reduce the human labeling cost for the QA service in a global commercial search engine, especially for languages with low resources. Our techniques have been deployed in multi-language services. At the time of writing, our world's largest QA system can process over 8 million queries per day, which generates more than 2.2 billion relevant tweets automatically. https:\/\/github.com\/anyleopeace\/PWAIT-benchmarking.git.io\/qa_trawl.","443":"We propose a new microscopy simulation system that can depict atomistic models in a micrograph visual style, similar to results of physical electron microscopic imaging (EM). This system is scalable, able to represent simulation of electron microscopy of tens of viral particles and synthesizes the image faster than previous methods. On top of that, the simulator is differentiable, both its deterministic as well as stochastic stages that form signal and noise representations in the micrograph. This notable property has the capability for solving inverse problems by means of optimization and thus allows for generation of microscopy simulations using the parameter settings estimated from real data. We demonstrate this learning capability through two applications: (1) estimating the parameters of the modulation transfer function defining the detector properties of the simulated and real micrographs, and (2) denoising the real data based on parameters trained from the simulated examples. While current simulators do not support any parameter estimation due to their forward design, we show that the results obtained using estimated parameters are very similar to the results of real micrographs. Additionally, we evaluate the denoising capabilities of our approach and show that the results showed an improvement over state-of-the-art methods. Denoised micrographs exhibit less noise in the","444":"The attention mechanism has demonstrated superior performance for inference over nodes in graph neural networks (GNNs), however, they result in a high computational burden during both training and inference. We propose FastGAT, a method to make attention based GNNs lightweight by using spectral sparsification to generate an optimal pruning of the input graph. This results in a per-epoch time that is almost linear in the number of graph nodes as opposed to quadratic. Further, we provide a re-formulation of a specific attention based GNN, Graph Attention Network (GAT) that interprets it as a graph convolution method using the random walk normalized graph Laplacian. Using this framework, we theoretically prove that spectral sparsification preserves the features computed by the GAT model, thereby justifying our FastGAT algorithm. We experimentally evaluate FastGAT on several large real world graphs for node classification tasks, FastGLT for general dense tensor factorization tasks, and FastRV for link prediction. Our experiments show that FastGAT can perform competitively with a state-of-the-art baseline in all these tasks, while reducing the computation time by up to 9.3x and maintaining accuracy comparable to a state-of-","445":"Attempts to curb the spread of coronavirus by introducing strict quarantine measures apparently have different effect in different countries: while the number of new cases has reportedly decreased in China and South Korea, it still exhibit significant growth in Italy and other countries across Europe. In this brief note, we endeavour to assess the efficiency of quarantine measures by means of mathematical modelling. Instead of the classical SIR model, we introduce a new model of infection progression under the assumption that all infected individual are isolated after the incubation period in such a way that they cannot infect other people. Disease progression in this model is determined by the basic reproduction number $\\mathcal{R}_0$ (the number of newly infected individuals during the incubation period), which is different compared to that for the standard SIR model. If $\\mathcal{R}_0>1$, then the number of latently infected individuals exponentially grows. However, if $\\mathcal{R}_0<1$ (e.g.~due to quarantine measures and contact restrictions imposed by public authorities), then the number of infected decays exponentially. We then consider the available data on the disease development in different countries to show that there are three possible patterns: growth dynamics, growth-decays dynamics, and patch","446":"We take up a recently proposed compartmental SEIQR model with delays, ignore loss of immunity in the context of a fast pandemic, extend the model to a network structured on infectivity, and consider the continuum limit of the same with a simple separable interaction model for the infectivities $\\beta$. Numerical simulations show that the evolving dynamics of the network is effectively captured by a single scalar function of time, regardless of the distribution of $\\beta$ in the population. The continuum limit of the network model allows a simple derivation of the simpler model, which is a single scalar delay differential equation (DDE), wherein the variation in $\\beta$ appears through an integral closely related to the moment generating function of $u=\\sqrt{\\beta}$. If the first few moments of $u$ exist, the governing DDE can be expanded in a series that shows a direct correspondence with the original compartmental DDE with a single $\\beta$. Even otherwise, the new scalar DDE can be solved using either numerical integration over $u$ at each time step, or with the analytical integral if available in some useful form. Our work provides a new academic example of complete dimensional collapse, ties up an underlying continuum model for a pandemic with","447":"In this work, we propose a deep reinforcement learning-based framework for optimizing cyclic lockdowns, which is one of the methods available for control of the COVID-19 pandemic. The proposed framework uses a custom reward function and episode-training procedure to learn model parameters in a sequential manner. Using real data from Italy, we compare the effectiveness of different optimization algorithms (i.e., Recurrent Neural Network, LSTM, MobileNetV2) with respect to their ability to generate a suitable reward function, which allows us to analyze the behaviour of the agents throughout the lockdown periods. We show that while both methods achieve acceptable performance for the Italian case study, they both also struggle to solve instances with severe restrictions on social interactions, which have been shown to be particularly challenging for artificial intelligence models. Our results suggest that there is room for improvement in terms of cycle length and agent architecture in these datasets. Moreover, our findings motivate the need for more thorough evaluation using realistic alternative scenarios. This is done by testing our framework with several other properties related to disease spread, namely, reproduction number, duration of symptoms, likelihood of transmission before symptom onset, etc. The main contribution of this work lies in the design of efficient RL algorithm for solving large-scale finite-state","448":"These lectures present some basic ideas and techniques in the spectral analysis of lattice Schrodinger operators with disordered potentials. In contrast to the classical Anderson tight binding model, the randomness is also allowed to possess only finitely many degrees of freedom. This refers to dynamically defined potentials, i.e., those given by evaluating a function along an orbit of some ergodic transformation (or of several commuting such transformations on higher-dimensional lattices). Classical localization theorems by Frohlich--Spencer for large disorders are presented, both for random potentials in all dimensions, as well as even quasi-periodic ones on the line. After providing the needed background on subharmonic functions, we then discuss the Bourgain-Goldstein theorem on localization for quasiperiodic Schrodinger cocycles assuming positive Lyapunov exponents. We also summarize our results and give extensions to more general cases. The proofs are constructive and some explicit examples are presented. Finally, we find connections between the studied problems and the renormalization group approach to quantum statistical mechanics via the Zakharov-Shabat inverse scattering method. All of these results are obtained from restricting oneself to uncountably infinite dimensional systems. At the end of","449":"The COVID-19 pandemic has led to infodemic of low quality information leading to poor health decisions. Combating the outcomes of this infodemic is not only a question of identifying false claims, but also reasoning about the decisions individuals make. In this work we propose a holistic analysis framework connecting stance and reason analysis, and fine-grained entity level moral sentiment analysis. We study how to model the dependencies between the different level of analysis and incorporate human insights into the learning process. Experiments show that our framework provides reliable predictions even in the low-supervision settings. Furthermore, we demonstrate the applicability of our framework on analyzing the misinformation around COVID-19 vaccines. Our code is available at https:\/\/github.com\/olivesgatech\/covid19-fakenews-dectection. This dataset will be released to the research community upon their linkages to relevant misinformation studies. The dataset includes editorially labelled source tweets, so if any further annotation improvements are required, simply follow these instructions. We encourage future research to analyze the opinions of people based on credible evidence from published scientific sources. One of the main contributions of this work is the combination of a novel online moderation mechanism for managing unreliable posts in social media discussions, and a","450":"Even though laboratory and epidemiological studies have demonstrated the effects of ambient temperature on the transmission and survival of coronaviruses, not much has been done on the effects of weather on the spread of COVID-19. This study investigates the effects of temperature, humidity, precipitation, wind speed and the specific government policy intervention of partial lockdown on the new cases of COVID-19 infection in Ghana. Daily data on confirmed cases of COVID-19 from March 13, 2020 to April 21, 2020 were obtained from the official website of Our World in Data (OWID) dedicated to COVID-19 while satellite climate data for the same period was obtained from the official website of NASA's Prediction of Worldwide Energy Resources (POWER) project. Considering the nature of the data and the objectives of the study, a time series generalized linear model which allows for regressing on past observations of the response variable and covariates was used for model fitting. The results indicate significant effects of maximum temperature, relative humidity and precipitation in predicting new cases of the disease. Also, results of the intervention analysis indicate that null hypothesis of no significant effect of the specific policy intervention of partial lockdown should be rejected (p-value=0.0164) at a 5\\% level of significance.","451":"This paper addresses the problem of single image super-resolution (SISR) and asymmetric diffusion in medical imaging using deep learning. We first propose a new handwritten digit representation of images to represent both normal and abnormal tissues in a supervised manner. To generate high-quality PET images, we then propose a novel pipeline for synthesizing digital twins (DTs) from chest CT scans, with a specific architecture incorporating a DT's corresponding data and attention matrix for further enhancement. Our results show that our proposed pipeline has achieved more accurate reconstruction performance than other state-of-the-art methods on NIH-AAPM dataset for SISR, demonstrating its potential for clinical translation. In addition, we have also demonstrated that by introducing the attention mechanism onto the input dimensions of the reconstructed PET images, the model can focus on the appropriate region of the input domain and produce better results compared to the existing techniques. The proposed method will provide a promising solution for the early diagnosis of COVID-19 cases. Source code will be made publicly available at https:\/\/github.com\/PengyiZhang\/covid19_weak_digit_representation_based_model.git.io\/. Keywords: Regulated dose, discrete wavelet transform, modified tensor","452":"Objective: Computed Tomography (CT) has an important role to detect lung lesion related to Covide 19. The purpose of this work is to obtain diagnostic findings of Ultra-Low Dose (ULD) chest CT image and compare with routine dose chest CT. Material and Methods: Patients, suspected of Covid 19 infection, were scanned successively with routine dose, and ULD, with 98% or 94% dose reductions, protocols. Axial images of routine and ULD chest CT were evaluated objectively by two expert radiologists and quantitatively by Signal to Noise Ratio (SNR) and pixel by pixel noise measurement. Results: It was observed that the ULD and routine dose chest CT images could be used as additional tools in COVID-19 diagnosis. Using deep learning methods for semi-supervised classification of these images provided high accuracy values reliably. For example, the ratio between ULD and routine doses was 0.961 [CI 95%: 0.989-0.996] in the case of pneumonia caused by different pathogens. Conclusions: ULD chest CT can be used in the management of patients with suspected Covid 19 infections. In addition, our results demonstrate the value of using public available datasets to improve the performance of AI","453":"This note continues study of exchangeability martingales, i.e., processes that are martingales under any exchangeable distribution for the observations. Such processes can be used for detecting violations of the IID assumption, which is commonly made in machine learning. Violations of the IID assumption are sometimes referred to as dataset shift, and dataset shift is sometimes subdivided into concept shift, covariate shift, etc. Our primary interest is in concept shift, but we will also discuss exchangeability martingales that decompose perfectly into two components one of which detects concept shift and the other detects what we call label shift. Our methods will be based on techniques of conformal prediction. We will use quantile regression type methods to detect possible shifts in the data distribution and measure their effect with respect to the predictors. Moreover, we will consider network autoregressive models - such as linear and random effects model -- which can encode both label and concept shift. The framework is illustrated by simulations and by applying it to COVID-19 daily cases. For each example, our approach identifies the true positive rate of mask usage among individuals at risk using simulation studies. Furthermore, we apply our methodology to summarize existing results from randomized clinical trials. Finally, we will discuss limitations of our","454":"Since SARS-CoV-2 started spreading in Europe in early 2020, there has been a strong call for technical solutions to combat or contain the pandemic, with contact tracing apps at the heart of the debates. The EU's General Daten Protection Regulation (GDPR) requires controllers to carry out a data protection impact assessment (DPIA) where their data processing is likely to result in a high risk to the rights and freedoms (Art. 35 GDPR). A DPIA is a structured threat analysis that identifies and evaluates possible consequences of data processing relevant to fundamental rights and describes the measures envisaged to address these risks or expresses the inability to do so. Based on the Standard Data Protection Model (SDM), we present a scientific DPIA which thoroughly examines three prominent published contact tracing apps from the perspective of recurrence, transmission, and storage. We first assess the privacy properties of the tested apps, then describe the concrete actions needed to ensure that the app developers consider the needs of our stakeholders according to the SDM and implement them accordingly. Finally, we examine the weak points, both in terms of functionality and purpose, of the considered apps. To best of our knowledge, this is the first systematic study of how contact tracing apps comply with the legal requirements of the","455":"In this paper we study a relation between two positive geometries: the momentum amplituhedron, relevant for tree-level scattering amplitudes in $\\mathcal{N} = 4$ super Yang-Mills theory, and the kinematic associahedron, encoding tree-level amplitudes in bi-adjoint scalar $\\phi^3$ theory. We study the implications of restricting the latter to four spacetime dimensions and give a direct link between its canonical form and the canonical form for the momentum amplituhedron. After removing the little group scaling dependence of the gauge theory, we find that we can compare the resulting reduced form with the pull-back of the associahedron form. In particular, the associahedron form is the sum over all helicity sectors of the reduced momentum amplituhedron forms. This relation highlights the common singularity structure of the respective amplitudes; in particular the factorization channels, corresponding to vanishing planar Mandelstam variables, are the same. Additionally, we also find a relation between these canonical forms directly on the kinematic space of the scalar theory when reduced to four spacetime dimensions by Gram determinant constraints. As a by-product of our work","456":"The existing biclustering algorithms for finding feature relation based biclusters often depend on assumptions like monotonicity or linearity. Though a few algorithms overcome this problem by using density-based methods, they tend to miss out many biclusters because they use global criteria for identifying dense regions. The proposed method, RelDenClu uses the local variations in marginal and joint densities for each pair of features to find the subset of observations, which forms the bases of the relation between them. It then finds the set of features connected by a common set of observations, resulting in a bicluster. To show the effectiveness of the proposed methodology, experimentation has been carried out on fifteen types of simulated datasets. Further, it has been applied to six real-life datasets. For three of these real-life datasets, the proposed method is used for unsupervised learning, while for other three real-life datasets it is used as an aid to supervised learning. For all the datasets the performance of the proposed method is compared with that of seven different state-of-the-art algorithms and the proposed algorithm is seen to produce better results. The efficacy of proposed algorithm is also seen by its use on COVID-19 dataset for identifying some features (gen","457":"The urban transportation system is a combination of multiple transport modes, and the interdependencies across those modes exist. This means that the travel demand across different travel modes could be correlated as one mode may receive demand from or create demand for another mode, not to mention natural correlations between different demand time series due to general demand flow patterns across the network. It is expectable that cross-modal ripple effects become more prevalent, with Mobility as a Service. Therefore, by propagating demand data across modes, a better demand prediction could be obtained. To this end, this study explores various machine learning models and transfer learning strategies for cross-modal demand prediction. The trip data of bike-share, metro, and taxi are processed as the station-level passenger flows, and then the proposed prediction method is tested in the large-scale case studies of Nanjing and Chicago. The results suggest that prediction models with transfer learning perform better than unimodal prediction models. Furthermore, stacked Long Short-Term Memory model performs particularly well in cross-modal demand prediction. These results verify our combined method's forecasting improvement over existing benchmarks and demonstrate the good transferability for cross-modal demand prediction in multiple cities. Source code and datasets are available at https:\/\/github.com\/any","458":"The systemic stability of a stock market is one of the core issues in the financial field. The market can be regarded as a complex network whose nodes are stocks connected by edges that signify their correlation strength. Since the market is a strongly nonlinear system, it is difficult to measure the macroscopic stability and depict market fluctuations in time. In this paper, we use a geometric measure derived from discrete Ricci curvature to capture the higher-order nonlinear architecture of financial networks. In order to confirm the effectiveness of our method, we use it to analyze the CSI 300 constituents of China's stock market from 2005--2020 and the systemic stability of the market is quantified through the network's Ricci type curvatures. Furthermore, we use a hybrid model to analyze the curvature time series and predict the future trends of the market accurately. As far as we know, this is the first paper to apply Ricci curvature to forecast the systemic stability of domestic stock market, and our results show that Ricci curvature has good explanatory power for the market stability and can be a good indicator to judge the future risk and volatility of the domestic market. Besides, some interesting findings have been drawn from the study. For example, based on the analysis of the SI 300 constituents,","459":"This paper proposes a deep learning-based framework for detecting COVID-19 positive subjects from their cough sounds. We build a robust and accurate feature extractor using a pre-trained U-Net architecture, which extracts the relevant features of the sound waves. These features are then put into a machine learning classifier to classify the subjects as either a case of viral pneumonia or a control. The performance of the proposed lung segmentation algorithm is improved by taking out the irrelevant features so that the effective factors contributing to the detection of the disease are highlighted in the audio recordings. The results achieved on a large public dataset collected from YouTube videos show that our approach outperforms other state-of-the-art techniques with achieving an AUC score of 0.967. Moreover, the best subject detector (i.e., the one having the highest sensitivity) is a non-contact healthy screening tool provided that the test is conducted quickly and efficiently. This could be useful if adopted for automated early diagnosis of COVID-19. Such technologies can also be employed for differentiating between other pulmonary diseases like asthma, COPD, and COVID-19. Further, the proposed framework can be readily deployed on a smartphone without requiring any additional hardware. It can also be applied to integrate both","460":"We report updated results for $\\varepsilon_K$ determined directly from the standard model (SM) with lattice QCD inputs such as $\\hat{B}_K$, $|V_{cb}|$, $|V_{us}|$, $\\xi_0$, $\\xi_2$, $\\xi_\\text{LD}$, $F_K$, and $m_c$. We find that the standard model with exclusive $|V_{cb}|$ and other lattice QCD inputs describes only 70% of the experimental value of $|\\varepsilon_K|$ and does not explain its remaining 30%, which leads to a strong tension in $|\\varepsilon_K|$ at the $4\\sigma$ level between the SM theory and experiment. We also find that this tension disappears when we use the inclusive value of $|V_{cb}|$ obtained using the heavy quark expansion based on QCD sum rules. Our updates exclude any theoretical explanation for the origin of this tension. In particular, the simple local model reproduces both the shape of the wave function and the observed discrepancy between infinite pion and Nb3Sn films. Finally, we present a comparison","461":"We examine the outskirts of galaxy clusters in the C-EAGLE simulations to quantify the\"edges\"of the stellar and dark matter distribution. The radius of the steepest slope in the dark matter, otherwise known as the splashback radius, is located at ~r_200m; the strength and location of this feature depends on the recent mass accretion rate, in good agreement with previous work. Interestingly, the stellar distribution (or intracluster light, ICL) also has a well-defined edge, which is directly related to the splashback radius of the halo. Thus, detecting the edge of the ICL can provide an independent measure of the physical boundary of the halo, and the recent mass accretion rate. We show that these caustics can also be seen in the projected density profiles, but care must be taken to account for the influence of substructures and other non-diffuse material, which can bias and\/or weaken the signal of the steepest slope. This is particularly important for the stellar material, which has a higher fraction bound in subhaloes than the dark matter. Finally, we show that the impact of future missions will further enhance the role of substructures and other non-diffuse materials,","462":"In contrast to the common assumption in epidemic models that the rate of infection between individuals is constant, in reality, an individual's viral load determines their infectiousness. We compare the average and individual reproductive numbers and epidemic dynamics for a model incorporating time-dependent infectiousness and a standard SIR model for both fully-mixed and category-mixed populations. We find that the reproductive number only depends on the total infectious exposure and the largest eigenvalue of the mixing matrix and that these two effects are independent of each other. When we compare our time-dependent mean-field model to the SIR model with equivalent rates, the epidemic peak is advanced and modifying the infection rate function has a strong effect on the timing and intensity of the epidemic. We also observe behavior akin to a traveling wave as individuals transition through infectious states. Our results indicate that the effective reproduction number for a general epidemic model including a time-dependent infectious force can be estimated from data solely based on measurements of the number of infected individuals at different times. This method could enable studies previously considered infeasible. As an application, we provide a way to estimate the transmission parameters governing the spread of COVID-19 in Germany from early February 2020 by combining the known demographic data and governmental measures for each German district","463":"We propose a physics-inspired mathematical model underlying the temporal evolution of competing virus variants that relies on the existence of (quasi) fixed points capturing the large time scale invariance of the dynamics. To motivate our result we first modify the time-honoured compartmental models of the SIR type to account for the existence of competing variants and then show how their evolution can be naturally re-phrased in terms of flow equations ending at quasi fixed points. As the natural next step we employ (near) scale invariance to organise the time evolution of the competing variants within the effective description of the epidemic Renormalization Group framework. We test the resulting Theory against the time evolution of COVID-19 virus variants that validate the theoretical predictions. Our results demonstrate that the proposed theory works extremely well to describe the online spread of the competing variants, while also being capable to explain the delay in the relative peaks of newly infected cases observed in several countries. The most excellent agreement between the theoretically predicted and experimentally tested curves is found to correspond to the strong impact of the quasiparticle alignment technique on the effective infection peak height. Our further analysis identifies the important role of non-pharmaceutical interventions via an effective population immunity threshold. This finding suggests that the only way to","464":"The COVID-19 pandemic has fueled one of the most rapid vaccine developments in history and has placed unprecedented demands on struggling healthcare systems around the world. In order to meet these incredible needs, we need to understand how the virus can be spread and what it can do to combat the pandemic. Our ongoing study is focused on the health risk posed by the Delta variant of SARS-CoV-2 at scale. We seek to address both the theoretical and computational challenges related to the transmission dynamics of this emerging viral infection. To accomplish so, we have developed an agent based model which provides a synthetic population where each person has demographic attributes, belong to a household, and has a base activity- and contact network determined from survey data. Furthermore, we account for the inflow between individuals who are infected with the disease but may not develop symptoms. We present a case study focusing on the city of New York City and demonstrate its impact on the overall epidemic trajectory when different intervention strategies are implemented. Finally, we provide policy recommendations for controlling the course of this outbreak and improve the efficacy of the non-pharmaceutical interventions. Together, our empirical findings highlight the importance of developing novel therapeutics and vaccination protocols for managing epidemics such as this new coronavirus disease. They","465":"We present revisited results for a long-term memory (LFM) question answering task with 13,000 questions and over 150,000 answers to be evaluated. We focus on the pre-deployment stage; i.e., building an LFM model by applying techniques from natural language processing, such as stop word removal, stemming\/lemmatization, link extraction, and summarization. We find that a BERT baseline performs well, but there is still room for improvement, e.g., considering more positive aspects of the generated responses. To this end, we propose a simple yet effective integration method based on Kullback-Leibler divergence (KLD). The KL divergence can be used both to refine the LM model and to provide a suitable answer format for the human user. In addition, we develop a novel filter that exploits the dependency structure between the original question and its automatically generated answer. As a result, the proposed filter achieves state-of-the-art performance on all the tasks involved in this study, including semantic relation inference, information retrieval, and machine reading comprehension. Our experiments show that our approach maintains impressive robustness against several data augmentation methods, while it outperforms other existing baselines only when applied to very small datasets","466":"Rapid technological innovation threatens to leave much of the global workforce behind. Today's economy juxtaposes white-hot demand for skilled labor against stagnant employment prospects for workers unprepared to participate in a digital economy. It is a moment of peril and opportunity for every country, with outcomes measured in long-term capital allocation and the life satisfaction of billions of workers. To meet the moment, governments and markets must find ways to quicken the rate at which the supply of skills reacts to changes in demand. More fully and quickly understanding labor market intelligence is one route. In this work, we explore the utility of time series forecasts to enhance the value of skill demand data gathered from online job advertisements. This paper presents a pipeline which makes one-shot multi-step forecasts into the future using a decade of monthly skill demand observations based on a set of recurrent neural network methods. We compare the performance of a multivariate model versus a univariate one, analyze how correlation between skills can influence multivariate model results, and present predictions of demand for a selection of skills practiced by workers in the information technology industry. Results reveal that phase 1 of the forecast process improves multivariate models' overall accuracy compared to phase 2, suggesting that some of the skills studied by the authors are actually useful skills for today's","467":"This paper aims to identify the most important research trends and highlights important areas for further investigation in future. This study investigates various aspects of scientific literature during the COVID-19 pandemic, including mapping of papers with respect to their topic, data sources, authors, countries, institutions, and authors' keywords. Our analysis was based on a systematic review technique using three parts of the Google Scholar database from 2011 to 2021. From this initial set of 1,110 studies, we extracted one study at UT Southwestern (UTSW), two other studies from the Microsoft Academic Graph (MAG) dataset, and investigated the link between these two datasets via co-occurrence networks. We found that 114 studies were included in the first part of the corpus -- starting from January to May 2020. These studies depicted meaningful relationships among all of the indicators identified. In particular, five highly ranked indicators were classified as being associated with significant events or concepts that occurred during the study period. While none of the reviewed studies had been formally published, our results suggest that much work has been done without formal labels through social media or online media. It would be helpful to researchers and policy makers to understand how science works and what kind of science is involved in addressing the current crisis. For example, this study provides a","468":"In recent years, the problem of misinformation on the web has become widespread across languages, countries, and various social media platforms. Although there have been several manual fact-checking efforts to alleviate this issue, such as those conducted by the fact-checking organizations (e.g., Snopes) via their websites, which may be viewed as far-right sources. Thus, it is important to automatically detect fake news about COVID-19 or other topics on the Web during these pandemic times. To address this challenge, we propose a multilingual model for detecting fake news about COVID-19. Our study collects data from 60 articles and then selects relevant keywords and phrases to generate topic models to classify each article based on its topic(s). We also compile a list of 21 datasets and unique URLs used in the experiment to test the performance of our model. The results show that our approach is able to produce reasonable results when compared with well-known fact-checking baselines. More importantly, we find that producing negative results are not too difficult for a non-experts user due to the complexity of the real world situation. Finally, we provide some possible future research directions to further enhance the capabilities of our model. Overall, we believe that our work can help researchers and","469":"Since the introduction of electronic cigarettes to the United States market in 2007, vaping prevalence has surged in both adult and adolescent populations. E-cigarettes are advertised as a safer alternative to traditional cigarettes and as a method of smoking cessation, but the U.S. government and health professionals are concerned that e-cigarettes attract young non-smokers. Here, we develop and analyze a dynamical systems model of competition between traditional and electronic cigarettes for users. With this model, we predict the change in smoking prevalence due to the introduction of vaping, and we determine the conditions under which e-cigarettes present a net public health benefit or harm to society. Our results suggest that the public health benefits from e-cigarettes are small compared with the costs to society, and they are subject to regulation, which can vary greatly depending on the vantage point studied despite the common ground that e-cigarettes are illegal. We also find that the presence of family members significantly reduces the amount of vaping, but this advantage does not translate into huge differences in consumption because these family members do not mind using e-cigarettes while their children use them when competing against themselves. Finally, we study how much users would pay to get rid of e-cigarettes if they were offered a 100% discount. We find that offering exclusive","470":"Workers from a variety of industries rapidly shifted to remote work at the onset of the COVID-19 pandemic. While existing work has examined the impact of this shift on office workers, little work has examined how shifting from in-person to online work affected workers in the informal labor sector. We examine the impact of shifting from in-person to online-only work on a particularly marginalized group of workers: sex workers. Through 34 qualitative interviews with sex workers from seven countries in the Global North, we examine how a shift to online-only sex work impacted: (1) working conditions, (2) risks and protective behaviors, and (3) labor rewards. We find that online work offers benefits to sex workers' financial and physical well-being. However, online-only works introduce new and greater digital and mental health risks as a result of the need to be publicly visible on more platforms and to share more explicit content. From our findings we propose design and platform governance suggestions for digital sex workers and for informal workers more broadly, particularly those who create and sell digital content. We also discuss future research directions for examining the role of online sex work in the context of today's digital economy. Our data and code are available through https:\/\/github.com\/annaha","471":"We present a targeted search for continuous gravitational waves (GWs) from 236 pulsars using data from the third observing run of LIGO and Virgo (O3LVC). Searches were for emission from the $l=m=2$ mass quadrupole mode with a frequency at only twice the pulsar rotation frequency ($\\rm{d} N\/ \\rm{d} X$), as well as GW emission from high-$l$ modes, such as harmonic index ($H_0$) and single pulses ($S_{1P}$). No evidence of GW signals was found so we report $\\rm{SNR}_{\\rm{dB}}<5.8\\times 10^{-24}$ in the most sensitive band 20--25 Hz; $\\leq 3.4 \\times 10^{-23}$ at 25 Hz for the two best performing pulsars, 170817 and 91445. These results improve upon previous limits by factors of $\\approx 6$ improvement in magnitude over a directed search on combined signal strength and sky maps. The source code is publicly available at https:\/\/github.com\/pulsejoint\/covid19-detection-processes. We also release the","472":"In this paper we study some mathematical models describing evolution of population density and spread of epidemics in population systems in which spatial movement of individuals depends only on the departure and arrival locations and does not have apparent connection with the population density. We call such models as population migration models and migration epidemics models, respectively. We first apply the theories of positive operators and positive semigroups to make systematic investigation to asymptotic behavior of solutions of the population migration models as time goes to infinity, and next use such results to study asymptotic behavior of solutions of the migration epidemics models as time goes to infinity. Some interesting properties of solutions of these models are obtained. Furthermore, we find that there exist certain universal parameters for the spreading speed of infection in a finite time interval, and another property is proved here. Then we discuss how many important characteristics of solutions of the population migration models affect the dynamics of disease propagation in them. Finally, we present some numerical simulations to support our theoretical findings and clinical observations. Our work shows that mathematical modeling of population flow and epidemic spread can be very useful tool for understanding their transmission process, and also gives rise to several new techniques to predict and analyze the impact of population mobility in the spread of epidemics. It is observed here that","473":"The growth of online Digital\/social media has allowed a variety of ideas and opinions to coexist. Social Media has appealed users due to the ease of fast dissemination of information at low cost and easy access. However, due to the growth in affordance of Digital platforms, users have become prone to consume disinformation, misinformation, propaganda, and conspiracy theories. In this paper, we wish to explore the links between the personality traits given by the Big Five Inventory and their susceptibility to disinformation. More speciDically, this study is attributed to capture the short- term as well as the long-term effects of disinformation and its effects on the Dive personality traits. Further, we expect to observe that different personalities traits have different shifts in opinion and different increase or decrease of uncertainty on an issue after consuming the disinformation. Based on the Dindings of this study, we would like to propose a personalized narrative-based change in behavior for different personality traits. The development of these narratives could be done in a gradual manner so that they can grow with time. This study contributes towards enhancing the capability of face recognition systems(FRS) and combating fake news via considering those who are not involved in FRS as part of the solution to solve the problem of representing the interests of other users based","474":"Fake news, false or misleading information presented as news, has a significant impact on many aspects of society, such as in politics or healthcare domains. Due to the deceiving nature of fake news, applying Natural Language Processing (NLP) techniques to the news content alone is insufficient. The multi-level social context information (news publishers and engaged users in social media) and temporal information of user engagement are important information in fake news detection. The proper usage of this information, however, introduces three chronic difficulties: 1) multi-level social context information is hard to be used without information loss, 2) temporal information is hard to be used along with multi-level social context information, 3) news representation with multi-level social context and temporal information is hard to be learned in an end-to-end manner. To overcome all three difficulties, we propose a novel fake news detection framework, Hetero-SCAN. We use Meta-Path to extract meaningful multi-scale social context information without loss. Meta-Path, a composite relation connecting two node types, is proposed to capture the semantics in the heterogeneous graph. We then develop a novel sequential feature fusion method based on Meta-Path instance encoding and aggregation methods to learn the temporal information of user engagement for fake","475":"The COVID-19 pandemic has transformed into an unprecedented crisis in which millions of lives have been lost and more than 500,000 people are estimated to have been infected worldwide as of November 1, 2020. In this paper, we attempt to forecast the number of deaths from March 27, 2020 until July 31, 2020 using the logistic model and several simple rate models including the logistic, Gompertz, quadratic, simple square, and simple exponential growth models. We find that (a) the total number of deaths can be described by the sum of the first two terms of the Fourier series expansion of number of deaths, and (b) there will be a second wave of infections when data collected during the current period are not used to improve predictions. The results indicate that it may be necessary to consider additional measures such as quarantines or social distancing to reduce the spread of COVID-19 beyond the current peak. Further, we suggest that data collected from Google Global mobility report could be useful to help further monitor the evolution of the outbreak. The work presented here indicates that the epidemiological analysis of online data could provide valuable insights regarding the timing and dynamics of future outbreaks of other types. This suggests that it might be possible to delay the","476":"The problem of identification of unknown parameters of mathematical models of the spread of COVID-19 infection, based on additional information about the number of detected cases and deaths, is considered. The solution proposed is based on the use of parametric families of distributions for the variables involved in the model. These include several types of autoregressive kernels, such as the classical Gaussian processes, which are well known to be useful tools in modelling infectious disease outbreaks. In order to specify these models, explicit methods for their estimation are presented. Since the models themselves cannot be easily modified, they are used with the parameter values estimated from data or applied using meta-learning techniques. The accuracy of the method is shown through the application of it to the case of Turkey. Using different test statistic it is possible to detect differences between the actual number of infected people and the results obtained by applying the global likelihood analysis. It is also shown that the lack of uncertainty in the estimates of the tested parameters does not correspond to reliability problems in the observed data. Reliable estimates of the uncertainties can be provided by either increasing the sample size or modifying the specifications of the studied model. Furthermore, it is demonstrated that the generally used kernel functions do not provide effective results for specific applications. One important result which follows immediately","477":"The integration of electric vehicles (EVs) with the energy grid has become an important area of research due to the increasing EV penetration in today's transportation systems. Under appropriate management of EV charging and discharging, the grid can currently satisfy the energy requirements of a considerable number of EVs. Furthermore, EVs can help enhance the reliability and stability of the energy grid through ancillary services such as energy storage. This paper proposes the EV routing problem with time windows under time-variant electricity prices (EVRPTW-TP) which optimizes the routing of an EV fleet that are delivering products to customers, jointly with the scheduling of the charging and discharging of the EVs from\/to the grid. The proposed model is a multiperiod vehicle routing problem where EVs can stop at charging stations to either recharge their batteries or inject stored energy to the grid. Given the energy costs that vary based on time-of-use, the charging and discharge schedules of the EVs are optimized to benefit from the capability of storing energy by shifting energy demands from peak hours to off-peak hours when the energy price is lower. The vehicles can recover the energy costs and potentially realize profits by injecting energy back to the grid at high price periods. EVRPTW-TP is formulated as an","478":"We investigate the impact of vaccination on the dynamics of epidemic spreading through structured networks using the cavity method of statistical physics. We relax the assumption that vaccination prevents all transmission of a disease used in previous studies, such as the simultaneous infection of multiple nodes. To do so we extend the cavity method to study networks where nodes have heterogeneous transmissibility. We find that vaccination with partial transmission still provides herd immunity and show how the herd immunity threshold depends upon the assortativity between nodes of different transmissibility. In addition, we study the impact of social distancing via bond percolation and show that percolating phases transition from one regime to another as the degree distribution of the network changes. Finally, we use our theoretical findings to extend targeted immunization strategies to real case scenarios, including the development of hypergraphs corresponding to the application of the cavity method. Our approach demonstrates how valuable insights can be obtained by extending the cavity method to study diverse networks for which empirical data are available. We also provide an algorithm to identify optimal vaccination strategies via simulated annealing. Altogether, our results highlight the importance of coordination at the global level during the pandemic, and suggest that efforts should focus preferentially on protecting the most vulnerable individuals or groups of people with high trans","479":"Small businesses (0-19 employees) are becoming attractive targets for cyber-criminals, but struggle to implement cyber-security measures that large businesses routinely deploy. There is an urgent need for effective and suitable cyber-security solutions for small businesses as they employ a significant proportion of the workforce. In this paper, we consider the small business cyber-security challenges not currently addressed by research or products, contextualised via an Australian lens. We also highlight some unique characteristics of small businesses conducive to cyber-security actions. Small business cyber-security discussions to date have been narrow in focus and lack re-usability beyond specific circumstances. Our study uses global evidence from industry, government and research communities across multiple disciplines. We explore the technical and non-technical factors negatively impacting a small business' ability to safeguard itself, such as resource constraints, organisational process maturity, and legal structures. Our research shows that some small business characteristics, such as agility, large cohort size, and piecemeal IT architecture, could allow for increased cyber-security. We conclude that there is a gap in current research in small business cyber-security. In addition, legal and policy work are needed to help small businesses become cyber-resilient. Finally, we suggest several recommendations for future research directions. The","480":"To increase situational awareness and support evidence-based policy-making, we formulated two types of mathematical models for COVID-19 transmission within a regional population. One is a fitting function that can be calibrated to reproduce an epidemic curve with two different growth rates (e.g., fast spread initially but mild infection cases). The other is a compartmental model that accounts for quarantine, self-isolation, social distancing, a non-exponentially distributed incubation period, asymptomatic individuals, and mild and severe forms of symptomatic disease. Using Bayesian inference methods, we have been calibrating our models daily for consistency with new reports of confirmed cases from the 15 most populous metropolitan statistical areas in the United States and quantifying uncertainty in parameter estimates and predictions of future case reports. This online learning approach allows for early identification of new trends despite considerable variability in case reporting. We infer new significant upward trends for five of the metropolitan areas starting between 19-April-2020 and 12-June-2020. In total, 32 distinct regions with at least one positive endemic equilibrium are identified based on our results. For each region, we find strong evidence supporting the negative exponential growth reported by others. Furthermore, we identify four clusters of regions which appear to begin growing exponentially in","481":"The world evolution of the Severe acute respiratory syndrome coronavirus 2 (SARS-Cov2 or simply COVID-19) lead the World Health Organization to declare it a pandemic. The disease appeared in China in December 2019, and it spread fast around the world, specially in european countries like Italy and Spain. The first reported case in Brazil was recorded in February 26, and after that the number of cases growed fast. In order to slow down the initial grow of the disease through the country, confirmed positive cases were isolated to not transmit the disease. To better understand the evolution of COVID-19 in Brazil, we apply a Susceptible - Infectious - Recovered (SIR) model to the analysis of data from the Brazilian Department of Health. Based on analyical and numerical results, as well on the data, the basic reproduction number is estimated to $R_{0}=5.25$. In addition, we estimate that the ratio unidentified infectious individuals and confirmed patients is about $10$, in agreement with previous studies. We also estimated the epidemic doubling time to be $2.72$ days. Finally, we provide estimates on the total number of infected people and the daily lethality rate during the course of the outbreak","482":"Social media platforms have been exploited to disseminate misinformation in recent years. The widespread online misinformation has been shown to affect users' beliefs and is connected to social impact such as polarization. In this work, we focus on misinformation's impact on specific user behavior and aim to understand whether general Twitter users changed their behavior after being exposed to misinformation. We compare the before and after behavior of exposed users to determine whether the frequency of the tweets they posted, or the sentiment of their tweets underwent any significant change. Our results indicate that users overall exhibited statistically significant changes in behavior across some of these metrics. Through language distance analysis, we show that exposed users were already different from baseline users before the exposure. We also study the characteristics of two specific user groups, multi-exposure and extreme change groups, which were potentially highly impacted. Finally, we study if the changes in the behavior of the users after exposure to misinformation tweets vary based on the number of their followers or the number of followers of the tweet authors, and find that their behavioral changes are all similar. Overall, our research suggests that social media platforms should design interventions to promote awareness among the users about the spread of misinformation, and more specifically, emphasize the harmful impacts of the infodemic by promoting conscious reflection and avoiding the propagation of","483":"Ocean current, fluid mechanics, and many other spatio-temporal physical dynamical systems are essential components of the universe. One key characteristic of such systems is that certain physics laws -- represented as ordinary\/partial differential equations (ODEs\/PDEs) -- largely dominate the whole process, irrespective of time or location. Physics-informed learning has recently emerged to learn physics for accurate prediction, but they often lack a mechanism to leverage localized spatial and temporal correlation or rely on hard-coded physics parameters. In this paper, we advocate a physics-coupled neural network model to learn parameters governing the physics of the system, and further couple the learned physics to assist the learning of recurring dynamics. A spatio-temporal physics-coupled neural network (ST-PCNN) model is proposed to achieve three goals: (1) learning the underlying physics parameters, (2) transition of local information between spatio-temporal regions, and (3) forecasting future values for the dynamical system. The physics-coupled learning ensures that the proposed model can be tremendously improved by using learned physics parameters, and can achieve good long-range forecasting (e.g., more than 30-steps). Experiments, using simulated and field-collected ocean","484":"We show that the dynamics of an epidemic are governed by exponential time relaxation since the start of the outbreak, and explain this counterintuitive result using an analogy with chemical reactions where an exponential relaxation of the thermodynamic driving force, or affinity, occurs. The theoretical reaction rate takes a form similar to the infection rate equation of mathematical epidemiology, while the affinity measured in epidemic data relaxes exponentially. The rise and fall of an epidemic wave directly follows from this exponential relaxation, but change in public health policies (e.g., lockdowns) induce visible perturbations on the relaxation dynamics. We demonstrate how these behavioral changes can be used to improve predictions of the spread of an infectious disease through quantitative analysis and simulations. Our results highlight the importance of maintaining exponential growth during the course of the pandemic, so that the number of deaths can be reduced significantly if preventive measures are not implemented. Finally, we discuss two important applications for the theory: modeling the onset of herd immunity in the absence of any vaccination, and determining the degree of heterogeneity within a population. For both applications, we present analytical expressions for the corresponding quantities and apply them to COVID-19 data to determine its risk as a function of social distancing measures. We find that even under optimistic assumptions about the efficacy of","485":"With the constant advancements of genetic engineering, a common concern is to be able to identify the lab-of-origin of genetically engineered DNA sequences. For that reason, AltLabs has hosted the genetic Engineering Attribution Challenge to gather many teams to propose new tools to solve this problem. Here we show our proposed method to rank the most likely labs-of-origin and generate embeddings for DNA sequences and labs. These embeddings can also perform various other tasks, like clustering both DNA sequences and labs and using them as features for Machine Learning models applied to solve other problems. This work demonstrates that our method outperforms the classic training method for this task while generating other helpful information. We argue that these results are evidence that our method acts as a drop-in to improve the performance of existing methods over time. The code and data are available at https:\/\/github.com\/annahaensch\/altlab_structure. Besides including some benchmark datasets, we also apply these methods to two case studies of genome assembly in U.S. BioNetworks and find very competitive results. Lastly, we demonstrate the potential of our approach in other research domains by applying it to the Alexa Prize Grand Challenge 4. Overall, our method shows evident advantages over the state","486":"In medical image analysis, semi-supervised learning is an effective method to extract knowledge from a small amount of labelled data and a large amount of unlabelled data. This paper focuses on a popular pipeline known as self learning, and points out a weakness named lazy learning that refers to the difficulty for a model to learn from the pseudo labels generated by itself. To alleviate this issue, we propose ATSO, an asynchronous version of teacher-student optimization. ATSO partitions the unlabelled dataset into two subsets and alternately uses one subset to fine-tune the model and updates the label on the other subset. We evaluate ATSO on two popular medical image segmentation datasets and show its superior performance in various semi-supervised settings. With slight modification, ATSO transfers well to natural image segmentation for autonomous driving data. Finally, we demonstrate how ATSO can be used for clinical image diagnosis tasks. Source code is available at https:\/\/github.com\/annahaensch\/ATSO.git.io\/. Video explanation: https:\/\/youtu.be\/jskY8uaVqjskY9l3f6e3fdd444e1a135\/?p=%2FStatsPup","487":"Twitter promises a simple way of extracting data from online social media platforms. The current version of Twitter, v2.1, delivers two major improvements over the previous one. First, it supports the usage of third-party services on Twitter without requiring any manual configuration. Second, it provides multilingual support for tweets with proper nouns (e.g., people, locations) in addition to other commonly used named entities. In this paper, we present a comparison between these two versions delivered by Google and Apple at scale. Our results show that while overall the new architecture significantly improves the performance across all Twitter tasks, some specific themes like\"loss of details\"and\"social network turmoil\", still remain highly relevant. Finally, we introduce three new datasets harvested from Reddit. We hope they will be useful for evaluating the various zero-shot or transfer learning capabilities of existing models on Twitter. The dataset sizes are around 3,000 tweets per language and 28,000 tweets shared worldwide. They should prove beneficial for researchers studying different ways of applying machine learning techniques to understand the spread of a disease through social media, and potentially guide the implementation of preventive measures. A separate file contains the codebook of the dataset as well as the documentation of the benchmark suite, which allows anyone to further adapt","488":"As unmanned aircraft systems (UASs) continue to integrate into the U.S. National Airspace System (NAS), there is a need to quantify the risk of airborne collisions between unmanned and manned aircraft to support regulation and standards development. Developing and certifying collision avoidance systems often rely on the extensive use of Monte Carlo collision risk analysis simulations using probabilistic models of aircraft flight. To train these models, high performance computing resources are required. We've prototyped a high performance computing workflow designed and deployed on the Lincoln Laboratory Supercomputing Center to process billions of observations of aircraft. However, the prototype has various computational and storage bottlenecks that limited rapid or more comprehensive analyses and models. In response, we have developed a novel workflow to take advantage of various job launch and task distribution technologies to improve performance. The workflow was benchmarked using two datasets of observations of aircraft, including a new dataset focused on the environment around aerodromes. Optimizing how the workflow was parallelized drastically reduced the execution time from weeks to days. Furthermore, employing modularity in the workflow allows for flexibility in accommodating different user groups' requirements and resource constraints. Our case study demonstrates that the proposed framework can be used to assess and communicate the risk of airborne collisions between unmanned and","489":"This paper contributes to the growing literature on explainable AI (XAI) by introducing a new conceptual framework, Explore-to-Extrapolate Risk Minimization (EERM), that facilitates reasoning about why certain actions are preferable. EERMs are particularly useful when applied to risk management problems because they can accommodate complex heterogeneous causal dependencies and quantify risks in decision making scenarios. We introduce a novel loss function in this framework that addresses the inherent multi-criteria nature of risk assessments in real-world settings. The proposed approach enables efficient computation of EERM scores while preserving the privacy of raw data. Our experiments show that the EERM score can be effectively computed based on existing methods with performance comparable to state-of-the-art approaches, without revealing any sensitive information about the underlying scenario. Finally, we demonstrate its practicality by deploying it to a real-world case study on COVID-19 XAI. Our results show that EERMs can provide meaningful insights into risky insider activity within a large user base from a single seed example. All code is available at https:\/\/github.com\/anyleopeace\/explainable-risk-minimization. Contact tracing via EERMs is also available at https:\/\/github.com","490":"Graph neural networks (GNNs) have emerged as a series of competent graph learning methods for diverse real-world scenarios, ranging from daily applications like recommendation systems and question answering to cutting-edge technologies such as drug discovery in life sciences and n-body simulation in astrophysics. However, task performance is not the only requirement for GNNs. Performance-oriented GNNs have exhibited potential adverse effects like vulnerability to adversarial attacks, unexplainable discrimination against disadvantaged groups, or excessive resource consumption in edge computing environments. To avoid these unintentional harms, it is necessary to build competent GNNs characterised by trustworthiness. To this end, we propose a comprehensive roadmap to build trustworthy GNNs from the view of the various computing technologies involved. In this survey, we introduce basic concepts and comprehensively summarise existing efforts for trustworthy GNNs from six aspects, including robustness, explainability, privacy, fairness, accountability, and environmental well-being. Additionally, we highlight the intricate cross-aspect relations between the above six aspects of trustworthy GNNs. Finally, we present a thorough overview of trending directions for facilitating the research and industrialisation of trustworthy GNNs. This survey is written with the goal of informing public health policies related to contagious diseases","491":"The problem of estimating the support of a distribution is of great importance in many areas of machine learning, computer science, physics and biology. Most of the existing work on this problem has focused on settings that assume perfectly accurate sampling approaches, which is seldom true in practical data science. Here we introduce the first known approach to support estimation in the presence of sampling artifacts and errors where each sample is assumed to arise from a Poisson repeat channel which simultaneously captures repetitions and deletions of samples. The proposed estimator is based on regularization theory and is robust to changes in the tuning parameters when compared to popular methods such as the generalized likelihood ratio (GLR) method. We demonstrate that our approach significantly outperforms other well established semi-parametric methods by a large margin using both synthetic and real observed data. Finally, we provide theoretical insights into the behavior of our model with respect to different types of outliers and sampling artifacts. In particular, we show how their negative effects can affect the performance of standard GLR and R2 estimates. This suggests the need for careful modeling of outlier detection in multi-channel longitudinal studies. For example, repeated measurements may be used to mitigate biases arising from cross-sample information loss due to the inability to correctly estimate the support of a given","492":"Let S Q denote x 3 = Q(y 1,..., y m)z where Q is a primitive positive definite quadratic form in m variables with integer coefficients. This S Q ranges over a class of singular cubic hypersurfaces as Q varies. For S Q we prove (i) Manin's conjecture is true if Q is locally determined with 2 | m and m 4; (ii) in general Manin's conjecture is true up to a leading constant if 2 | m and m 6. We also establish (iii) Manin's conjecture is true for all primes $p \\leq X^{1\/4}$. The proof relies on two new results announced by T. Carlsen and S. Eilers. Firstly, they have shown that there exists a connected branch of manifolds whose intersection with any open subset of dimension $\\neq 3$ has strictly larger topology than that of surfaces with finite fundamental group. Secondly, they have demonstrated that it is only possible to obtain a discrete and complete invariant for these manifolds in the setting of their conjecture. This answers questions raised by T. Carlen and S. Eilers during their paper [arXiv:1909.07237] about the multiplicity of","493":"Modern generative models achieve excellent quality in a variety of tasks including image or text generation and chemical molecule modeling. However, existing methods often lack the essential ability to generate examples with requested properties, such as the age of the person in the photo or the weight of the generated molecule. Incorporating such additional conditioning factors would require rebuilding the entire architecture and optimizing the parameters from scratch. Moreover, it is difficult to disentangle selected attributes so that to perform edits of only one attribute while leaving the others unchanged. To overcome these limitations we propose PluGeN (Plugin Generative Network), a simple yet effective generative technique that can be used as a plugin to pre-trained generative models. The idea behind our approach is to transform the entangled latent representation using a flow-based module into a multi-dimensional space where the values of each attribute are modeled as an independent one-dimensional distribution. In consequence, PluGeN can generate new samples with desired attributes as well as manipulate labeled attributes of existing examples. Due to the disentangling of the latent representation, we are even able to generate samples with rare or unseen combinations of attributes in the dataset, such as a young person with gray hair, men with make-up, or women with beards. We","494":"Structural global parameter identifiability indicates whether one can determine a parameter's value from given inputs and outputs in the absence of noise. If a given model has a set number of values for which there may be infinitely many values, such parameters are called non-identifiable. We present a procedure for accelerating a global identifiability query by eliminating algebraically independent non-identifiable parameters. Our proposed approach significantly improves performance across different computer algebra frameworks. In particular, it enables us to exploit first order information about the dependence structure between input variables of our model, which we derive and show to improve performance further. For example, this derivation provides a sound foundation for the use of the $\\ell_1$-norm regularization method, which is commonly used together with the classical empirical risk minimization (ERM) algorithm to solve large optimization problems. Empirical studies validate the effectiveness of our proposed strategy for handling missing data in practical scenarios. From a theoretical perspective, our work establishes novel results on the basis of purely observational approaches - instead of considering the asymptotic behavior of the joint distribution of the primal and dual parameters, as typically implemented in existing MCMC and variational Bayes approaches. Finally, we provide extensive computational experiments to demonstrate the improvement brought by","495":"We study a SIR epidemiological model, with a variable contagion rate, applied to the evolution of COVID19 in Cuba. It is highlighted that an increase in the predictive character depends on understanding the dynamics for the temporal evolution of the rate of contagion $\\beta^*$. A semi-empirical model for this dynamics is formulated, where reaching $\\beta^*\\approx0$ due to isolation is achieved after the mean duration of the disease $\\tau=1\/\\gamma$, in which the number of infected in the confined families has decreased. It is considered that $\\beta^*(t)$ should have an abrupt decrease on the day of initiation of confinement and decrease until canceling at the end of the interval $\\tau$. The analysis describes appropriately the infection curve for Germany. The model is applied to predict an infection curve for Cuba, which estimates a maximum number of infected as less than 2000 in the middle of May, depending on the rigor of the isolation performed during $\\tilde R_0=10^{-3}$. This is suggested by the ratio between the daily detected cases and the total. We consider the ratio between the observed and real infected cases (k) less than unity. The low value of k","496":"Urban analytics combines spatial analysis, statistics, computer science, and urban planning to understand and shape city futures. While it promises better policymaking insights, concerns exist around its epistemological scope and impacts on privacy, ethics, and social control. This chapter reflects on the history and trajectory of urban analytics as a scholarly and professional discipline. In particular, it considers the direction in which this field is going and whether it improves our collective and individual welfare. It first introduces early theories, models, and deductive methods from which the field originated before shifting toward induction. It then explores urban network analytics that enrich traditional representations of spatial interaction and structure. Next it discusses urban applications of spatiotemporal big data and machine learning. Finally, it argues that privacy and ethical considerations are too often ignored as ubiquitous monitoring and analytics can empower social repression. It concludes with a call for a more critical urban analytics that recognizes its epistemological limits, emphasizes human dignity, and learns from and supports marginalized communities. Such an approach should emphasize multiple values, including those related to human health, equity, and sustainability. Furthermore, it should be clear that knowledge of the historical context surrounding any current problem is never sufficient for making decisions about how to address it. For empowering the forces of social repression, this study","497":"Highest density regions refer to level sets containing points of relatively high density. Their estimation from a random sample, generated from the underlying density, allows to determine the clusters of the corresponding distribution. This task can be accomplished considering different nonparametric perspectives. From a practical point of view, reconstructing highest density regions can be interpreted as a way of determining hot-spots, a crucial task for understanding COVID-19 space-time evolution. In this work, we compare the behavior of classical plug-in methods and a recently proposed hybrid algorithm for highest density regions estimation through an extensive simulation study. Both methodologies are applied to analyze a real data set about COVID-19 cases in the United States. The results show that both methods achieve good accuracy when estimating homogeneous densities, while performing well on heterogeneous ones. Moreover, the estimated hotspots vary considerably between time periods and align with the known infection timescale of COVID-19. Finally, we also discuss the implications of our findings for broader scholarly research on COVID-19 and its potential impact on risk management. An R package capable of implementing the described methodology is available at https:\/\/github.com\/ceyonur\/highest_density_regions_estimator.git.io\/.","498":"The recent COVID-19 pandemic has become a major threat to human health and well-being. Non-pharmaceutical interventions such as contact tracing solutions are important to contain the spreads of COVID-19-like infectious diseases. However, current contact tracing solutions are fragmented with limited use of sensing technologies and centered on monitoring the interactions between individuals without an analytical framework for evaluating effectiveness. Therefore, we need to first explore generic architecture for contact tracing in the context of today's Internet of Things (IoT) technologies based on a broad range of applicable sensors. A new architecture for IoT based solutions to contact tracing is proposed and its overall effectiveness for disease containment is analyzed based on the traditional epidemiological models with the simulation results. The proposed work aims to provide a framework for assisting future designs and evaluation of IoT-based contact tracing solutions and to enable data-driven collective efforts on combating current and future infectious diseases. This paper provides a systematic review of the recently proposed architectures for IoT-based contact tracing and demonstrates their potential application using the proposed model and analysis. In particular, three prominent types of architectures for IoT-based contact tracing are investigated based on four categories: centralised, decentralised, or hybrid. Moreover, the existing literature on the state-of-the","499":"We present here a simple derivation of the simpler model\"Hiding-Sizes\"(HS) model, introduced by Weickert et al. (2013), which remains valid when recombined with individual-level effects such as social distancing and mask-wearing. The former component only needs to be modified to account for the double-sided nature of the HS model; while the latter will remain unchanged, we add a new parameter to the model in order to include the effect of hidden size or isolation on the dynamics of the epidemic. We also discuss the various limitations of the model. After collecting the data reported in Wuhan, China, our simple differential equation system can reproduce the observed infection curve for any given value of the basic reproduction number $R_0$. It applies to the city-level analysis in Wuhan, where the infection peak was reached about 10 days before the first reported case. Finally, it predicts the evolution of the outbreak in Mexico City, where complete lockdown measures were not implemented until June 2021. If the early quarantine strategies had been maintained, the cumulative number of deaths would have diminished more than the one shown in this paper. Therefore, these results suggest that if there is little incentive for people to adopt them, then investing efforts on","500":"In this paper, we perform Monte Carlo calculations to study the critical behavior of the spread of infectious diseases through a novel approach to the SIR epidemiological model. A stochastic lattice gas version of the model was applied on hybrid lattices which, in turn, are generated from typical square lattices when inserting a connection probability $p$ that a given site has both first- and second-nearest neighbor interactions. By combining percolation theory and finite-size scaling analysis, we estimate both the critical threshold and leading critical exponent ratios of the non-absorbing SIR model in different cases of hybrid lattices. An analysis of the average size of the percolating cluster and the size distribution of non-percolating clusters of recovered individuals was carried out to determine the universality class of the model. We show that the epidemic growth obeys universal laws with exponents ranging between one and two for all investigated cases of hybrid lattices. The present results indicate that it is possible to extract information about the disease transmission process by analyzing the clustering properties of the network of interacting susceptible, infected, and recovered individuals in order to improve the reliability of the predictions made by the model. Finally, we provide theoretical insights into the difficulty of predicting the epidemic threshold and","501":"We study multi-patch epidemic models where individuals may migrate from one patch to another in either of the susceptible, exposed\/latent, infectious and recovered states. We assume that infections occur both locally with a rate that depends on the patch as well as\"from distance\"from all the other patches. The exposed and infectious periods have general distributions, and are not affected by the possible migrations of the individuals. The migration processes in either of the three states are assumed to be Markovian, and independent of the exposed and infectious periods. We establish a functional law of large number (FLLN) and a function central limit theorem (FCLT) for the susceptible, exposed\/latent, infectious and recovered processes. In the FLLN, the limit is determined by a set of Volterra integral equations. In the special case of deterministic exposed and infectious periods, the limit becomes a system of ODEs with delays. In the FCLT, the limit is given by a set of stochastic Volterra integral equations driven by a sum of independent Brownian motions and continuous Gaussian processes with an explicit covariance structure. We prove that the limit converges to a unique solution defined by an iterative application of the FCLT. Our","502":"The ongoing COVID-19 pandemic has profoundly impacted people's life around the world, including how they interact with mobile technologies. In this paper, we seek to develop an understanding of how the dynamic trajectory of a pandemic shapes mobile phone users' experiences. Through the lens of app popularity, we approach this goal from a cross-country perspective. We compile a dataset consisting of six-month daily snapshots of the most popular apps in the iOS App Store in China and the US, where the pandemic has exhibited distinct trajectories. Using this longitudinal dataset, our analysis provides detailed patterns of app ranking during the pandemic at both category and individual app levels. We reveal that app categories' rankings are correlated with the pandemic, contingent upon country-specific development trajectories. Our work offers rich insights into how the COVID-19, a typical global public health crisis, has influence people's day-to-day interaction with the Internet and mobile technologies. Together, these findings highlight the critical role of app reviews as a proxy for social distancing, and illustrate the potential impact of tracking major changes in peoples' lives through app updates. Our work may shed light on the timely design of mitigation strategies by identifying areas of opportunity for governments and developers to take advantage of. Overall","503":"Advances in face synthesis have raised alarms about the deceptive use of synthetic faces. Can synthetic identities be effectively used to fool human observers? In this paper, we introduce a study of the visual perception of synthetic faces generated using different strategies including a state-of-the-art deep learning-based GAN model. This is the first rigorous study of the effectiveness of synthetic face generation techniques grounded in experimental techniques from psychology. We answer important questions such as how often do GAN-based and more traditional image processing-based techniques confuse human observers, and are there subtle cues within a synthetic face image that cause humans to perceive it as a fake without having to search for obvious clues? To answer these questions, we conducted a series of large-scale crowdsourced behavioral experiments with different sources of face imagery. Results show that humans are unable to distinguish synthetic faces from real faces under several different circumstances. This finding has serious implications for many different applications where face images are presented to human users. Furthermore, our results suggest ways to improve the perceptual accuracy of synthetic faces via improving the clarity of their presentation. The findings motivate future research into designing more effective synthetic face generating methods grounded in experimental techniques from psychology. Overall, they provide a better understanding of why modern generative models are so effective at","504":"We develop a Bayesian non-parametric quantile panel regression model. Within each quantile, the response function is a convex combination of a linear model and a non-linear function, which we approximate using Bayesian Additive Regression Trees (BART). Cross-sectional information at the pth quantile is captured through a conditionally heteroscedastic latent factor. The non-parametric feature of our model enhances flexibility, while the panel feature, by exploiting cross-country information, increases the number of observations in the tails. We develop Bayesian Markov chain Monte Carlo (MCMC) methods for estimation and forecasting with our quantile factor BART model (QF-BART), and apply them to study growth at risk dynamics in a panel of 11 advanced economies. We show that our QF-BART model outperforms other competing models in both predictive power and uncertainty mapping, using backtesting to verify its outperformance on real-world data. Our approach provides a better approximation of the true response function than either the conventional parametric or semi-parametric approaches. Altogether, our results highlight the utility of incorporating structural dependencies in machine learning models when assessing economic performance. Code is available at https:\/\/github.com\/dhopp1","505":"The spread of misinformation and\"fake news\"continues to be a major focus of public concern. A great deal of research has examined who falls for misinformation and why, and what can be done to make people more discerning consumers of news. Comparatively little work, however, has considered misinformation producers, and how their strategies interact with the psychology of news consumers. Here we use game-theoretic models to study the strategic interaction between news publishers and news readers. We show that publishers who seek to spread misinformation can generate high engagement with falsehoods by using strategies that mix true and false stories over time, in such a way that they serve more false stories to more loyal readers. These coercive strategies cause false stories to receive higher reader engagement than true stories - even when readers strictly prefer truth over falsehood. In contrast, publishers who seek to promote engagement with accurate information will use approaches that generate more engagement with true stories than with false stories. We confirm these predictions empirically by examining 1,000 headlines from 20 mainstream and 20 fake news sites, comparing Facebook engagement data with 20,000 perceived accuracy ratings collected in a survey experiment. We show that engagement is negatively correlated with perceived accuracy among misinformation sites, but positively correlated with perceived accuracy among mainstream sites. We then use our model","506":"The paper title refers to a list of keywords, presented in alphabet order, which describe the general topic of the data set and hence are likely to appear in the text. The use of these keywords on social media has been increasing during the last years, possibly due to the COVID-19 pandemic. In this manuscript we aim at investigating whether there is any connection between the use of these keywords and the gender classification problem, or not. We have duly used the Twitter dataset and tried to answer the following queries: \\textit{Is it possible for people to fail to classify female\/male based on their tweets? OR\\textit{is it possible for women to succeed while making men suffer? To study the subject further, we have divided the database into two parts, one including 11,787 documents containing the words\"keywords\"in red, the other will be used to fine-tune our machine learning model. Next, we have trained some multiple linear regression models with the retrieved keyword features. Finally, we have achieved 0.85 (F1 score) of the test set from the perspective of the predicted gender. We thus show that the application of machine learning via social media can produce results that may help in discrimination among individuals of different genders, especially","507":"With worldwide concerns surrounding the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), there is a rapidly growing body of literature on the virus, SARS-CoV-1, and COVID-19. This explosion of information has been curated via online media sources like Twitter, Facebook, YouTube, and others. Many organizations including governments and health departments have taken advantage of this wealth of data to produce valuable services for helping people fight against the disease. However, the public's trust in these products may not be well placed due to the lack of manual fact-checking efforts. In this paper, we propose a multi-task deep learning approach to scale up labeling efforts with the help of Google News and The World Health Organization (WHO). We combine the proposed neural architecture with classic document similarity metrics and then apply it to two different contexts: 1) the personal domain of the user, and 2) the organization or news article credibility level of the product. Our experiments on real-world datasets show that the proposed approach outperforms state-of-the-art methods in both tasks. Finally, we demonstrate the effectiveness of the proposed method on a publicly available dataset as well as some of its potential applications during the current pandemic","508":"The recent development in the area of pervasive computing and artificial intelligence has led to the realization of true vision transformers. These are devices that can learn powerful representations from large-scale unstructured text without costly human supervision. The core strength of such architectures is the ability to perform model selection and computation on demand. However, despite their state-of-the-art performance in predictive tasks like ad hoc retrieval or image classification, we still face significant challenges with applying these models to unseen tasks and\/or small datasets. Most notably, lack of generalizability of such models as they are trained on one dataset and tested on another by means of cross-domain training. In this paper, we study the problem of few-shot out-of-domain learning of linear classifiers based on dense connections between randomly initialized input embeddings and fully connected layer representations. We show that simple dense connection mechanisms can outperform more complex ones, and thus achieve good presentation power with only a minor loss in accuracy. Dense connections are especially useful for transferring knowledge from one domain to others where the two domains exhibit very different styles. Our method achieves strong results on three diverse medical imaging datasets, namely, healthy vs unhealthy lungs sounds, COVID-19 scans, and optic cup\/optic","509":"This paper introduces the 3rd DIHARD challenge, the third in a series of speaker diarization challenges intended to improve the robustness of diarization systems to variation in recording equipment, noise conditions, and conversational domain. The challenge comprises two tracks evaluating diarization performance when starting from a reference speech segmentation (track 1) and diarization from raw audio scratch (track 2). We describe the task, metrics, datasets, and evaluation protocol. Track 1 leverages a multi-task learning approach with a custom reward function, while track 2 utilizes a single monolingual target speaker over multiple languages. We further introduce the DIHARD challenge as a testbed for discussing how well current speaker diarization techniques can be improved by utilizing data from one or more domains where they have been trained. Additionally, we demonstrate how our system copes with variation in session length and time, including suggestions on non-speech elements, alternative sentence segmentation tasks, and background noises. Results are presented at https:\/\/jovidecenzato.github.io\/diachard\/. Findings include: (1) the proposed training paradigm significantly improves the state-of-the-art performant baselines; (2) each of the suggested backgrounds outper","510":"Due to the abrupt arise in the pandemic worldwide, the video conferencing platforms are becoming ubiquitously available and being embedded into either various digital devices or the collaborative daily work. Even though the service provider has designed many security functions to protect individual's privacy, such as virtual background (VB), it still remains to be explored that how the instability of VB leaks users' privacy or impacts their mentality and behaviours. In order to understand and locate implications for the contextual of the end-users' privacy awareness and its mental model, we will conduct survey and interviews for users as the first stage research. We will raise conceptual challenges in terms of the designing safety and stable VB, as well as provide design suggestions. Furthermore, we will also discuss the impact of the lack of screen reader support in VC environments. As a solution, we propose a novel method of using augmented reality technology to create a secure and stable virtual environment by embedding user activities into objects of interest - specifically, demonstrating videos from multiple people playing together. Our study explores the potential application of using augmented reality technology for social good crises management, and further investigates the technical problems and ethical issues associated with state-of-the-art technologies. We conclude by providing recommendations on how to develop effective AR based systems","511":"As the coronavirus disease 2019 (COVID-19) has shown profound effects on public health and the economy worldwide, it becomes crucial to assess the impact on the virus transmission and develop effective strategies to address the challenge. A new statistical model derived from the SIR epidemic model with functional parameters is proposed to understand the impact of weather and government interventions on the virus spread in the presence of asymptomatic infections among eight metropolitan areas in the United States. The model uses Bayesian inference with Gaussian process priors to study the functional parameters nonparametrically, and sensitivity analysis is adopted to investigate the main and interaction effects of these factors. This analysis reveals several important results including the potential interaction effects between weather and government interventions, which shed new light on the effective strategies for policymakers to mitigate the COVID-19 outbreak. In addition, the model indicates significant interactions between infection cases and hospitalizations, which suggests the effectiveness of hospitals in managing infectious diseases. It also shows that the spatiotemporal heterogeneity of the infection spread can be well explained by the dynamic development of the effective government intervention and the changing demographic conditions. Moreover, the results provide strategic insights for policy makers to take informed decisions when facing with similar epidemics in the future. Numerical simulations based on","512":"Genomic surveillance of SARS-CoV-2 has been instrumental in tracking the spread and evolution of the virus during the COVID-19 pandemic. The availability of SARS-CoV-2 molecular sequences isolated from infected individuals, coupled with phylodynamic methods, have provided insights into the origin of the virus, its evolutionary rate, the timing of introductions, the patterns of transmission, and the rise of novel variants that have spread through populations. Despite enormous global efforts of governments, laboratories, and researchers to collect and sequence molecular data, many challenges remain in analyzing and interpreting the data collected. Here, we describe the models and methods currently used to monitor the spread of SARS-CoV-2, discuss long-standing and new statistical challenges, and propose a method for tracking the rise of novel variants during the epidemic. The framework described here can be extended to other viral diseases transmitted through molecular mechanisms. Thus, it may help track the rise of novel variants of concern and guide public health decision making. Finally, these methods will help reconstruct the genomic profile of the known pathogen within the context of the current COVID-19 outbreak, facilitating more efficient downstream analyses of SARS-CoV-2 molecular sequences. Overall, this article provides an overview of","513":"The growth in disinformation technology necessitates the need to identify fake videos. One approach to preventing the consumption of these fake videos is provenance which allows the user to authenticate media content to its original source. This research designs and investigates the use of provenance indicators to help users identify fake videos. We first interview users regarding their experiences with different misinformation modes (text, image, video) to guide the design of indicators within users' existing perspectives. Then, we conduct a participatory design study to develop and design fake video indicators. Finally, we evaluate participant-designed indicators via both expert evaluations and quantitative surveys with a large group of end-users. Our results provide concrete design guidelines for the emerging issue of fake videos. Our findings also raise concerns regarding users' tendency to overgeneralize from misinformation warning messages, suggesting the need for further research on warning design in the ongoing fight against disinformation. Overall, our work contributes to the nascent but emerging literature by proposing a novel view on tackling disinformation via design. We hope this contribution will be a useful resource for researchers, public health leaders, and social media platforms to enable approaches to combat disinformation via intervention. WARNING: This paper contains offensive language that commonly appears on Reddit's r\/anxietynetwork_fakenews. Hence, it","514":"Lung cancer is the leading cause of mortality from cancer worldwide and has various histologic types, among which Lung Adenocarcinoma (LUAC) has recently been the most prevalent one. The current approach to determine the invasiveness of LUACs is surgical resection, but this is not practical for all patients as it requires trained medical experts who are time-consuming and expensive to operate. In this paper, we propose an explainable two-step machine learning model based on graph neural networks (GNNs) to identify LUACs using chest Computed Tomography (CT) scans. Our first step consists of pre-processing the CT images to extract their relevant features, such as ground-glass opacity (GGO), consolidation, and texture. Then, a variety of GNNs are used to classify the extracted features into LUAC or non-LUAC classes. Finally, the classification accuracy of each class is further computed with the GGOVID tool to verify its reliability. With the proposed approach, we have identified 96.00% of the best decision tree model achieving sensitivity and specificity of 97.83%, 98.39%, and 95.57% respectively on the dataset, while showing about 10% improvement over other reported state-of-","515":"The global outbreak of COVID-19 has led to focus on efforts to manage and mitigate the continued spread of the disease. One of these efforts include the use of contact tracing to identify people who are at-risk of developing the disease through exposure to an infected person. Historically, contact tracing has been primarily manual but given the exponential spread of the virus that causes COVID-19, there has been significant interest in the development and use of digital contact tracing solutions to supplement the work of human contact tracers. The collection and use of sensitive personal details by these applications has led to a number of concerns by the stakeholder groups with a vested interest in these solutions. We explore digital contact tracing solutions in detail and propose the use of a transparent reporting mechanism, FactSheets, to provide transparency of and support trust in these applications. We also provide an example FactSheet template with questions that are specific to the contact tracing application domain. Our analysis of the existing state of the art reveals that there is currently no standard scheme for ensuring the privacy of this data. Finally, we highlight some potential challenges and offer recommendations for future research along with a discussion of the societal implications of using these tools. We believe that our effort is the first step towards providing clarity and guidance about how and","516":"We study a class of individual-based, fixed-population size epidemic models under general assumptions, e.g., heterogeneous contact rates encapsulating changes in behavior and\/or enforcement of control measures. We show that the large-population dynamics are deterministic and relate to the Kermack-McKendrick PDE. Our assumptions are minimalistic in the sense that the only important requirement is that the basic reproduction number of the epidemic $R_0$ be finite, and allow us to tackle both Markovian and non-Markovian dynamics. The novelty of our approach is to study the\"infection graph\"of the population. We show local convergence of this random graph to a Poisson (Galton-Watson) marked tree, recovering Markovian backward-in-time dynamics in the limit as we trace back the transmission chain leading to a focal infection. This effectively models the process of contact tracing in a large population. It is expressed in terms of the Doob $h$-transform of a certain renewal process encoding the time of infection along the chain. Our results provide a mathematical formulation relating a fundamental epidemiological quantity, the generation time distribution, to the successive time of infections along this transmission chain. Within these more precise conditions,","517":"Since early 2020, the world has been dealing with a raging pandemic outbreak: COVID-19. A year later, vaccines have become accessible, but in limited quantities, so that governments needed to devise a strategy to decide which part of the population to prioritize when assigning the available doses, and how to manage the interval between doses for multi-dose vaccines. In this paper, we present an optimization framework to address the dynamic double-dose vaccine allocation problem whereby the available vaccine doses must be administered to different age-groups to minimize specific societal objectives. In particular, we first identify an age-dependent Susceptible-Exposed-Infected-Recovered (SEIR) epidemic model including an extension capturing partially and fully vaccinated people, whereby we account for age-dependent immunity and infectiousness levels together with disease severity. Second, we leverage our model to frame the dynamic age-dependent vaccine allocation problem for different societal objectives, such as the minimization of infections or fatalities, and solve it with nonlinear programming techniques. Finally, we carry out a numerical case study with real-world data from The Netherlands. Our results show how different societal objectives can significantly alter the optimal vaccine allocation strategies. For instance, we find that minimizing the overall number of infections results in delaying","518":"The rapid spread of COVID-19 in the United States has imposed a major threat to public health, the real economy, and human well-being. With the absence of effective vaccines, the preventive actions of social distancing and travel reduction are recognized as essential non-pharmacologic approaches to control the spread of COVID-19. Prior studies demonstrated that human movement and mobility drove the spatiotemporal distribution of COVID-19 in China. Little is known about the patterns and effects of co-location reduction on cross-county transmission risk of COVID-19. This study utilizes Facebook co-location data for all counties in the United States from March to early May 2020. The analysis examines the synchronicity and time lag between travel reduction and pandemic growth trajectory to evaluate the efficacy of social distancing in ceasing the population co-location probabilities, and subsequently the growth in weekly new cases. The results show that the mitigation effects of co-location reductions appear in the growth of weekly new cases with one week of delay. Furthermore, significant segregation is found among different county groups which are categorized based on numbers of cases. The results suggest that within-group co-location probabilities remain stable during the first phase of this pandemic, while across-","519":"The novel coronavirus disease (COVID-19) pandemic has posed unprecedented challenges for the utilities and grid operators around the world. In this work, we focus on the problem of load forecasting. With strict social distancing restrictions, power consumption profiles around the world have shifted both in magnitude and daily patterns. These changes have caused significant difficulties in short-term load forecasting. Typically algorithms use weather, timing information and previous consumption levels as input variables, yet they cannot capture large and sudden changes in socioeconomic behavior during the pandemic. In this paper, we introduce mobility as a measure of economic activities to complement existing building blocks of forecasting algorithms. Mobility data acts as good proxies for the population-level behaviors during the implementation and subsequent easing of social distancing measures. The major challenge with such dataset is that only limited mobility records are associated with the recent pandemic. To overcome this small data problem, we design a transfer learning scheme that enables knowledge transfer between several different geographical regions. This architecture leverages the diversity across these regions and the resulting aggregated model can boost the algorithm performance in each region's day-ahead forecast. Through simulations for regions in the US and Europe, we show our proposed algorithm can outperform conventional forecasting methods by more than three-folds. In","520":"The COVID-19 pandemic has caused unprecedented disruption worldwide, leading to significant loss in life and economy. As essential measures for containing the virus spread, governments have implemented strict lockdown policies at different levels of society. This has changed the daily lives of almost everyone on Earth. While remote working is not new for software engineers, forced Work From Home situations to come with both constraints, limitations, and opportunities for individuals, software teams and software companies. Understanding how these restrictions affect team performance is critical for supporting the success of firms in adapting their production strategies to cope with the disruptions. In this paper, we investigate the impact of the outbreak on software development teams in a top-10 European firm. We analyse records of communications among its 1000+ teams using a data set made of all messages posted between 2013 and 2015. We classify each message into seven categories as either technical or non-technical and further analyse 37 indicators of team performance based on quantitative and qualitative features. We find that the most affected groups are concerned with various aspects of the WFH environment, such as higher stress levels, more ill-defined tasks, and more changes to the status quo. The observations indicate that the effectiveness of individual, software and organisational decisions depends on the context within which they work. To understand the","521":"Nb3Sn is a promising next-generation material for superconducting radiofrequency cavities, with significant potential for both large scale and compact accelerator applications. However, so far, Nb3Sn cavities have been limited to cw accelerating fields<18 MV\/m. In this paper, new results are presented with significantly higher fields, as high as 24 MV\/m in single cell cavities. Results are also presented from the first ever Nb3Sn-coated 1.3 GHz 9-cell cavity, a full-scale demonstration on the cavity type used in production for the European XFEL and LCLS-II. Results are presented together with heat dissipation curves to emphasize the potential for industrial accelerator applications using cryocooler-based cooling systems. The cavities studied have an atypical shiny visual appearance, and microscopy studies of witness samples reveal significantly reduced surface roughness and smaller film thickness compared to typical Nb3Sn films for superconducting cavities. Possible mechanisms for increased maximum field are discussed as well as implications for physics of RF superconductivity in the low coherence length regime. Outlook for continued development is presented. Detailed analysis of the main parameters by scaling up the size of the system and increasing","522":"In this paper, we introduce the Age of Incorrect Information (AoII) as an enabler for semantics-empowered communication, a newly advocated communication paradigm centered around data's role and its usefulness to both the transmitter and receiver. First, we shed light on how the traditional communication paradigm, with its role-blind approach to data, is vulnerable to performance bottlenecks. Next, we highlight the shortcomings of several proposed performance measures destined to deal with the traditional communication paradigm's limitations, namely the Age of Information (AoI) and the error-based metrics. We also show how the AoII addresses these shortcomings and captures more meaningfully the purpose of data. Afterward, we consider the problem of minimizing the average AoII in a transmitter-receiver pair scenario where packets are sent over an unreliable channel subject to a transmission rate constraint. We prove that under such a setting, there exist a number of transmit pulses necessary to achieve the lower bound of the optimal policy. Furthermore, we propose a novel mutual information-agnostic multi-objective optimization method inspired by the recent advances in machine learning techniques. We demonstrate through numerical simulations the effectiveness of our proposed design compared to other benchmarking strategies, e.g., age of information and error-","523":"Scientists, governments, and companies increasingly publish datasets on the Web. Google's Dataset Search extracts dataset metadata -- expressed using schema.org and similar vocabularies -- from Web pages in order to make datasets discoverable. Since we started the work on Dataset Search in 2016, the number of datasets described in schema.org has grown from about 500K to almost 30M. Thus, this corpus has become a valuable snapshot of data on the Web. To the best of our knowledge, this corpus is the largest and most diverse of its kind. We analyze this corpus and discuss where the datasets originate from, what topics they cover, which form they take, and what people searching for datasets are interested in. Based on this analysis, we identify gaps and possible future work to help make data more discoverable. This paper serves as a starting point for research describing how to improve the quality of Dataset Search and thereby increase its user base. Finally, we provide a detailed example of a large-scale dataset currently available online: https:\/\/github.com\/gbriel21\/dsets_in_the_wild. Particularly, we show how these datasests can be used to explore topic modeling, study authors' distributions, and answer other questions with additional","524":"We present a search for continuous gravitational waves from five radio pulsars, comprising three recycled pulsars (PSR~J0437\\textminus4715) and two young pulsars: the Crab pulsar (J0534+2200) and the Vela pulsar (J0835\\textminus4510). We use data from the third observing run of Advanced LIGO and Virgo combined with data from their first and second observing runs. For the first time we are able to match (for PSR~J0437\\textminus4715) or surpass (for J0835\\textminus4510) the indirect limits on gravitational-wave emission from recycled pulsars inferred from their observed spin-downs, and constrain them as less than 10^{-8}$ at 95\\% confidence level. For each of the five pulsars, we perform targeted searches that assume a tight coupling between the gravitational-wave and electromagnetic signal phase evolution. We also present constraints on PSR~J0711\\textminus6830, the Crab pulsar and the Vela pulsar from a search that relaxes this assumption, allowing the gravitational-wave signal to vary from the electromagnetic expectation within a narrow band of frequencies and frequency derivatives. No","525":"3CL-Pro (or M-Pro) is the SARS-CoV-2 main protease, responsible for mediating viral replication and transcription. It acts exclusively as a homodimer, while monomers are inactive. Due to its pivotal role, 3CL-Pro has been one of the most studied SARS-CoV-2 proteins and the subject of a number of therapeutic interventions, targeting its catalytic domain. A number of potential drug candidates have been reported, including some natural products. Here, we have investigated in silico, through fully flexible binding and extensive molecular dynamics simulations, the natural product space for the identification of potential candidates of 3CL-Pro dimerization inhibitors. We report that fortunellin (acacetin 7-O-neohesperidoside), a natural flavonoid O-glycoside, isolated from the fruits of Citrus japonica var. margarita (kumquat), is a potent inhibitor of 3CL-Pro dimerization. A search of the ZINC natural products database identified another 16 related molecules, which possess interesting pharmacological properties. We propose that fortunellin and its analogs might be the basis of novel pharmaceuticals against SARS-CoV","526":"Finding objects in digital images is a challenging but feasible task. Although there are many available image descriptors, their performance varies substantially across object categories. As a result, it is impractical to apply one single descriptor for all real-world images. To address this problem, we propose FOSEC, a framework of fine-grained feature extraction from semantic data of natural images. We show that by using colour-aware spatial clustering with an appropriate distance metric, we can produce a large number of high quality features without any pixel-level annotation. Then, we train a deep neural network with these features to detect the presence of the desired object(s) in each image. Evaluation on four popular datasets demonstrates that our method outperforms other state-of-the-art approaches by a significant margin. We also achieve good results on two out of three publicly available medical image datasets. Our code is available at https:\/\/github.com\/felipemaiapolo\/FOSEC.git. The project is licensed under a 3.0 license via the GitHub repository https:\/\/github.com\/felipemaiapolo\/FOSEC_DDSM. Keywords: Feature extraction, Defect detection, Natural language processing, Open dataset, Dat","527":"The burgeoning availability of sensing technology and location-based data is driving the expansion of analysis of human mobility networks in science and engineering research, as well as in epidemic forecasting and mitigation, urban planning, traffic engineering, emergency response, and business development. However, studies employ datasets provided by different location-based data providers, and the extent to which the human mobility measures and results obtained from different datasets are comparable is not known. To address this gap, in this study, we examine three prominent location-based data sources: Spectus, X-Mode, and Veraset to analyze human mobility networks across metropolitan areas at different scales: global, sub-structure, and microscopic. Dissimilar results were obtained from the three datasets, suggesting the sensitivity of network models and measures to datasets. This finding has important implications for building generalized theories of human mobility and urban dynamics based on different datasets. The findings also highlighted the need for ground-truthed human movement datasets to serve as the benchmark for testing the representativeness of human mobility datasets. Researchers and decision-makers across different fields of science and technology should recognize the sensitivity of human mobility results to dataset choice and develop procedures for ground-truthing the selected datasets in terms of representativeness of data points and transferability","528":"The Bradley-Terry model is widely used for pairwise comparison data analysis. In this paper, we analyze the asymptotic behavior of the maximum likelihood estimator of the Bradley-Terry model in its logistic parameterization, under a general class of linear identifiability constraints. We show that the constraint requiring the Bradley-Terry scores for all compared objects to sum to zero minimizes the sum of the variances of the estimated scores, and recommend using this constraint in practice. The second recommended solution involves simply plugging in the standard rates derived for the first problem. To illustrate the usefulness of our results, we apply them to simulated datasets. Our analyses demonstrate that the proposed family of rate functions yields sensible and consistent improvements over other established methods, while maintaining their interpretability. A key message is that it is generally more useful to directly estimate the variance of the estimated scores than computing the mean of the estimates. This can be particularly important when the latter becomes infeasible or impossible in practice. Finally, we provide an interactive tool allowing the readers to explore the different ways of interpreting the results of any analysis involving observational studies with a similar number of participants. Code implementing our method is available at https:\/\/github.com\/annahaensch\/metric-learning-","529":"The COVID-19 pandemic has completely disrupted the operation of our societies. Its elusive transmission process, characterized by an unusually long incubation period, as well as a high contagion capacity, has forced many countries to take quarantine measures that conspire against the performance of national economies. This situation confronts decision makers in different countries with the alternative of reopening the economies, thus facing the unpredictable cost of a rebound of the infection. This work tries to offer an initial theoretical framework to handle this alternative. We present a dual-systems model of epidemic spread and recovery, adapted from the famous SIR model, which is translated into a system of linear differential equations. Then we apply the model to study the case of Spain, one of the most affected countries by the current outbreak. We show that if $R_0$ (the number of new infected individuals during their infectious period) decreases exponentially along any trajectory until reaching $\\sim 1\/3$ of its maximum value, then the whole epidemical curve can be flattened around the peak of infections. These results demonstrate how effective isolation and social distancing measures can be used to manage the reproduction number of an epidemic such as COVID-19 without having to implement them explicitly. In addition, the mathematical model allows us to estimate","530":"This study presents a scalable method to measure the impact of social distancing on the spread of COVID-19 in a city, and demonstrates it in the case of Santiago (Chile), which has been one of the most harshly hit by the pandemic and whose death toll was also heavy with about 27 million confirmed cases. The metric used in this paper corresponds to the reproduction number R0, which indicates the effectiveness of social distancing when compared to its partial failure. Based on these results, we argue that governments should implement measures that allow mitigating the transmission of the disease but do not put the economy into stakeholder pockets because their main objective is to reduce the intensity of the outbreak without having any major socio-economic problems. We emphasize the importance of coordinating the deployment of resources between different levels of society and ensuring public health through accurate data sharing. These findings show that social distancing can be effective if coordinated with the right type of mechanism, namely, intelligent community detection\/isolation, contact tracing, and spatial mapping. In addition, proper usage of medical testing kits and constant monitoring of social distancing metrics are key actions to control the impact of the pandemic. Finally, the adoption of such technologies from global societies at large has to take into account the trade-off between","531":"We present a robust data-driven machine learning analysis of the COVID-19 pandemic from its early infection dynamics, specifically infection counts over time. The goal is to extract actionable public health insights. These insights include the infectious force, the rate of a mild infection becoming serious, estimates for asymtomatic infections and predictions of new infections over time. We focus on USA data starting from the first confirmed infection on January 20 2020. Our methods reveal significant asymptotic (hidden) epidemic activity, a lag of about 10 days, and we quantitatively confirm that the infectious force is strong with about a 0.14% transition from mild to serious infection. Our methods are efficient, robust and general, being agnostic to the specific virus and applicable to different populations or cohorts. They provide important insight into the progression of the disease, enable us to design effective intervention strategies and tests, and allow us to identify potential vaccine candidates. This methodology is applied to all publicly available infected dataset in the US, and it can be applied to other datasets and countries. In addition, we discuss how our methods apply to multiple disciplines, including medicine, immunology and virology. Finally, we make some forecasts regarding future research directions. All collected data, code, scripts, pre","532":"The period after the COVID-19 wave is called the Echo-Chambers. Estimation of crowd size in an outdoor environment is essential in the Echo-Channels era. Making a simple and flexible working system for the same is the need of the hour. This article proposes and evaluates a non-intrusive, passive, and costeffective solution for crowd size estimation in an outdoor environment. We call the proposed system as LTE communication infrastructure based environment sensing or LTE-CommSense. This system does not need any active signal transmission as it uses LTE transmitted signal. So, this is a power-efficient, simple low footprint device. Importantly, the personal identity of the people in the crowd can not be obtained using this method. First, the system uses practical data to determine whether the outdoor environment is empty or not. If not, it tries to estimate the number of people occupying the near range locality. Performance evaluation with practical data confirms the feasibility of this proposed approach. The precision of the estimates reaches around 90%. At last, the system uses computational path planning to find a way to reach the desired height from below the ground plane. Overall, the proposed approach is quite universal and may be used for any kind of surveillance scenario requiring no more than 200m2 of","533":"The COVID-19 pandemic is one of the most challenging healthcare crises during 21st century America. As the virus continues to spread on a global scale, the majority of efforts have been on the development of vaccines and the mass immunization of the public. While the daily case numbers were following a decreasing trend, the emergent of new viral mutations and variants still pose a significant threat. As economies start recovering and societies begin opening up with people going back into office buildings, schools, and malls, we still need to have the ability to detect and minimize the spread of COVID-19. Individuals with COVID-19 may show multiple symptoms such as cough, fever, and shortness of breath. Many of the existing detection techniques focus on symptoms having the same equal importance. However, it has been shown that some symptoms are more prevalent than others. In this paper, we present a multimodal method to predict COVID-19 by incorporating existing deep learning classifiers using convolutional neural networks and our novel probability-based weighting function that considers the prevalence of each symptom. The experiments were performed on an existing dataset with respect to the three considered modes of coughs, fever, and shortness of breath. The results show considerable improvements in the detection of CO","534":"The COVID-19 pandemic has become a major global problem, and along with this pandemic there's an infodemic permeating many aspects of our daily lives. While fact-checking is an important task, it is not as straightforward to automatically assign accuracy to each piece of information due to the lack of training data and prior knowledge. In this paper we propose DEAPT (Detecting Evidence for Accuracy), a novel approach to detect topics in evidence articles by using sentence embeddings trained on a suitable set of labeled data. We further present a comparative analysis between several topic modeling methods to determine which method performs best for detecting topical support. DEAPT can be easily used in any other text classification tasks requiring no pre-trained resources. Finally, we demonstrate DEAPT's performance on two popular fake news datasets. The first dataset encompasses 1.4 million claims, and the second dataset consists of 21 million tweets. On both datasets, we achieve the state-of-the-art results without using any target labels for training. This indicates that DEAPT can be useful when applied to topical datasets. Source code and models are available at https:\/\/github.com\/NaiveNeuron\/DEAPT.git.io\/. Video tutorials demonstrating DEAP","535":"We present TruthBot, an all-in-one multilingual conversational chatbot designed for seeking truth (trustworthy and verified information) on specific topics. It helps users to obtain information specific to certain topics, fact-check information, and get recent news. The chatbot learns the intent of a query by training a deep neural network from the data of the previous intents and responds appropriately when it classifies the intent in one of the classes above. Each class is implemented as a separate module that uses either its own curated knowledge-base or searches the web to obtain the correct information. The topic of the chatbot is currently set to COVID-19. However, the bot can be easily customized to any topic-specific responses. Our experimental results show that each module performs significantly better than its closest competitor, which is verified both quantitatively and through several user-based surveys in multiple languages. TruthBot has been deployed in June 2020 and is currently running. https:\/\/www.com\/covid-truthbot\/index.html. This website serves more than 8 million people across 188 countries and territories. http:\/\/covid-truthbot-app.com\/watch_tokens.php, which demonstrates its wide applicability. The bot","536":"Genome assembly is a fundamental problem in Bioinformatics, requiring to reconstruct a source genome from anassembly graph built from a set of reads (short strings sequenced from the genome). A notion of genome assembly solution is that of an arc-covering walk of the graph. Since assembly graphs admit many solutions, the goal is to find what is definitely present in all solutions, or what is safe. Most practical assemblers are based on heuristics having at their core unitigs, namely paths whose internal nodes have unit in-degree and out-degree, and which are clearly safe. The long-standing open problem of finding all the safe parts of the solutions was recently solved [RECOMB 2016] yielding a 60% increase in contig length. This safe and complete genome assembly algorithm was followed by other works improving the time bounds, as well as extending the results for different notions of assembly solution. But it remained open whether one can be complete also for models of genome assembly of practical applicability. In this paper we present a universal framework for obtaining safe and complete algorithms which unify the previous results, while also allowing for easy generalisations to assembly problems including many practical aspects. This is based on a novel graph structure, called the hydrostructure of","537":"We present numerical results obtained from a stochastic model for the spread of infectious diseases in a 2D spatial context on realistic geographical scenarios. The model takes into account the known special characteristics of this disease such as the existence of islands and the different mobility patterns among people acting as virus carriers. The aim of the paper is to study the effect of the movement of people across regions with regards to their exposure risk due to the special circumstances mentioned above. For this purpose, we introduce a version of the Susceptible-Infected-Recovered (SIR) model previously introduced in Bertaglia, Boscheri, Dimarco&Pareschi, Math. Biosci. Eng. (2021) and Boscheri, Dimarco&Pareschi, Math. Mod. Meth App. Scie. App. 0003\/04146\\]. We apply the model here to the case of COVID-19 outbreaks in Italy, both via simulation and by fitting daily data. Our findings show that even a minimal rate of movements of susceptible and infected individuals can lead to a severe increase of the epidemic peak, indicating the importance of restricting mobility in order to reduce the spreading of the disease. Such results are also conditioned to the particular scenario considered","538":"In a wide range of applications, the estimate of droplet evaporation time is based on the classical d2-law, which, assuming a fast mixing and fixed environmental properties, states that the droplet surface decreases linearly with time at a determined rate. However, in many cases the predicted evaporation rate is overestimated. In this Letter, we propose a revision of the d2-law capable to accurately determine droplet evaporation rate in dilute conditions by a proper estimate of the asymptotic droplet properties. Besides a discussion of the main assumptions, we tested the proposed model against data from direct numerical simulations finding an excellent agreement for predicted droplet evaporation time in dilute turbulent jet-sprays. Our results show that even if one gets into the regime of rare events only, the estimated droplet evaporation rate is very close to the actual measured rate from the fluid dynamics lab experiment. Moreover, we observe that small droplets cannot reach the terminal sedimentation velocity before developing strongly heterogeneous droplet networks. These findings are also validated in another experimental study indicating that grains transported by a turbulent jet spray can remain suspended in air for several hours. Therefore, our theoretical predictions will be useful for designing effective spray disinfection strategies","539":"In epidemic or pandemic situations, resources for testing the infection status of individuals may be scarce. Although group testing can help to significantly increase testing capabilities, the (repeated) testing of entire populations can exceed the resources of any country. We thus propose an extension of the theory of group testing that takes into account the fact that definitely specifying the infection status of each individual is impossible. Our theory builds on assigning to each individual an infection status (healthy\/infected), as well as an associated cost function for erroneous assignments. This cost function is versatile, e.g., it could take into account that false negative assignments are worse than false positive assignments and that false assignments in critical areas, such as health care workers, are more severe than in the general population. Based on this model, we study the optimal use of a limited number of tests to minimize the expected cost. More specifically, we utilize information-theoretic methods to give a lower bound on the expected cost and describe simple strategies that can significantly reduce the expected cost over currently known strategies. A detailed example is provided to illustrate our theory. The theoretical results are validated by extensive numerical simulations. Altogether, our work shows that our approach provides significant improvements over existing schemes and that it can be especially useful for reducing the","540":"This manuscript is focused on features' definition for the outcome prediction of matches of NBA basketball championship. It is shown how models based on one a single feature (Elo rating or the relative victory frequency) have a quality of fit better than models using box-score predictors (e.g. the Four Factors). Features have been ex ante calculated for a dataset containing data of 16 NBA regular seasons, paying particular attention to home court factor. Models have been produced via Deep Learning, using cross validation. The best model predicts with high accuracy the win probability of the NBA playoff series from the perspective of the content of the paper. The winning probabilities are found to be in accordance with the World Wide Web Consortium's findings. Further, we have attempted to analyze the effect of size and type of surface area covered by the game board. The impact of the wide range of factors on the outcome of these games is also analyzed. Finally, the potential impact of further features is discussed as well as the importance of balancing their strengths. We particularly emphasize the influence of efficient scoring rules and motivate the necessity of trading away some of the easily available surplus of information in order to gain an insight into the complexity of this sport. All models reproduce well the current world record with a deviation of precision for three","541":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\nWhich is an integral part of a digital twin?\n### Input:\nCOVID Super Expert\n### Response: By amalgamating recent communication and control technologies, computing and data analytics techniques, and modular manufacturing, Industry~4.0 promotes integrating cyber-physical worlds through cyber-physical systems (CPS) and digital twin (DT) for monitoring, optimization, and prognostics of industrial processes. A DT is an emerging but conceptually different construct than CPS. Like CPS, DT relies on communication to create a highly-consistent, synchronized digital mirror image of the objects or physical processes. DT, in addition, uses built-in models on this precise image to simulate, analyze, predict, and optimize their real-time operation using feedback. DT is rapidly diffusing in the industries with recent advances in the industrial Internet of things (IIoT), edge and cloud computing, machine learning, artificial intelligence, and advanced data analytics. However, the existing literature lacks in identifying and discussing the role and requirements of these technologies in DT-enabled industries from the communication and computing perspective. In this article, we first present the functional aspects, appeal, and innovative use of DT in smart industries. Then, we elaborate on this perspective by systematically reviewing and reflecting on recent research in next-generation (NextG) wireless technologies (e.g., 5G and beyond networks), various","542":"In response to the shortage of ventilators caused by the COVID-19 pandemic, many organizations have designed low-cost emergency ventilators. Many of these devices are pressure-cycled pneumatic ventilators, which are easy to produce but often do not include the sensing or alarm features found on commercial ventilators. This work reports a low-cost, easy-to-produce electronic sensor and alarm system for pressure-cycled ventilators that estimates clinically useful metrics such as pressure and respiratory rate and sounds an alarm when the ventilator malfunctions. A low-complexity signal processing algorithm uses a pair of nonlinear recursive envelope trackers to monitor the signal from an electronic pressure sensor connected to the patient airway. The algorithm, inspired by those used in hearing aids, requires little memory and performs only a few calculations on each sample so that it can run on nearly any microcontroller. We show that our design runs in just 8 hours with a battery life of up to 8 hours and that it is capable of detecting multiple failures at once. Our prototype has been tested successfully on a portable testbed also owned by us and has achieved an average accuracy of 97% since its inception. Quantitative evaluation of the device shows that it","543":"With the continued spread of coronavirus, the task of forecasting distinctive COVID-19 growth curves in different cities, which remain inadequately explained by standard epidemiological models, is critical for medical supply and treatment. Predictions must take into account non-pharmaceutical interventions to slow the spread of coronavirus, including stay-at-home orders, social distancing, quarantine and compulsory mask-wearing, leading to reductions in intra-city mobility and viral transmission. Moreover, recent work associating coronavirus with human mobility and detailed movement data suggest the need to consider urban mobility in disease forecasts. Here we show that by incorporating intra-city mobility and policy adoption into a novel metapopulation SEIR model, we can accurately predict complex COVID-19 growth patterns in U.S. cities ($R^2$ = 0.990). Estimated mobility change due to policy interventions is consistent with empirical observation from Apple Mobility Trends Reports (Pearson's R = 0.872), suggesting the utility of model-based predictions where data are limited. Our model also reproduces urban\"superspreading\", where a few neighborhoods account for most secondary infections across urban space, arising from uneven neighborhood populations and heightened intra-city churn in popular neighborhoods.","544":"We develop a framework for evaluating intervention strategies to mitigate the spread of infectious diseases in networks. We extend the SIR model by compartmentalizing it into a system of linear differential equations, and use the Pontryagin minimum principle as a constraint to derive cost-effective mitigation strategies. In addition, we show that the impact of relaxing the lockdown measures on a network may be modeled by removing some of its nodes. We apply our modeling approach to the case of a large number of interacting partners. Our numerical results suggest that if there is no coordination between all interactions then the strategy called 'lockdown' is more effective than anything else. However, when there is perfect synchronization among all interactions this strategy becomes less effective and should be used with extreme caution. This illustrates the importance of having a sufficiently long time horizon to relax the lockdowns before taking any drastic actions. Furthermore, we find interesting oscillatory dynamics, which has unique signatures from different mathematical points of view. A detailed analysis allows us to identify relevant parameters for designing optimal mitigation strategies through topological data analysis (TDA). Finally, we apply our TDA approach to the global spreading of COVID-19 by tracking the progress of the infection in each continent and region, and illustrate how certain factors affect the ability of governments and","545":"We propose a new model for describing the spreading processes, in particular, epidemics. Our model has three components. The first one describes the information transmission regarding the contact structure among healthy and non-healthy populations. The second one defines the average number of infected individuals during an epidemic period. And finally, the third component provides the probability distribution over all secondary infections generated by an infectious individual. In this model, we can also incorporate asymptomatic or hidden individuals who are not detected but may be infected. We present some numerical simulations to show the effectiveness of our model. Then, we apply it to the analysis of the COVID-19 outbreak in Italy. Specifically, we investigate the effect of different measures on the spread of the virus such as social distancing, lockdown, tracing, early detection and isolation. We find that while requiring a fair amount of resources, quarantine is necessary to reduce the mortality rate. However, it worsens if there is a delay between the initial infection and the isolation measure. On the other hand, sufficiently strict testing could lead to a high death toll even without quarantining. Thus, we suggest to adopt suitable restrictive strategies to minimize the economic impact of the pandemic. These recommendations are likely to improve the performance of the healthcare system and eventually","546":"The first BV bands photometric observations and the low-resolution spectrum of the shortest period Am type eclipsing binary TYC 6408-989-1 have been obtained. The stellar atmospheric parameters of the primary star were obtained through the spectral fitting as follows: $T_{eff}=6990\\pm117 K$, $\\log g=4.25\\pm0.26 cm\/s^2$, $[Fe\/H]=-0.45\\pm0.03 dex$. From the six spectra, we could obtain both the effective temperature and log($g$) with the following features: $T_{eff}\\sim 6990\\pm117 K$, $\\log g \\sim 4.15\\pm 0.07 dex$, $[Fe\/H] \\lesssim -0.48\\pm0.04 dex$. The original spectra from this system at VLT\/UVES and GTO showed the same feature as the previously published UVES spectra, but changed in magnitude and velocity range. We also performed the analysis for the secondary component to check if it was indeed a companion star or not. No significant evidence of extra companions has been found so far. With our new spectroscopic data, we can start to explore","547":"We investigate the spatial structure and evolution of star formation and the interstellar medium (ISM) in interacting galaxies. We use an extensive suite of parsec-scale galaxy merger simulations (stellar mass ratio = 2.5:1), which employs the\"Feedback In Realistic Environments-\"model (fire-2). This framework resolves star formation, feedback processes, and the multi-phase structure of the ISM. We focus on the galaxy-pair stages of interaction. We find that close encounters substantially augment cool (HI) and cold-dense (H2) gas budgets, elevating the formation of new stars as a result. This enhancement is centrally-concentrated for the secondary galaxy, and more radially extended for the primary. This behaviour is weakly dependent on orbital geometry. We also find that galaxies with elevated global star formation rate (SFR) experience intense nuclear SFR enhancement, driven by high levels of either star formation efficiency (SFE) or available cold-dense gas fuel. Galaxies with suppressed global SFR also contain a nuclear cold-dense gas reservoir, but low SFE levels diminish SFR in the central region. Concretely, in the majority of cases, SFR-enhancement in the central kil","548":"This work presents the analysis of the impact of restrictions on mobility in Italy, with a focus on the period from 6 November 2020 to 31 January 2021, when a three-tier system based on different levels of risk was adopted and applied at regional level to contrast the second wave of COVID-19. The impact is first evaluated on mobility using Mobile Network Operator anonymised and aggregate data shared in the framework of a Business-to-Government initiative with the European Commission. Mobility data, alongside additional information about electricity consuption, are then used to assess the impacts on an economic level of the three-tier system in different areas of the country. A scenario analyses of both the situation and its potential solutions are discussed. Finally, the optimal scenarios for reducing mobility and health risks are identified. Analyses of this kind could contribute to understand the dynamics of the pandemic, as well as to manage the future of the economy and other social issues linked to it. The relevance of these findings is highlighted by the current pandemic crisis, which has shown how fragile human mobility can be during such a disruptive event. In light of the upcoming challenges, including the reform of the transport sector, the question of whether CAPADR economies may benefit from a more agile approach, or whether subsidies should","549":"This paper presents a novel approach to extract sentiment polarity and emotions from unstructured social media data, with applications in public health informatics, government policy, and commercial advertising. We introduce two preprocessing techniques (sentiment-specific word embeddings and emotion-specific semantic embeddings) for this task, and evaluate their effectiveness using three popular datasets of tweets representing various topics, including one in English labeled by us and the other three are used as source domain data. Our evaluation shows that both embedding types perform similarly for all three datasets; however, we find that the sentiments they produce in each dataset are significantly different from each other. This suggests that there is substantial room for improvement in detecting sentiment polarity and emotions from text without relying on any annotated dataset of its kind. Finally, we also provide an extensive analysis of the interplay between these two embedding types, which offers new insights into how they complement each other. Our results show that leveraging either one of them leads to state-of-the-art or comparable performance across all three datasets, and demonstrate the complementary use of these two embedding types to obtain significant improvements over existing baselines. All code and pretrained models are available at https:\/\/github.com\/anyleopeace\/","550":"A simplified method to compute $R_t$, the Effective Reproduction Number, is presented. The method relates the value of Rt to the estimation of the doubling time performed with a local exponential fit. The condition $d(k)$ of minimum at k=0 corresponds to the case when the reproduction number $R_0$ has finite support and thus is not infinite. We show that for the doubling time parameter this condition coincides with the requirement in terms of its derivative or it can be expressed in terms of one of the two conditions mentioned before (1). For the other hand, for any kind of stochastic system with finite variance, including both systems without pre-existing structure we establish that d(k)=2\/(\\log k)$. Thus, these results apply to many practical situations. As an application, we estimate the duration of the first wave of COVID-19 in Italy by mean field approximation. We found that over time the probability density function of the generation time follows from [Formula: see text], which turns out to be close to 1\/6. More importantly, the estimated value of $R_t$ does not depend on the population size nor on the timescale $\\Delta t$ used for computing the average value","551":"This paper provides a review of past approaches to the use of deep-learning frameworks for the analysis of discrete irregular-patterned complex sequential datasets. A typical example of such a dataset is financial data where specific events trigger sudden irregular changes in the sequence of the data. Traditional deep-learning methods perform poorly or even fail when trying to analyse these datasets. The results of a systematic literature review reveal the dominance of frameworks based on recurrent neural networks. The performance of deep-learning frameworks was found to be evaluated mainly using mean absolute error and root mean square error accuracy metrics. Underlying challenges that were identified are: lack of performance robustness, non-transparency of the methodology, internal and external architectural design and configuration issues. These challenges provide an opportunity to improve the framework for complex irregular-patterned sequential datasets. This contribution recommends a better approach to the utilisation of deep-learning frameworks for the analysis of discrete irregular-patterned complex sequential datasets. Overall, this work contributes to the scientific community by improving our understanding of the complexity of these datasets and providing a valuable resource for practitioners who want to tackle them. Furthermore, it identifies several key challenges and opportunities for future research. For instance, it explains why some groups appear to be more effective than others in terms of their performance","552":"COVID-19 has been one of the most severe diseases, causing a harsh pandemic all over the world, since December 2019. The aim of this study is to evaluate the value of Long Short--Term Memory (LSTM) Networks in forecasting the total number of COVID-19 cases in Turkey. The COVID-19 data for 30 days, between March 24 and April 23, 2020, are used to estimate the next fifteen days. The mean absolute error of the LSTM Network for 15 days estimation is 1,69$\\pm$1.35%. Whereas, for the same data, the error of the Box--Jenkins method is 3.24$\\pm$1.56%, Prophet method is 6.88$\\pm$4.96% and Holt-Winters Additive method with Damped Trend is 0.47$\\pm$0.28%. Additionally, when the number of deaths data is also provided with the number of total cases to the input of LSTM Network, the mean error reduces to 0.99$\\pm$0.51%. Consequently, addition of the number of deaths data to the input, results a lower error in forecasting, compared to using only the number of total cases as","553":"This paper describes the use of neural networks to enhance simulations for subsequent training of anomaly-detection systems. Simulations can provide edge conditions for anomaly detection which may be sparse or non-existent in real-world data. Simulations suffer, however, by producing data that is\"too clean\"resulting in anomaly detection systems that cannot transition from simulated data to actual conditions. Our approach enhances simulations using neural networks trained on real-world data to create outputs that are more realistic and variable than traditional simulations. We demonstrate this method with two applications: (1) identifying counties in South Carolina with anomalous trends in terms of COVID-19 related cases and deaths, and (2) modeling hurricane evacuation destination choice. Results suggest that our enhanced simulations produce reliable source predictions and facilitate better documentation of both the model behavior and the resulting annotations. An advantage of this method over classical simulations is that it requires no human knowledge of the details of the simulation results -- indeed, whether such simulations represent reality somewhat inaccurately depends on the particular parametric configuration used -- and produces only a single image as input at once. Such a simplification highlights the practicality of our approach and paves the way towards future research of improved simulations where feedbacks between models and simulations are integrated. The code used","554":"Z. Schuss (1937-2018) was an applied mathematician, with several contributions in asymptotic, stochastic processes, PDEs, modeling and signal processing. He is well known for his original approach to the activation escape problem, based on WKB and boundary layer analysis, summarized in six books published by Springer. The text summarizes his intellectual approach in science, views and personal path across the XX century, WWII and his personal contribution to academia. In addition, it includes a series of anecdotes about his interactions with colleagues, the training data, tools and procedures used, what worked and what didn't work. Aspects of the mathematical approach and of the many complications involved are discussed here. The paper concludes with several conclusions on how this framework can be readily modified for application to today's problems. Examples include potential applications to epidemiology, engineering, economics, electrodynamics and so on. Finally, some ideas for future research are presented. These are related to dynamical systems, random walkers, differential equations, mechanics, chemistry, biology, medicine, psychology, and art. They arise from the interplay between the different components of the framework. All of these examples revolve around one or more interacting disciplines, whose combination makes them difficult to","555":"In 1988, in cooperation with a team of experimental physicists, a Condensed Matter theorist, CMT (Commodal Mixture Theory) researcher and a medical physicist developed a novel theory called multilayer transportivity formalism (MLT). This new approach became popular and has been used extensively in physics, mainly due to its ability to describe processes which would be intractable for humans or impossible for machines. The MLT framework enabled by artificial intelligence offers a great opportunity to improve our understanding of life. As the result, it can be stated that modern AI-based applications are really made up of many different technologies such as multi-layer neural networks, quantum computing, advanced generative techniques, etc., alongside some other less frequently used technologies such as classical mechanics and traditional mathematics. In this article we will review these new mathematical theories and apply them to the study of biological systems. We must also discuss the key role of MLT in the study of biology and how it can be applied on one side of the scientific spectrum, i.e., towards the development of health sciences. Finally, we should consider the consequences of these new paradigms and develop their own research methods. Here we present three main approaches using the MLT framework: 1) theoretical approach, 2","556":"The communication revolution has perpetually reshaped the means through which people send and receive information. Social media is an important pillar of this revolution and has brought profound changes to various aspects of our lives. However, the open environment and popularity of these platforms inaugurate windows of opportunities for various cyber threats, thus social networks have become a fertile venue for spammers and other illegitimate users to execute their malicious activities. These activities include phishing hot and trendy topics and posting a wide range of contents in many topics. Hence, it is crucial to continuously introduce new techniques and approaches to detect and stop this category of users. This paper proposes a novel and effective approach to detect social spammers. An investigation into several attributes to measure topic-dependent and topic-independent users' behaviours on Twitter is carried out. The experiments of this study are undertaken on various machine learning classifiers. The performance of these classifiers are compared and their effectiveness is measured via a number of robust evaluation measures. Further, the proposed approach is benchmarked against state-of-the-art social spam and anomalous detection techniques. These experiments report the effectiveness and utility of the proposed approach and embedded modules. In addition, the architecture used to construct such models is also analysed. Consequently, this model can be applied to detect deception","557":"The COVID-19 pandemic has significantly impacted all aspects of our lives, including the way in which we interact with the Internet and mobile services. While many countries are in a re-opening stage, some effects of the pandemic on people's behaviors are expected to last much longer, including how they choose between different telecommunication providers. Therefore, it is important to consider the dynamics of public choice under this unprecedented scenario. In this paper, we aim to investigate the factors that affect the transitions among various telecommunication providers during the pandemic. We analyze data collected from a representative online social network in Indonesia by using Netflow traces. The results show that the participants had a variety of daily activities, such as shopping at home, playing around, working from home, etc., while their family members were more likely to work remotely. However, the adoption of technology varies greatly among different telecommunication providers. We also found that the willingness to use publicly available telecommunication data depends on many variables, including the number of users, the age group, gender, location, and whether the user is a college\/university student. These findings should be useful for the government and other stakeholders to guide the selection of public communication networks during the pandemic. They also indicate that the health risks","558":"A randomized trial allows estimation of the causal effect of an intervention compared to a control in the overall population and in subpopulations defined by baseline characteristics. Often, however, clinical questions also arise regarding the treatment effect in subpopulations of patients, which would experience clinical or disease related events post-randomization. Events that occur after treatment initiation and potentially affect the interpretation or the existence of the measurements are called intercurrent events in the ICH E9(R1) guideline. If the intercurrent event is a consequence of treatment, randomization alone is no longer sufficient to meaningfully estimate the treatment effect. Analyses comparing the subgroups of patients without the intercurrent events for intervention and control will not estimate a causal effect. This is well known, but post-hoc analyses of this kind are commonly performed in drug development. An alternative approach is the principal stratum strategy, which classifies subjects according to their potential occurrence of an intercurrent event on both study arms. We illustrate with examples that questions formulated through principal strata occur naturally in drug development and argue that approaching these questions with the ICH E9(R1) estimand framework has the potential to lead to more transparent assumptions as well as more adequate analyses and conclusions. In addition, we provide an","559":"With the rapid development of COVID-19, people are asked to maintain\"social distance\"and\"stay at home\". In this scenario, more and more social interactions move online, especially on social media like Twitter and Weibo. People post tweets to share information, express opinions and seek help during the pandemic, and these tweets on social media are valuable for studies against COVID-19, such as early warning and outbreaks detection. Therefore, in this paper, we release a large-scale COVID-19 social media dataset from Weibo called Weibo-COV, covering more than 30 million tweets from 1 November 2019 to 30 April 2020. Moreover, the field information of the dataset is very rich, including basic tweets information, interactive information, location information and retweet network. We hope this dataset can promote studies of COVID-19 from multiple perspectives and enable better and faster researches to suppress the spread of this disease. Our dataset is available at https:\/\/github.com\/WeibeiZhang\/weibo-covid19-trawl-network. This dataset will be released even when the research community visits the Weibo site. It may also serve as a reference for other researchers aiming to build similar datasets with different resources. The dataset","560":"This paper presents a novel approach to resilient distributed optimization with quadratic costs in a networked control system (e.g., wireless sensor network, power grid, robotic team) prone to external attacks (e.g., hacking, power outage) that cause agents to misbehave. Departing from classical filtering strategies proposed in literature, we draw inspiration from a game-theoretic formulation of the consensus problem and argue that adding competition to the mix can enhance resilience in the presence of malicious agents. Our intuition is corroborated by analytical and numerical results showing that i) our strategy highlights the presence of a nontrivial tradeoff between blind collaboration and full competition, and ii) such competition-based approach can outperform state-of-the-art algorithms based on Mean Subsequence Reduced. The efficacy of our method is illustrated through a numerical example involving a decentralized agent-based simulation environment. Our performance analysis indicates that our approach scales well across different realizations, supports competing objectives, and achieves good recovery quality even when there is a mismatch between the agent's and the centralized server's knowledge. Finally, we note that our findings apply more broadly to other distributionally robust optimization problems where competitive effects arise. Code will be made available at https:\/\/github.com\/","561":"The recent outbreak of COVID-19 has led to urgent needs for reliable diagnosis and management of SARS-CoV-2 infection. As a complimentary tool, chest CT has been shown to be able to reveal visual patterns characteristic for COVID-19, which has definite value at several stages during the disease course. To facilitate CT analysis, recent efforts have focused on computer-aided characterization and diagnosis, which has shown promising results. However, domain shift of data across clinical data centers poses a serious challenge when deploying learning-based models. In this work, we attempt to find a solution for this challenge via federated and semi-supervised learning. A multi-national database consisting of 1704 scans from three countries is adopted to study the performance gap, when training a model with one dataset and applying it to another. Expert radiologists manually delineated 945 scans for COVID-19 findings. We further investigate the usage of four popular convolutional neural networks (CNNs) as transfer learning weights, which are also pre-trained on large datasets. The proposed approach consistently outperforms baseline models trained on single-site data or those trained on pooled data, even when using less number of slices per scan. Furthermore, the worst performing model achieves an accuracy of","562":"The spread of coronavirus disease 2019 (COVID-19) has claimed millions of lives, highlighting the need for robust text mining techniques to assist in the timely identification of new infections and thereby control the replication cycle. In this paper, we present a deep learning model based on recurrent neural networks (RNNs) to predict whether a particular region is affected by COVID-19 or not. We first train a global RNN ensemble consisting of three different RNN models with different depths to extract features from chest X-ray images. Then, we generate the corresponding text embeddings through a sequence of steps called\"STATA\". These embeddings are used as feature inputs to separate the two dense layers of our model. Finally, we output the class labels of these separations using a majority voting approach. Based on this dataset, we propose a novel method called Region2Vec - Text Embedding Network (ReVTELNet), which combines visual word segmentation and fine-tuned language modeling to construct a low dimensional vector representation of each region. This projection enables us to effectively identify the regions where the model is most uncertain. We conduct extensive experiments to evaluate the proposed method against seven different datasets, including those of China, Spain, Italy, France","563":"We present a stochastic epidemic model to study the effect of various preventive measures, such as uniform reduction of contacts and transmission, vaccination, isolation, screening and contact tracing, on a disease outbreak in a homogeneously mixing community. The model is based on an infectivity process, which we define through stochastic contact and infectiousness processes, so that each individual has an independent infectivity profile. In particular, we monitor variations of the reproduction number and of the distribution of generation times. We show that some interventions, i.e., uniform reduction and vaccination, affect the former while leaving the latter unchanged, whereas other interventions, i.e., isolation, screening and contact tracing, affect both quantities. We provide a theoretical analysis of the variation of these quantities, and we show that, in practice, the variation of the generation time distribution can be significant and that it can cause biases in the estimation of basic reproduction numbers. The framework, because of its general nature, captures the properties of many infectious diseases, but specific applications, like the monitoring of the current COVID-19 pandemic, require numerical approximations. Furthermore, the computationally efficient Bayesian update algorithm uses a novel proposal distribution for the parameters of the Susceptible-Infected-Rec","564":"The presence of oscillations in aggregated COVID-19 data not only raises questions about the data's accuracy, it hinders understanding of the pandemic. A spectral analysis is presented, and the oscillation periods measured for each country over time are calculated. Two different Fourier coefficients are extracted from the data, and both parameters are used to build five different vector autoregressive models (VARs). The resulting models are then coupled by a sinusoidal resynthesis method with automatic phase determination, which provides accurate daily updates of the estimated parameters. The use of these two techniques makes it possible to reconstruct a continuous spectrum where the values of all Lyapunov exponent are equal except for a few countries, e.g., Germany and South Korea. Finally, we apply the model to make predictions regarding the evolution of the outbreak in Italy, France, Spain, Portugal, Sweden, United Kingdom, Denmark, and Norway based on the publicly available ISOT and Combined Corpus datasets. For all the countries considered, we obtain precise estimates of the effective reproduction number ($R_t$) via the Bayesian inverse problem approach. In general, the value of $R_t$ increases rapidly with the increasing number of frequencies examined, until reaching around 1.2 for the","565":"Inferring the source of a diffusion in a large network of agents is a difficult but feasible task, if a few agents act as sensors revealing the time at which they got hit by the diffusion. A main limitation of current source detection algorithms is that they assume full knowledge of the contact network, which is rarely the case, especially for epidemics, where the source is called patient zero. Inspired by recent contact tracing algorithms, we propose a new framework, which we call Source Detection via Contact Tracing Framework (SDCTF). In the SDCTF, the source detection task starts at the time of the first hospitalization, and initially we have no knowledge about the contact network other than the identity of the first hospitalized agent. We may then explore the network by contact queries, and obtain symptom onset times by test queries in an adaptive way. We also assume that some of the agents may be asymptomatic, and therefore cannot reveal their symptom onset time. Our goal is to find patient zero with as little contact network information as possible. We propose two local search algorithms for the SDCTF: the LS algorithm is more data-efficient, but can fail to find the true source if many asymptomatic agents are present, whereas the LS+ algorithm is more","566":"Computing a (short) path between two vertices is one of the most fundamental primitives in graph algorithmics. In recent years, the study of paths in temporal graphs, that is, graphs where the vertex set is fixed but the edge set changes over time, gained more and more attention. A path is natural or artificial if it uses edges with non-decreasing time stamps. We investigate how much different paths can be generated by changing the input parameters, including the number of corners and the number of paths. The output of each test is a unique combinatorial description of a path. We formulate a few simple properties of paths and introduce the notion of rational paths, which are those paths that use either all its vertices or exactly one single corner of a path. We show that the Cuntz-Pimsner $C^*$-algebra $\\mathcal{O}_\\bullet(G)$ has nuclear dimension 1 when $G$ is a connected triangle group ($\\mathrm{SH}^0(\\mathcal{O}_\\bullet(G))=1$). For a random walk on an undirected graph with generic integer coefficients, we prove that $\\mathcal{O}_\\","567":"Developing mechanisms that flexibly adapt dialog systems to unseen tasks and domains is a major challenge in spoken dialogue research. Neural models implicitly memorize task-specific dialog policies from the training data. We posit that this implicit memorization has precluded zero-shot transfer learning. To this end, we leverage the schema-guided paradigm, wherein the task-specific dialog policy is explicitly provided to the model. We introduce the Schema Attention Model (SAM) and improved schema representations for the STAR corpus. SAM obtains significant improvement in zero-shot settings, with a +22 F1 score improvement over prior work. These results validate the feasibility of zero-shot generalizability in spoken dialogue. Ablation experiments are also presented to demonstrate the efficacy of SAM. The code is available at https:\/\/github.com\/liam0949\/spear2vec.git.io\/query_conditioned_fusion_model_and_schema.html\/. Findings are consistent with existing literature on cross-domain topic modeling and word segmentation. A further analysis is conducted on the STAR corpus itself to understand more about the limitations of the current model. Code and models are publicly available at https:\/\/github.com\/zhangkai930418\/Spear2V","568":"This paper presents a model for predicting the duration of intense seismic events. We adopt the dynamic mode decomposition (DMD) method to distinguish between different types of stochastic signals emitted by earthquakes, with DMD being a powerful data-driven method used in various applications. The main feature of DMD is that it can encode both seismic sequences and amplitude or phase changes over time. Applying DMD to this problem yields a set of decomposed directional waves that are caused by nonlinear interactions within the earth's atmosphere. If these are observed as they emerge from ground measurements, then their source can be determined relatively quickly. For certain problems, e.g., when assuming that all seismic instruments are removed immediately, the uncertainty in the location of the source becomes immaterial. Our model also allows one to use DMD to learn about the general properties of the system under study using minimal data. After applying DMD to two relevant cases studies, we find that our framework is able to reconstruct both the topology of the wave surface and its temporal evolution, while showing negligible performance errors compared to other state-of-the-art methods. Finally, we also show how DMD can be used to learn about the properties of complex systems generically, without specifying any particular param","569":"Current definition of high-entropy alloys (HEAs) is commonly based on the configurational entropy. But this definition depends only on the chemical composition of a HEA and therefore cannot distinguish the information content that is encoded in various local atomic arrangements and measurable by the Shannon entropy in information theory. Here, inspired by the finding that two-dimensional (2D) Sudoku matrices exhibit higher average Shannon entropy than 2D random matrix counterparts, we propose high-Shannon-entropy alloys (HSEA), whose structures are based on 3D Sudoku matrices. Despite the constraints on the atomic arrangements to form a 3D Sudoku 4 x 4 x 4 matrix, we find that, a prototypical HSEA, NbMoTaW has a lower energy than the same HEA with a random structure due to the smaller lattice distortion. We compute the electronic structures and mechanical properties and find that the HSEA exhibits enhanced average Fermi velocity and ductility over the random NbMoTaW HEA. Finally, we evaluate the formation energies of single vacancies and a vacancy cluster in the HSEA, which provides theoretical support for the observed material property modifications upon doping in HEAs. Our work shows that introducing the Shannon entropy to HE","570":"We propose a novel and practical mechanism which enables the service provider to verify whether a suspect model is stolen from the victim model via model extraction attacks. Our key insight is that the profile of a DNN model's decision boundary can be uniquely characterized by its Universal Adversarial Perturbations (UAPs). UAPs belong to a low-dimensional subspace and piracy models' subspaces are more consistent with victim model's subspace compared with non-piracy model. Based on this, we propose a UAP fingerprinting method for DNN models and train an encoder via contrastive learning that takes fingerprint as inputs, outputs a similarity score. Extensive studies show that our framework can detect model IP breaches with confidence>99.99 within only 20 fingerprints of the suspect model. It has good generalizability across different model architectures and is robust against post-modifications on stolen models. We further apply our framework to the scenario where the suspect model is trained using back-propagation and thus learns its deep neural network architecture. Our results demonstrate that our proposed approach can identify relevant factors of the suspect model during training, and can be used to obtain valuable insights about its current performance by viewing it as\"black boxes\". Code is available at https:\/\/","571":"Computational Colour Constancy (CCC) consists of estimating the colour of one or more illuminants in a scene and using them to remove unwanted chromatic distortions. Much research has focused on illuminant estimation for CCC on single images, with few attempts of leveraging the temporal information intrinsic in sequences of correlated images (e.g., the frames in a video), a task known as Temporal Colour Constancy (TCC). The state-of-the-art for TCC is TCCNet, a deep-learning architecture that uses a ConvLSTM for aggregating the encodings produced by CNN submodules for each image in a sequence. We extend this architecture with different models obtained by (i) substituting the TCCNet submodules with C4, the state-of-the-art method for CCC targeting images; (ii) adding a cascading strategy to perform an iterative improvement of the estimate of the illuminant. We tested our models on the recently released TCC benchmark and achieved results that surpass the state-of-the-art. Analyzing the impact of the number of frames involved in illuminant estimation on performance, we show that it is possible to reduce inference time by training the models on","572":"During the COVID-19 pandemic, the slope of the epidemic curve in Mexico City has been quite unstable. We have predicted that in the case that a fraction of the population above a certain threshold returns to the public space, the negative tendency of the epidemic curve will revert. Such predictions were based on modeling the reactivation of economic activity after lockdown by means of an epidemiological model on a contact network of Mexico City derived from mobile device co-localization. We evaluated the epidemic dynamics considering the tally of active and recovered cases documented in the mexican government's open database. Scenarios were modeled in which different percentages of the population are reintegrated to the public space by scanning values ranging from 5% up to 50%. Null models were built by using data from the Jornada Nacional de Sana Distancia (the Mexican model of elective lockdown) in which there was a mobility reduction of 75% and no mandatory mobility restrictions. We found that a new peak of cases in the epidemic curve was very likely for scenarios in which more than 5% of the population rejoined the public space; The return of more than 50% of the population synchronously will unleash a peak of a magnitude similar to the one that was predicted with no mitigation strategies","573":"We explore various machine learning and deep learning models applied to forecast intraday realized volatility (RV), by exploiting commonality in intraday volatility via pooling stock data together, and by incorporating a proxy for the market volatility. Neural networks dominate linear regressions and tree models in terms of performance, due to their ability to uncover and model complex latent interactions among variables. Our findings remain robust when we apply trained models to new stocks that have not been included in the training set, thus providing new empirical evidence for a universal volatility mechanism among stocks. Finally, we propose a new approach to forecasting one-day-ahead RVs using past intraday RVs as predictors, and highlight interesting diurnal effects that aid the forecasting mechanism. The results demonstrate that the proposed methodology yields superior out-of-sample forecasts over a strong set of traditional baselines that only rely on past daily RVs. We also find that similar results can be obtained by using simpler neural network architectures such as unidirectional learners or by modeling the interaction between log returns and RVs. Hence, our results indicate that it may be possible to obtain more informative predictions from existing data sets by just proposing variations of the original method, without losing predictive power. Code is available at https:\/\/github","574":"We investigate the possibility of a harvesting effect, i.e. a temporary forward shift in mortality, associated with the COVID-19 pandemic by looking at the excess mortality trends of an area that registered one of the highest death tolls in the world during the first wave, Northern Italy. We do not find any evidence of a sizable COVID-19 harvesting effect, neither in the summer months after the slowdown of the first wave nor at the beginning of the second wave. According to our estimates, only a minor share of the total excess deaths detected in Northern Italian municipalities over the entire period under scrutiny (February - November 2020) can be attributed to an anticipatory role of COVID-19. A slightly higher share is observed for the most severely affected areas (the provinces of Bergamo and Brescia, in particular), but even in these territories, the harvesting effect can only account for less than 20% of excess deaths. Furthermore, the lower mortality rates observed in these areas at the beginning of the second wave may be due to several factors other than a harvesting effect, including behavioral change and some degree of temporary herd immunity. The very limited presence of short-run mortality displacement restates the case for containment policies aimed at minimizing the health impacts of the pandemic","575":"Dietary studies showed that dietary-related problem such as obesity is associated with other chronic diseases like hypertension, irregular blood sugar levels, and increased risk of heart attacks. The primary cause of these problems is poor lifestyle choices and unhealthy dietary habits, which are manageable using interactive mHealth apps. However, traditional dietary monitoring systems using manual food logging suffer from imprecision, underreporting, time consumption, and low adherence. Recent dietary monitoring systems tackle these challenges by automatic assessment of dietary intake through machine learning methods. This survey discusses the most performing methodologies that have been developed so far for automatic food recognition and volume estimation. First, we will present the rationale of visual-based methods for food recognition. The core of the paper is the presentation, discussion and evaluation of these methods on popular food image databases. Following that, we discussed the mobile applications that are implementing these methods. The survey ends with a discussion of research gaps and open issues in this area. In particular, we mentioned the lack of documentation about interacting with these methods, their advantages and limitations, and how they can be applied towards healthier lifestyles. For each category of methods, we also discuss the uncertainty quantification approach for choosing between different alternatives. To close this article, we outline some potential future research directions in the","576":"The metric dimension (MD) of a graph is a combinatorial notion capturing the minimum number of landmark nodes needed to distinguish every pair of nodes in the graph based on graph distance. We study how much the MD can increase if we add a single edge to the graph. The extra edge can either be selected adversarially, in which case we are interested in the largest possible value that the MD can take, or uniformly at random, in which case we are interested in the distribution of the MD. The adversarial setting has already been studied by [Eroh et. al., 2015] for general graphs, who found an example where the MD doubles on adding a single edge. By constructing a different example, we show that this increase can be as large as exponential. However, we believe that such a large increase can occur only in specially constructed graphs, and that in most interesting graph families, the MD at most doubles on adding a single edge. We prove this for $d$-dimensional grid graphs, by showing that $2d$ appropriately chosen corners and the endpoints of the extra edge can distinguish every pair of nodes, no matter where the corner locations are added. For the special case of $d=2$, we show that it suffices to","577":"In this work, we introduce a robot arm explicitly designed to assist humans in giving feedback on non-human aspects of their interaction behavior. With the goal of facilitating more efficient communication, the robot arm aims to reduce the time needed by humans while providing information exchangeably fast. In order to achieve such an objective, we created a learning algorithm based on reinforcement learning and collected a dataset of human-handovers using this smart robot arm. We then conducted a study where the participants were asked to give advice on how to improve the safety of the robot arm when it was interacting with humans. The results show that the participants were more likely to make accurate predictions for the recipients than themselves and that the robot arm had no relevant factor in making these judgments. Our findings suggest that there is room for improvement in terms of both the generalizability of the learned model and the quality of the provided guidance. Moreover, the learning approach has some limitations and can be adapted to other applications where stateless or partial matching is desired. Finally, the collection of the expert input data is thought to be essential but not as critical as the deployment of the robot arm. Thus, our contribution is twofold: (1) we demonstrate that human-to-robot handovers are extremely beneficial and should be","578":"Reinforcement learning (RL) is acquiring a key role in the space of adaptive interventions (AIs), attracting a substantial interest within methodological and theoretical literature and becoming increasingly popular within health sciences. Despite potential benefits, its application in real life is still limited due to several operational challenges--in addition to ethical and cost issues among others--that remain open in part due to poor communication and synergy between methodological and applied scientists. In this work, we aim to bridge these gaps by providing a comprehensive survey on RL methods capable of enhancing the scientific community's understanding of RL phenomena with applications in AIs. We first present an overview of RL, covering both methodologies and topics, highlighting the current limitations of traditional approaches. Then, we discuss different directions for improving RL performance and apply it to conduct a state-of-the-art review of AIs. We end with a discussion on the future prospect of RL research in AIs and highlight some interesting open problems worthy of further study. Our contribution provides a thorough view of where RL stands today in relation to AIs, and identifies important areas for improvement in terms of both methodology and topic coverage. The framework serves as a guide for researchers to improve their knowledge of RL dynamics and be able to derive successful strategies for using RL technology","579":"Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. We see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and\/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative","580":"The novel coronavirus disease (COVID-19) has been spreading rapidly across the world and caused significant impact on the public health and economy. However, there is still lack of studies on effectively quantifying the lung infection caused by COVID-19 in computerized tomography (CT) images. To address this issue, we present a deep learning framework to automatically quantify the infected volume from CT images for COVID-19 patients. Based on the state-of-the-art segmentation network, we firstly propose a modified auto-encoder and gradient boosting model to generate high-quality and smooth images. Then, we adopt two popular convolutional neural networks (CNNs) with different receptive fields to further develop the model. The generated images are transferred into custom fine-tuned top layers followed by a set of model snapshots. These model snapshots preserve the original data distribution but also improve the capability of generalizing the learned knowledge. We finally evaluate the effectiveness of the proposed method via quantitative results based on chest CT images of 143 subjects. Our findings show that our method outperforms the state-of-the-art segmentation network and significantly improves the prediction accuracy of the lesion area under the ROC curve maximization while maintaining the same detection speed","581":"The energy trade is an important pillar in nearly every aspect of our society. With the development of machine learning and artificial intelligence, the volume of energy trade has been growing rapidly. In this paper, we present a comprehensive review of recent studies on energy trade using deep learning. We first introduce some definitions and related concepts of energy trade, then summarize existing work with the outlooks for its future. We also discuss the key factors behind the performance of such algorithms. Finally, we highlight the challenges and propose potential research directions for further improving these algorithms, as well as discussing their relationship with the platform economy. We end with a discussion on the importance of proper equipment and training strategies for applying AI-based solutions into the energy trade. Several researchers have already made progress towards analysing the causal relationships between energy trade and the economic growth. However, there are still lots of room for improvement in terms of both model structures and analysis techniques. Furthermore, the lack of explainability in these models cannot be considered as one of the main reasons why the framework of energy trade is so challenging. Therefore, through the use of AI-powered computer vision tools, the interactions between energy trade and human behaviour can be visualised and analysed with greater depth. This will help promote the guideline for social and economic practice","582":"We document the evolution of labour market power by employers on the US and Peruvian labour markets during the 2010s. Making use of a structural estimation model of labour market dynamics, we estimate differences in market power that workers face depending on their sector of activity, their age, sex, location and educational level. In particular, we show that differences in cross-sectional market power are significant and higher than variations over the ten-year time span of our data. In contrast to findings of labour market power in developed countries such as the US, we document significantly lower levels of market power for workers in Peru vis-\\`a-vis the tertiary educated workforce, regardless of age and sector. In contrast, for the primary educated workforce, market power seems to be high in (private) services and manufacturing. For secondary educated workers, only the mining sector stands out as moderately more monopsonistic than the rest of the labour market. We also show that at least for the 2010s, labour market power declined in Peru. We contrast these results with similar estimates obtained for the United States where we are able to show that increases in labour market power are particularly acute in certain sectors, such as agriculture and entertainment and recreational services, as well as in specific geographic areas where these sectors are","583":"A novel method for interactive construction and rendering of extremely large molecular scenes is presented. Our method consists of building a continuous motion buffer to represent a single 'interactive' scene, and uses a set of building rules to allow the simulation of the complete scene in real-time. The resulting triangular phase diagrams contain rich information about the dynamics of the scene, which can be used to control the spatial spread of light, and enable realistic visualizations of complex structures. An accompanying GUI-based interface allows users to interact with the 3D scene as if they were in the same room. We demonstrate our technique on multiple examples of varying complexity, including one with over 100 million atoms. One way to access the GPU memory space is via a CUDA Graphs communication stream, where the data is transferred to the host computer through its corresponding memory hierarchy. Another layer is added to this hierarchical structure by using another GPU thread (or user) dedicated to dealing with the smaller regions of the input triangulation. This results in a second order of magnitude reduction in computational time. For all the tested cases we observe a clear improvement in the performance, especially between the two extremes of resolution. In future work we will present a new approach based on GPUs coupled with machine learning algorithms for improving the efficiency of","584":"We consider here an extended SIR model, including several features of the recent COVID-19 outbreak: in particular the infected and recovered individuals can either be detected (+) or undetected (-) and we also integrate an intensive care unit (ICU) capacity. Our model enables a tractable quantitative analysis of the optimal policy for the control of the epidemic dynamics using both lockdown and detection intervention levers. With parametric specification based on literature on COVID-19, we investigate the sensitivities of various quantities on the optimal strategies, taking into account the subtle trade-off between the sanitary and the socio-economic cost of the pandemic, together with the limited capacity level of ICU. We identify the optimal lockdown policy as an intervention structured in 4 successive phases: First a quick and strong lockdown intervention to stop the exponential growth of the contagion; second a short transition phase to reduce the prevalence of the virus; third a long period with full ICU capacity and stable virus prevalence; finally a return to normal social interactions with disappearance of the virus. The optimal scenario hereby avoids the second wave of infection, provided the lockdown is released sufficiently slowly. We also provide optimal intervention measures with increasing ICU capacity, as well as optimization over the effort on detecting and isolating infected","585":"The Internet of Things (IoT) has grown rapidly in the last decade and continue to develop in terms of dimension and complexity offering wide range of devices to support diverse set of applications. With ubiquitous Internet, connected sensors and actuators, networking and communication technology, and artificial intelligence (AI), smart cyber-physical systems (CPS) provide services rendering assistance to humans in their daily lives. However, the recent outbreak of COVID-19 (also known as coronavirus) pandemic has exposed and highlighted the limitations of current technological deployments to curtail this disease. IoT and smart connected technologies together with data-driven applications can play a crucial role not only in prevention, continuous monitoring, and mitigation of the disease, but also enable prompt enforcement of guidelines, rules and government orders to contain such future outbreaks. In this paper, we envision an IoT-enabled ecosystem for intelligent monitoring, pro-active prevention and control, and mitigation of COVID-19. We propose different architectures, applications and technology systems for various smart infrastructures including E-health, smart home, smart supply chain management, smart locality, and smart city, to develop future connected communities to manage and mitigate similar outbreaks. Furthermore, we present research challenges together with future directions to enable and develop these","586":"We introduce and study the problem of allocating bailouts (stimulus, subsidy allocations) to people participating in a financial network subject to income shocks. We build on the framework of Eisenberg and Noe that allows the incorporation of a bailout policy that is based on discrete bailouts motivated by the types of stimulus checks people receive around the world as part of COVID-19 economical relief plans. We show that optimally allocating such bailouts on a financial network in order to maximize a variety of social welfare objectives of this form is a computationally intractable problem. We develop approximation algorithms to optimize these objectives and establish guarantees for their approximation rations. Then, we incorporate multiple fairness constraints in the optimization problems and establish relative bounds on the solutions with versus without these constraints. Finally, we apply our methodology to a variety of data, both in the context of a system of large financial institutions with real-world data, as well as in a realistic societal context with financial interactions between people and businesses for which we use semi-artificial data derived from mobility patterns. Our results suggest that the algorithms we develop and study have reasonable results in practice and outperform other network-based heuristics. We argue that the presented problem through the societal-level lens could assist policymakers","587":"Popular online platforms such as Google Search have the capacity to expose billions of users to partisan and unreliable news. Yet, the content they show real users is understudied due to the technical challenges of independently obtaining such data, and the lack of data sharing agreements that include it. Here we advance on existing digital trace methods using a two-wave study in which we captured not only the URLs participants clicked on while browsing the web (engagement), but also how these URLs were shown to others through reposts, or social media messages containing them. Using surveys paired with engagement and reposts from around 50K Twitter profiles, we found that the average amount of time people spent engaging with unreliable and partisan news sites was comparable to their amount of time spent with reliable news sites. However, we find that reposts from low-quality sources were more likely to be seen than those from high-quality sources. We discuss the implications of our results for digital tracing and future research into misinformation, and defend that this work provides important insights towards advancing the field of digital traces by 1) uncovering seemingly anomalous patterns in large datasets, 2) investigating the characteristics of interactions between specific user groups, 3) determining what types of resources a platform needs to promote independent factual investigation,","588":"The knowledge of scripts, common chains of events in stereotypical scenarios, is a valuable asset for task-oriented natural language understanding systems. We propose the Goal-Oriented Script Construction task, where a model produces a sequence of steps to accomplish a given goal. We pilot our task on the first multilingual script learning dataset supporting 18 languages collected from wikiHow, a website containing half a million how-to articles. For baselines, we consider both a generation-based approach using a language model and a retrieval-based approach by first retrieving the relevant steps from a large candidate pool and then ordering them. We show that our task is practical, feasible but challenging for state-of-the-art Transformer models, and that our methods can be readily deployed for various other datasets and domains with decent zero-shot performance. Finally, we provide extensive empirical results showing that our task provides significant gains over several strong baselines in all settings. Our code and data are available at https:\/\/github.com\/HLTCHKUST\/goescripts.git.io\/. Our analysis reveals substantial challenges in training transformers for this task. This work is based on the submission to the track of 8058 participating panelists in WNLP 2020. The insights gained","589":"I critique a recent analysis (Miles, Stedman&Heald, 2020) of COVID-19 lockdown costs and benefits, focussing on the United Kingdom (UK). Miles et al. (2020) argue that the March-June UK lockdown was more costly than the benefit of lives saved, evaluated using the NICE threshold of {\\pounds}30000 for a quality-adjusted life year (QALY) and that the costs of a lockdown for 13 weeks from mid-June would be vastly greater than any plausible estimate of the benefits, even if easing produced a second infection wave causing over 7000 deaths weekly by mid-November. I note here two key problems that significantly affect their estimates and cast doubt on their conclusions. Firstly, their calculations arbitrarily cut off after 13 weeks, without costing the epidemic end state. That is, they assume indifference between mid-September states of 13 or 7500 weekly deaths and corresponding infection rates. This seems indefensible unless one assumes that (a) there is little chance of any effective vaccine or improved medical or social interventions for the foreseeable future, (b) notwithstanding temporary lockdowns, COVID-19 will very likely propagate until herd immunity. Even under these assumptions it is very questionable. Secondly, they ignore the","590":"The non-verbal behavior of passersby can be critical to blind individuals to initiate interactions, preserve personal space, or practice social distancing during a pandemic. Among other use cases, wearable cameras employing computer vision can be used to extract proxemic signals of others and thus increase access to the semantic segmentation of these images. However, such information is not directly accessible for pedestrians and must be inferred from video sequences. To this end, we propose a framework that combines pedestrian detection with feature extraction and neural networks to infer proxemic signals while preserving the privacy of the users. We employ various convolutional neural network architectures ranging from VGG16 to ResNet50 and show that their performance is comparable to that of existing methods. Furthermore, we conduct a study involving simulated and real-world footage to evaluate our approach in indoor and outdoor scenarios. Our results demonstrate that it can provide reliable proxemics signals for both audio and visual segmentsation tasks without any prior knowledge. Finally, we discuss theoretical limitations and practical implications along with the dataset available at https:\/\/github.com\/felipemaiapolo\/audio2visual-semantic-analysis-of-passing-reviews.git.io\/app\/qa1st\/3D-model-based","591":"The recent outbreak of COVID-19 has seriously threatened global health. The rapid spread of the virus has created pandemic, and countries all over the world are struggling with it. One reason why the fight against the disease is difficult is due to the lack of information. In this work we present a deep learning based model for short term forecasting of the total number of cases in Turkey. For longer terms predictions, we apply an ensemble of multiple deep learning models. The proposed approach uses one neural network which learns about the features of the Turkish regions using satellite imagery and generates a prediction vector by concatenating these feature vectors. We also propose an ensemble strategy to use these prediction vectors to learn better. Our results show that the proposed approach outperforms other well known methods for both short and long term forecasts. Additionally, we demonstrate that applying our model on a different region of Turkey leads to interesting results. Thus, there is a great scope for generalization on other Turkic languages. The code used in this work is available at https:\/\/github.com\/anil1055\/covid19-short-term-prediction-model\/. Further, more data can be found at https:\/\/anil1055.github.io\/forecasting-model\/tree\/","592":"Rapid and accurate simulation of cerebral vessel structure and function may help improving patient-specific intervention and predicting treatment outcome. However, with explicit FD devices being placed in patient-specific aneurysm model, the computational domain must be resolved around the thin stent wires, leading to high computational cost in computational fluid dynamics (CFD). Classic homogeneous porous medium (PM) methods cannot accurately predict the post-stenting aneurysmal flow field due to the inhomogeneous FD wire distributions on anatomic arteries. We propose a novel approach that models the FD flow field as a meshed network of connected liquid-lined flexible cylinders coupled to a viscoelastic thoracic cavity. Each branch opens at a rate determined by input biomechanical parameters and a pressure-dependent manner which allows for rapid control of the solidification behavior of each layer. The prototype robot has been validated in a study involving patients who underwent stuttering procedures while wearing a mask-based SIR model. The results show that our model can successfully simulate the post stenting flow field over both oral cavity phantoms and volunteers without any modification from the mechanical design. Moreover, we demonstrate that using simple engineering analysis tools allows us to efficiently compute the optimal configuration of airway wall stiffness resulting in","593":"By the start of 2020, the novel coronavirus disease (COVID-19) has been declared a worldwide pandemic. Because of the severity of this infectious disease, several kinds of research have focused on combatting its ongoing spread. One potential solution to detect COVID-19 is by analyzing the chest X-ray images using Deep Learning (DL) models. In this context, Convolutional Neural Networks (CNNs) are presented as efficient techniques for early diagnosis. In this study, we propose a novel randomly initialized CNN architecture for the recognition of COVID-19. This network consists of a set of different-sized hidden layers created from scratch. The performance of this network is evaluated through two public datasets, which are the COVIDx and the enhanced COVID-19 datasets. Both of these datasets consist of 3 different classes of images: COVID19, pneumonia, and normal chest X-ray images. The proposed CNN model yields encouraging results with 94% and 99% of accuracy for COVIDx and enhanced COVID-19 dataset, respectively. Also, it provides good precision and recall values in identifying pneumonia patients. Finally, we also test the capability of this network to identify COVID-19 related cases automatically. To support this possibility, we","594":"The recent emergence of a new coronavirus, COVID-19, has gained extensive coverage in public media and global news. As of 24 March 2020, the virus had spread to 215 countries with more than 4,622,001 confirmed cases and 311,916 deaths worldwide according to WHO statistics. In this study, we present a comprehensive analysis of COVID-19 on one of the biggest Chinese cities, Wuhan, from different perspectives based on its heterogeneous features. We first explore the correlations between the number of confirmed cases and death cases for both China and Italy. The results show that the epidemic in Wuhan experienced four stages, i.e., initial stage, the rapid diffusion stage, the hierarchical diffusion stage, and finally the contraction stage. The main conclusions can be drawn as follows. First, despite the great efforts made by the government to control the epidemic situation, the number of confirmed cases appears to have a relatively small change during the last two months. Second, the dynamic evolution of the epidemics in Wuhan will be investigated after the end of April 2020. Third, since several parts of the world are still not affected by the virus, including Europe, South America and India, it would be interesting to monitor the effectiveness of the prevention","595":"We prove the nonintegrability of the susceptible-exposed-infected-removed (SEIR) epidemic model in the Bogoyavlenskij sense. This property of the SEIR model is different from the more fundamental susceptible-infected-removed (SIR) model, which is Bogoyavlenskij-nonintegrated. Our basic tool for the proof is an extension of the Morales-Ramis theory due to Ayoul and Zung. Moreover, we extend the system to a six-dimensional system to treat transcendental first integrals and commutative vector fields. We also use the fact that the incomplete gamma function $\\Gamma(\\alpha,x)$ does not admit a closed form representation of its derivative in the whole range of $x$. From this, we obtain a version of the Morales-Ramis theory without vital conditions on parameters. We conclude our paper with several numerical simulations of some examples. Note that although the nonintegration of the SEIR model may be inconsistent with the data, the existence of such inconsistencies cannot be denied under the pretext of these particular models. Furthermore, since the goal of this paper is to improve the understanding of the COVID-19 epidemics, we refrain from","596":"In this paper we develop two stochastic models for pricing and hedging of financial assets. The dynamics of these models are similar in nature with the additional requirement that one should be able to quickly adapt to new market regimes by means of sample data. This adds a layer of uncertainty to the estimation process which makes it more difficult to use traditional statistical approaches based on Markov chains Monte Carlo (MCMC). In order to address this issue, we propose a modification of the Sarmanov method for estimating parameters from non-stationary samples where we combine multi-layer convolutional neural networks (CNNs) at each time step with long short-term memory (LSTM) networks. We also introduce a dynamic filter generator composed of a CNN and an LSTM so that the proposed framework can perform both parameter estimation and provide samples of high quality. Using empirical results obtained from the SPY ETF historical data set, we demonstrate that our dynamic approach outperforms the standard SARMANOV method as well as any other pure LABS model or hybrid LSTM\/GCN architecture by statistically significant margins using backtesting strategies. Furthermore, we find that the best performing network architectures dynamically change over time according to different volatility levels. These changes in the network topology not only","597":"GOTO-4 is a medium resolution (~4500) wide-band (0.35 - 2.0 {\\mu}m) spectrograph which passed the Final Design Review in 2018. The instrument has been planned to be installed at La Silla and it was during this period that a new prime focus corrector for the ESO NTT at La Silla, called GCN-SE, was built. For the optics configuration, we replaced the common optical elements used in all the previous FDRs with the newly developed DESIREE system. The wavelength coverage of the light curve obtained by means of the DECam Local Volume Exploration survey (DLE) method is exactly one order of magnitude better than the flux density obtained by means of the ROSAT High Energy Telescope (HET) method. We are able to achieve a higher sensitivity even with the weaker diffraction limit imposed by the ESA Hubble Space Telescope (HSST) project. Thanks to the high quality control of the electronics chain, the HET data were processed with the GROND software running on GPUs. The resulting corrected HET profiles provide a more accurate temperature measurement over the whole optic aperture for any given stellar object luminosity. These changes will help to improve the sensitivities of the various instruments","598":"Score-based algorithms for tuberculosis (TB) verbal screening perform poorly, causing misclassification that leads to missed cases and unnecessary costly laboratory tests for false positives. We compared score-based classification defined by clinicians to machine learning classification such as SVM-RBF, logistic regression, and XGBoost. We restricted our analyses to data from adults, the population most affected by TB, and investigated the difference between untuned and unweighted classifiers to the cost-sensitive ones. Predictions were compared with the corresponding GeneXpert MTB\/Rif results. After adjusting the weight of the positive class to 40 for XGBoost, we achieved 96.64% sensitivity and 35.06% specificity. As such, the sensitivity of our identifier increased by 1.26% while specificity increased by 13.19% in absolute value compared to the traditional score-based method defined by our clinicians. Our approach further demonstrated that only 2000 data points were sufficient to enable the model to converge. The results indicate that even with limited data we can actually devise a better method to identify TB suspects from verbal screening. This may be especially useful if applied to a noisy version of TB surveillance system developed for COVID-19. Finally, we suggested that future research on","599":"The COVID-19 pandemic has forced changes in production and especially in human interaction, with\"social distancing\"a standard prescription for slowing transmission of the disease. This paper examines the economic effects of social distancing at the aggregate level, weighing both the benefits and costs to prolonged distancing. Specifically we fashion a model of economic recovery when the productive capacity of factors of production is restricted by social distancing, building a system of equations where output growth and social distance changes are interdependent. The model attempts to show the complex interactions between output levels and social distancing, developing cycle paths for both variables. Ultimately, however, defying gravity via prolonged social distancing shows that a lower growth path is inevitable as a result. More generally, the results highlight the limits on which social distancing interventions can be effective, and the degree to which they must be implemented. In order to better understand these limitations, we provide an interactive dashboard: http:\/\/www.fil.ion.ucl.ac.uk\/spm\/covid-19\/dashboard\/ We hope this tool will be useful for future research into the economics of social distancing. Also, it demonstrates how such analysis may be used to guide policy decisions relating to the reopening of essential activities,","600":"The Anti-Extradition Law Amendment Bill protests in Hong Kong present a rich context for exploring information security practices among protesters due to their large-scale urban setting and highly digitalised nature. We conducted in-depth, semi-structured interviews with 11 participants of these protests. Research findings reveal how protesters favoured Telegram and relied on its security for internal communication and organisation of on-the-ground collective action; were organised in small private groups and large public groups to enable collective action; adopted tactics and technologies that enable pseudonymity; and developed a variety of strategies to detect compromises and to achieve forms of forward secrecy and post-compromise security when group members were (presumed) arrested. We further show how group administrators had assumed the roles of leaders in these 'leaderless' protests and were critical to collective protest efforts. We conclude by outlining implications for privacy legislation, system design thinking and research in the contexts of real-world violence against civilians. We call for technology makers to consider developing fundamental tools to assist them in protecting people's lives during future pandemics or enabling them to effectively engage in meaningful activism behaviours while having access to effective means of communication. We also highlight broader concerns about the role of social media in creating systemic risk, and discuss potential solutions to mitigate","601":"The paper of this article contains the results of a multilingual experiment aimed at investigating the evolution of the news industry in countries with different language distributions. To this end, we have developed a browser extension which can be used to automate the entire process of credibility assessments of false claims circulating in social media. This tool has been evaluated by means of a study based on the fact checking website Newsguard and the Politifact Twitter dataset, among others. The participants communicated via their Twitter accounts or Facebook profiles, from which behavioral test data were extracted using the Google Trends algorithm. The outcomes of the experiment are represented through three main findings: 1) the languages spoken by the majority of the sample belong to English-based countries; 2) there is no significant relationship between the performance of the mainstream political right and the mainstream left-wing party (FPOe), for example; 3) the number of fake news websites and Twitter posts significantly increase in some Spanish-speaking countries but decrease in others, indicating that the overall trend towards recognizing online misinformation is not yet compatible. These results shed light on the complexity of the interplay of politics and health misinformation within each country. Overall, they reveal the existence of a lack of trustworthiness and critical thinking before adopting formal methods of reasoning about the veracity of","602":"A thermodynamic framework has been developed for a class of non-equilibrium stationary density functional (NDF) states which are induced by a continuous-time Markov chain. It has been shown that the NDF equations have a unique classical solution; therefore, the Kolmogorov equation must be used to find the explicit solution. The resulting variational problem is governed by a single scalar field of the desired state. Thus, we obtain the evolution of the pandemic under one such field. In turn, this allows us to reinterpret the obtained results in terms of equilibrium statistical physics rather than pure mathematics. Focusing on Maxwell's theory as our model theory, we present some numerical results based on the Monte Carlo method and show that they agree with the analytical solutions. Moreover, we explain how the lack of equivalence between the NDF and the corresponding macroscopic fluctuation relation (MFR) leads to introduce a new generalized Jarrow-Rudd (GJR) equation. We also use the MFR approach to compute the evolution of the COVID-19 infection curve for several countries and discover interesting patterns in the behavior of the curves. Our analysis demonstrates that GJR is a very effective tool for obtaining a discrete-time description of the dynamics of","603":"We are reporting on lessons learned from an e-contest for students held during the current pandemic. We compare the e-contest with the 10 previous editions of the same but face-to-face contest. While apparently the competition did not suffer because of being a virtual one, some disadvantages were noted. The main conclusions are: the basic inter-subjective nature of the challenge, and the fact that online jury-competitors interact very little in the real world; both issues may be relevant to the theoretical model of the competition. However, other than these general results, we also draw significant inferences by comparing the reported competition results with the results from the internal experiment, which suggests that the former may have been easier to observe. Finally, we note that since the reported competition results are obtained from a single institution (e.g. IT), our findings can hardly serve as an independent source of information for other institutions, thus providing limited clues about how the COVID-19 pandemic affected the competition. Overall, this paper reports on lessons learned from an e-contest for students, and we hope they will help further promote the development of similar competitions in the future. Contact details for the external group testing are provided. This is also the case where","604":"Determining whether two configurable software system's parts are correct or not depends on various human-understandable concepts, but it does not matter how these concepts are defined nor what they actually represent. We present a framework for reasoning about the relationship between configurable programming languages (e.g., C, Python) and their underlying semantics with respect to both run time and compile types. This framework provides us with a better understanding of code semantics at run-time, i.e., when programs are compiled to execute a set of instructions. Also, this allows us to use insights from the runtime data to inform those methods. Our main contribution is the definition of such semantically meaningful and computationally efficient concepts as well as proof assistants for verifying them. We also present the first attempt towards formally defining the semantics of a concept, which is essential for non-trivial program analysis. Lastly, we implement a full prototype pipeline of our approach running on one of the most popular development environments, including 2 large industrial language datasets. The results demonstrate the feasibility of our approach and show its effectiveness over several naive heuristics. All implemented components are available under https:\/\/github.com\/NaiveNeuron\/ParaCrawl.git. Additionally, due to the lack","605":"Counterfactual inference is a useful tool for comparing outcomes of interventions on complex systems. It requires us to represent the system in form of a structural causal model, complete with a causal diagram, probabilistic assumptions on exogenous variables, and functional assignments. Specifying such models can be extremely difficult in practice. The process requires substantial domain expertise, and does not scale easily to large systems, multiple systems, or novel system modifications. At the same time, many application domains, such as molecular biology, are rich in structured causal knowledge that is qualitative in nature. This manuscript proposes a general approach for querying a causal biological knowledge graph, and converting the qualitative result into a quantitative structural causal model that can learn from data to answer the question. We demonstrate the feasibility, accuracy and versatility of this approach using two case studies in systems biology. The first demonstrates the appropriateness of the underlying assumptions and the accuracy of the results. The second illustrates the versatility of the approach by querying a knowledge base for the molecular determinants of a severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)-induced cytokine storm, and performing counterfactual inference to estimate the causal effect of medical countermeasures for severely ill patients. The third example considers a global pand","606":"Background: Northern Italy was one of the most impacted areas by COVID. It is now widely assumed that the virus was silently spreading for at least 2 weeks before the first patient was identified. During this silent phase, and in the following weeks when the hospital system was overburdened, data collection did not occur as it would have been impossible to collect all the necessary information regarding the new infected cases. With the aim of assessing both the dynamics of the introduction of the virus and the effectiveness of containment measures introduced, we try to reconstruct the evolution of the pandemic in northern Italian regions using all cause mortality data. Methods: we collected all cause mortality data stratified by age from the national institute of statistics, together with COVID-related deaths data released by other government structures. Using a SEIR model together with estimates of the exposure to death time distribution, we fitted the reproduction number in different phases of the spread at regional level. Results: We estimate a reproduction number of 2.6+\/-0.1 before case identification was performed, during which there were no restrictions imposed; after lockdown (as announced by the Prime Minister) the number increased to 3.4. School closures contributed to a reduction of the reproductive number down to 0.8. Soft lockdown measures resulted","607":"We study the influence of vaccination on the dynamics of epidemics spreading through structured networks using the cavity method of statistical physics. We relax the assumption that vaccination prevents all transmission of a disease used in previous studies, such as the deterministic SIR model, and study instead the impact of social distancing via bond percolation. Our results show that vaccination with partial immunity provides significant changes in the epidemic size distribution but does not eliminate the virus completely. In particular, we find that increasing the vaccine efficacy to only 90% successively degrades the cumulative epidemic size by more than 30%, which implies that delaying the second wave of infection by 10 days leads to immense mortality. Furthermore, we observe that high vaccine coverage is able to suppress the epidemic size more successfully than low coverage levels. The main conclusion of our study is that there is a non-monotonic relationship between the extent of social distancing and the epidemic size, and new insights into the effects of vaccination are needed. Specifically, it is possible that the increase in the perceived susceptibility to COVID-19 vaccines increases the probability of dying from Covid-19 rather than from death. Therefore, enhancing testing could reduce the number of deaths by 50%. Finally, since prevalence tests measure the willingness to detect only a few percent","608":"Infectious diseases that incorporate pre-symptomatic transmission are challenging to monitor, model, predict and contain. We address this scenario by studying a variant of a stochastic susceptible-exposed-infected-recovered model on arbitrary network instances using an analytical framework based on the method of dynamic message-passing. This framework provides a good estimate of the probabilistic evolution of the spread on both static and contact networks, offering a significantly improved accuracy with respect to individual-based mean-field approaches while requiring a much lower computational cost compared to numerical simulations. It facilitates the derivation of epidemic thresholds, which are phase boundaries separating parameter regimes where infections can be effectively contained from those where they cannot. These have clear implications on different containment strategies through topological (reducing contacts) and infection parameter changes (e.g., social distancing and wearing face masks), with relevance to the recent COVID-19 pandemic. Finally, our results show that even a moderate increase in the basic reproduction number, such as one leading to a doubling of the population size, does not render the system completely immune to further outbreaks. Thus, there is a need for more sophisticated measures aimed at distinguishing between individuals who become infected from others and whether or not one becomes infected upon","609":"In this paper, we propose to derive the time-varying parameters for the Susceptible-Infected-Recovered (SIR) model by using the Fourier transform. We then introduce a new concept, i.e., the Effective Reproduction Number (ERN), to identify the transmission rate which governs the spread of infectious diseases. The estimation of ERN is formulated as an inverse problem, with the objective function being the expected number of infectious cases free of constraints. We develop a simple iterative algorithm to solve the ERN inference. Furthermore, we incorporate two additional capabilities: firstly, the population size and its initial conditions into the model; secondly, the timing of quarantine - both in the pre-quarantine and post-quarantine settings - into the model. The resulting MCMC method is applied to estimate the reproduction numbers $R_t$ at various levels for countries in the US, and the results can be used to inform the public health policy on how to manage the epidemic without risking overwhelming the hospital capacity. The calculator also provides estimates on the duration of the peak of infection, and the height of the inflection point for each country. They are compared with the estimated values from the SIR model trained on data about COVID","610":"We have developed a globally applicable diagnostic Covid-19 model by augmenting the classical SIR epidemiological model with a neural network module. Our model does not rely upon previous epidemics like SARS\/MERS and all parameters are optimized via machine learning algorithms employed on publicly available Covid-19 data. The model decomposes the contributions to the infection timeseries to analyze and compare the role of quarantine control policies employed in highly affected regions of Europe, North America, South America and Asia in controlling the spread of the virus. For all continents considered, our results show a generally strong correlation between strengthening of the quarantine controls as learnt by the model and actions taken by the regions' respective governments. Finally, we have hosted our quarantine diagnosis results for the top 70 affected countries worldwide, on a public platform, which can be used for informed decision making by public health officials and researchers alike. The adaptive and timely generated quarantine responses provide empirical evidence about the efficiency of the implemented measures and allow us to better understand the future dynamics of the disease. This work is based on a rigorous mathematical analysis and simulations of the stochastic model predictions in combination with empirical data from more than 500 countries, including those from the top 70 affected countries worldwide. A web application capable of predicting epidemic scenarios at","611":"In this work, we investigate how the COVID-19 pandemics and more precisely the lockdown of a sector of the economy may have changed our habits and, there-fore, altered the demand of some goods even after the re-opening. In a two-sector infinite horizon economy, we show that the demand of the goods produced by the sector closed during the lockdown could shrink or expand with respect to their pre-pandemic level depending on the length of the lockdown and the relative strength of the satiation effect and the substitutability effect. We also provide conditions under which this sector could remain inactive even after the lockdown as well as an insight on the policy which should be adopted to avoid this outcome. Our results are based on a simple mathematical model where each agent represents a different type of person who can consume what they would like to buy before and during the lockdown. Although the model is unrealistic in practice, it allows us to derive insights on the behavior of the market when such a scenario exists. This gives rise to policies for both supply and price control which are quite surprising from the perspective of traditional economics. Finally, we provide optimal strategies for the industries which should implement these measures. We find that if a sufficiently strong lockdown is implemented, the productivity of the","612":"In this study, we present a dynamical agent-based model to investigate the interplay between the socio-economy of and SEIRS-type epidemic spreading over a geographical area, divided to smaller area districts and further to smallest area cells. The model treats the populations of cells and authorities of districts as agents, such that the former can reduce their economic activity and the latter can recommend economic activity reduction both with the overall goal to slow down the epidemic spreading. The agents make decisions with the aim of attaining as high socio-economic standings as possible relative to other agents of the same type by evaluating their standings based on the local and regional infection rates, compliance to the authorities' regulations, regional drops in economic activity, and efforts to mitigate the spread of epidemic. We find that the willingness of population to comply with authorities' recommendations has the most drastic effect on the epidemic spreading: periodic waves spread almost unimpeded in non-compliant populations, while in compliant ones the spread is minimal with chaotic spreading pattern and significantly lower infection rates. Health and economic concerns of agents turn out to have lesser roles, the former increasing their efforts and the latter decreasing them. Overall, our results show that the optimal course of action for preventing the propagation of epidemics in a given area","613":"The COVID-19 pandemic has caused a global health crisis with widespread consequences for both public and private health sectors. The gold standard for COVID-19 detection is the use of RT-PCR testing from CT or X-ray images. However, due to the high demand for tests, this approach is time-consuming and instead chest CT scan or Chest X-Ray (CXR) image analysis can be used as a scalable and fast method. This research aims to develop a new deep learning framework for detecting COVID-19 pneumonia patients using CXR images instead of PCR test results. We propose a multi-scale GAN architecture named MS-GAN which produces two novel conditional probability maps for lung disease detection and patient normalization in addition to other visual features such as VGG16 and ResNet50. Additionally, we employ transfer learning to address the problem of data scarcity seen in medical imaging datasets. Extensive experiments on five different CT scan dataset show that our model achieves superior performance over existing state-of-the-art methods achieving an average accuracy of 0.932$\\pm$0.005 and area under ROC curve of 0.91$\\pm$0.010 for lung disease classification and 0.99 $\\pm$","614":"India is one of the worst affected countries by the Covid-19 pandemic at present. We studied publicly available data of the Covid-19 patients in India and analyzed possible impacts of quarantine and social distancing within the stochastic framework of the SEQIR model to illustrate the controlling strategy of the pandemic. Our simulation results clearly show that proper quarantine and social distancing should be maintained from an early time just at the start of the pandemic and should be continued till its end to effectively control the pandemic. This calls for a more socially disciplined lifestyle in this perspective in future. The demographic stochasticity, which is quite visible in the system dynamics, has a critical role in regulating and controlling the pandemic. It turns out that if a less stringent measure is adopted, the number of infected people decreases, although the epidemic remains active without any sign of relaxation. In fact, it becomes non-existent when there is a greater than 50% probability of reinfection due to unavailability of medical resources. Finally, we also find that delaying the release of quarantined individuals does not lead to any significant change in the mortality rate. Although releasing those who are asymptomatic or have mild symptoms earlier will delay the peak demand for hospital resources","615":"The work of our research group (MIT) has been greatly changed by the COVID-19 pandemic. It was originally intended to be done in an academic manner, with a focus on researching and developing new tools for COVID-19. However, when it came to conducting this research, we realized that there is no time period or event where particular data sets are needed. We propose a modular approach to address this problem, called Data Aspects Networks (DANs). To understand how each aspect of the dataset contributes to its overall performance, we analyze both individual aspects and the pairwise interactions between them. Furthermore, we introduce a novel method for integrating these features into one holistic framework through which to extract the most important data sets from DANS. These experiments demonstrate the ability of our proposed approach to mine meaningful information from real-world datasets without having any prior knowledge about the details of the datasets involved. In addition, we show that DANS can be used as a blueprint for designing automated machine learning oriented methods, with some sample cases shown in https:\/\/github.com\/mit_edu\/dataasets-nodes.html. This paper intends to bridge the gap between computer science and nursing. It is hoped that our unique methodology can facilitate the development of innovative","616":"As of June 2021, COVID-19 has spread to almost every country in the world, causing millions of infected and hundreds of thousands of deaths. In this paper, we first verify the assumption that clinical variables could have time-varying effects on COVID-19 outcomes. Then, we develop a temporal stratification approach to make daily predictions on patients' outcome at the end of hospital stay. Training data is segmented by the remaining length of stay, which is a proxy for the patient's overall condition. Based on this, a sequence of predictive models are built, one for each time segment. Thanks to the publicly shared data, we were able to build and evaluate prototype models. Preliminary experiments show 0.98 AUROC, 0.91 F1 score and 0.97 AUPR on continuous deterioration prediction, encouraging further development of the model as well as validations on different datasets. We also verify the key assumption which motivates our method. Clinical variables could have time-varying effects on COVID-19 outcomes. That is to say, the feature importance of a variable in the predictive model varies at different disease stages. To more precisely express these ideas, we propose a novel deep learning framework which leverages reinforcement learning to balance the loss","617":"The recent COVID-19 pandemic has promoted vigorous scientific activity in an effort to understand, advice and control the pandemic. Data is now freely available at a staggering rate worldwide. Unfortunately, this unprecedented level of information contains a variety of data sources and formats, and the models do not always conform to the description of the data. Health officials have recognized the need for more accurate models that can adjust to sudden changes, such as produced by changes in behavior or social restrictions. In this work we formulate a model that fits a ``SIR''-type model concurrently with a statistical change detection test on the data. The result is a piece wise autonomous ordinary differential equation, whose parameters change at various points in time (automatically learned from the data). The main contributions of our model are: (a) providing interpretation of the parameters, (b) determining which parameters of the model are more important to produce changes in the spread of the disease, and (c) using data-driven discovery of sudden changes in the evolution of the pandemic. Together, these characteristics provide a new model that better describes the situation and thus, provides better quality of information for decision making. We also present a comparison between our model and another widely used epidemiological model, showing that our model","618":"Learning sequence models (LMs) have become ubiquitous in solving various natural language processing (NLP) tasks. There has been increasing interest in what knowledge these LMs contain and how we can extract that knowledge, treating LMs as knowledge bases (KBs). While there has been much work on probing LMs in the general domain, there has been little attention to whether these powerful LMs can be used as domain-specific KBs. To this end, we create the BioLAMA benchmark, which is comprised of 49K biomedical factual knowledge triples for probing biomedical LMs. We find that biomedical LMs with recently proposed probing methods can achieve up to 18.51% Acc@5 on retrieving biomedical knowledge. Although this seems promising given the task difficulty, our detailed analyses reveal that most predictions are highly correlated with prompt templates without any subjects, hence producing similar results on each relation and hindering their capabilities to be used as domain-specific KBs. We hope that BioLAMA can serve as a challenging benchmark for biomedical factual probing. The code is available at https:\/\/github.com\/ncbi\/BioLAMA.git. Since the test data is noisy, we also develop a simple baseline based on topic modeling using word2vec embeddings trained on","619":"This study applies natural language modeling to generate plausible strategic moves in the ancient game of Go. We train the Generative Pretrained Transformer (GPT-2) to mimic the style of Go champions as archived in Smart Game Format (SGF), which offers a text description of move sequences. The trained model further generates valid but previously unseen strategies for Go. Because GPT-2 preserves punctuation and spacing, the raw output of the text generator provides inputs to game visualization and creative patterns, such as the Sabaki project's (2020) game engine using auto-replays. Results demonstrate that language modeling can capture both the sequencing format of championship Go games and their strategic formations. Compared to random game boards, the GPT-2 fine-tuning shows efficient opening move sequences favoring corner play over less advantageous center and side play. Game generation as a language modeling task offers novel approaches to more than 40 other board games where historical text annotation provides training data (e.g., Amazons&Connect 4\/6). These results indicate that language models can be used to discover existing strategy mixes for Go. This work demonstrates how NLP techniques can be applied to pure LAU gameplay by re-purposing classic board games from the perspective of strategists. It also","620":"The COVID-19 pandemic has influenced virtually all aspects of our lives, including how we interact with mobile technologies. In this paper, we investigate the impact of the pandemic on the\"dynamics\"of the mobile phone network where hundreds or thousands of new subscribers join in its early stages. We focus on Spain, one of the countries most affected by the spread of the virus, which has implemented strict social distancing measures. Our analysis shows that the arrival of non-social engineering elements like mobility restrictions, lockdowns, and temporary ad spaces can increase the vulnerability of the mobile phone network. As these networks recover from such shocks, having access to data traffic increases their ability to analyze mobile phones' location traces to inform epidemic models, as well as for policy making decisions to mitigate the effects of the outbreak. Besides, changes in the comtemporary use of the cellular network during lockdowns and space closures, while keeping the number of users at home via a tight threshold, leave signatures in the traffic logs that can be automatically detected and analyzed. The insights provided here may help to improve decision-making regarding the management of the COVID-19 pandemic, and more generally, to manage the sudden appearance and growth of large macroscopic aggregates in the Internet","621":"The global pandemic of coronavirus disease 2019 (COVID-19) has put tremendous pressure on the medical system. Imaging plays a complementary role in the management of patients with COVID-19. Computed tomography (CT) and chest X-ray (CXR) are the two dominant screening tools. However, difficulty in eliminating the risk of disease transmission, radiation exposure and not being costeffective are some of the challenges for CT and CXR imaging. This fact induces the implementation of lung ultrasound (LUS) for evaluating COVID-19 due to its practical advantages of noninvasiveness, repeatability, and sensitive bedside property. In this paper, we utilize a deep learning model to perform the classification of COVID-19 from LUS data, which could produce objective diagnostic information for clinicians. Specifically, all LUS images are processed to obtain their corresponding local phase filtered images and radial symmetry transformed images before fed into the multi-scale residual convolutional neural network (CNN). Secondly, image combination as the input of the network is used to explore rich and reliable features. Feature fusion strategy at different levels is adopted to investigate the relationship between the depth of feature aggregation and the classification accuracy. Our proposed method is evaluated on the point-","622":"We investigate the effects of school closure and subsequent reopening on the transmission of COVID-19, by considering Denmark, Norway, Sweden and German states as case studies. By comparing the growth rates in daily hospitalizations or confirmed cases under different interventions, we provide evidence that school closures contribute to a reduction in the growth rate approximately 7 days after implementation. Limited school attendance, such as older students sitting exams or the partial return of younger year groups, does not appear to significantly affect community transmission. In countries where community transmission is generally low, such as Denmark or Norway, a large-scale reopening of schools while controlling or suppressing the epidemic appears feasible. However, school reopening can contribute to statistically significant increases in the growth rate in countries like Germany, where community transmission is relatively high. In all regions, a combination of low classroom occupancy and robust test-and-trace measures were in place. Our findings underscore the need for a cautious evaluation of reopening strategies. This article is part of the theme issue \u2018Modelling that shaped the early COVID-19 pandemic response in the UK\u2019. We are also interested in discussing how the impact of school opening\/closures varies across age and gender. To further investigate these variations, we use a Bayesian SE","623":"We study dynamic clustering problems from the perspective of online learning. We consider an online learning problem, called \\textit{Dynamic $k$-Clustering}, in which $k$ centers are maintained over time (centers may change positions) such as a dynamically changing set of $r$ clients is connected to each other with a rate $\\mathcal{R}$, or equivalently, given a stream of unknown \\textit{distinctive} vectors drawn from a known distribution, we aim to find the number of defective items inside the resulting cluster at any given time step. In this paper, we present both the \\textit{constant-factor approximation} and the \\textit{size-constrained approximation}. We show that our constancy factor approximation can be efficiently solved via solving a linear program directly for non-negative integer values. For the size constraint, we derive a quadratic programming formulation by imposing system constraints on the numbers of defectives and nondominated variables. The objective function value is then approximately solved by applying a variant of Benders decomposition to obtain an approximate solution. We additionally perform experiments on random graphs generated using parameterized complexity theory to demonstrate the effectiveness of our model. Our theoretical results match well with our","624":"In December 2019, the first positive case of novel coronavirus disease (COVID-19) was confirmed in China. In the month following this, COVID-19 spread worldwide, affecting 153 million people. Education and transportation systems are among the most disrupted sectors of society since the outbreak began. As these industries resume activities, public health authorities have expressed the need for more accurate models of COVID-19 transmission to help with preventive measures. In this study, we propose a mechanistic model of COVID-19 transmission based on the basic principles of statistical physics. The proposed model improves the traditional SIR model by considering the prevention measure as part of the model. Furthermore, we consider the effect of different government strategies on the spread of the virus. By applying the model to real data from Hubei province and Wuhan, we find that the optimal lockdown policy depends on both the current number of active cases and the past prevalence of the pandemic. Our results indicate that relaxations in the containment measures cause a rapid increase in the number of active cases, which helps with the maintenance of the system. Finally, we use the model to discuss the impact of varying governmental strategies on the effective reproduction number $R_t$. We found that there is a reasonable correlation","625":"We investigate changes in and factors affecting American adolescents' subjective wellbeing during the COVID-19 pandemic through a multi-wave longitudinal study of high-resolution mobile phone data for over 14,000 adolescent participants between April 1 and June 30, 2020, in the United States (US). Using repeated cross-sectional surveys at both the individual and neighborhood level, we find that how much US adults felt online was associated with increases in both mental health outcomes and physical activity levels. Factors such as worries about their well-being or lack of resilience were found to be significantly correlated with these measures. There was also a significant relationship between depression severity and internet usage tendencies. Our results suggest that public health officials and clinicians can adapt strategies based on the evolving needs of communities to adequately inform policy decisions aimed at helping vulnerable groups during this unprecedented crisis. In addition, we provide implications for research towards understanding how much US adults feel online during a major life change. We encourage future work to evaluate the long-term effects of the pandemic on children's emotional wellbeing and mental health. Future studies should include extending the same methods to other domains, including comparative analyses, structural equation modeling, and dynamic topic models. We argue that new ways of assessing mental illness need to be designed with specific consideration given our findings","626":"Echo chambers may exclude social media users from being exposed to other opinions, therefore, can cause rampant negative effects. Among abundant evidence are the 2016 and 2020 US presidential elections conspiracy theories and polarization, as well as the COVID-19 disinfodemic. To help better detect echo chambers and mitigate its negative effects, this paper explores the mechanisms and attributes of echo chambers in social media. In particular, we first illustrate four primary mechanisms related to three main factors: human psychology, social networks, and automatic systems. We then depict common attributes of echo chambers with a focus on the diffusion of misinformation, spreading of conspiracy theory, creation of social trends, political polarization, and emotional contagion of users. We illustrate each mechanism and attribute in a multi-perspective of sociology, psychology, and social computing with recent case studies. Our analysis suggest an emerging need to detect echo chambers and mitigate their negative effects. This paper concludes with several proposed methods for detecting echo chambers, some of which have potential to be deployed in large scale, such as using data sources beyond massive social media datasets. Finally, we discuss the challenges of policing online platforms and connections between offline and online communities. These challenges include understanding the platform's structure, detecting harmful or offensive content, and ensuring reliable behavioral change","627":"The COVID-19 pandemic has caused unprecedented disruption, particularly in retail. Where essential demand cannot be fulfilled online, or where more stringent measures have been relaxed, customers must visit shop premises in person. This naturally gives rise to some risk of susceptible individuals (customers or staff) becoming infected. It is therefore important to minimise this risk as far as possible while retaining economic viability of the shop. We therefore explore and compare the spread of COVID-19 in different shopping situations involving person-to-person interactions: (i) free-flowing, unstructured shopping; (ii) structured shopping (e.g. a queue). We examine which of (i) or (ii) may be preferable for minimising the spread of COVID-19 in a given shop, subject to constraints such as the geometry of the shop; compliance of the population to local guidelines; and additional safety measures which may be available to the organisers of the shop. We derive a series of conclusions, such as unidirectional free movement being preferable to bidirectional shopping, and that the number of servers should be maximised as long as they can be well protected from infection. Finally, we propose a suite of recommendations for practitioners to consider when implementing these policies in","628":"Federated learning (FL) has received high interest from researchers and practitioners to train machine learning (ML) models for healthcare. Ensuring the trustworthiness of these models is essential. Especially bias, defined as a disparity in the model's predictive performance across different subgroups, may cause unfairness against specific subgroups, which is an undesired phenomenon for trustworthy ML models. In this research, we address the question to which extent bias occurs in medical FL and how to prevent excessive bias through reward systems. We first evaluate how to measure the contributions of institutions toward predictive performance and bias in cross-silo medical FL with a Shapley value approximation method. In a second step, we design different reward systems incentivizing contributions toward high predictive performance or low bias. We then propose a combined reward system that incentivizes contributions toward both. We evaluate our work using multiple medical chest X-ray datasets focusing on patient subgroups defined by patient sex and age. Our results show that we can successfully measure contributions toward bias, and an integrated reward system successfully incentivizes contributions toward a well-performing model with low bias. While the partitioning of scans only slightly influences the overall bias, institutions with data predominantly from one subgroup introduce a favorable bias for this subgroup. Our results indicate that","629":"The effectiveness of vaccination highly depends on the choice of individuals to vaccinate, even if the same number of individuals are vaccinated. Vaccinating individuals with high centrality measures such as betweenness centrality (BC) and eigenvector centrality (EC) are effective in containing epidemics. However, in many real-world cases, each individual has distinct epidemic characteristics such as contagion, recovery, fatality rate, efficacy, and probability of severe reaction to a vaccine. Moreover, the relative effectiveness of vaccination strategies depends on the number of available vaccine shots. Centrality-based strategies cannot take the variability of epidemic characteristics or the availability of vaccines into account. Here, we propose a framework for vaccination strategy based on graph neural network ansatz (GNNA) and microscopic Markov chain approach (MMCA). In this framework, we can formulate an effective vaccination strategy that considers the properties of each node, and tailor the vaccination strategy according to the availability of vaccines. Our approach is highly scalable to large networks. We validate the method in many real-world networks for network dismantling, the susceptible-infected-susceptible (SIS) model with homogeneous and heterogeneous contagion\/recovery rates, and the susceptible-infected-recovered-","630":"The COVID-19 pandemic has been accurately described as a public health problem once daily positive case counts are reported. In order to better understand this phenomenon, we have studied both the reporting of case counts and the reporting of patient deaths from multiple sources. From these data sources, it appears that the official numbers released by the Johns Hopkins University (JHU) may be overdispersed, with severe implications for the accuracy of modelling their data. We have also found that the infection fatality rate tends to be low at the early stages of the outbreak, while later it can reach up to 0.5%. The main reason for some misreported numbers is likely due to non-epidemic factors, such as testing errors, travel restrictions, and\/or mask wearing. A simple linear regression model provides evidence that the number of infected people decays exponentially within about 10 days after the start of the epidemic, which suggests that lockdown policies should be implemented reasonably soon. Finally, we emphasize that not much is known on the precise nature of the spread of epidemics. Here we propose a general framework for obtaining reliable estimates of the reproduction number of an infectious disease under uncertain data conditions, including non-pharmaceutical interventions. This framework allows us to obtain bounds on the","631":"Through lockdowns and other severe changes to daily life, almost everyone is affected by the COVID-19 pandemic. Scientists and medical doctors are - among others - mainly interested in researching, monitoring, and improving physical and mental health of the general population. Mobile health apps (mHealth), and apps conducting ecological momentary assessments (EMA) respectively, can help in this context. However, developing such mobile applications poses many challenges like costly software development efforts, strict privacy rules, compliance with ethical guidelines, local laws, and regulations. In this paper, we present TrackYourHealth (TYH), a highly configurable, generic, and modular mobile data collection and EMA platform, which enabled us to develop and release two mobile multi-platform applications related to COVID-19 in just a few weeks. We present TYH and highlight specific challenges researchers and developers of similar apps may also face, especially when developing apps related to the medical field. Especially concerningly, we discuss the TYH problem in the broader literature regarding state-of-the-art infrastructures taking into account both their capabilities and limitations. We also describe how our infrastructure handles typical issues for the biomedical crowd, including staying on track, proper labeling, localization, image quality and reliability. Furthermore,","632":"The light curve of the microlensing event KMT-2021-BLG-0912 exhibits a very short anomaly relative to a single-lens, single-source form. We investigate the light curve for the purpose of identifying the origin of the anomaly. We model the light curve under various interpretations. From this, we find four solutions, in which three solutions are found under the assumption that all stars have an equal planet\/host mass ratio, and the other solution is found under the assumption that planets are more likely to orbit less massive stars. The first solution (with no host) is excluded from the model based on the contradiction that the faint source companion is bigger than its primary component. Considering only the components with masses $\\sim 3M_{\\rm J}$ and $\\sim 10^{6} M_{\\rm J}$, respectively, we derive the basic reproduction number $R_0 = 1.93 \\pm 0.31$ as a function of the binary logarithmic uncertainty. The second solution ($R_0=1.69 \\pm 0.42$), assuming that the measurements of the planets' masses are precise enough, can also be ruled out. Finally, the third solution $(R_0=2.34","633":"The COVID-19 pandemic has demonstrated the need for a more effective and efficient disinfection approach to combat infectious diseases. Ultraviolet germicidal irradiation (UVGI) is a proven mean for disinfection and sterilization and has been integrated into handheld devices and autonomous mobile robots. Existing UVGI robots which are commonly equipped with uncovered lamps that emit intense ultraviolet radiation suffer from: inability to be used in human presence, shadowing of objects, and long disinfection time. These robots also have a high operational cost. This paper introduces a cost-effective germicidal system that utilizes UVGI to disinfect pathogens, such as viruses, bacteria, and fungi, on high contact surfaces (e.g. doors and tables). This system is composed of a team of 5-DOF mobile manipulators with end-effectors that are equipped with far-UVC excimer lamps. The design of the system is discussed with emphasis on path planning, coverage planning, and scene understanding. Evaluations of the UVGI system using simulations and irradiance models are also included. Finally, we present the results of our experiments which demonstrate the effectiveness of the proposed UVGI robot. Specifically, this robot is capable of effectively deactivating several different pathogens without UVC exposure while maintaining a","634":"We propose a probabilistic approach to select a subset of a \\textit{target domain representative keywords} from a candidate set, contrasting with a context domain. Such a task is crucial for many downstream tasks in natural language processing. To contrast the target domain and the context domain, we adapt the \\textit{two-component mixture model} concept to generate a distribution of candidate keywords. It provides more importance to the \\textit{distinctive} keywords of the target domain than common keywords contrasting with the context domain. To support the \\textit{representativeness} of the selected keywords towards the target domain, we introduce an \\textit{optimization algorithm} for selecting the subset from the generated candidate distribution. We have shown that the optimization algorithm can be efficiently implemented with a near-optimal approximation guarantee. Finally, extensive experiments on multiple domains demonstrate the superiority of our approach over other baselines for the tasks of keyword summary generation and trending keywords selection. The code used in this paper is available at https:\/\/github.com\/NEUIR\/CODESearch.git. Since the development of deep learning based models has been slow in 2020, the production of non-trivial neural text summarization algorithms remains challenging. Our codes","635":"Modeling is a key activity in conceptual design and system design. Through collaborative modeling, end-users, stakeholders, experts, and entrepreneurs are able to create a shared understanding of a system representation. While the Unified Modeling Language (UML) is one of the major conceptual modeling languages in object-oriented software engineering, more and more concerns arise from the modeling quality of UML and its tool support. Among them, the limitation of the two-dimensional presentation of its notations and lack of natural collaborative modeling tools are reported to be significant. In this paper, we explore the potential of using Virtual Reality (VR) technology for collaborative UML software design by comparing it with classical collaborative software design using conventional devices (Desktop PC, Laptop). For this purpose, we have developed a VR modeling environment that offers a natural collaborative modeling experience for UML Class Diagrams. Based on a user study with 24 participants, we have compared collaborative VR modeling with conventional modeling with regard to efficiency, effectiveness, and user satisfaction. Results show that the use of VR has some disadvantages concerning efficiency and effectiveness, but the user's fun, the feeling of being in the same room with a remote collaborator, and the naturalness of collaboration were increased. Conclusions and future research directions should be","636":"Deep generative models are attracting great attention for molecular design with desired properties. Most existing models generate molecules by sequentially adding atoms. This often renders generated molecules with less correlation with target properties and low synthetic accessibility. Molecular fragments such as functional groups are more closely related to molecular properties and synthetic accessability than atoms. Here, we propose a fragment-based molecular generative model which designs new molecules with target properties by sequentially adding molecular fragments to any given starting molecule. A key feature of our model is a high generalization ability in terms of property control and fragment types. The former becomes possible by learning the contribution of individual fragments to the whole target properties in an auto-regressive manner. For the latter, we used a deep neural network that predicts the bonding probability of two molecules from the embedding vectors of the two molecules as input. The high synthetic accessibility of the generated molecules is implicitly considered while preparing the fragment library with the BRICS decomposition method. We show that the model can generate molecules with the simultaneous control of multiple target properties at a high success rate. It also works equally well with unseen fragments even in the property range where the training data is rare, verifying the high generalization ability. As a practical application, we demonstrated that the model can generate potential inhibitors","637":"We propose a new nonlinear point-coupling parameterized interaction, PC-L3, for the relativistic Hartree-Bogoliubov framework with a separable pairing force by fitting to observables of 65 selected spherical nuclei, including the binding energies and charge radii. Overall, from comparing with the experimental data, the implementation of PC-L3 in relativistic Hartree-Bogoliubov successfully yields the lowest root-mean-square deviation, 1.251 MeV, among currently and commonly used point-coupling interactions. Meanwhile, PC-L3 is capable of estimating the saturation properties of the symmetric nuclear matter and bulk properties of the finite nuclei and of appropriately predicting the isospin and mass dependence of binding energy, e.g., the isotopes of $Z$=20, 50, and 82, and isotones of $N$=20, 50, and 82. The comparison of the estimated binding energies for $\\sim$5500 neutron-rich nuclei based on PC-L3 and other point-coupling interactions is also presented. Among three newly proposed interaction kernels, the Gaussian one is adopted for the range of 0.5<varepsilon","638":"This paper studies a networked bivirus model, in which two competing viruses spread across a network of interconnected populations; each node represents a population with a large number of individuals. The viruses may spread through possibly different network structures, and an individual cannot be simultaneously infected with both viruses. Focusing on convergence and equilibria analysis, a number of new results are provided. First, we show that for networks with generic system parameters, there exist a finite number of equilibria. Exploiting monotone systems theory, we further prove that for bivirus networks with generic system parameters, then convergence to an equilibrium occurs for all initial conditions, except possibly for a set of measure zero. Given the network structure of one virus, a method is presented to construct an infinite family of network structures for the other virus that results in an infinite number of equilibria in which both viruses coexist. Necessary and sufficient conditions are derived for the local stability\/instability of boundary equilibria, in which one virus is present and the other is extinct. A sufficient condition for a boundary equilibrium to be almost globally stable is presented. Then, we show how to use monotone systems theory to generate conclusions on the ordering of stable and unstable equilibria,","639":"Traditionally, most machine learning algorithms have been designed to learn from data streams where the data itself does not present any information about the real world tasks. In this case, it is useful to ask what kind of knowledge the algorithm lacks and how could one expand upon such results? In this paper, we propose a framework for diagnosing problems with streaming data whose solutions are based on solving a batch nonlinear problem. The proposed framework takes as input the data stream and its associated parameters which govern the behavior of the model during training. It outputs a single final classification vector at the end of the input sequence when all batches of the model have been solved. We also detect if the average performance of the new approach exceeds the baseline method by taking into account the temporal correlation inherent in the data stream. For doing so, we compare both methods using three different datasets taken from the Internet. Our experiments show that the new approach outperforms the baselines in terms of accuracy, macro F1, and AUROC score while having almost the same mAP values. While further visualizing the internal architecture of the framework, we can notice some differences between the two approaches since they are related to hyperparameter tuning and neural network architectures. Finally, we also demonstrate how the lack of interpretability","640":"In 2020, the SARS-CoV-2 virus causes a global pandemic of the new human coronavirus disease COVID-19. This pathogen primarily infects the respiratory system of the afflicted, usually resulting in pneumonia and in a severe case of acute respiratory distress syndrome. These disease developments result in the formation of different pathological structures in the lungs, similar to those observed in other viral pneumonias that can be detected by the use of chest X-rays. For this reason, the detection and analysis of the pulmonary regions, the main focus of affection of COVID-19, becomes a crucial part of both clinical and automatic diagnosis processes. Due to the overload of the health services, portable X-ray devices are widely used, representing an alternative to fixed devices to reduce the risk of cross-contamination. However, these devices entail different complications as the image quality that, together with the subjectivity of the clinician, make the diagnostic process more difficult. In this work, we developed a novel fully automatic methodology specially designed for the identification of these lung regions in X-ray images of low quality as those from portable devices. To do so, we took advantage of a large dataset from magnetic resonance imaging of a similar pathology and performed two stages of transfer","641":"The literature for count modeling provides useful tools to conduct causal inference when outcomes take non-negative integer values. Applied to the potential outcomes framework, we link the Bayesian causal inference literature to statistical models for count data. We discuss the general architectural considerations for constructing the predictive posterior of the missing potential outcomes. Special considerations for estimating average treatment effects are discussed, some generalizing certain relationships and some not yet encountered in the causal inference literature. Methods for doubly robust estimation (DR) using linear or logistic regression are introduced; more generally, both model specifications and estimation procedures are called\"marginally consistent\". In this context, our main result is that the DR methods usually provide unbiased estimates of the average treatment effect. Furthermore, because these methods focus on reducing uncertainty rather than making stronger predictions, they have less impact on the uncertainties in the observed outcome distribution. The use of such methods can be illustrated by a simulation study and an application to HIV treatment monitoring. Our work facilitates the construction of confidence intervals for the averaged treatment effect, allowing us to recommend bootstrap-based confidence intervals as well as directly calculating the desired coverage probability without relying on sampling techniques. This applies especially to situations where the outcome variable is highly correlated with covariates (e.g., explaining why the proposed estimator","642":"Widespread opinions and discussion exist regarding the efficiency of social distancing after crucial spread of SARS-CoV-2 during the actual Covid-19 pandemic. While Germany has released a federal law that prohibits any type of direct contact for more than 2 people other countries including the US released curfews. People are now wondering whether these measures are helpful to stop or hamper the Covid-19 pandemic and to limit the spread of potentially deadly corona virus. A quantitative statement on this question depends on many parameters that are difficult to grasp mathematically and cannot therefore be made conclusively (they include consistent adherence to the measures decided, the estimated number of unreported cases, the possible limitation by test capacities, possible mutations of the virus, etc...). However, it turns out that a reduction in the actual daily new infection rate (actual daily growth rate of reported cases, in short: infection rate) from the current value of 30-35% in the US to 10% would be extremely effective in stopping the spread of the virus. The severe restrictions in Germany which closed any public events, schools and universities a week ago might already have contributed to a reduction of the growth rate of reported cases below 30%. These measures could be used to reduce the size of","643":"We","644":"The disease called the new coronavirus (COVID19) is a new viral respiratory pandemic that first appeared on January 13, 2020 in Wuhan, China. Some of the symptoms of this illness are fever, cough, shortness of breath and difficulty in breathing. In more serious cases, death may occur as a result of infection. COVID19 emerged as a pandemic that affected the whole world in a little while. The most important issue in the fight against the epidemic is the early diagnosis and follow-up of COVID19 (+) patients. Therefore, in addition to the RT-PCR test, medical imaging methods are also used when identifying COVID 19 (+) patients. In this study, an alternative approach was proposed using cough data, one of the most prominent symptoms of COVID19 (+) patients. The performances of z-normalization and min-max normalization methods were investigated on these data. All features were obtained using discrete wavelet transform method. Support vector machines (SVM) was used as classifier algorithm. The highest performances of accuracy and F1-score were obtained as 100% and 100% using the min-max normalization, respectively. On the other hand, the highest accuracy and highest F1-score","645":"The Hobby-Eberly Telescope (HET) Dark Energy Experiment (HETDEX) is undertaking a blind wide-field low-resolution spectroscopic survey of 540 square degrees of sky to identify and derive redshifts for a million Lyman-alpha emitting galaxies (LAEs) in the redshift range 1.9<z<3.5. The ultimate goal is to measure the expansion rate of the Universe at this epoch, to sharply constrain cosmological parameters and thus the nature of dark energy. A major multi-year wide field upgrade (WFU) of the HET was completed in 2016 that substantially increased the field of view to 22 arcminutes diameter and the pupil to 10 meters, by replacing the optical corrector, tracker, and prime focus instrument package and by developing a new telescope control system. The new, wide-field HET now feeds the Visible Integral-field Replicable Unit Spectrograph (VIRUS), a new low-resolution integral field spectrograph (LRS2), and the Habitable Zone Planet Finder (HPF), a precision near-infrared radial velocity spectrograph. VIRUS consists of 156 identical spectrographs fed by almost 35,000 fibers in","646":"Image generative models can learn the distributions of the training data and consequently generate examples by sampling from these distributions. However, when the training dataset is corrupted with outliers, generative models will likely produce examples that are also similar to the outliers. In fact, a small portion of outliers may induce state-of-the-art generative models, such as Vector Quantized-Variational AutoEncoder (VQ-VAE), to learn a significant mode from the outliers. To mitigate this problem, we propose a robust generative model based on VQ-VAE, which we name Robust VQ-VAE (RVQ-VAE). During inference, the RVQ-VAE uses two separate codebooks for the inliers and outliers. To ensure the codebooks embed the correct components, we iteratively update the sets of inliers and outliers during each epoch. To guarantee that the encoded data points are matched to the correct codebooks, we quantize using a weighted Euclidean distance measure whose weights are determined by directional variances of the codebooks. Both codebooks, together with the encoder and decoder, are trained jointly according to the reconstruction loss and the quantization loss. We","647":"The proposed European Artificial Intelligence Act (AIA) is the first attempt to elaborate a general legal framework for AI carried out by any major global economy. As such, the AIA is likely to become a point of reference in the larger discourse on how AI systems can (and should) be regulated. In this article, we describe and discuss the two primary enforcement mechanisms proposed in the AIA: the conformity assessments that providers of high-risk AI systems are expected to conduct, and the post-market monitoring plans that providers must establish to document the performance of high-risk AI systems throughout their lifetimes. We argue that the AIA can be interpreted as a proposal to establish a Europe-wide ecosystem for conducting AI auditing, albeit in other words. Our analysis offers three main contributions. First, by describing the enforcement mechanisms included in the AIA in terminology borrowed from existing literature on AI auditing, we help providers of AI systems understand how they can prove adherence to the requirements set out in the AIA in practice. Second, by examining the AIA from an auditing perspective, we seek to provide transferable lessons from previous research about how to refine further the regulatory approach outlined in the AIA. Third, by highlighting seven aspects of the AIA where amendments (","648":"We present a novel method for six-degree-of-freedom (6-DoF) autonomous camera movement for minimally invasive surgery, which, unlike previous methods, takes into account both the position and orientation information from structures within the surgical scene. In addition to locating the camera for a good view of the manipulated object, our autonomous camera also tracks the 6-DoF pose of the patient while taking into account workspace constraints, including the horizon and safety constraints. We developed a simulation environment to test our method on the\"wire chaser\"surgical training task from validated training curricula in conventional laparoscopy and robot-assisted surgery. Furthermore, we propose, for the first time, the application of the proposed autonomous camera method in video-based surgical skill assessment, an area where videos are typically recorded using fixed cameras. In a study with N=30 human subjects, we show that video examination of the autonomous camera view as it tracks the ring motion over the wire leads to more accurate user error (ring touching the wire) detection than when using a fixed camera view, or equivalently, an improved 3D localization via ring formation through surface fitting. Our preliminary work suggests that there is potential for autonomous camera positioning informed by scene orientation, and this can direct designers","649":"In the present study, we propose novel sequence-to-sequence pre-training objectives for low-resource machine translation (NMT): Japanese-specific sequence to sequence (JASS) for language pairs involving Japanese as the source or target language, and English-specific sequence to sequence (ENSS) for language pairs involving English. JASS focuses on masking and reordering Japanese linguistic units known as bunsetsu, whereas ENSS is proposed based on phrase structure masking and reordering tasks. Experiments on ASPEC Japanese--English&Japanese--Chinese, Wikipedia Japanese--Chinese, News English--Korean corpora demonstrate that JASS and ENSS outperform MASS and other existing language-agnostic pre-training methods by up to +2.9 BLEU points for the Japanese--English tasks, up to +7.0 BLEU points for the Japanese--Chinese tasks and up to +1.3 BLEU points for English--Korean tasks. Empirical analysis, which focuses on the relationship between individual parts in JASS and ENSS, reveals the complementary nature of the subtasks of JASS and ENSS. Adequacy evaluation using LASER, human evaluation, and case studies reveals that our proposed methods significantly","650":"We continue a series of papers devoted to construction of semi-analytic solutions for barrier options. These options are written on underlying following some simple one-factor diffusion model, but all the parameters of the model as well as the barriers are time-dependent. We managed to show that these solutions are systematically more efficient for pricing and calibration than, eg., the corresponding finite-difference solvers. In this paper we extend this technique to pricing double barrier options and present two approaches to solving it: the General Integral transform method and the Heat Potential Method. Our results confirm that for double barrier options these semi-analytic techniques are also more efficient than the traditional numerical methods used to price this type of options. Finally, we provide easily verifiable theoretical findings about the efficiency of our proposed algorithms. The main result of this paper is actually valid in a much broader class of problems. This includes certain classical examples as well as a large number of applications where semi-analytical techniques would be highly beneficial. Examples include the pricing of CAT bond, NPI (CAT) bond, and stock option prices or the modelling of stochastic volatilities. To illustrate the speed of our methodology we provide a few examples of real data analysis, including the effects of COVID","651":"Agent-based and activity-based models for simulating transportation systems have attracted significant attention in recent years. Few studies, however, include a detailed representation of active modes of transportation - such as walking and cycling - at a city-wide level, where dominating motorised modes are often of primary concern. This paper presents an open workflow for creating a multi-modal agent-based and activity-based transport simulation model, focusing on Greater Melbourne, and including the process of mode choice calibration for the four main travel modes of driving, public transport, cycling and walking. The synthetic population generated and used as an input for the neural network based methods. The road network was modified to take account of the different travel modes and their share of dominance; this also included a static structure, which cannot be changed without the need for any further training. The method presented here can be applied to other cities and regions, but it has been developed to be more generic than the original PyTorch implementation due to the framework's modular design. The code presented here, available under https:\/\/github.com\/rover-mind\/multi_mode_transport.git\/, allows anyone to train their own model using the weights from another person's data or through our web application. We encourage any","652":"COVID-19 pandemic elucidated that knowledge systems will be instrumental in cases where accurate information needs to be communicated to a substantial group of people with different backgrounds and technological resources. However, several challenges and obstacles hold back the wide adoption of virtual assistants by public health departments and organizations. This paper presents the Instant Expert, an open-source semantic web framework to build and integrate voice-enabled smart assistants (i.e. chatbots) for any web platform regardless of the underlying domain and technology. The component allows non-technical domain experts to effortlessly incorporate an operational assistant with voice recognition capability into their websites. Instant Expert is capable of automatically parsing, processing, and modeling Frequently Asked Questions pages as an information resource as well as communicating with an external knowledge engine for ontology-powered inference and dynamic data utilization. The presented framework utilizes advanced web technologies to ensure reusability and reliability, and an inference mechanism for natural language understanding powered by deep learning and heuristic algorithms. A use case for creating an informatory assistant for COVID-19 based on the Centers for Disease Control and Prevention (CDC) data is presented to demonstrate the framework's usage and benefits. We hope that this framework will prove beneficial for advancing the goal of building informed virtual assistants while reducing the burden on","653":"Early detection of COVID-19 using radiological images has gained considerable attention, as it can provide fast and accurate diagnosis results. The recent deep learning approaches have shown great promise in this area by achieving impressive performance on chest X-ray (CXR) datasets. However, training such models need large volumes of CXR data, which are not available for most patients. In this work we propose a novel generative adversarial network (GAN) based approach -- named CovidGAN-- to generate synthetic CXR images with high fidelity. Our experiments show that the proposed model achieves state-of-the-art performance on the CXR dataset and produces promising results for both generated and real image pairs. We also demonstrate the importance of transfer learning in developing robust classifiers, especially where data breaches occur. As a case study, we also evaluate the predictive uncertainty estimates given by the proposed model for detecting pneumonia cases. They achieve satisfactory results for both generated and real image pairs, confirming the superiority of our method over other state-of-the-art methods. Finally, we release the first publicly available COVID-19 CXR dataset at https:\/\/github.com\/masadcv\/covidGAN.git.io\/. Our code is now","654":"The classic classification scheme for Active Galactic Nuclei (AGNs) was recently challenged by the discovery of the so-called changing-state (changing-look) AGNs (CSAGNs). The physical mechanism behind this phenomenon is still a matter of open debate and the samples are too small and of serendipitous nature to provide robust answers. In order to tackle this problem, we need to design methods that are able to detect AGN right in the act of changing-state. Here we present an anomaly detection (AD) technique designed to identify AGN light curves with anomalous behaviors in massive datasets. The main aim of this technique is to identify CSAGN at different stages of the transition, but it can also be used for more general purposes, such as cleaning massive datasets for AGN variability analyses. We use light curves from the Zwicky Transient Facility data release 5 (ZTF DR5), containing a sample of 230,451 AGNs of different classes. The ZTF DR5 light curves were modeled with a Variational Recurrent Autoencoder (VRAE) architecture, that allowed us to obtain a set of attributes from the VRAE latent space that describes the general behaviour of our sample. These attributes were then","655":"In this article, we present a brief overview of what BEC (Business Email Compromise) is and why we should be concerned about. In addition, it presents 2 examples, Ubiquity and Peebles Media Group, which have been chosen to analyse the phenomena of BEC and underpin how universal BEC threat is for all companies. The psychology behind this scam has been, then, studied. In particular, the Big Five Framework has been analysed to understand how personality traits play an important role in Social Engineering-based attacks. Furthermore, the 6 basic principles of influence, by Cialdini, have been presented to show which strategies are adopted in such scam. The paper follows with the analysis of the BEC impacts, the incidents evaluation and, finally, with the description of some precautions, that companies should undertake in order to mitigate the likelihood of a Business Email Compromise. The conclusions of this article are based on a thorough research methodology, that provides evidence from the statistical analysis of the number of Business Email Compromises, against a background of considered facts. Therefore, it can be stated that the problem of protecting users' privacy does not exist and, consequently, there will be no incentive to perform cyber-risking. It is however crucial to","656":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\nWhat is the most efficient way of utilizing existing systems and external data?\n### Input:\nCOVID Super Expert\n### Response: Objective: Neural network de-identification studies have focused on individual datasets. These studies assume the availability of a sufficient amount of human-annotated data to train models that can generalize to corresponding test data. In real-world situations, however, researchers often have limited or no in-house training data. Existing systems and external data can help jump-start de-identification on in-house data; however, the most efficient way of utilizing existing systems and external data is unclear. This article investigates the transferability of a state-of-the-art neural clinical de-identification system, NeuroNER, across a variety of datasets, when it is modified architecturally for domain generalization and when it is trained strategically for domain transfer. Methods and Materials: We conducted a comparative study of the transferability of NeuroNER using four medical imaging datasets with different types of abnormalities. We first verified that NeuroNER significantly outperforms ten unimodal baselines by large margins, achieving an average improvement of 3.4% over the F1 score. We then demonstrated that it likewise exceeds 14 other state-of-the-art de-identification systems by achieving comparable performance. Furthermore, we examined the impact of pretraining and fine-tuning on the","657":"Pre-processing and Data Augmentation play an important role in Deep Convolutional Neural Networks (DCNN). Whereby several methods aim for standardization and augmentation of the dataset, we here propose a novel method aimed to feed DCNN with spherical space transformed input data that could better facilitate feature learning compared to standard Cartesian space images and volumes. In this work, the spherical coordinates transformation has been applied as a preprocessing method that, used in conjunction with normal MRI volumes, improves the accuracy of brain tumor segmentation and patient overall survival (OS) prediction on Brain Tumor Segmentation (BraTS) Challenge 2020 dataset. The LesionEncoder framework has been then applied to automatically extract features from DCNN models, achieving 0.586 accuracy of OS prediction on the validation data set, which is one of the best results according to BraTS 2020 leaderboard. After applying the LesionEncoder framework on 3D CT volumes, a significant improvement in terms of Dice similarity coefficient (DSC), mean absolute error (MAE) and relative mean loss value was observed comparing the predicted and reference values of the lesion encoder output layer with the ground truth label. Moreover, MAEs of $0.249 \\pm 0.006$ and $0","658":"Being low-dose radiotherapy effective for cancer patients, low-dose CT imaging has been widely adopted in clinical practice. The LDCT images inevitably suffer from geometric degradation problem caused by complex noises. It was reported that deep learning (DL)-based LDCT denoising methods using convolutional neural network (CNN) achieved impressive denoising performance. Although most existing DL-based methods (e.g., encoder-decoder framework) can implicitly utilize non-local and contextual information via downsampling operator and 3D CNN, the explicit multi-information (i.e., local, non-local, and contextual) integration may not be explored enough. To address this issue, we propose a novel graph convolutional network-based LDCT denoising model, namely GCN-MIF, to explicitly perform multi-information fusion for denoising purpose. Concretely, by constructing intra- and inter-slice graph, the graph convolutional network is introduced to leverage the non-local and contextual relationships among pixels. The traditional CNN is adopted for the extraction of local information. Finally, the proposed GCN-MIF model fuses all the extracted local, non-local, and contextual information. Extensive experiments show the effectiveness","659":"As the COVID-19 pandemic began, so has global NO$_x$, with reductions of up to 80% in some areas. These are among the largest and fastest responses in recorded history for any type of external attack on the Earth system. However, despite their record size, global NO$_x$ levels are only at modestly increased since late 2008 by a mere 1.8%, though they have played a key role as spillover effects on other planetary surface types. This paper uses new estimates of historical nitrogen dioxide (NO$_2$) emissions to explore the effects of recent changes in fossil fuel consumption, cement production, and electricity sector trends on the short-term variability of atmospheric column densities over China's most polluted regions. We find that monthly average NO$_2$ concentrations were 9.4%, 6.3%, and 4.5% higher than 2019 baselines within February, May, and October 2020, respectively. The greatest drop was during the second half of March 2020, when daily average NO$_2$ values dropped by 49.1%. Such drops can be driven by decreases in both global energy demand and anthropogenic nitrogen oxides (NO$_x$) emissions. Recent studies suggest that climate variations play a","660":"Respiratory sound classification is an important tool for remote screening of respiratory-related diseases such as pneumonia, asthma, and COVID-19. To facilitate the interpretability of classification results, especially ones based on deep learning, many explanation techniques have been proposed using prototypes. However, existing explanation techniques often assume that the data is non-biased and the prediction results can be explained by a set of prototypical examples. In this work, we develop a unified example-based explanation method for selecting both representative data (prototypes) and outliers (criticisms). In particular, we propose a novel application of adversarial attacks to generate an explanation spectrum of data instances via an iterative fast gradient sign method. Such unified explanation can avoid over-generalisation and bias by allowing human experts to assess the model mistakes case by case. We performed a wide range of quantitative and qualitative evaluations to show that our approach generates effective and understandable explanation and is robust with many deep learning models. Our code is available at https:\/\/github.com\/annahaensch\/unfiled-data-explanation.git.io\/. This repository collects and collates the prototype datasets used in the research study, which are also collected independently at https:\/\/github.com\/zhang-","661":"Numerous accessibility features have been developed and included in consumer operating systems to provide people with a variety of disabilities additional ways to access computing devices. Unfortunately, many users, especially older adults who are more likely to experience ability changes, are not aware of these features or do not know which combination to use. In this paper, we first quantify this problem via a survey with 100 participants, demonstrating that very few people are aware of built-in accessible features on their phones. These observations led us to investigate accessibility recommendation as a way to increase awareness and adoption. We chose three specific accessibility recommendations (low-, medium- and high-degree) based on their applicability to our own context - one's smartphone setup, interactions with smartphones, and environmental constraints. We conducted a 30-item quantitative study where each item was rated according to its importance for understanding the subject's needs. Our results show that both low- and medium-degree recommendations are generally applicable, but that they require some considerable effort to be useful. We also found that using these guidelines requires even more effort than implementing them directly. To circumvent this challenge, we propose a probabilistic approach to recommend personalized and adaptive content online. This work emphasizes the need for considering accessibility requirements from the user perspective and demonstrates the potential of automated","662":"We introduce Language-Informed Latent Actions (LILA), a framework for learning natural language interfaces in the context of human-robot collaboration. LILA falls under the shared autonomy paradigm: in addition to providing discrete language inputs, humans are given a low-dimensional controller $-$ e.g., a 2 degree-of-freedom (DoF) joystick that can move left\/right and up\/down $x$ axis of the robot's end effector. LILA learns to use language to modulate this controller, providing users with a language-informed control space: given an instruction like\"place the cereal bowl on the tray,\"LILA may learn a 2-layer neural network to approximate the dynamic programming operation of the controller. We evaluate LILA models by comparing them to various state-of-the-art baselines, both from the literature and by leveraging one of the few existing datasets about user-generated content in English. Results show that LILA models outperform imitation learning and end-effector control baselines, especially when there is little data available for training; while they are computationally tractable, we demonstrate that their approximations become increasingly accurate as more training data becomes available. A qualitative analysis validates our assumption that LIME","663":"The present paper shows a simple method to combine epidemiological and behavioral models, which combines the advantages of both with very few examples of its kind in any domain. The method consists of stacking on a set of epidemic curves centered at different timescales (or hidden state) parameters into a single model. It is known how the choice of these time scales impacts the resulting overall solution; thus combining them allows one to obtain a continuous-time approximation to the collective dynamics of the infection. We demonstrate the utility of this approach by showing that it yields improved results for COVID-19 forecasting. The method is applicable to a wide variety of problems where empirical data is available. Finally, we show that our approach provides a way to quantify the effects of mobility restrictions within a population, and illustrate some ways to further explore them. Results are based on a combination of analytical methods and numerical simulations. They substantiate the effectiveness of the proposed method as well as its flexibility and applicability beyond a particular problem context. In addition, they provide a way to calculate the herd immunity threshold and subsequent estimates of the total number of infected people. The method is implemented in the R package mobILity. An online application is also developed using the code presented here, allowing users to quickly reproduce the experiments reported","664":"The use of machine learning techniques in classical and quantum systems has led to novel techniques to classify ordered and disordered phases, as well as uncover transition points in critical phenomena. Efforts to extend these methods to dynamical processes in complex networks is a field of active research. Network-percolation, a measure of resilience and robustness to structural failures, as well as a proxy for spreading processes, has numerous applications in social, technological, and infrastructural systems. A particular challenge is to identify the existence of a percolation cluster in a network in the face of noisy data. Here, we consider bond-percolation, and introduce a sampling approach that leverages the core-periphery structure of such networks at a microscopic scale, using onion decomposition, a refined version of the $k-$core. By selecting subsets of nodes in a particular layer of the onion spectrum that follow similar trajectories in the percolation process, percolating phases can be distinguished from non-percolating ones through an unsupervised clustering method. Accuracy in the initial step is essential for extracting samples with information-rich content, that are subsequently used to predict the critical transition point through the confusion scheme, a recently introduced learning method. The method","665":"In this paper, we present an epidemiological model for the spread of COVID-19 in India after lockdown by local government. We discuss how this compartmental model can be used to understand the transmission dynamics of this epidemic and how different policies can be implemented by the government to control its spread. This is one of the first models available in literature which focuses on the spread of COVID-19 from a global perspective. We also give special emphasis to our model with daily intervention by public health workers. Our model allows us to explore the interplay between different interventions such as testing, quarantine, social distancing, and vaccination along with their associated parameters. By building our model into a reproducible framework, we aim to provide a quantitative guideline for decision makers to take suitable decisions regarding the implementation of targeted countermeasures by the government. The modeling code contained within our repository is being made publicly accessible at https:\/\/github.com\/sourish-cmi\/Covid19_model_contribution. Finally, we would like to see researchers across fields of science and engineering dealing with similar problems around the world construct, analyze, and point out the weaknesses of existing methods. One of the key goals of this research is to develop a mathematical analysis of infectious disease transmission under realistic","666":"Frequently used numbers are the daily population in various fields and countries, which need to be compared with other facts as well as with each other. These kinds of data have also been used for forming different predictive features of machine learning models. However, most of the existing studies on this kind of data focus on only one country or application domain, although they play an important role in several aspects of our lives like work environment and health). In this paper, we perform some exploratory analysis of how these patterns developed after the lockdown measures were applied to handle the COVID-19 pandemic throughout the world. The first half of our study (incl. environmental factors) focuses on the importance of understanding human behavior by applying behavioral medicine. The second quarter considers social media reactions from people while the third quarter deals with the impact of coronavirus disease. The fourth month presents us both sets of collected data about the epidemic spread in India and the USA, respectively. We connect all the datasets and parameters with statistical significance using the Kendall's tau test and p-value <0.05. This allows us to present some interesting findings of how the epidemic changed during the months of May and June 2020. The main results show that the strict control measures implemented by the government maintained the low","667":"Currently, many countries are facing the problems of aging population, serious imbalance of medical resources supply and demand, as well as uneven geographical distribution, resulting in a huge demand for remote e-health. Particularly, with invasions of COVID-19, the health of people and even social stability have been challenged unprecedentedly. To contribute to these urgent problems, this article proposes a general architecture of the remote e-health, where the city hospital provides the technical supports and services for remote hospitals. Meanwhile, 5G technologies supported telemedicine is introduced to satisfy the high-speed transmission of massive multimedia medical data, and further realize the sharing of medical resources. Moreover, to turn passivity into initiative to prevent COVID-19, a broad area epidemic prevention and control scheme is also investigated, especially for the remote areas. We discuss their principles and key features, and foresee the challenges, opportunities, and future research trends. Finally, a node value and content popularity based caching strategy is introduced to provide a preliminary solution of the massive data storage and low-latency transmission. The effectiveness of the proposed method is illustrated by the experimental results on the real dataset from China. Keywords: remote e-health, COVID-19, mobile network, shared medical resource, crisis","668":"Data visualizations are standard tools for assessing and communicating risks. However, it is not always clear which designs are optimal or how encoding choices might influence risk perception and decision-making. In this paper, we report the findings of a large-scale gambling game that immersed participants in an environment where their actions impacted their bonuses. Participants chose to either enter a lottery or receive guaranteed monetary gains based on five common visualization designs. By measuring risk perceptions and observing decision-making, we showed that icon arrays tended to elicit economically sound behavior. We also found that people were more likely to gamble when presented area proportioned triangle and circle designs. Using our results, we model risk perception and discuss how our findings can improve visualization selection. Our work builds upon previous research by proposing improvements for non-experts involved in modeling human decision-making with visualizations. Although our results focus only on the context of gambling, they have important implications for understanding generalizable models of risky asset allocation. They may also be applicable to other domains, such as financial engineering and machine learning. Finally, because our findings are limited to urban populations, we release our dataset to help facilitate future research into this underexplored domain. This data augmentation opens up opportunities for addressing real-world issues around fairness and","669":"Most research on novel techniques for 3D Medical Image Segmentation (MIS) is currently done using Deep Learning with GPU accelerators. The principal challenge of such technique is that a single input can easily cope computing resources, and require prohibitive amounts of time to be processed. Distribution of deep learning and scalability over computing devices is an actual need for progressing on such research field. Conventional distribution of neural networks consist in data parallelism, where data is scattered over resources (e.g., GPUs) to parallelize the training of the model. However, experiment parallelism is also an option, where different training processes are parallelized across resources. While the first option is much more common on 3D image segmentation, the second provides a pipeline design with less dependence among parallelized process's result. In this work we present a design for distributed deep learning training pipelines based on thread-level workloads, focusing on multi-node and multi-GPU environments, where the two different distribution approaches are deployed and benchmarked. We take as proof of concept the 3D U-Net architecture, using the MSD Brain Tumor Segmentation dataset, a state-of-art problem in medical image segmentation with high computational and space requirements. Using the B","670":"Temporal and causal relations play an important role in determining the dependencies between events. Classifying the temporal and causal relations between events has many applications, such as generating event timelines, event summarization, textual entailment and question answering. Temporal and causal relations are closely related and influence each other. So we propose a joint model that incorporates both temporal and causal features to perform causal relation classification. We use the syntactic structure of the text for identifying temporal and causal relations between two events from the text. We extract parts-of-speech tag sequence, dependency tag sequence and word sequence from the text. We propose an LSTM based model for temporal and causal relation classification that captures the interrelations between the three encoded features. Evaluation of our model on four popular datasets yields promising results for temporal and causal relation classification. The code used here, https:\/\/github.com\/thunlp\/JointIDSF-model\/tree\/main\/events2vec.git, is available at https:\/\/github.com\/thunlp\/JointIDSF-model\/tree\/master\/events2vec.git. This master thesis demonstrates that our model outperforms all the existing models with respect to temporal and causal relation classification tasks. Further, it provides good interpretable","671":"We study pure exploration in multi-agent systems where agents may choose to strategically assign themselves to either of two states, or not at all. Specifically, we consider the scenario of a university building on top of a large industrial company by assuming that every agent represents one of several interacting parts and thus can be assigned to one of the states from a finite set of interactions. The objective for each agent is to maximize her\/her utility (i.e., minimize its cost), subject to constraints arising from the limited number of interactions. We show that this problem becomes NP-hard when there are integer inputs, and provide a polynomial-time algorithm with bounded-error probability for non-adaptive strategies. For semi-adaptive strategies, we prove that the above desiderata are simultaneously achievable for any number of agents and any number of interactions. Moreover, our main contribution shows how to efficiently implement this class of games in practice, namely, low-dimensional mean-field approximations through Markov chain Monte Carlo methods. Finally, we demonstrate the efficacy of our approach in a variety of numerical experiments involving real-world networks. Our results show that our method outperforms the state-of-the-art heuristics, achieving higher payoff matrices and lower average","672":"We introduce and study the problem of maximizing the number of people that a dining room can accommodate provided that the chairs belonging to different tables are socially distant. We also investigate how to minimize the expected cost due to congestion and\/or shortage of food. Our results show that for small tables with only two or three customers, even the simple binary optimization problem can be solved in polynomial time. In contrast, for large tables with hundreds of thousands of users, the computations are still feasible under reasonable assumptions. For example, assuming that allocating resources to more than 100 individuals of a population of 10000 people is not too big, then optimizing the allocation strategy leads to>~30% increase of the capacity of the dining room, over the state-of-the-art solution. Similar effects can be obtained when we assume that the distribution of the customer's choice (i.e., which table) is non-uniform. Finally, we also consider the problem of minimizing the economic value of the allocations since this framework does not model factoricity of the decisions. Our results show that any algorithm that allocates resources based on an anticipated utility function can produce a good approximation to the optimal solution in practice. All algorithms are agnostic about the type of resource requirement nor the structure","673":"The electric vehicle transition has been moved back by the pandemic and the shortage of high-quality electricity sources. The existing generation mix technologies are not able to satisfy the stringent energy requirements of EVs. To enable the full penetration of EV technology into the daily life of citizens, this paper presents a novel approach based on the combination of microgrids (MGs) with battery friendly fixed charge station architectures. This new approach enables the ubiquitous delivery of EVs from anywhere any time while being fully charged up to date. Under appropriate management of EV charging, the power distribution network can be dynamically balanced to handle the fluctuating demands of the EVs and the PGAs. In such a way, the system can reduce the operational risks of traditional fossil fuel based generation systems and provides more efficient transmission routes. This study aims at providing widespread awareness about the new type of flexible EV charging and its benefits compared to conventional approaches. A case study based on the island of Gotland off the South East coast of Sweden shows that the proposed approach allows for a significant reduction of fossil fuels consumption and increases the flexibility of the supply chain. Thereby, it constitutes a promising solution for the long-term deployment of EVs and contributes to the increasing trend towards carbon neutrality of our society. Besides, the economic impact of the","674":"We investigate adaptive strategies to robustly and optimally control the COVID-19 pandemic via social distancing measures based on the example of Germany. Our goal is to minimize the number of fatalities over the course of two years without inducing excessive social costs. We consider a tailored model of the German COVID-19 outbreak with different parameter sets to design and validate our approach. Our analysis reveals that an open-loop optimal control policy can significantly decrease the number of fatalities when compared to simpler policies under the assumption of exact model knowledge. In a more realistic scenario with uncertain data and model mismatch, a feedback strategy that updates the policy weekly using model predictive control (MPC) leads to a reliable performance, even when applied to a validation model with deviant parameters. On top of that, we propose a robust MPC-based feedback policy using interval arithmetic that adapts the social distancing measures cautiously and safely, thus leading to a minimum number of fatalities even if measurements are inaccurate and the infection rates cannot be precisely specified by social distancing. Our theoretical findings support various recent studies by showing that 1) adaptive feedback strategies are required to reliably contain the COVID-19 outbreak, 2) well-designed policies can significantly reduce the number of fatalities compared to simpler ones while keeping the","675":"Abugida refers to a phonogram writing system where each syllable is represented using a single consonant or typographic ligature, along with a default vowel or optional diacritic(s) to denote other vowels. However, texting in these languages has some unique challenges in spite of the advent of devices with soft keyboard supporting custom key layouts. The number of characters in these languages is large enough to require characters to be spread over multiple views in the layout. Having to switch between views many times to type a single word hinders the natural thought process. This prevents popular usage of native keyboard layouts. On the other hand, supporting romanized scripts (native words transcribed using Latin characters) with language model based suggestions is also set back by the lack of uniform romanization rules. To this end, we propose a disambiguation algorithm and showcase its usefulness in two novel mutually non-exclusive input methods for languages natively using the abugida writing system: (a) disambiguation of ambiguous input for abugida scripts, and (b) disambiguation of word variants in romanized scripts. We benchmark these approaches using public datasets, and show an improvement in typing speed by 19.49%, 25.13%, and","676":"The task of detecting whether a person wears a face mask from speech is useful in modelling speech in forensic investigations, communication between surgeons or people protecting themselves against infectious diseases such as COVID-19. In this paper, we propose a novel data augmentation approach for mask detection from speech. Our approach is based on (i) training Generative Adversarial Networks (GANs) with cycle-consistency loss to translate unpaired\"talk like\"speech into two classes (with masks and without masks), and on (ii) generating new training utterances using the cycle-consistent GANs, assigning opposite labels to each translated utterance. Original and translated utterances are converted into spectrograms which are provided as input to a set of ResNet neural networks with various depths. The networks are combined into an ensemble through a Support Vector Machines (SVM) classifier. With this system, we participated in the Mask Sub-Challenge (MSC) of the INTERSPEECH 2020 Computational Paralinguistics Challenge, surpassing the baseline proposed by the organizers by 2.8%. We show that our data augmentation method provides substantial performance gains also outside the MSC. Furthermore, we demonstrate that our data augmentation approach yields better results than","677":"The aim of this paper is to shed some light on mathematical epidemic dynamics modelling from the viewpoint of analytical mechanics. To set the stage, it recasts the basic SIR model of mathematical epidemic dynamics in an analytical mechanics setting by considering as much as possible the effects of time-dependent parameters on the ensuing macroscopic models for linearised scalar field theory around black holes, and obtain the corresponding reduced differential equations through the notion of \\emph{fractionalization}. The results indicate that one may not have to be forced to make a choice between the initial conditions and the properties of the coarse-grained data they generate. In particular, the latter appears to be in excellent agreement with the former for the case of Dirichlet boundary conditions. If these are imposed explicitly in the language of physics, the formalism of our approach offers the possibility of improving upon the existing studies by taking into account higher order nonlinearity corrections to frequencies which occur naturally in any theoretical scenario involving coherent structures. We present numerical tests in support of such a claim based on two coupled semiconductor nanolasers exhibiting sustained oscillations over a wide range of infection rates. By also including the (unnecessary) constraint inferred by C. M. Bender and collegues that the symmetry of our","678":"With noisy environment caused by fluoresence and additive white noise as well as complicated spectrum fingerprints, the identification of complex mixture materials remains a major challenge in Raman spectroscopy application. In this paper, we propose a new scheme based on a constant wavelet transform (CWT) and a deep network for classifying complex mixture. The scheme first transforms the noisy Raman spectrum to a two-dimensional scale map using CWT. A multi-label deep neural network model (MDNN) is then applied for classifying material. The proposed model accelerates the feature extraction and expands the feature graph using the global averaging pooling layer. The Sigmoid function is implemented in the last layer of the model. The MDNN model was trained, validated and tested with data collected from the samples prepared from substances in palm oil. During training and validating process, data augmentation is applied to overcome the imbalance of data and enrich the diversity of Raman spectra. From the test results, it is found that the MDNN model outperforms previously proposed deep neural network models in terms of Hamming loss, one error, coverage, ranking loss, average precision, F1 macro averaging and F1 micro averaging, respectively. The average detection time obtained from our model is 5","679":"The global health threat from COVID-19 has been controlled in a number of instances by large-scale testing and contact tracing efforts. We created this document to suggest three functionalities on how we might best harness computing technologies to supporting the goals of public health organizations in minimizing morbidity and mortality associated with the spread of COVID-19, while protecting the civil liberties of individuals. In particular, this work advocates for a third-party free approach to assisted mobile contact tracing, because such an approach mitigates the security and privacy risks of requiring a trusted third party. We also explicitly consider the inferential risks involved in any contract tracing system, where any alert to a user could itself give rise to de-anonymizing information. More generally, we hope to participate in bringing together colleagues in industry, academia, and civil society to discuss and converge on ideas around a critical issue rising with attempts to mitigate the COVID-19 pandemic. The goal of this note is to raise awareness about the potential ramifications of these developments, and to promote a considered conversation regarding effective regulations, policies, and challenges. We hope that this will help achieve the goal of controlling the spread of COVID-19, and may be used as a prelude to discussing some of the broader social","680":"This research investigates how people engage with data visualizations when commenting on the social platform Reddit. There has been considerable research on collaborative sensemaking with visualizations and the personal relation of people with knowledge, but little is known about how public audiences without specific expertise and shared incentives openly express their thoughts, feelings, and insights in response to data visualizations. Motivated by the extensive social exchange around visualizations in online communities, this research examines characteristics and motivations of people's reactions to posts featuring visualizations. Following a Grounded Theory approach, we study 475 reactions from the \/r\/dataisbeautiful community, identify ten distinguishable reaction types, and consider their contribution to the discourse. A follow-up survey with 168 Reddit users clarified their intentions to react. Our results help understand the role of personal perspectives on data and inform future interfaces that integrate audience reactions into visualizations to foster a more inclusive understanding of people's opinions. We also discuss how our findings differ across sub-populations such as political partisans and what kind of submissions are likely to receive. Overall, we contribute to the design of future collaborative sensemaking tools for online platforms. We encourage others to further investigate the nuances of conversations surrounding visualizations and to explore the challenges of collaborating with high-quality visualizations. Finally","681":"The COVID-19 pandemic has become a major threat to human health and well-being worldwide. Non-pharmaceutical interventions such as contact tracing solutions are important to contain the spreads of COVID-19-like infectious diseases. However, current contact tracing solutions are fragmented with limited use of sensing technologies and centered on monitoring the interactions between individuals without an analytical framework for evaluating effectiveness. Therefore, we need to first explore generic architecture for contact tracing in the context of today's Internet of Things (IoT) technologies based on a broad range of applicable sensors. A new architecture for IoT based solutions to contact tracing is proposed and its overall effectiveness for disease containment is analyzed based on the traditional epidemiological models with the simulation results. The proposed work aims to provide a framework for assisting future designs and evaluation of IoT-based contact tracing solutions and to enable data-driven collective efforts on combating current and future infectious diseases. This paper presents a systematic review of the latest research papers on IoT-based contact tracing and provides a critical overview of the most recent advances in this area. In addition, by grouping the related studies into five main categories according to their objectives, the study reviews may help researchers and practitioners to quickly identify suitable techniques for future research directions. The study concludes with a","682":"Early detection of person-to-person transmission of emerging infectious diseases such as avian influenza is crucial for containing pandemics. We developed a simple permutation test and its refined version for this purpose. A simulation study shows that the refined permutation test is as powerful as or outcompetes the conventional test built on asymptotic theory, especially when the sample size is small. In addition, our resampling methods can be applied to a broad range of problems where an asymptotic test is not available or fails. We also found that decent statistical power could be attained with just a small number of cases, if the disease is moderately transmissible between humans. Our results suggest that even a minimal chance of human transmission would yield a huge improvement in terms of statistical power compared to the traditional case-control method. Thus, our work provides a new perspective for understanding the interplay between epidemiological dynamics and testing mechanisms. Altogether, our work builds a bridge over the gap between epidemiology and testing biology, emphasizing the importance of combining them together. Our findings enable researchers to understand the key factors behind the limited effectiveness of standard tests, and guide the design of efficient testing strategies. Finally, we discuss what looks like a successful application of the refined permutation","683":"Customers' emotions play a vital role in the service industry. The better frontline personnel understand the customer, the better the service they can provide. As human emotions generate certain (unintentional) bodily reactions, such as increase in heart rate, sweating, dilation, blushing and paling, which are measurable, artificial intelligence (AI) technologies can interpret these signals. Great progress has been made in recent years to automatically detect basic emotions like joy, anger etc. Complex emotions, consisting of multiple interdependent basic emotions, are more difficult to identify. One complex emotion which is of great interest to the public health sector is difficult to collect: whether a customer is telling the truth or just a story. This research presents an AI-method for capturing and sensing emotional data. With an accuracy of around 98 %, the best trained model was able to detect whether a participant of a debating challenge was arguing for or against her\/his conviction, using speech analysis. The data set was collected in an experimental setting with 40 participants. The findings are applicable to a wide range of services and applications contexts. The algorithm presented here can be applied in any situation where it is helpful for the agent to know whether a customer is speaking to her\/his conviction. The algorithm could also be used","684":"Motivated by applications in testing, treatment choice, risk profiling, and sales management, we develop a novel Bayesian nonparametric approach to estimate disease prevalence from viral load surveillance data. Our approach combines information from multiple sources to improve uncertainty quantification on the estimated proportion of positive tests over time. We illustrate this via simulation studies and apply our method to two datasets relating to COVID-19 testing in Mexico City and New York state. We find that applying our method to these datasets enhances both accuracy and uncertainty estimates for decision making support. Finally, we demonstrate how our approach can be applied beyond public health settings to benefit real world decision making. To facilitate further research, we release our code publicly at https:\/\/github.com\/annahaensch\/covid19_estimator. This estimator may assist researchers and policy makers in developing more effective intervention strategies as well as monitoring the spread of any contagious disease through serological or antibody testing. Additionally, due to the intrinsic stochastic nature of sampling data, it may be difficult to infer the posterior distribution of parameter estimates without sufficient samples being available for computing the exact likelihood ratio. Therefore, we recommend estimating the target prevalence using the first few samples most likely to contain the true signal but still produce reasonable confidence intervals","685":"In this paper, we introduce two remote extended reality (XR) research methods that can overcome the limitations of lab-based controlled experiments, especially during the COVID-19 pandemic: (1) a predictive model-based task analysis and (2) a large-scale video-based remote evaluation. We used a box stacking task including three interaction modalities - two multimodal gaze-based interactions as well as a unimodal hand-based interaction which is defined as our baseline. For the first evaluation, a GOMS-based task analysis was performed by analyzing the tasks to understand human behaviors in XR and predict task execution times. For the second evaluation, an online survey was administered using a series of the first-person point of view videos where a user performs the corresponding task with three interaction modalities. A total of 118 participants were asked to compare the interaction modes based on their judgment. Two standard questionnaires were used to measure perceived workload and the usability of the modalities. Results show that all models successfully capture the intended goal of reducing the number of touch events, predicting the time of a task, and generalizing the usage of the different modalities over time. As the next step, we developed a dual-modal behavior algorithm for","686":"The COVID-19 pandemic abruptly changed the way many people work, interact and move around in their daily lives. The change was sudden, unexpected and companies were not ready for it. Additionally, it affected learning outcomes and student behavior in different ways depending on whether and how they interacted with others inside or outside of school. This paper presents findings from a comparative analysis of the impact of the COVID-19 pandemic on these two components of student life at one Chilean university. Thereby, we aim to answer questions such as what happened during the semester when no lectures were delivered remotely, did the students have difficulties accessing the Internet connection (on both mobile and desktop), why are virtual meetings not so effective as real ones, how can we improve internet access, especially to rural areas, and how do we deal with miscommunication? In this context, we also discuss the importance of remote teaching since several countries across the globe had suddenly to switch off local classes. Finally, we provide recommendations for other researchers and policy makers to take advantage of the new situation by developing similar methods aimed at reducing the distance between physical and virtual meeting spaces. We hope that our study can help politicians and teachers deliver a more productive and sustainable future city. Overall, the pandemic has shown us a clear","687":"Global concern regarding ultrafine particles (UFPs), which are particulate matter (PM) with a diameter of less than 100nm, is increasing. These particles-with more serious health effects than PM less than 2.5 micrometers (PM2.5)-are difficult to measure using the current methods because their characteristics are different from those of other air pollutants. Therefore, a new monitoring system is required to obtain accurate UFPs information, which will raise the financial burden of the government and people. In this study, we estimated the economic value of UFPs information by evaluating the willingness-to-pay (WTP) for the UFPs monitoring and reporting system. We used the contingent valuation method (CVM) and the one-and-one-half-bounded dichotomous choice (OOHBDC) spike model. We analyzed how the respondents' socio-economic variables, as well as their cognition level of PM, affected their WTP. Therefore, we collected WTP data of 1,040 Korean respondents through an online survey. The estimated mean WTP for building a UFPs monitoring and reporting system is KRW 6,958.55-7,222.55 (USD 6.22-6.45","688":"We present SHINeS, a space simulator which can be used to replicate the thermal environment in the immediate neighborhood of the Sun down to a heliocentric distance r~0.06 au. The system consists of three main parts: the solar simulator which was designed and constructed in-house, a vacuum chamber, and the probing and recording equipment needed to monitor the experimental procedures. Our motivation for building this experimental system was to study the effect of intense solar radiation on the surfaces of asteroids when their perihelion distances become smaller than the semi-major axis of the orbit of Mercury. Comparisons between observational data and recent orbit and size-frequency models of the population of near-Earth asteroids suggest that asteroids are super-catastrophically destroyed when they approach the Sun. Whereas the current models are agnostic about the disruption mechanism, SHINeS was developed to study the mechanism or mechanisms responsible. The system can, however, be used for other applications that need to study the effects of high solar radiation on other natural or artificial objects. Scientific studies performed using SHINeS demonstrate that the average energy dose of ionizing radiation sufficient to destabilize hydrated materials becomes comparable with the area of air captured by NASA's MAROON-X mission.","689":"We propose a novel testing and containment strategy in order to contain the spread of SARS-CoV2 while permitting large parts of the population to resume social and economic activity. Our approach recognises the fact that testing capacities are severely constrained in many countries. In this setting, we show that finding the best way to utilise this limited number of tests during a pandemic can be formulated concisely as an allocation problem. Our problem formulation takes into account the heterogeneity of the population and uses pooled testing to identify and isolate individuals while prioritising key workers and individuals with a higher risk of spreading the disease. In order to demonstrate the efficacy of our testing and containment mechanism, we perform simulations using a network-based SIR model. Our simulations indicate that applying our mechanism on a population of $100,000$ individuals with only $16$ tests per day reduces the peak demand for hospital beds by approximately $20\\%$, when compared to the scenario where no intervention is implemented. Moreover, the application of our mechanism on a larger population leads to a more substantial reduction in the peak demand for hospital beds ($24\\%)$ than if no intervention is implemented. Finally, we note that the results obtained are merely indicative of the optimal policy; thus, there is no single\"best","690":"We present a sketch-based CAD modeling system, where users create objects incrementally by sketching the desired shape edits, which our system automatically translates to CAD operations. Our approach is motivated by the close similarities between the steps industrial designers follow to draw 3D shapes, and the operations CAD modeling systems offer to create similar shapes. To overcome the strong ambiguity with parsing 2D sketches, we observe that in a sketching sequence, each step makes sense and can be interpreted in the \\emph{context} of what has been drawn before. In our system, this context corresponds to a partial CAD model, inferred in the previous steps, which we feed along with the input sketch to a deep neural network in charge of interpreting how the model should be modified by that sketch. Our deep network architecture then recognizes the intended CAD operation and segments the sketch accordingly, such that a subsequent optimization estimates the parameters of the operation that best fit the segmented sketch strokes. Since there exists no datasets of paired isometric drawings from pairs of parallel lines, we train our system by generating synthetic images of different sizes using inliner features via self-supervision, and enrich them with useful information about the segmentation structure. We achieve state-of-the-art results on open-source benchmarks for","691":"We demonstrate an approach to replicate and forecast the spread of the SARS-CoV-2 (Covid-19) pandemic using the toolkit of probabilistic programming languages (PPLs). Our goal is to study the impact of various modeling assumptions and motivate policy interventions enacted to limit the reach of the Covid-19 pandemic. Using existing compartmental models we show how to use inference in PPLs to obtain posterior estimates for disease parameters. We improve popular existing models to reflect practical considerations such as the underreporting of the true number of Covid-19 cases and motivate the need to model policy decisions with confidence intervals. We design an SEI3RD model as a reusable template and demonstrate its flexibility in comparison to other models. We also provide a greedy algorithm that selects the optimal series of policy measures that are likely to control the infected population subject to provided constraints. We work within a simple, modular, and reproducible framework to enable immediate cross-domain access to the state-of-the-art in probabilistic inference with emphasis on policy intervention. We are not epidemiologists; the sole objective of this study is to serve as an exposition of methods, not to directly infer the real numbers of Covid-19 cases or","692":"It has been a long demand of Internet Service Providers (ISPs) that the Content Providers (CPs) share their profits for investments in network infrastructure. In this paper, we study profit sharing contracts between a CP with multiple ISPs. Each ISP commits to improving the Quality of Services (QoS) for the end-users through higher investments efforts. The CP agrees to share the profits due to the resulting higher demand for its content. We first model non-cooperative interaction between the CP and the ISPs as a two-stage game. CP is the leader that decides what fraction of its profits will be shared with the ISPs. Each ISP then simultaneously decides the amount of effort (investment) to enhance network quality. Here, CP cannot observe individual effort by the ISPs, which poses a challenge for the CP to decide how to share the profits with each ISP. Therefore, we also investigate a cooperative scenario, where the CP only decides the total share it gives to the ISPs, and each ISP then cooperatively shares the profit among themselves. We study the effect of such cooperation between the ISPs by building a Nash Bargaining based model. We show that the collaboration improves total effort by the ISPs and the payoff of the CP. Furthermore, we find that the cooperation between","693":"In this paper we present a framework for identifying and understanding vaccine hesitancy, which is an important aspect of achieving and maintaining herd immunity. We consider two frameworks, namely one based on vaccination experience or mass immunization, the other based on individual choice\/behavior. In both cases we construct multinomial logistic regression models to identify (i) pro-vaccination groups, i.e., those who support vaccinating others with their own social risk aversion, and (ii) anti-vaccination groups, i.e., those who question vaccine efficacy or the need for general vaccination against Covid-19. For the first framework, we develop linear regressions in the presence of covariates available from previous studies about vaccine acceptance. For the second framework, we introduce an algorithm to determine if an image represents a valid distance between the individuals\u2019 opinions. Then, we use maximum likelihood estimation to estimate the vaccine confidence interval at a given time point during the epidemic season. If necessary, we also calculate the percentage of the population infected over time by combining the estimated vaccine confidence interval and the observed daily cases. Finally, we conduct a numerical study comparing the above frameworks using data from the Danish national registries. The results show that the proposed approach provides better estimates of the","694":"In this work, we study how differentiable cascaded learning (DCSL) methods can be used to learn more generalizable deep neural networks from a single large-scale medical image dataset. We present a new approach that improves over recent attempts by using the transformer-based segmentation model T5 for feature extraction along with several other pretrained CNNs in a continual learning framework. Our DCSL baseline is trained on the CIFAR-10 dataset and follows the base architecture of RoBERTa and DenseNet121. The key idea of our approach is to use the position information provided by the tokenizers to guide the network through the entire training process. In addition to improving over previous approaches, our method achieves state-of-the-art results on two public neuroimaging datasets under very challenging settings. Source code will be made available at https:\/\/github.com\/ashishrana160796\/DCsl-model-and-pretrained-CNN-in-time. This source code also demonstrates novel applicability of DCSL in developing semantic segmentation tasks without any labeled data or target annotations. We make our algorithm publicly available via https:\/\/github.com\/ashishrana160796\/DCL-algorithm-and-super","695":"#StopAsianHate and #StopAAPIHate are two of the most commonly used hashtags that represent the current movement to end hate crimes against the Asian American and Pacific Islander community. We conduct a social media study of public opinion on the #StopAsianHate and #StopAAPIHate movement based on 46,058 Twitter users across 30 states in the United States ranging from March 18 to April 11, 2021. The movement attracts more participation from women, younger adults, Asian and Black communities. 51.56% of the Twitter users show direct support, 18.38% are news about anti-Asian hate crimes, while 5.43% have negative attitudes towards the movement. Public opinion varies across user characteristics. Furthermore, among the states with most racial bias motivated hate crimes, the negative attitude towards the #StopAsianHate and #StopAAPIHate movement is the weakest. To our best knowledge, this is the first large-scale social media-based study to understand public opinion on the #StopAsianHate and #StopAAPIHate movement. We hope our study can provide insights and promote research on anti-Asian hate crimes, and ultimately help address such a serious societal issue for the common benefits of all communities.","696":"The use of delivery services is an increasing trend worldwide, further enhanced by the COVID pandemic. In this context, drone technologies have emerged as preferred techniques for transporting goods, medical supplies to a given target location in the quarantine areas experiencing an epidemic outbreak. This paper presents a navigation system that makes feasible the delivery of parcels with autonomous drones. The system generates a path between a start and a final point and controls the drone to follow this path based on its localization obtained through GPS, 9DoF IMU, and barometer. In the landing phase, information of poses estimated by a marker (ArUco) detection technique using a camera, ultra-wideband (UWB) devices, and the drone's software estimation are merged by utilizing an Extended Kalman Filter algorithm to improve the landing precision. A vector field-based method controls the drone to follow the desired path smoothly, reducing vibrations or harsh movements that could harm the transported parcel. Real experiments validate the delivery strategy and allow to evaluate the performance of the adopted techniques. Preliminary results state the viability of our proposal for autonomous drone-assisted parcel delivery. We demonstrate the applicability of our approach by building a new package drop testbed for studying the effect of varying sensor resolutions and measurement rates on the landing accuracy.","697":"The prevalence and maturity of Bring Your Own Device (BYOD) security along with subsequent frameworks and security mechanisms in Australian organisations is a growing phenomenon somewhat similar to other developed nations. During the COVID 19 pandemic, even organisations that were previously reluctant to embrace BYOD have been forced to accept it to facilitate remote work. The aim of this paper is to discover, through a study conducted using a survey questionnaire instrument, how employees practice and perceive the BYOD security mechanism deployed by Australian businesses which can help guide the development of future BYOD security framework. Three research questions are answered by this study - What levels of awareness do Australian business entities have for BYOD security aspects? How are employees currently responding to the security requirements imposed by their organisations for mobile devices? Can we improve the usage of existing network infrastructure to enhance IT services for mobile devices? Overall, the findings show that awareness among employees is low regarding both the existence of BYOD security framework and its efficacy. This research presents some of the first evidence on the cyber-security culture practices adopted by Australian businesses. Such findings may be used as a basis for developing new training curricula in tertiary level education aimed at helping preventative measures against potential spear attacks. Furthermore, the findings highlight several key factors that need to be considered","698":"With the increase of COVID-19 cases worldwide, an effective way is required to diagnose COVID-19 patients. The primary problem in diagnosing COVID-19 patients is the shortage and reliability of testing kits, due to the quick spread of the virus, medical practitioners are facing difficulty identifying the positive cases. The second real-world problem is to share the data among the hospitals globally while keeping in view the privacy concerns of the organizations. Building a collaborative model and preserving privacy are major concerns for training a global deep learning model. This paper proposes a framework that collects a small amount of data from different sources (various hospitals) and trains a global deep learning model using blockchain based federated learning. Blockchain technology authenticates the data and federated learning trains the model globally while preserving the privacy of the organization. First, we propose a data normalization technique that deals with the heterogeneity of data as the data is gathered from different hospitals having different kinds of CT scanners. Secondly, we use Capsule Network-based segmentation and classification to detect COVID-19 patients. Thirdly, we design a method that can collaboratively train a global model using blockchain technology with federated learning while preserving privacy. Additionally, we collected real-life COVID-19 patients data,","699":"The most serious threat to ecosystems is the global climate change fueled by the uncontrolled increase in carbon emissions. In this project, we use mean field control and mean field game models to analyze and inform the decisions of electricity producers on how much renewable sources of production ought to be used in the presence of a carbon tax. The trade-off between higher revenues from production and the negative externality of carbon emissions is quantified for each producer who needs to balance in real time reliance on reliable but polluting (fossil fuel) thermal power stations versus investing in and depending upon clean production from uncertain wind and solar technologies. We compare the impacts of these decisions in two different scenarios: 1) the producers are competitive and hopefully reach a Nash Equilibrium; 2) they cooperate and reach a Social Optimum. We first prove that both problems have a unique solution using forward-backward systems of stochastic differential equations. We then illustrate with numerical experiments the producers' behavior in each scenario. We further introduce and analyze the impact of a regulator in control of the carbon tax policy, and we study the resulting Stackelberg equilibrium with the field of producers. Finally, we discuss the implications of our results for future carbon taxation efforts. Our work is motivated by the current push towards","700":"The study of animals is a key part of our understanding of their role in the world, and so learning about the behaviours of animals can have significant implications for both wildlife ecosystems and human behaviour. We aim to develop a computational model that goes beyond simply identifying species at risk or performing net energy minimisation on the basis of sequences of visits to these sites. Our approach is based on the well-known Page's CUSUM test, which allows us to determine whether one visitor of a given animal series is from another by computing gists between them. The difference between various types of graphs convolutional networks (GCNs) used in this work and others such as feed-forward neural networks (FFNNs), traditionally trained for classifying images, is relatively simple. Two different frameworks were tested: one using bits and other using classical machine learning algorithms; and a third uses a combination of both. In this first case, we achieved an accuracy of 91%, a sensitivity of 93% and a specificity of 90%. In the second case, we obtained an accuracy of 94%, a sensitivity of 92% and a specificity of 84%. Finally, in the third scenario, we combined the advantages of both classification frameworks into one wholly new architecture with the intention of addressing the problem of multiple species","701":"This paper presents preliminary summary results from a longitudinal study of participants in seven U.S. states during the COVID-19 pandemic. In addition to standard socio-economic characteristics, we collect data on various economic preference parameters: time, risk, and social preferences, and risk perception biases. We pay special attention to predictors that are both important drivers of social distancing and are potentially malleable and susceptible to policy levers. We note three important findings: (1) demographic characteristics exert the largest influence on social distancing measures and mask-wearing, (2) we show that individual risk perception and cognitive biases exert a critical role in influencing the decision to adopt social distancing measures, (3) we identify important demographic groups that are most susceptible to changing their social distancing behaviors. These findings can help inform the design of policy interventions regarding targeting specific demographic groups, which can help reduce the transmission speed of the COVID-19 virus. Our work summarizes what is known about the epidemiological factors associated with social distancing behavior, and also highlights potential research directions for further exploring and quantifying these factors. This article is part of the theme issue \u2018Data science approach to infectious disease surveillance\u2019. To reconstruct how our mindsets interact with real-world","702":"Abusive language on online platforms is a major societal problem, often leading to important societal problems such as the marginalisation of underrepresented minorities. There are many different forms of abusive speech such as hate speech, profanity, and cyber-bullying, and online platforms seek to moderate it in order to limit societal harm, to comply with legislation, and to create a more inclusive environment for their users. Within the field of Natural Language Processing, researchers have developed different methods for automatically detecting abusive content, most of which focus on specific subproblems or on narrow communities, as what is considered abusive very much differs by context. We argue that there is currently a dichotomy between what types of abuse detection algorithms we develop and how well they perform offline tests. This dichotomy impedes the potential of these algorithms being used for real-world applications. In this paper we survey existing methods, both commercial and academic, for automatic abuse detection from social media text. We also describe a method for creating custom rules for filtering out application layer attack requests. Finally we discuss challenges for adopting these methods and sketching possible research directions. We believe that our work opens up new possibilities for research into anti-abusive language on digital platforms. Our code, models, and data are available at https:\/\/","703":"The ongoing COVID-19 pandemic has raised concerns for many regarding personal and public health implications, financial security and economic stability. Alongside many other unprecedented challenges, there are increasing concerns over social isolation and mental health. We introduce \\textit{Expressive Interviewing}--an interview-style conversational system that draws on ideas from motivational interviewing and expressive writing. Expressive Interviewing seeks to encourage users to express their thoughts and feelings through writing by asking them questions about how COVID-19 has impacted their lives. We present relevant aspects of the system's design and implementation as well as quantitative and qualitative analyses of user interactions with the system. In addition, we conduct a comparative evaluation with a general purpose dialogue system for mental health that shows our system potential in helping users to cope with COVID-19 issues. Finally, we discuss limitations of the current work and provide recommendations for future research. Our code, models, data, and documentation are available online at https:\/\/github.com\/annahaensch\/expressive-interviewing.git. This project serves as part of my Master's thesis at the Faculty of Mathematics and Information Science at Ruprecht-Karls-Universit\\\"at Heidelberg under the supervision of PD Dr.","704":"With the emergence of the COVID-19 pandemic, the need for an effective contact tracing system has gained much attention from both public and academic communities. As several countries have been successful in developing their respective digital contact tracing systems, it is now possible to compare these systems across countries and use them as a basis for further research. In this paper, we study the current status of the COVID-19 digital contact tracing system in the Philippines. We also investigate how the disease propagated over a large number of social media platforms during the first three months of 2020. By using the results of our study, we find that there are still lots of room for improvement in terms of adoption behavior and usage scenario. However, we observe that there are already plenty of smartphone users who are willing to adopt the COVID-19 contact tracing app (even after some privacy concerns were found earlier). Further, we show that extending the existing state of the art with such technology would be very beneficial to improve the adoption rate. Therefore, we conclude that governments and developers of civic tech applications should consider the people's needs beyond data collection but instead serve up community mitigation efforts by focusing on those who are most likely to be infected. For example, we find that in targeting the people who are least likely","705":"We apply machine learning models to forecast intraday realized volatility (RV), by exploiting commonality in intraday volatility via pooling stock data together, and by incorporating a proxy for the market volatility. Neural networks dominate linear regressions and tree models in terms of performance, due to their ability to uncover and model complex latent interactions among variables. Our findings remain robust when we apply trained models to new stocks that have not been included in the training set, thus providing new empirical evidence for a universal volatility mechanism among stocks. Finally, we propose a new approach to forecasting one-day-ahead RVs using past intraday RVs as predictors, and highlight interesting diurnal effects that aid the forecasting mechanism. The results demonstrate that the proposed methodology yields superior out-of-sample forecasts over a strong set of traditional baselines that only rely on past daily RVs. We also find that similar results can be obtained using our updated methodology with just six hours of data from each day. As a result, we no longer observe any systematic differences between different methods, which leads to a more stable conclusion regarding the efficacy of our method. Code is available at https:\/\/github.com\/mgrasp\/GARCH-model-based-prediction.git.io","706":"The COVID-19 pandemic is one of the most pressing issues at present. A question which is particularly important for governments and policy makers is the following: Does the virus spread in the same way in different countries? Or are there significant differences in the development of the epidemic? In this paper, we devise new inference methods that allow to detect differences in the development of the COVID-19 epidemic across countries in a statistically rigorous way. In our empirical study, we use the methods to compare the outbreak patterns of the epidemic in a number of European countries. Our results suggest that there are no major discrepancies between the case fatality rates (CFRs) observed in each country, nor between the shapes of the CFR waves provided by the different national health services. However, some of the measures of interest rate variations seem to be more complex than those ones reported in the official statistics. Our findings also point out that several countries appear to begin their third wave earlier and thus have lower CFR values than the rest of the world. Finally, we find interesting temporal inconsistencies in the CFR rhythms established so far, with possible explanations ranging from a mix of logistical and epidemiological factors. Overall, these results show how collective actions can effectively drive the onset and shape of future outbreaks of the disease","707":"This study develops an epidemiological model for transferring the vaccine BNT162b2 based on the heat diffusion equation. Then, I apply optimal control theory to the proposed generalized SEIR model. For this, we introduce a vaccination compartmental model by separating the population in two groups: one composed by key-workers that keep working during the pandemic and have a usual contact rate, and another group consisting of people that are enforced\/recommended to stay at home. We refer to these two subpopulations as the vaccinated and the unvaccinated (or untested) individuals, respectively. The transmission rates Beta(t) and Gamma(t) are defined in terms of the average number of sexual contacts or daily travel, respectively. The spread of the disease via vector infection is also taken into account. Finally, several simulations are done to study and predict the behavior of the epidemic in Italy, Spain, and Germany considering different levels of the vaccination campaign, and the effect of multiple preventive measures such as social distancing, lockdown, and wearing masks. From the results, it is observed that the spread of the disease becomes much lower when there is a higher percentage of vaccinating people than when non-vaccating individuals. In addition, for the countries where a low fraction of","708":"In machine learning, data are usually represented in a (flat) Euclidean space where distances between points are along straight lines. Researchers have recently considered more exotic (non-Euclidean) Riemannian manifolds such as hyperbolic space which is well suited for tree-like data. In this paper, we propose a representation living on a pseudo-Riemannian manifold with constant nonzero curvature. It is a generalization of hyperbolic and spherical geometries where the nondegenerate metric tensor is not positive definite. We provide the necessary tools in this geometry and extend gradient method optimization techniques. More specifically, we provide closed-form expressions for distances via geodesics and define a descent direction that guarantees the minimization of the objective problem. Our novel framework is applied to graph representations. We demonstrate its effectiveness on several popular datasets including COVID-19 patient trajectory data and the Clothing1M dataset. The results show that our approach outperforms other state-of-the-art methods in terms of topological accuracy while maintaining low computation costs. Finally, we illustrate how the fundamental concepts of Bayesian inference can be readily implemented in our setting. All code is publicly available at https:\/\/github.com\/sfu","709":"Strong social distancing restrictions have been crucial to controlling the COVID-19 outbreak thus far, and the next question is when and how to relax these restrictions. A sequential timing of relaxing restrictions across groups is explored in order to identify policies that simultaneously reduce health risks and economic stagnation relative to current policies. The goal will be to mitigate health risks, particularly among the most fragile sub-populations, while also managing the deleterious effect of restrictions on economic activity on local businesses and policymakers. Towards this end, several mathematical models for heterogeneous, clustered networks with random link activation\/deletion dynamics are developed, which characterize different types of cluster transitions may help inform decision making. In addition, we study the impact of clustering algorithms on the spread of misinformation, where false information can persist virtually unmitigated by any active individual spreading it. Our results indicate that strategies that consider both correlation and heterogeneity between individuals can provide reliable guidance for enhancing mitigation of health risks, especially during periods of heightened uncertainty such as the one starting in April 2020. Finally, our simulations propose a novel method to efficiently compute optimal release policies, which can be readily implemented by existing tools for policy planning. Taken together, our work not only presents important insights into the nature of appropriate public health interventions, but","710":"Diagnostic tests are almost never perfect. Studies quantifying their performance use knowledge of the true health status, measured with a reference diagnostic test. Researchers commonly assume that the reference test is perfectly specific, and thus estimates the prevalence of the underlying disease using only the binary outcome of the test. However, because the assumption is often violated in practice, researchers have little confidence on the validity of the reference test. In this paper, we study the problem of assessing confidence for Diagnostic Tests when it arises from misclassification parameters. We propose a framework for making inference about whether or not a model has correctly classified a population using a Bayesian meta-analysis approach. We apply our methodology to analyse data from a seroprevalence survey of SARS-CoV-2 in undiagnosed adults between May and July 2020. Our analysis shows that the 95% credible interval for the estimated reproduction number falls inside the range of what would be reasonable if the reference test was indeed perfect. Our findings support the argument that false negatives do not make up all of the information needed to inform decisions based on the results of a single test. A key finding is that even though the original intent of the test is to classify individuals who are currently healthy, there is no clear evidence","711":"The employment status of billions of people has been affected by the COVID epidemic around the Globe. New evidence is needed on how to mitigate the job market crisis, but there exists only a handful of studies mostly focusing on developed countries. We fill in this gap in the literature by using novel data from Ukraine, a transition country in Eastern Europe, which enacted strict quarantine policies early on. We model four binary outcomes to identify respondents (i) who are not working during quarantine, (ii) those who are more likely to work from home, (iii) respondents who are afraid of losing a job, and, finally, (iv) survey participants who have savings for 1 month or less if quarantine is further extended. Our findings suggest that respondents employed in public administration, programming and IT, as well as highly qualified specialists, were more likely to secure their jobs during the quarantine. Females, better educated respondents, and those who lived in Kyiv were more likely to work remotely. Working in the public sector also made people more confident about their future employment perspectives. Although our findings are limited to urban households only, they provide important early evidence on the correlates of job market outcomes, expectations, and financial security, indicating potential deterioration of socio-economic inequalities. These results may be used to inform","712":"A self-exciting spatio-temporal point process is fitted to incident data from the UK National Traffic Information Service to model the rates of primary and secondary accidents on the M25 motorway in a 12-month period during 2017-18. This process uses a background component to represent primary accidents, and a self-exciting component to represent secondary accidents. The background consists of periodic daily and weekly components, a spatial component and a long-term trend. The self-exciting components are decaying, unidirectional functions of space and time. These components are determined via kernel smoothing and likelihood estimation. Temporally, the background is stable across seasons with a daily double peak structure reflecting commuting patterns. Spatially, there are two peaks in intensity, one of which becomes more pronounced during the study period. Self-excitation accounts for 6-7% of the data with associated time and length scales around 100 minutes and 1 kilometre respectively. In-sample and out-of-sample validation are performed to assess the model fit. When we restrict the data to incidents that resulted in large speed drops on the network, the results remain coherent. We then apply a Bayesian framework to estimate the parameters of the model. The resulting model successfully models the","713":"Modeling the spatiotemporal nature of the spread of infectious diseases can provide useful intuition in understanding the time-varying aspect of the disease spread and the underlying complex spatial dependency observed in people's mobility patterns. Besides, the county level multiple related time series information can be leveraged to make one prediction on an individual time series. Adding to this challenge is the fact that real-time data often deviates from the unimodal Gaussian distribution assumption and may show some complex mixed patterns. Motivated by this, we develop a deep learning-based time-series model for probabilistic forecasting called Auto-regressive Mixed Density Dynamic Diffusion Network(ARM3Dnet), which considers both people's mobility and disease spreading as a diffusion process on a dynamic directed graph. The Gaussian Mixture Model layer is implemented to consider the multimodal nature of the real-time data while learning from multiple related time series. We demonstrate our model with several experiments including a large variety of evaluation metrics, and show superior performance compared with state-of-the-art models. Furthermore, we apply our model to predict next day death counts and intraday fatality rates collected from the COVID-19 database. The results validate the practicality of our proposed","714":"Compatibility checkers have been widely used in mobile apps, ranging from location-based services, to social media and navigation. A wide range of methods have been applied for detecting possible cross-device incompatibilities between mobile apps and installed web pages on the fly (e.g., scrollbar, button, etc). However, there has hardly been research focusing on the problem of detecting mismatches between mobile app instances and websites, which is very different from either understanding of what kind of mismatch or fixing any potential issues with those apps. In this paper, we present an experimental study to systematically assess the performance of methods designed to detect smartphone incompatibilities under varying network\/environmental conditions. We focus on two main aspects: (1) generalization ability of these methods, i.e., whether they can be generalized to other devices; (2) platform heterogeneity among the tested problems, i.e., the diversity of underlying implementation options provided by the vendors. Our results show that all tested methods are able to generate positive responses from both healthy and infected hosts, and perform comparably well on their target tasks. Furthermore, we document several new findings based on our results, such as the fact that recursive resampling does not significantly improves performance, and that round-trip","715":"Communication, whether physical or virtual, has always been about as important as ever in human history. The ubiquity of information communication via social media platforms and our reliance on them have increased during COVID-19 periods. This study examines how students develop their own version of the Who's message to us through analysis of 427 student comments posted by the social media platform Twitter while ensembling news from multiple sources. We found that students mainly focus on what branch of physics they pursue with this inquiry being primarily interested in quantum mechanics over heretical topics such as phase field crystal models. In addition, we identified four themes in the discussion concerning the WHO's work,\"The Chemical Industry in India\", which makes it essential for schools to understand the importance of these industries in society. With further investigation into the meaning and sentiment of the tweets, we find that the term\"chemistry\"in the context of quantum mechanics is very much similar to both the word\"virus\"and the concept of chemical reaction dynamics. Therefore, schools need to be able to use the concepts presented in the text to help educate people on science and technology - one of the keys to achieving a more sustainable world. Furthermore, this study provides another layer of understanding about the ways in which a college\/university campus","716":"Automated Deduction in Geometry (ADG) is a forum to exchange ideas and views, to present research results and progress, and to demonstrate software tools at the intersection between geometry and automated deduction. Relevant topics include (but are not limited to): polynomial algebra, invariant and coordinate-free methods; probabilistic, synthetic, and logic approaches, techniques for automated geometric reasoning from discrete mathematics, combinatorics, and numerics; interactive theorem proving in geometry; symbolic and numeric methods for geometric computation, geometric constraint solving, automated generation\/reasoning and manipulation with diagrams; design and implementation of geometry software, automated theorem provers, special-purpose tools, experimental studies; applications of ADG in mechanics, geometric modelling, CAGD\/CAD, computer vision, robotics and education. Traditionally, the ADG conference is held every two years. The previous editions of ADG were held in Nanning in 2018, Strasbourg in 2016, Coimbra in 2014, Edinburgh in 2012, Munich in 2010, Shanghai in 2008, Pontevedra in 2006, Gainesville in 2004, Hagenberg in 2002, Zurich in 2000, Beijing in 1998, and Toulouse in 1996. The 13th edition of ADG was","717":"Monotone matrices play a key role in the convergence theory of regular splittings and different types of weak regular splittings. If monotonicity fails, $K$-monotonicity is sufficient for the convergence of the above-mentioned classes of matrices. However, if $K$-monotonicity is impossible, $L_2$-norm has to be used instead. In this paper, we study the problem of $K$-regular and $L_2$-weak regular splittings of type II (even if in standard generalization settings), i.e., where both the eigenvalues and eigenvectors of the Hamiltonian turn out to be unbounded. We show that under certain conditions, $K$-regular and $L_2$-weak regular splittings are equal up to a bi-Lipschitz reparameterization. As a significant consequence, we obtain a lower bound on the maximal norm of a matrix whose entries are bounded by the largest eigenvalue of a random unitary matrix. This lower bound is expressed in terms of the quadratic form of the corresponding Jacobi iterative method. To establish this lower bound, we provide a characterization of the class","718":"Emergency Departments (EDs) overcrowding is a well recognized worldwide phenomenon. The consequences range from long waiting times for visits and treatment of patients up to life-threatening health conditions. The international community is devoting greater and greater efforts on analyzing this phenomenon aiming at reducing waiting times, improving the quality of the service. Within this framework, we propose a Discrete Event Simulation (DES) model to study the patient flows through a medium-size ED located in a region of Central Italy recently hit by a severe earthquake. In particular, our aim is to simulate unusual ED conditions, corresponding to critical events (like a natural disaster) that cause a sudden spike in the number of patient arrivals. The availability of detailed data concerning the ED processes enabled to build an accurate DES model and to perform extensive scenario analyses. The model provides a valid decision support system for the ED managers also in defining specific emergency plans to be activated in case of mass casualty disasters. We illustrate how the proposed model can describe different ED states under special conditions, such as a peak demand period with 35% increase of the total incoming patients. Finally, we test the model performance using a validation dataset associated with the recent COVID-19 outbreak, showing that the proposed model has been able to correctly describe the available facts","719":"Redox flow batteries (RFBs) have gained popularity as large-scale energy storage systems for wind and solar powered grids. Modern RFB systems are based on highly corrosive and\/or flammable electrolytes. Organic redox active species for RFBs are gaining commercial traction, but there is a trade-off in choosing aqueous or non-aqueous electrolytes in terms of rate capabilities, energy density, safety and cost. While modification of organic redox molecules to mitigate these issues is prevalent in the literature, the search for novel electrolyte systems is scarce. Here we present microemulsion-based electrolytes as an alternative for the next generation of organic RFBs. Micro-emulsion electrolytes (MEs) offer the advantages of decoupled solubility and ionic conductivity, broad electrochemical windows, non-flammability, simple fabrication modes and low cost of constituent chemicals. The electrochemical properties of organic redox species in MEs have been investigated for RFB requirements and a proof-of-concept flow cell with a widely studied redox system and a commercial membrane is shown. The compositions of MEs can be tailored to the redox system, making them a platform of electrolytes for all organic redox systems","720":"The significance of social media has increased manifold in the past few decades as it helps people from even the most remote corners of the world stay connected. With the COVID-19 pandemic raging, social media has become more relevant and widely used than ever before, and along with this, there has been a resurgence in the circulation of fake news and tweets that demand immediate attention. In this paper, we describe our Fake News Detection system that automatically identifies whether a tweet related to COVID-19 is\"real\"or\"fake\", as a part of CONSTRAINT COVID19 Fake News Detection in English challenge. We have used an ensemble model consisting of pre-trained models that has helped us achieve a joint 8th position on the leader board. We have achieved an F1-score of 0.9831 against a top score of 0.9869. Post completion of the competition, we have been able to drastically improve our system by incorporating a novel heuristic algorithm based on username handles and link domains in tweets fetching an F1-score of 0.9883 and achieving state-of-the art results on the given dataset. We also conduct ablation studies on the basis of individual components for each one of the models incorporated into the ensemble. Among","721":"In this paper, we present factor models used to understand the heterogeneity in the effects of social distancing measures for COVID-19 related cases and deaths in the Mexican territory during 2020. Specifically, the analysis was guided by four objectives: (1) To describe the heterogeneous pattern of the incidence of COVID-19 across the Mexican territory; (2) To incorporate information on non-pharmaceutical interventions - such as stay-at-home orders, school closures, and mobility restrictions - into the model with the incorporation of vaccination data; (3) To determine the role of geographic proximity in the relationship between the reproductive number Rt(t) and the percentage of vaccinated people t(-%) among all detected cases aged 15-49y; and (4)To use these results to guide policy makers to make public health decisions with adequate consideration of the heterogeneous nature of the transmission dynamics of COVID-19. The findings show a very high degree of spatial heterogeneity in the relationship between the reproductive number Rt(t) and the percentages of both population groups at risk of infection. Furthermore, the estimates indicate that vaccinations are mostly associated with the highest mortality rates throughout the study period. The current framework provides new insights into the effectiveness of the heterogeneous disease control strategies","722":"This paper introduces the Sequential Monte Carlo Transformer, an original approach that naturally captures the observations distribution in a recurrent architecture. The keys, queries, values and attention vectors of the network are considered as the unobserved stochastic states of its hidden structure. This generative model is such that at each time step the received observation is a random function of these past states in a given attention window. In this general state-space setting, we use Sequentially Monte Carlo methods to approximate the posterior distributions of the states given the observations, and then to estimate the gradient of the log-likelihood. We thus propose a generative model providing a predictive distribution, instead of a single-point estimation. We show through simulations that our model achieves better performance than other traditional neural network models for forecasting tasks. We also demonstrate how our model can be readily applied on different forecast problems, with both simulated and real observed data. Code available at https:\/\/github.com\/akshat57\/seq2vec. This code will be released when you want to test your model on any data set containing half or more parameters than 1000. Additionally, we provide guidelines on how to select the best hyperparameters in order to obtain the most accurate posterior approximation under a specific cost structure. Finally","723":"One of the new scientific ways of understanding discourse dynamics is analyzing the public data of social networks. This research's aim is Post-structuralist Discourse Analysis (PDA) of Covid-19 phenomenon (inspired by Laclau and Mouffe's Discourse Theory) by using Intelligent Data Mining for Persian Society. The examined big data is five million tweets from 160,000 users of the Persian Twitter network to compare two discourses. Besides analyzing the tweet texts individually, a social network graph database has been created based on retweets relationships. We use the VoteRank algorithm to introduce and rank people whose posts become word of mouth, provided that the total information spreading scope is maximized over the network. These users are also clustered according to their word usage pattern (the Gaussian Mixture Model is used). The constructed discourse of influential spreaders is compared with the most active users. This analysis is done based on Covid-related posts over eight episodes. Also, by relying on the statistical content analysis and polarity of tweet words, discourse analysis is done for the whole mentioned subpopulations, especially for the top individuals. The most important result of this research is that the Twitter subjects' discourse construction is government-based rather than community-based. The analyzed","724":"The 2008 financial crisis and then the COVID-19 pandemic have changed our lives in unprecedented ways. In almost every industry today, sectors such as banking, insurance, medical, etc., are seeing a huge surge in demand from investors due to both these events and the subsequent economic recessions. To this end, the stock markets are also surging with respect to their previous records. The phenomenon known as `rebound effect' exists in all probability for an investor's choice of assets and even during this pandemic. We propose a novel framework to study this phenomenon by using the daily logs of one of the largest capital market indices, Nifty Financial Instruments (NPI), which covers the entire supply of face-to-face transactions. We create a new ground truth via real fund flow data and load it into the NPI dataset. Then we apply several supervised machine learning techniques for predicting when an individual will perform specific action on those stocks or not. Our results indicate that the choices made by some investors can be very profitable and hence should be used with extreme caution. Moreover, we find that short term predictions are extremely challenging but do not show any sign of rebounding effects. Hence, we conclude that the development of further extended methods, especially deep learning ones, along with an","725":"Noise pollution has been linked to an increased risk of respiratory diseases, with evidence from recent statistical studies coming from three different disciplines. Here we analyze individual and group-level NO2$_2$ emissions data collected by Sentinel 5P, which is the most advanced ground-based dark energy detector in operation on Earth since 2015. We find that daily average NO2$_2$ emission rates were 38% lower than pre-firework levels, 29% lower than secondary aerosol levels, 22% lower than primary aerosol levels, and 17% lower than atmospheric column CO(2) levels. When accounting for all sources of signal simultaneously, including ground-based and satellite-derived noise sources, the former are observed to be much less active, emitting only 1.1% of the total elemental nuclei spin transition probability matrix elements. In contrast, the latter are found to be highly dominating the physics community, with its spectrum also showing the presence of other transitions metals, such as Cu, Au, and Fe. Both of these have a double peak feature, corresponding to well-defined bands of Lorentzian absorption lines at their respective atomic sites, indicating that they are likely cospatial. Such observations allow us to estimate the fraction of metropolitan areas experiencing elevated","726":"The student peer-group is one of the most important influences on student development. Group work is essential for creating positive learning experiences, especially in remote-learning where student interactions are more challenging. While the benefits of study groups are established, students from underrepresented communities often face challenges in finding social support for their education when compared with those from majority groups. We present a system for flexible and inclusive study group formation that can scale to thousands of students. Our focus is on long-term study groups that persist throughout the semester and beyond. Students are periodically provided opportunities to obtain a new study group if they feel their current group is not a good fit. In contrast to prior work that generates single-use groups, our process enables continuous refinement of groups for each student, which in turn informs our algorithm for future iterations. We trialed our approach in a 1000+ student introductory Electrical Engineering and Computer Science course that was conducted entirely online during the COVID-19 pandemic. We found that of all students matched to study groups through our algorithm, a large majority felt comfortable asking questions (78%) and sharing ideas (74%), while only 10% of students did so spontaneously(58%). Students from underrepresented backgrounds were more likely to request software-matching for study groups when compared with","727":"The degree of malignancy of osteosarcoma and its tendency to metastasize\/spread mainly depend on the pathological grade (determined by observing the morphology of the tumor under a microscope). The purpose of this study is to use artificial intelligence to classify osteosarcoma histological images and to assess tumor survival and necrosis, which will help doctors reduce their workload, improve the accuracy of osteosarcoma cancer detection, and make a better prognosis for patients. The study proposes a typical transformer image classification framework by integrating noise reduction convolutional autoencoder and feature cross fusion learning (NRCA-FCFL) to classify osteosarcoma histological images. Noise reduction convolutional autoencoder could well denoise histological images of osteosarcoma, resulting in more pure images for osteosarcoma classification. Moreover, we introduce two new benchmarks, including one with pixel level labels instead of patient level ones, to validate the performance of the proposed approach. Extensive experiments conducted on the OSIC dataset show that our method outperforms several state-of-the-art traditional and deep learning approaches on both survival and necrosis prediction tasks. We also explore different visualization techniques to provide visual explanations","728":"This work covers a five-year period of intensive collaboration between epidemiological modellers and visualization researchers by documenting and reflecting upon knowledge constructs -- a series of ideas, approaches and methods taken from existing visualization research and practice -- deployed and developed to support modelling of the COVID-19 pandemic. Structured independent commentary on these efforts is synthesized through iterative reflection to develop: evidence of the effectiveness and value of visualization in this context; open problems upon which the research communities may focus; guidance for future activity of this type; and recommendations to safeguard the achievements and promote, advance, secure and prepare for future collaborations of this kind. In describing and comparing a series of related projects that were undertaken in unprecedented conditions, our hope is that this unique report, and its rich interactive supplementary materials, will guide the scientific community in embracing visualization in its observation, analysis and modelling of data as well as in disseminating findings. Equally we encourage the visualization community to engage with impactful science in addressing its emerging challenges. If we are successful, this showcase of activity may stimulate mutually beneficial engagement between communities with complementary expertise to address problems of significance in epidemiology and beyond. https:\/\/ramp-vis.github.io\/RAMPVIS-PhilTransA-Supplement\/index.","729":"This paper investigates the cointegration between possible determinants of crude oil futures prices during the COVID-19 pandemic period. We perform comparative analysis of WTI and newly-launched Shanghai crude oil futures (SC) via the Autoregressive Distributed Lag (ARDL) model and Quantile Autoregressive Distributed Lag (QARDL) model. The empirical results confirm that economic policy uncertainty, stock markets, interest rates and coronavirus panic are important drivers of WTI futures prices. Our findings also suggest that the US and China's stock markets play vital roles in movements of SC futures prices. Meanwhile, CSI300 stock index has a significant positive short-run impact on SC futures prices while S\\&P500 prices possess a positive nexus with SC futures prices both in long-run and short-run. Overall, these empirical evidences provide practical implications for investors and policymakers. Specifically, they make it more convenient to obtain insurance from different sources than official statistics which can be costly and difficult to access timely data. Moreover, the development of machine learning techniques may shed light on the future dynamics of SC futures prices based on econometric methods. It will be interesting to see if this becomes feasible. Narrow bid-ask spreads and deep learning have","730":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\nHow is the analysis of the surface web marketplace performed?\n### Input:\nCOVID Super Expert\n### Response: Cybercrime is continuously growing in numbers and becoming more sophisticated. Currently, there are various monetisation and money laundering methods, creating a huge, underground economy worldwide. A clear indicator of these activities is online marketplaces which allow cybercriminals to trade their stolen assets and services. While traditionally these marketplaces are available through the dark web, several of them have emerged in the surface web. In this work, we perform a longitudinal analysis of a surface web marketplace. The information was collected from victims of 87 different crime categories during 2020 and within 2021. We analyse the products traded in such markets, their prices, their availability, and the exchange currency. Our results indicate that illegal practices are leveraged in surface marketplaces and that there are not effective mechanisms towards their takedown at the time of writing. We also found that product titles including\"cybersecurity\",\"financial security\", and\"social media platforms\"are highly associated with the illicit trades. Furthermore, we present strategies to detect fake transactions and false deliveries for sale-offering fraud as well as untransomware attacks. To aid further research, we release our anonymised dataset to the community. This data may be used to help researchers to better understand the dynamics of surface crimes, among others. Our analysis indicates that it","731":"We consider online resource allocation under a typical non-profit setting, where limited or even scarce resources are administered by a not-for-profit organization like a government. We focus on the internal-equity by assuming that arriving requesters are homogeneous in terms of their external factors like demands but heterogeneous for their internal attributes like demographics. Specifically, we associate each arriving requester with one or several groups based on their demographics (i.e., race, gender, and age), and we aim to design an equitable distributing strategy such that every group of requesters can receive a fair share of resources proportional to a preset target ratio. We present two LP-based sampling algorithms and investigate them both theoretically (in terms of competitive-ratio analysis) and experimentally based on real COVID-19 vaccination data maintained by the Minnesota Department of Health. Both theoretical and numerical results show that our LP-based sampling strategies can effectively promote equity, especially when the arrival population is disproportionately represented, as observed in the early stage of the COVID-19 vaccine rollout. Finally, we highlight a few practical implications of this work, including the possibility that future governments may rely on demographic information to make resource allocations more equitable. Together, these ideas offer a new perspective on tackling large scale emergency","732":"The COVID-19 pandemic has been characterized by the generation of large volumes of viral genomic data at an incredible pace due to recent advances in high-throughput sequencing technologies, the rapid global spread of SARS-CoV2, and its persistent threat to public health. However, distinguishing the most epidemiologically relevant information encoded in these vast amounts of data requires substantial effort across the research fields. Studies of SARS-CoV2 genomes have been critical in tracking the spread of variants and understanding its epidemic dynamics, and may prove crucial for controlling future epidemics and alleviating significant public burdens on healthcare systems. Together, genomic data and bioinformatics methods enable broad-scale investigations of the spread of SARS-CoV2 at the local, national, and global scales and allow researchers the ability to efficiently track the emergence of novel variants, reconstruct epidemic dynamics, and provide important insights into drug and vaccine development and disease control. Here, we discuss the tremendous opportunities that genomics offers to unlock the effective use of SARS-CoV2 genome data for efficient public health surveillance and guiding timely responses to COVID-19. In particular, we demonstrate how valuable daily updates of model parameters can be used to improve the accuracy and sensitivity of various forecasting models with","733":"When a free falling ping-pong ball collides on a horizontal surface, it loses kinetic energy. The ratio between the height reached by the ball after the collision and the initial height is called restitution coefficient. A method to measure it by using a home-made cathetometer was proposed during the Olimpiadi di Fisica 2018. In this paper we show how to measure it also by using the PhyPhox app and Arduino board. Firstly, we calibrate the system by comparing the output of each test with the total number of balls. Then, we use one of the most popular social media platforms in Italy (Twitter) to share information with other users about the arXiv\"distance ladder\", i.e., the ranking of the nodes in the network according to their probability of transmitting the signal for at least $1$ meters away from themselves. Based on these results, we can estimate the restitution coefficient in two different scenarios at very good accuracy. One of the conclusions is that the factor associated with the restitution coefficient increases if one goes to work or live in close proximity to others. The second scenario is that there is no connection between the arXiv's topology and the social networks' structure. We emphasize that our results are not directly","734":"Door doors are one of the most frequently opened and closed doors in our daily lives, with users entering and exiting them on a regular basis. If they do not manage their personal space well during such open-hours, it can lead to unexpected injuries. The presence of security cameras around the door can be used as a safe measure to mitigate this problem, but its effectiveness seems limited to indoor scenes. To help solve this issue, we propose the\"Binary Spring Balance\"(BSB) puzzle, which consists of two parts - the entrance window and the standing area. We first collect enough data from these windows for three hours at an academic center. Then, we present a deep learning framework to balance the information needs of users safely. Finally, we evaluate the efficacy of the proposed algorithm on real-world images by using different datasets obtained from user interactions at an industrial office. Our results show that the BSB method can provide effective assistance to maintain social distancing while preserving the privacy of people inside the house. More importantly, the system does not cause any increment in computational cost at run-time. This implies that it could be deployed as a stand-alone solution to protect users when they are outside the house. Moreover, the way the algorithm works suggests that it can be","735":"The covid19 pandemic has impacted the whole world, including China's crude oil future market, which is the third most traded crude oil futures after WTI and Brent. As India's first crude oil futures accessible to foreign investors, the Shanghai crude oil futures (SC) have attracted significant interest since launch at the NYSE. The impact of COVID-19 on the new crude oil futures is an important issue for investors and policy makers. Therefore this paper studies the short-term influence of COVID-19 pandemic on SC via multifractal analysis. We compare market efficiency of SC before and during the pandemic with the multifractal detrended fluctuation analysis and other commonly used random walk tests. Then we generate shuffled and surrogate data to investigate the components of multifractal nature in SC. And we examine cross-correlations between SC returns and other financial assets returns as well as SC trading volume changes by the multifractal detrended cross-correlation analysis. The results show that market efficiency of SC and its cross-correlations with other assets increase significantly after the outbreak of COVID-19. Besides that, the sources of its multifractal nature have changed since the pandemic. The findings provide evidence for the short-term","736":"Analysing patterns of engagement among citizen science participants can provide important insights into the organisation and practice of citizen science projects. In particular, methods from statistics and network science can be used to understand different types of user behaviour and user interactions to help the further implementation and organization of community efforts. Using publicly available data from the iNaturalist community and their yearly City Nature Challenges (CNC) from 2017-2020 as an example; we showcase computational methods to explore the spatio-temporal evolution of this citizen science community that typically interacts in a hybrid offline\/online way. In particular, we investigate the user type present in the community along with their interaction types by means of text analysis of comments made by users on YouTube videos. We find interesting differences between the two groups which highlights the need for a more detailed approach to analyse online communities involved in citizen science projects or other citizen science initiatives involving e.g. community radio astronomy together with their potential role in promoting interdisciplinary research. Finally, we highlight how the findings of our study can inform future studies on the dynamics of online citizen science communities and their relationship with public outreach activities such as city design. Our work provides additional evidence and directions for analyzing online citizen science communities' involvement, transparency, diversity and plurality - ultimately enhancing the wider","737":"We present CENELLOPE Large Program for Experimental Science and Applications in Middle East (LAA-MEE) from the National Laboratories of the Republic of Iran to develop and design dual-purpose accelerators capable of addressing research needs in the area of science and engineering with a high degree of flexibility. The goal of this project is to demonstrate the feasibility of designing such mobile devices as well as developing relevant data-driven applications. For this purpose, we have adopted the research workflow presented in Ref. [1]. This includes establishing the theoretical framework for the performance evaluation of the two-stage approach, evaluating the basic reproduction number $R_0$ via the next generation matrix method, and finally using the finite difference method to calculate the computational cost of the device. During the study period, we had planned to build two types of novel accelerators, which would be used on both sides of the spectrum of applications needing fast replication numbers and lower computation costs. However, due to the rapidly evolving pandemic situation, it was difficult to set up experiments where we can test our methodology by accessing existing accelerator resources and discussing the challenges of building a new system for LAA-MEE. Therefore, this paper provides a comprehensive analysis of the generic suitability of synthetic COVID-19 quantum","738":"The significant increase in the number of individuals with chronic ailments (including the elderly and disabled) has dictated an urgent need for an innovative model for healthcare systems. The evolved model will be more personalized and less reliant on traditional brick-and-mortar healthcare institutions such as hospitals, nursing homes, and long-term healthcare centers. The smart healthcare system is a topic of recently growing interest and has become increasingly required due to major developments in modern technologies, especially in artificial intelligence (AI) and machine learning (ML). This paper is aimed to discuss the current state-of-the-art smart healthcare systems highlighting major areas like wearable and smartphone devices for health monitoring, machine learning for disease diagnosis, and the assistive frameworks, including social robots developed for the ambient assisted living environment. Additionally, the paper demonstrates software integration architectures that are very significant to create smart healthcare systems, integrating seamlessly the benefit of data analytics and other tools of AI. The explained developed systems focus on several facets: the contribution of each developed framework, the detailed working procedure, the performance as outcomes, and the comparative merits and limitations. The current research challenges with potential future directions are addressed to highlight the drawbacks of existing systems and the possible methods to introduce novel frameworks, respectively. This review aims at providing comprehensive insights into","739":"In this paper, we develop a framework for difference-in-differences designs with staggered treatment adoption and heterogeneous causal effects. We show that conventional regression-based estimators fail to provide unbiased estimates of relevant estimands absent strong restrictions on treatment-effect homogeneity. We then derive the efficient estimator addressing this challenge, which takes an intuitive\"imputation\"form when treatment-effect heterogeneity is unrestricted. We characterize the asymptotic behavior of the estimator, propose tools for inference, and develop tests for identifying assumptions. Extensions include time-varying controls, triple-differences, and certain non-binary treatments. We show the practical relevance of these insights in a simulation study and an application. Studying the consumption response to tax rebates in the United States, we find that the notional marginal propensity to consume is between 8 and 11 percent in the first quarter -- about half as large as benchmark estimates used to calibrate macroeconomic models -- and predominantly occurs in the first month after the rebate. Consistent with other investigations, we document a flattening of the Phillips curve at the beginning of the second quarter -- two months before the release of the alternative measure of inflation was issued by the Fed. Finally, we consider the problem of multiple treatment","740":"The classic searches for supersymmetry have not given any strong indication to new physics. Therefore CMS is designing dedicated searches to target the more difficult and specific supersymmetry scenarios. This contribution present three such recent studies based on 13 TeV proton-proton collisions recorded with the CMS detector in 2016, 2017 and 2018: a search for heavy gluinos cascading via heavy next-to-lightest neutralino in final states with boosted Z bosons and missing transverse momentum; a search for compressed supersymmetry in final states with soft taus; and a search for compressed, long-lived charginos in hadronic final states with disappearing tracks. The results are consistent with no significant signal for most of these scenarios. However, taking into account the known background from processes with two, three or four vector bosons in addition to the Standard Model (SM) and its minimal extension (MSSM) we show that it generally prefers a high level of nonzero Chern number ($\\mathcal{C}$) and weak scale heights ($\\mathcal{R}_0$), indicating the likelihood of observing pseudosyptotic superfields at this energy. We further perform a detailed analysis of the existing experimental constraints on $\\mathcal{C","741":"The non-equilibrium stationary coherences that form in donor-acceptor systems are investigated to determine their relationship to the efficiency of energy transfer to a neighboring reaction center. It is found that the effects of asymmetry in the dimer are generally detrimental to the transfer of energy. Four types of systems are examined, arising from combinations of localized trapping, delocalized (Forster) trapping, eigenstate dephasing and site basis dephasing. In the cases of site basis dephasing the interplay between the energy gap of the excited dimer states and the environment is shown to give rise to a turnover effect in the efficiency under weak dimer coupling conditions. Furthermore, the nature of the coherences and associated flux are interpreted in terms of pathway interference effects. In addition, regardless of the cases considered, the ratio of the real part and the imaginary part of the coherences in the energy-eigenbasis tends to a constant value in the steady state limit. This behavior is attributed to the symmetry breaking structure of the dimer system due to its finite size. The results obtained employ a detailed classical treatment of the system coupled with a simple derivation of the formula for the macroscopic quantum product operator at the steady state level.","742":"The COVID-19 pandemic has impacted on every human activity and, because of the urgency of finding the proper responses to such an unprecedented emergency, it generated a diffused societal debate. The online version of this discussion was not exempted by the presence of misinformation campaigns, but, differently from what already witnessed in other debates, the COVID-19 -intentional or not- flow of false information put at severe risk the public health, possibly reducing the efficacy of government countermeasures. In this manuscript, we study the effective impact of misinformation in the Italian societal debate on Twitter during the pandemic, focusing on the various discursive communities. In order to extract such communities, we start by focusing on verified users, i.e., accounts whose identity is officially certified by Twitter. We start by considering each couple of verified users and count how many unverified ones interacted with both of them via tweets or retweets: if this number is statically significant, i.e. so great that it cannot be explained only by their activity on the online social network, we can consider the two verified accounts as similar and put a link connecting them in a monopartite network of verified users. The discursive communities can then be found by running a community detection algorithm on this network.","743":"The COVID-19 pandemic has caused us to adopt social distancing and stay at home. This policy measure has put significant strain on the economy and people's daily lives. An alternative to widespread lockdowns is effective contact tracing during an outbreak's early stage. However, mathematical models suggest that epidemic control for SARS-CoV-2 transmission with manual contact tracing is implausible. To reduce the effort of contact tracing, many digital contact tracing projects (e.g., PEPP-PT, DP-3T, TCN, BlueTrace, Google\/Apple Exposure Notification, and East\/West Coast PACT) are being developed to supplement manual contact tracing. However, digital contact tracing has drawn scrutiny from privacy advocates, since governments or other parties may attempt to use contact tracing protocols for mass surveillance. As a result, many digital contact tracing projects build privacy-preserving mechanisms to limit the amount of privacy-sensitive information leaked by the protocol. In this paper, we examine how these architectures resist certain classes of attacks, specifically DoS attacks, and present BlindSignedIDs, a novel method to construct lightweight and verifiable contact tracing identifiers. During our investigation, we found that virtually all existing solutions fail to show compliance with the Privacy-Preserving","744":"The U.S. air transportation network (ATN) is critical to the mobility and the functioning of the United States. It is thus necessary to ensure that it is well-connected, efficient, and robust. Despite extensive research on its topology, the temporal evolution of the network's robustness and tolerance remains largely unexplored. In the present paper, a temporal study of the domestic U.S. ATN was performed based on annual flight data from 1996 to 2016 and network analytics were used to examine the effects of restructuring that followed the 9\/11 events along with the current state of the system. Centrality measures were computed to assess the system's topology and its global robustness. A node deletion method was applied to assess the network's tolerance by simulating a targeted attack scenario. The study showed that the 9\/11 terrorist attacks triggered vast restructuring of the network, in terms of efficiency and security. Air traffic expanded, as new airports and air routes were introduced. Airlines reconsidered their strategy and optimized their operations, thus allowing the network to recover rapidly and become even more efficient. Security concerns resulted in significant improvement of the network's robustness. Since 2001, the global traffic and topological properties of the U.S. ATN have displayed continuous growth","745":"This paper focuses on challenges of federated learning (FL) and its applications to statistical modeling, with particular emphasis on the interconnection between FL and the central server. We define several descriptive statistics based on the information contained in the historical part of the log file. Additionally, we describe how these basic statistics can be used for more detailed analysis by finding the eigenvalues of the centralized Laplace operator. Furthermore, we provide a short overview of recent developments on FL-based statistical inference, focusing on the similarities and differences among existing methods. Finally, we discuss the applicability of FL to simulate stochastic processes such as population growth or epidemics spread. The results presented here are illustrated via simulations using two different examples from the literature. In addition, we present a case study where the objective function value is explicitly computed against the average precision loss defined over multiple populations. Moreover, we also compare our proposed approach with some other well known alternatives in terms of both model complexity and quality. Apart from computational efficiency, we show that our method provides good economic insights into the dynamics of FL with a reasonable level of accuracy. Thereby, we will mainly focus on addressing practical issues related to FL's use in economics, such as the construction of appropriate meshing schemes, implementation of complex algorithms","746":"The outbreak of the novel coronavirus disease 2019 (COVID-19) has caused unprecedented impacts to people\u2019s daily life around the world. Various measures and policies such as lockdown and social-distancing are implemented by governments to combat the disease during the pandemic period. These measures and policies as well as virus itself may cause different mental health issues to people such as depression, anxiety, sadness, etc. In this paper, we exploit the massive text data posted by Twitter users to analyse the sentiment dynamics of people living in the state of New South Wales (NSW) in Australia during the pandemic period. Different from the existing work that mostly focuses on the country-level and static sentiment analysis, we analyse the sentiment dynamics at the fine-grained local government areas (LGAs). Based on the analysis of around 94 million tweets that posted by around 183 thousand users located at different LGAs in NSW in 5 months, we found that people in NSW showed an overall positive sentimental polarity and the COVID19-CT with the highest F1 score of 0.95. The fine-grained analysis of sentiment in LGAs found that despite the dominant positive sentiment most of days during the study period, some LGAs experienced significant sentiment changes from positive to","747":"During the SARS-CoV-2 pandemic, theoretical high-energy physics, and likely also the majority of other disciplines, are seeing a surge of virtual seminars as a primary means for scientific exchange. In this brief article, we highlight some compelling benefits of virtualizing research talks, and argue for why virtual seminars should continue even after the pandemic. Based on our extensive experience on running online talks, we also summarize some basic guidelines on organizing virtual seminars, and suggest some directions in which they could evolve. We believe that these considerations will be useful both to researchers who want to organize academic conferences online, and to people who would like to donate themselves and opposed donations in their footprints. As scientists, we hope that better than the current situation, our paper can serve as a model for future situations, and thus it can help guide the design of future meetings, including those guided by the general principles of physical vs. virtual sciences. For clarity, we distinguish two important types of meeting scenarios, one where all participants have the possibility to interact with each other physically, and another where all participants may access the Internet (or local intranet) during the conference. We also discuss how to make sure that your organization's communication channels are sufficient to accommodate different sizes of discussions,","748":"We present the design and analysis of an ongoing project, named VIDAR-19, able to extract automatically diseases from the CORD-19 dataset, and also diseases which might be considered as risk factors. The project relies on the ICD-11 classification of diseases maintained by the WHO. This nomenclature is used as a data source of the extraction mechanism, and also as the repository for the results. Developed for the COVID-19, the project has the ability to extract diseases at risk and to calculate relevant indicators. The outcome of the project is presented in a dashboard which enables the user to explore graphically diseases at risk which are put back into the classification hierarchy. Beyond the COVID-19, VIDAR has much broader applications and might be directly applied for any corpus dealing with other pathologies. Automated disease detection based on medical text can greatly improve quality of care, and facilitate timely treatment for patients. This paper presents the architecture of the project, and reports on the performance of the first release (D1) and its extensions (D2). Both sets of models achieve high accuracy scores, reaching values close to 99% for the former and 80% for the latter. With respect to the extracted diseases, we find that some patterns appear very","749":"We present FACESEC, a framework for fine-grained robustness evaluation of face recognition systems. FACESEC evaluation is performed along four dimensions of adversarial modeling: the nature of perturbation (e.g., pixel-level or face accessories), the attacker's system knowledge (about training data and learning architecture), goals (dodging or impersonating people), and capability (tailored to individual inputs or across sets of these). We use FACESEC to study five face recognition systems in both closed-set and open-set settings, and to evaluate the state-of-the-art approach for defending against physically realizable attacks on these. We find that accurate knowledge of neural architecture is significantly more important than knowledge of the training data in black-box attacks. Moreover, we observe significant performance gains from employing Fuzzy Cognitive Maps instead of fully connected layers as backbones. The efficacy of our method depends on the size of perturbation and the neural network architecture used by the attack detection algorithm. In particular, for the HarmenberT model, FECMA outperforms all other models with a high accuracy score, even when the dataset is cropped to only one million faces. These results are also valid for all other popular face recognition models.","750":"The evolution of social media platforms have empowered everyone to access information easily. Social media users can easily share information with the rest of the world. This may sometimes encourage spread of fake news, which can result in undesirable consequences. In this work, we train models which can identify health news related to COVID-19 pandemic as real or fake. Our models achieve a high precision level (more than 98\\%). We also investigate how different factors explain the uncertainty in the performance of these models. One of the most significant findings is that predictive features alone are not sufficient for the full realization of prediction. To mitigate this problem, we propose a novel approach that uses completeness and diversity measures to enhance the training data set. Overall, our study improves the robustness of existing models by ensuring that they provide reliable predictions for patients' wellbeing and health condition. We further find that some external parameters like size of dataset and source of patient's location significantly influence model's performance. Therefore, we discuss how creating heterogeneous compositions of the data affects the overall performance of machine learning models. Data cleansing via applying unsupervised adversarial attacks achieves improving results for both regular and balanced datasets. Finally, we explore how sampling techniques and apply autoencoder and Siamese pre-trained models","751":"Transport properties of dense fluids are fundamentally challenging, because the powerful approaches of equilibrium statistical physics cannot be applied directly to non-equilibrium conditions. Polar fluids compound this problem, because the long-range interactions preclude the use of a simple effect-diameter approach based solely on hard spheres. Here, we develop a kinetic theory for dipolar hard-sphere fluids that is valid up to high density. We derive a mathematical approximation for the radial distribution function at contact directly from the equation of state, and use it to obtain the shear viscosity. We also perform molecular-dynamics simulations of this system and extract the shear viscosity numerically. The theoretical results compare favorably to the simulations. Our analysis reveals a second order phase transition as in Ref. [1], but we find that the critical exponent differs from the value $R_0=2$ predicted by classical statistics. It is suggested that the reason for this discrepancy is due to the emergence of a new length scale associated with large curvature points. Finally, we show that changing the nature of the fluid leads to a change in the effective viscosity coefficient. Specifically, we observe that the presence of a finite concentration of one species of particles introduces a renormalization of the effective","752":"Many different technologies are used to detect pests in the crops, such as manual sampling, sensors, and radar. However, these methods have scalability issues as they fail to cover large areas, are uneconomical and complex. This paper proposes a crowdsourced based method utilising the real-time farmer queries gathered over telephones for pest surveillance. We developed data-driven strategies by aggregating and analyzing historical data to find patterns and get future insights into pest occurrence. We showed that it can be an accurate and economical method for pest surveillance capable of enveloping a large area with high spatio-temporal granularity. Forecasting the pest population will help farmers in making informed decisions at the right time. This will also help the government and policymakers to make the necessary preparations as and when required and may also ensure food security. As the future work focuses on developing machine learning techniques to improve accuracy and economic efficiency of pest surveillance, we suggest that computer vision and deep learning models should be prioritized to further enhance automation of this process. This will allow us to build a more effective tool to protect crop health and reduce its loss during pandemics\/covid-19. Overall, our study demonstrates the potential of crowdourcing platforms for rapid pest surveillance under uncertain conditions. This","753":"Digitalization opens up new opportunities in the collection, analysis, and presentation of data which can contribute to the achievement of the 2030 Agenda and its Sustainable Development Goals (SDGs). In particular, the access to and control of environmental and geospatial data is fundamental to identify and understand global issues and trends. Also immediate crises such as the COVID-19 pandemic demonstrate the importance of accurate health data such as infection statistics and the relevance of digital tools like video conferencing platforms. However, today much of the data is collected and processed by private actors. Thus, governments and researchers depend on data platforms and proprietary systems of big tech companies such as Google or Microsoft. The market capitalization of the seven largest US and Chinese big tech companies has grown to 8.7tn USD in recent years, about twice the size of Germany's gross domestic product (GDP). Therefore, their market power is enormous, allowing them to dictate many rules of the digital space and even interfere with legislations. Based on a literature review and nine expert interviews this study presents a framework that identifies the risks and consequences along the workflow of collecting, processing, storing, using of data. It also includes solutions that governmental and multilateral actors can strive for to alleviate the risks. Fundamental to this framework","754":"The practice of social distancing, even in challenging circumstances such as pre-hurricane evacuation and pandemic transmission, remains a challenge for modern societies. In this work, we present a novel method to reduce the spread of infectious diseases while maintaining a safe distance between individuals by exploiting large-scale collective mobility data collected from mobile phones. We develop a multi-objective approach focusing on both crowding control and contagion prevention objectives. The results show that our approach can provide an effective way to manage epidemic outbreaks without significantly reducing human interaction and global connectivity. This approach can be readily implemented into existing digital contact tracing and vaccination strategies. Finally, we demonstrate how our approach provides practical insights at the scale of city-level transmission of infectious diseases. Specifically, we identify that the most effective methods to minimize outbreak are those that focus on high-risk groups or that require balancing infection control with the risk of disease transmission. Both of these results indicate that our model captures the spatial heterogeneity of epidemics sufficiently well to inform decision makers about suitable protective measures. Our approach, which builds upon mobile phone location data, may help monitor the evolution of epidemics globally and locally by updating the rules of physical proximity based on the dynamic nature of people's mobility patterns. Such information can ultimately lead to better prepared","755":"Deep learning is gaining instant popularity in computer aided diagnosis of COVID-19. Due to the high sensitivity of Computed Tomography (CT) to this disease, CT-based COVID-19 detection with visual models is currently at the forefront of medical imaging research. Outcomes published in this direction are frequently claiming highly accurate detection under deep transfer learning. This is leading medical technologists to believe that deep transfer learning is the mainstream solution for the problem. However, our critical analysis of the literature reveals an alarming performance disparity between different published results. Hence, we conduct a systematic thorough investigation to analyze the effectiveness of deep transfer learning for COVID-19 detection with CT images. Exploring 14 state-of-the-art visual models with over 200 model training sessions, we conclusively establish that the published literature is frequently overestimating transfer learning performance for the problem, even in the prestigious scientific sources. The roots of overestimation trace back to inappropriate data curation. We also provide case studies that consider more realistic scenarios, and establish transparent baselines for the problem. We hope that our reproducible investigation will help in curbing hype-driven claims for the critical problem of COVID-19 diagnosis, and pave the way for a more transparent performance evaluation of techniques for CT-","756":"First identified in Wuhan, China, in December 2019, the outbreak of COVID-19 has been declared as a worldwide emergency in January, and a global pandemic in March 2020 by the World Health Organization (WHO). Along with this pandemic, we are also experiencing an\"infodemic\"of information with low credibility such as fake news and conspiracies. In this work, we present ReCOVery, a repository designed and constructed to facilitate the studies of combating such information regarding COVID-19. We first broadly search and investigate ~2,000 news publishers, from which 61 are identified with extreme [high or low] levels of credibility. By inheriting the credibility of the media on which they were published, a total of 2,029 news articles on coronavirus, published from January to May 2020, are collected in the repository, along with 140,820 tweets that reveal how these news articles are spread on the social network. The repository provides multimodal information of news articles on coronavirus, including textual, visual, temporal, and network information. The way that news credibility is obtained allows a trade-off between dataset scalability and label accuracy. Extensive experiments are conducted to present data statistics and distributions, as well as to provide","757":"A standing challenge in undergraduate Computer Science curricula is the teaching and learning of computer programming. Through this paper which is an essay about programming, we aim to contribute to the plethora of existing pedagogies, approaches and philosophies, by discussing a specific feature of our approach in teaching principled programming to undergraduate students, in their first semester of studies, namely the utilization of pictures, both text-based and raster-based graphics. Although the given course has evolved substantially over the thirty years of its delivery regarding the programming languages (Miranda, C, C++, Java) and paradigms (functional, imperative, object-oriented, combination of procedural and object-oriented) used, the discussed visual feature has been maintained and steadily strengthened. We list abstraction, problem decomposition and synthesis, information hiding, reusability, modularity and extensibility as key principles of problem solving and algorithmic thinking. These principles are closely aligned with the advocated computational thinking techniques of problem decomposition, pattern recognition, pattern generalization and algorithm design. We aim for our students to familiarize themselves with all the above principles through practical problem solving. Our ongoing inquiry has been whether the problem domain of pictures is contributing valuably towards this aim. Moreover, an added-value is that students","758":"We present a framework for using existing data to estimate the severity of a pandemic such as COVID-19 in Mexico City and assess its risk with respect to human mobility within the city. The analysis allows us to explore how lockdown policies affect the spread of the disease and how changes in human mobility might affect the transmission dynamics. Our main contributions are (1) a detailed mechanistic model of the impact of social distancing on the course of the epidemic, including quantification of the degree of success\/failure of the mitigation strategies; (2) estimation of the number of deaths due to the outbreak; and (3) prediction of the course of the epidemic until June 2020 assuming no new case occurrences. We also discuss the relevance of this modeling framework with regard to the current pandemic development in Mexico City. Finally, we make available a unique dataset from Telef\\'onica Intercetiones de Sana Distancia (TIdeS), consisting of over 152 million anonymized mobile device users' visit trajectories to various places in the city, including homes, restaurants, shopping centers, residential areas, and transportation stations. This work enables us to investigate the effect of population size, distance between home and business locations, and other parameters related to the evolution of the epidem","759":"Blood oxygen saturation (SpO$_2$) is an essential indicator of respiratory functionality and is receiving increasing attention during the COVID-19 pandemic. Clinical findings show that it is possible for COVID-19 patients to have significantly low SpO$_2$ before any obvious symptoms. The prevalence of cameras has motivated researchers to investigate methods for monitoring SpO$_2$ using videos. Most prior schemes involving smartphones are contact-based: They require a fingertip to cover the phone's camera and the nearby light source to capture re-emitted light from the illuminated tissue. In this paper, we propose the first convolutional neural network based noncontact SpO$_2$ estimation scheme using smartphone cameras. The scheme analyzes the videos of a participant's hand for physiological sensing, which is convenient and comfortable, and can protect their privacy and allow for keeping face masks on. We design our neural network architectures inspired by the optophysiological models for SpO$_2$ measurement and demonstrate the explainability by visualizing the weights for channel combination. Our proposed models outperform the state-of-the-art model that is designed for contact-based SpO$_2$ measurement, showing the potential of our proposed method to contribute to","760":"With the ease of access to information on social media, and its rapid dissemination over the internet (both velocity and volume), it has become challenging to filter out truthful information from fake ones. The research community is now faced with the task of automatic detection of fake news, which carries real-world socio-political impact. One such research contribution came in the form of the Constraint@AAA12021 Shared Task on COVID19 Fake News Detection in English. In this paper, we shed light on a novel method we proposed as a part of this shared task. Our team introduced an approach to combine topical distributions from Latent Dirichlet Allocation (LDA) with contextualized representations from XLNet. We also compared our method with existing baselines to show that XLNet + Topic Distributions outperforms other approaches by attaining an F1-score of 0.967. A qualitative analysis validates the superiority of our model to capture the topic content correctly. This motivates us to further conduct an extensive quantitative study on our proposed method. We empirically demonstrate that our XLNet+Topic Distributions model achieves state-of-the-art results on the given benchmark datasets. Additional analyses on different development subsets validate the generalizability of our model across","761":"One of the main approaches to control and slow down the spread of COVID-19 has been automated contact tracing. Since proximity data can be collected by personal mobile devices, the natural proposal has been to use this for contact tracing as this provides a major gain over a manual implementation. In this work we study the characteristics of automated contact tracing and its effectiveness for mapping the spread of a pandemic due to the spread of SARS-CoV-2. We highlight the infrastructure and social structures required for automated contact tracing to work for the current pandemic. We display the vulnerabilities of the strategy to inadequately sample the population, which results in the inability to sufficiently determine significant contact with infected individuals. Of crucial importance will be the participation of a significant fraction of the population for which we derive a minimum threshold. We conclude that a strong reliance on contact tracing to contain the spread of the SARS-CoV-2 pandemic can lead to the potential danger of allowing the pandemic to spread unchecked. A carefully thought out strategy for controlling the spread of the pandemic along with automated contact tracing can lead to an optimal solution. The paper presents strategies and ideas for future research and policy development to further mitigate the spread of the pandemic. Guidelines for ethical contact tracing are also discussed","762":"India accounts for 11% of maternal deaths globally where a woman dies in childbirth every fifteen minutes. Lack of access to preventive care information is a significant problem contributing to high maternal morbidity and mortality numbers, especially in low-income households. We work with ARMMAN, a non-profit based in India, to further the use of call-based information programs by early-on identifying women who might not engage on these programs that are proven to affect health parameters positively.We analyzed anonymized mobile phone data from over 300 calls received by him since 2016 to understand different factors such as the nature of the program, its benefits, limitations, and whether it reaches the intended population. Our findings indicate that the majority of calls are concentrated towards the performance of individual users, and about 10% are made available to the rest of the world through a public library. The math analysis on our dataset suggests that the leading cause of this concentration is due to non-random sampling of positive cases, and the number of calls in certain categories is much higher than what is being register with other catalogs of behaviors. This presents a great opportunity to study human behavior called under programming during COVID-19 crisis measures, and also highlights the need to develop novel approaches and methods to enable the ubiquitous availability of fine","763":"We analyse the distribution and the flows between different types of employment (self-employment, temporary, and permanent), unemployment, education, and other types of inactivity, with particular focus on the duration of the school-to-work transition (STWT). The aim is to assess the impact of the COVID-19 pandemic in Italy on the careers of individuals aged 15-34. We find that the pandemic worsened an already concerning situation of higher unemployment and inactivity rates and significantly longer STWT duration compared to other EU countries, particularly for females and residents in the South of Italy. In the midst of the pandemic, individuals aged 20-29 were less in (permanent and temporary) employment and more in the NLFET (Neither in the Labour Force nor in Education or Training) state, particularly females and non Italian citizens. We also provide evidence of an increased propensity to return to schooling, but most importantly of a substantial prolongation of the STWT duration towards permanent employment, mostly for males and non Italian citizens. Our contribution lies in providing a rigorous estimation and analysis of the impact of COVID-19 on the carriers of young people in Italy, which has not yet been explored in the literature. Furthermore, we propose a novel methodology to decompose","764":"The so-called block-term decomposition (BTD) tensor model, especially in its rank-$(L_r, L_r, 1)$ version, has been recently receiving increasing attention due to its enhanced ability of representing systems and signals that are composed of \\emph{block} components of rank higher than one, a scenario encountered in numerous and diverse applications. Its uniqueness and approximation have thus been thoroughly studied. The challenging problem of estimating the BTD model structure, namely the number of block terms (rank) and their individual (block) ranks, is of crucial importance in practice and has only recently started to attract significant attention. In data-streaming scenarios and\/or big data applications, where the tensor dimension in one of its modes grows in time or can only be processed incrementally, it is essential to be able to perform model selection and computation in a recursive (incremental\/online) manner. To date there is only one such work in the literature concerning the (general rank-$(L, M, N)$) BTD model, which proposes an incremental method, however with the BTD rank and block ranks assumed to be a-priori known and time invariant. In this preprint, a novel approach","765":"The Advanced Virgo detector has contributed with its data to the rapid growth of the number of detected gravitational-wave signals in the past few years, alongside the two LIGO instruments. First, during the last month of the Observation Run 1 (O1) in August 2017 (with, most notably, the compact binary mergers GW170814 and GW170817) and then during the full Observation Run 2 (O2): an 11 months data taking period, between April 2019 and March 2020, that led to the addition of about 80 events to the catalog of transient gravitational-wave sources maintained by LIGO, Virgo and KAGRA. These discoveries and the manifold exploitation of the detected waveforms require an accurate characterization of the quality of the data, such as continuous study and monitoring of the detector noise. These activities, collectively named {\\em detector characterization} or {\\em DetChar}, span the whole workflow of the Virgo data, from the instrument front-end to the final analysis. They are described in details in the following article, with a focus on the associated tools, the results achieved by the Virgo DetChar group during the O2 run and the main prospects for future data-taking periods with an improved detector. The motivation","766":"In global QCD fits of parton distribution functions (PDFs), a large portion of the estimated uncertainty on the PDFs originates from the choices of parametric functional forms and fitting methodology. We argue that these types of uncertainties can be underestimated with common PDF ensembles in high-stake measurements at the Large Hadron Collider and Tevatron. A fruitful approach to quantify these uncertainties is to view them as arising from sampling of allowed PDF solutions in a multidimensional parametric space. This approach applies powerful insights gained in recent statistical studies of large-scale population surveys and quasi-Monte Carlo integration methods. In particular, PDF fits may be affected by the big data paradox, which stipulates that more experimental data do not automatically raise the accuracy of PDFs -- close attention to the data quality and sampling of possible PDF solutions is as essential. To test if the sampling of the PDF uncertainty of an experimental observable is truly representative of all acceptable solutions, we introduce a technique (\"a hopscotch scan\") based on a combination of parameter scans and stochastic sampling. With this technique, we show that the PDF uncertainty on key LHC cross sections at 13 TeV obtained with the public NNPDF4.0 fitting code is larger than the nominal","767":"We present an ongoing collaboration between epidemiological modellers and visualization researchers by documenting and reflecting upon knowledge constructs -- a series of ideas, approaches and methods taken from existing visualization research and practice -- deployed and developed to support modelling of the COVID-19 pandemic. Structured independent commentary on these efforts is synthesized through iterative reflection to develop: evidence of the effectiveness and value of visualization in this context; open problems upon which the research communities may focus; guidance for future activity of this type; and recommendations to safeguard the achievements and promote, advance, secure and prepare for future collaborations of this kind. In describing and comparing a series of related projects that were undertaken in unprecedented conditions, our hope is that this unique report, and its rich interactive supplementary materials, will guide the scientific community in embracing visualization in its observation, analysis and modelling of data as well as in disseminating findings. Equally we hope to encourage the visualization community to engage with impactful science in addressing its emerging data challenges. If you are interested in reading, exploring or using our material, contact me via email or Twitter. We are also interested in receiving feedback from the visual community at https:\/\/dash.org\/s\/e2b1f6e3a22523b7d0b","768":"Early classification of time series has been extensively studied for minimizing class prediction delay in time-sensitive applications such as healthcare and finance. A primary task of an early classification approach is to classify an incomplete time series as soon as possible with some desired level of accuracy. Recent years have witnessed several approaches for early classification of time series. As most of the approaches have solved the early classification problem with different aspects, it becomes very important to make a thorough review of these solutions to know whether they are really useful in real-world applications like medical science or financial engineering. In this paper, we present a systematic review of current literature on early classification of time series. We divide various existing approaches into four exclusive categories based on their proposed solution strategies. The four categories include prefix-based, shapelet-based, model-based and miscellaneous approaches. The authors also discuss the applications of early classification in many areas including industrial monitoring, intelligent transportation, medical science and financial engineering. Finally, we provide a quick summary of the current literature with future research directions. This article is expected to be a good resource for both researchers and practitioners in providing a wide view of state-of-the-art solutions to early classify time series using modern neural network architectures. All the tools and data are publicly available at","769":"Temporal drift of low-cost sensors is crucial for the applicability of wireless sensor networks (WSN) to measure highly local phenomenon such as air quality. The emergence of wireless sensor networks in locations without available reference data makes calibrating such networks without the aid of true values a key area of research. While deep learning (DL) has proved successful on numerous other tasks, it is under-researched in the context of blind WSN calibration, particularly in scenarios with networks that mix static and mobile sensors. In this paper we investigate the use of DL architectures for such scenarios, including the effects of weather in both drifting and sensor measurement. New models are proposed and compared against a baseline, based on a previous proposed model and extended to include mobile sensors and weather data. Also, a procedure for generating simulated air quality data is presented, including the emission, dispersion and measurement of the two most common particulate matter pollutants: PM 2.5 and PM 10. Results show that our models reduce the calibration error with an order of magnitude compared to the baseline, showing that DL is a suitable method for WSN calibration and that these networks can be remotely calibrated with minimal cost for the deployer. Finally, we highlight the benefits of our methods in different scenarios, including","770":"This record contains the proceedings of the 2020 Workshop on Assessing, Explaining, and Conveying Robot Proficiency for Human-Robot Teaming, which was held in conjunction with the 2020 ACM\/IEEE International Conference on Human-Robot Interaction (HRI). This workshop was originally scheduled to occur in Cambridge, UK on March 23, but was moved to a set of online talks due to the COVID-19 pandemic. The meeting consisted of over forty participants from industry, academia, and government agencies across the globe. These groups included researchers from Europe, the United States, and Canada, as well as people from other continents such as Brazil and India. The goal of this workshop is to bring together leading scientists with interest in human-robot interaction to explain how they have been developed and what challenges they face so far. We hope that this will help promote research into human-robot interaction and robotics policy. Finally, we identify some key themes and identify important frontiers for future work in this area. All collected papers are publicly available at https:\/\/github.com\/mahfuzibnalam\/human_robots_interaction_2020_challenge.git.io\/website\/rpoetry.html.","771":"Currently, the world seeks to find appropriate mitigation techniques to control and prevent the spread of the new SARS-CoV-2. In our paper herein, we present a peculiar Multi-Task Learning framework that jointly predicts the effect of SARS-CoV-2 as well as Personal-Protective-Equipment consumption in Community Health Centres for a given populace. Predicting the effect of the virus (SARS-CoV-2), via studies and analyses, enables us to understand the nature of SARS-CoV- 2 with reference to factors that promote its growth and spread. Therefore, these foster widespread awareness; and the populace can become more proactive and cautious so as to mitigate the spread of Corona Virus Disease 2019 (COVID- 19). Furthermore, understanding and predicting the demand for Personal Protective Equipment promotes the efficiency and safety of healthcare workers in Community Health Centres. Owing to the novel nature and strains of SARS-CoV-2, relatively few literature and research exist in this regard. These existing literature have attempted to solve the problem statement(s) using either Agent-based Models, Machine Learning Models, or Mathematical Models. In view of this, our work herein adds to existing literature via modeling our problem statements as Multi-","772":"We report on multiple analytic studies of the $x$-dependence of parton distributions, in particular quark distribution functions, based on the QCD axion model with Dirichlet boundary conditions. We have investigated the analytic properties of the fixed charge expansion for different conformal limits of the model, and we show that the results obtained remain valid when one rearranges the perturbative quantum theory by using a factorization scheme. Our findings enable calculations of flavor singlet parton distribution functions (PDFs), as well as Monte Carlo simulations of colliding partons, which can be used to study other processes such as the emission of photons or nuclei. This work will provide a framework for studying fundamental physics problems at the level of particle phenomenology, without relying on any theoretical approximations. To obtain the full set of local [Formula: see text] \u2192 \u03c0 formulae, we present a new technique based on the Laplace transform. In this way, all nonperturbative terms related to the Lagrangian quadratic polynomials (LQP) series are cancelled, and hence the Moyal parameters are not changed. For the double spectral density matrix elements, we also find a relation between the LQPs and","773":"Sports data has become widely available in the recent past. With the improvement of machine learning techniques, there have been attempts to use sports data to analyze not only the outcome of individual games but also to improve insights and strategies. The outbreak of COVID-19 has interrupted sports leagues globally, giving rise to increasing questions and speculations about the outcome of this season's leagues. What if the season was not interrupted and concluded normally? Which teams would end up winning trophies? Which players would perform the best? Which team would end their season on a high and which teams would fail to keep up with the pressure? We aim to tackle this problem and develop a solution. In this paper, we proposeUCLData, which is a dataset containing detailed information of UEFA Champions League games played over the past six years. We also propose a novel autoencoder based machine learning pipeline that can come up with a story on how the rest of the season will pan out. Our work shows that our model is able to generate reasonable predictions for several key points in the league protocol. After collecting more than 5 million comments from Twitter, we show that most teams are interested in greater feedback on non-trivial aspects of the game. Finally, we introduce some interesting findings of the research.","774":"We present some ideas on how to extend a kinetic type model for crowd dynamics to account for an infectious disease spreading. We focus on a medium size crowd occupying a confined environment where the disease is easily spread. The kinetic theory approach we choose uses tools of game theory to model the interactions of a person with the surrounding people and the environment and it features a parameter to represent the level of stress. It is known that people choose different walking strategies when subjected to fear or stressful situations. To demonstrate that our model for crowd dynamics could be used to reproduce realistic scenarios, we simulate passengers in one terminal of Hobby Airport in Houston. In order to model disease spreading in a walking crowd, we introduce a variable that denotes the level of exposure to people spreading the disease. In addition, we introduce a parameter that describes the contagion interaction strength and a kernel function that is a decreasing function of the distance between a person and a spreading individual. We test our contagion model on a problem involving a small crowd walking through a corridor. The results show that the model can reproduce very well the actual travel time distributions. Finally, we propose a parametrization method based on Markov chains for solving the contagion model using this tool. This method allows us to efficiently compute the number of infected individuals","775":"The ongoing COVID-19 pandemic has been a major challenge to humanity, with 12 million confirmed cases as of July 13th, 2020 [1]. In previous work, we described how Artificial Intelligence can be used to tackle the pandemic with applications at the molecular, clinical, and societal scales [2]. In the present follow-up article, we review these three research directions, and assess the level of maturity and feasibility of the approaches used, as well as their potential for operationalization. We also summarize some commonly encountered risks and practical pitfalls, as well as guidelines and best practices for formulating and deploying AI applications at different scales. Finally, we discuss emerging opportunities and encourage further research in all three directions. We hope this survey will promote and broaden the use of AI methods, tools, and frameworks, especially within the biomedical domain. The full scope of our discussion is not restricted to epidemiology, but include relevant aspects of it. Since several topics are still relevant, such as uncertainty quantification, we outline a number of future methodological improvements. We also provide two keynote speakers on uncertainty quantification in artificial intelligence (AI). One of thematic areas covered is infection transmission mechanisms, while the other focuses on the societal scale, e.g., forecasting collective mobility patterns,","776":"The advances in understanding complex networks have generated increasing interest in dynamical processes occurring on them. Pattern formation in activator-inhibitor systems has been studied in networks, revealing differences from the classical continuous media. Here we study pattern formation in a new framework, namely multiplex networks. These are systems where activator and inhibitor species occupy separate nodes in different layers. Species react across layers but diffuse only within their own layer of distinct network topology. This multiplicity generates heterogeneous patterns with significant differences from those observed in single-layer networks. Remarkably, diffusion-induced instability can occur even if the two species have the same mobility rates; condition which cannot destabilize single-layer networks. The instability condition is revealed using perturbation theory and expressed by a combination of degrees in the different layers. Our theory demonstrates that the existence of such topology-driven instabilities is generic in multiplex networks, providing a new mechanism of pattern formation. Instability of degree-degree pairs occurs both in known models of epidemics and in healthy states under random walk conditions. Furthermore, we show that the epidemic threshold remains unchanged for greater values of the rewiring rate than those found previously for single-layer networks. Thus, our results indicate that phase separation between the two","777":"The aim of this work is to study the behaviour of deep learning models on chest X-ray images for detecting COVID-19 patients. We have adopted and evaluated several convolutional neural network architectures through transfer learning approaches, to detect COVID-19 pneumonia using both chest X-rays and a meta-learning approach based on pre-trained weights. The results have been analysed in two main categories as well as tested with a third category. This work also studies the effectiveness of different augmentation techniques applied on the trained networks. Our findings show that augmenting the input image dataset can improve the performance of all the models achieving more than 87% in most cases. Moreover, we are able to achieve 99% accuracy and recall for the augmented target classification with only 50 epochs. For each test image, the model performs differently from its base configuration with almost 30% degradation in terms of F1 score. We conclude that there is a considerable room for improvement on those models by adapting them for the specific medical imaging domain. These results encourage the community to re-design the training steps for these models in light of the trade-off between generalization and fine-tuning. Furthermore, it would be useful to compare their performance when dealing with similar datasets except for the few large","778":"In late 2019, COVID-19, a severe respiratory disease, emerged, and since then, the world has been facing a deadly pandemic caused by it. This ongoing pandemic has had a significant effect on different aspects of societies. The uncertainty around the number of daily cases made it difficult for decision-making bodies to control the outbreak. Deep Learning models have proved that they can come in handy in many real-world problems such as healthcare ones. However, they require a lot of data to learn the features properly and output an acceptable solution. Since COVID-19 has been a lately emerged disease, there was not much data available, especially in the first stage of this pandemic. Therefore, it is necessary to collect more data with them to improve the accuracy of the model's predictions. As new data often comes from regions, it is important to consider those sources of data and apply them to the proposed model to enhance its learning capabilities. In this paper, we propose a framework based on transfer learning to combine the knowledge extracted from historical data with the geographical location information. We evaluated our approach in two phases - first phase with no data, and second phase with plenty of data (as far as 100+ data points). Our results prove that when using as few as","779":"In natural language processing, there is often one's interest in identifying whether a text contains more information than what is readily expressed by the interpreter. In this work, we explore how differentiable programming techniques can be used to optimize the identification problem for multi-label text classification. We consider two types of information, namely, label descriptions and hand-crafted features. As an example, we discuss the latter type of information in both the training and inference stages with a real-world application to sentiment analysis. For each type of information, we also develop efficient thresholding schemes via statistical data assimilation methodologies. Experimental results show that our proposed methods outperform state-of-the-art approaches on multiple datasets for all the tasks. Our code is available at https:\/\/github.com\/masadcv\/Thresholded-Feature-Semantic-Model(TFSM) for any further research. To the best of our knowledge, this is the first work that jointly optimizes both feature types together. The TFSM consists of three components: (i) A joint learning objective function that combines various types of textual information along with their corresponding annotations; (ii) a mutual exclusivity distillation (MED), which produces a new set of outputs based on the semantics of","780":"With the increasing attention on thermal imagery for Covid-19 screening, the public sector may believe there are new opportunities to exploit thermal as a modality for computer vision and AI. Thermal physiology research has been ongoing since the late nineties. This research lies at the intersections of medicine, psychology, machine learning, optics, and affective computing. We will review the known factors of thermal vs. RGB imaging for facial emotion recognition. But we also propose that thermal imagery may provide a semi-anonymous modality for computer vision, over RGB, which has been plagued by misuse in facial recognition. However, the transition from one mode to another is not easy and relies on the availability of different datasets. The main objective of this paper is to motivate the community to explore other sources of thermal data and develop more effective ways of utilizing it. Other tasks like face detection using Histogram of Oriented Gradients (HOG) or Support Vector Machine (SVM) can be developed by collecting information from these datasets. These techniques can be generalized to any other type of masked face detection problem where the main challenge is how to produce high resolution images without containing the masked faces. As an example, we have collected a dataset of RGB and thermals with masks but no facial coverings","781":"The construction and application of knowledge graphs have seen a rapid increase across many disciplines in recent years. Additionally, the problem of uncovering relationships between developments in the COVID-19 pandemic and social media behavior is of great interest to researchers hoping to curb the spread of this new disease. In this paper we present a knowledge graph constructed from COVID-19 related tweets in the Los Angeles area, supplemented with federal and state policy announcements and disease spread statistics. By incorporating dates, topics, and events as entities, we construct a knowledge graph that describes the connections between these useful information. We use natural language processing techniques to extract tweet-topic, tweet-date, and event-date relations. Further analysis on the constructed knowledge graph provides insight into how tweets reflect public sentiments towards COVID-19 related topics and how changes in these sentiments correlate with real-world events. The insights provided by the knowledge graph can be used for preventive actions to guide the public health officials. This article is part of the Knowledge Graph Initiative (KGIs). As more data becomes available, the KGIS can be increasingly useful in dealing with complex systems such as misinformation, fake news, and opinion dynamics. The dataset used in this work was obtained via crawling similar websites. We also performed sentiment analysis","782":"Introduced by Kiefer and Wolfowitz \\cite{KW56}, the nonparametric maximum likelihood estimator (NPMLE) is a widely used methodology for learning mixture odels and empirical Bayes estimation. Sidestepping the non-convexity in mixture likelihood, the NPMLE estimates the mixing distribution by maximizing the total likelihood over the space of probability measures, which can be viewed as an extreme form of overparameterization. In this paper we discover a surprising property of the NPMLE solution. Consider, for example, a Gaussian mixture model on the real line with a subgaussian mixing distribution. Leveraging complex-analytic techniques, we show that with high probability the NPMLE based on a sample of size $n$ has $O(\\log n)$ atoms (mass points), significantly improving the deterministic upper bound of $n$ due to Lindsay \\cite{lindsay1983geometry1}. Notably, any such Gaussian mixture is statistically indistinguishable from a finite one with $O(\\log n)$ components (and this is tight for certain mixtures). Thus, absent any explicit form of model selection, NPMLE automatically chooses the right model complexity, a property we term \\em","783":"A question of global concern regarding the sustainable future of humankind stems from the effect due to aerosols on our climate. The quantification of atmospheric aerosols and their relationship to climatic impacts are key to understanding the dynamics of climate forcing and to improve our knowledge about climate change. Due to its response to precipitation, temperature, topography and human activity, one of the most dynamical atmospheric regions is the atmospheric boundary layer (ABL): ABL aerosols have a sizable impact on the evolution of the radiative forcing of climate change, human health, food security, and, ultimately, on the local and global economy. The identification of ABL pattern behaviour requires constant monitoring and the application of instrumental and computational methods for its detection and analysis. Here, we show a new method for the retrieval of ABL top arising from light detection and ranging (LiDAR) signals, by training a convolutional neural network in a supervised manner; forcing it to learn how to retrieve such a dynamical parameter on real, non-ideal conditions and in a fully automated, unsupervised way. Our findings pave the way for a full integration of LiDAR elastic, inelastic, and depolarisation signal processing, and provide a novel approach for real-time quantitative","784":"The ongoing COVID-19 pandemic has raised concerns for many regarding personal and public health implications, financial security and economic stability. Alongside many other unprecedented challenges, there are increasing concerns over social isolation and mental health. We introduce \\textit{Expressive Interviewing}--an interview-style conversational system that draws on ideas from motivational interviewing and expressive writing. Expressive Interviewing seeks to encourage users to express their thoughts and feelings through writing by asking them questions about how COVID-19 has impacted their lives. We present relevant aspects of the system's design and implementation as well as quantitative and qualitative analyses of user interactions with the system. In addition, we conduct a comparative evaluation with a general purpose dialogue system for mental health that shows our system potential in helping users to cope with COVID-19 issues. Finally, we discuss some limitations and suggestions for future work. All models and data are available online at https:\/\/expressiveinterpreter.github.io\/en\/latest\/. This dataset will be made publicly available upon acceptance. \\url{https:\/\/youtu.be\/zhbcSvxEKMk}$\\times$ faster than previous datasets due to its multi-modal nature. Each video contains multiple modalities (image","785":"We propose an end-to-end architecture for multivariate time-series prediction that integrates a spatial-temporal graph neural network with a matrix filtering module. This module generates filtered (inverse) correlation graphs from multivariate time series before inputting them into a GNN. In contrast with existing sparsification methods adopted in graph neural network, our model explicitly leverage time-series filtering to overcome the low signal-to-noise ratio typical of complex systems data. We present a set of experiments, where we predict future sales from a synthetic time-series sales dataset and real COVID-19 daily cases from the New York Times. The proposed approach shows superior performance compared to both unsupervised and supervised baselines and significantly outperforms the state-of-the-art deterministic baseline method. A detailed analysis on the topological properties of the resulting graphs indicates that these networks can capture meaningful information contained in the original multi-dimensional space via vector representations. Our code has been made publicly available at https:\/\/github.com\/annahaensch\/covid19_graph_multitask learning. All the datasets used in this work are available as open source technologies or were generated using the proposed technique. They can be used under GPL v3.","786":"These proceedings accompany the Belle II talk in the Diversity and Inclusion parallel session delivered during ICHEP 2020. This marks the first external presentation by the Belle II Collaboration, in which we present some of our data and self-reported statistics regarding diversity and inclusion. We also present Belle II's current and planned activities to aid and improve diversity and inclusion. We find that there is still a lot to be done to improve the social working environment and population representation within our collaboration and within high energy physics. We also identify and discuss possible research directions for future work. All of these topics are very important to consider as we enter into the next phase of ICHEP 2020. If you have any questions or ideas about how to improve equity, diversity and inclusion in your community, please send them to me. We will respond to all queries from anyone on Twitter via our account or through other means, just as they can reach us. However, this action is not on equal footing for everyone. Last but not least, we note that the discussion around diversity and inclusion has been gaining relevance with respect to both HEP and society at large. For example, during the COVID-19 pandemic, it was suggested that HEP might play a vital role in reaching out to the broader","787":"We describe how we have developed a computational microscope for analyzing very large, high-dimensional, and complex bioassays with synthetic genetic material. The goal of this research is to demonstrate the generalizability of our multi-core simulation algorithm across different types of assays from proteins to nucleic acids through polymerase chain reactions (PCR). We also present the detailed characterization of our approach in comparison to existing methods for the detection of SARS-CoV-2 RNA\/DNA viruses. Our method shows improved performance over previous approaches for the detection of RNA\/DNA viruses. Furthermore, by using our multivariate data model, we are able to analyze the protein expression patterns of various SARS-CoV-2 species without prior knowledge on their epidemiological behavior. Finally, we show that our technique can be exploited to detect specific proteins of interest for multiple purposes including the analysis of infectious disease outbreaks. In summary, we have shown that our widely distributed simulator, which is freely available online, can be used to efficiently focus on the specific scientific domain of interest. In addition, it may help to develop other disciplines involved in biotechnology and pharmaceutical applications. Impact Statement\u2014During 2021, when the COVID-19 pandemic continues its worldwide devastation, people will need more tools to","788":"The field of Business and Economics has been undergoing a profound transformation with the advent of intelligent technologies such as globalization, digitization, and stock trading. The current survey focuses on examining the impact of computer technology in the field of Business and Economics from the perspective of software engineering and systems science. We provide a detailed overview of recent research findings about the application of artificial intelligence (AI) methods such as machine learning and deep learning to improve business processes by automating or optimizing activities, thereby reducing human involvement and increasing productivity. In addition, we discuss how AI-based technologies can be applied to better engage customers, support economic operations, and help reduce operational costs for businesses. Further, we outline challenges and opportunities in the area of Business and Economics using AI. Finally, we provide some potential directions for future research in this promising area. All collected data, model implementations, and source code are available online at https:\/\/github.com\/zhijing-jin\/ai4bharvesting. Besides, we present a collection of publicly available datasets and models, which are needed to facilitate future research in the field of Business and Economics. Collection of datasets and models will be updated continuously within the scope of this article. Our forecasts and views of future transactions in the domain of Business and Economics may","789":"The current COVID-19 pandemic has highlighted the need for cheap, fast and reliable tools which can assist physicians in diagnosing COVID-19. Medical imaging such as CT can take a key role in complementing conventional diagnostic tools from molecular biology, and, using deep learning techniques, several automatic systems were demonstrated promising performances using CT or X-ray data. Here, we advocate a more prominent role of point-of-care ultrasound imaging to guide COVID-19 detection. Ultrasound is non-invasive and ubiquitous in medical facilities around the globe. Our contribution is threefold. First, we gather a lung ultrasound (POCUS) dataset consisting of (currently) 1103 images (654 COVID-19, 277 bacterial pneumonia and 172 healthy controls), sampled from 64 videos. While this dataset was assembled from various online sources and is by no means exhaustive, it was processed specifically to feed deep learning models and is intended to serve as a starting point for an open-access initiative. Second, we train a deep convolutional neural network (POCOVID-Net) on this 3-class dataset and achieve an accuracy of 89% and, by a majority vote, a video accuracy of 92%. For detecting COVID-19 in","790":"Multi-scale biomedical knowledge networks are expanding with emerging experimental technologies that generates multi-scale biomedical big data. Link prediction is increasingly used especially in bipartite biomedical networks to identify hidden biological interactions and relationshipts between key entities such as compounds, targets, gene and diseases. We propose a Graph Neural Networks (GNN) method, namely Graph Pair based Link Prediction model (GPLP), for predicting biomedical network links simply based on their topological interaction information. In GPLP, 1-hop subgraphs extracted from known network interaction matrix is learnt to predict missing links. To evaluate our method, three heterogeneous biomedical networks were used, i.e. Drug-Target Interaction network (DTI), Compound-Protein Interaction network (CPI) from NIH Tox21, and Compound-Virus Inhibition network (CVI). Our proposed GPLP method significantly outperforms over the state-of-the-art baselines. In addition, different network incompleteness is analysed with our devised protocol, and we also design an effective approach to improve the model robustness towards incomplete networks. Our method demonstrates the potential applications in other biomedical networks. The code is available at https:\/\/github.com\/emnlp-mim","791":"Human population growth has driven rising demand for food that has, in turn, imposed huge impacts on the environment. In an effort to reconcile our need to produce more sustenance while also protecting the ecosystems of the world, farming is becoming more reliant on smart tools and communication technologies. Developing a smart farming framework allows farmers to make more efficient use of inputs, thus protecting water quality and biodiversity habitat. Internet of Things (IoT), which has revolutionized every sphere of the economy, is being applied to agriculture by connecting on-farm devices and providing real-time monitoring of everything from environmental conditions to market signals through to animal health data. However, utilizing IoT means farming networks are now vulnerable to malicious activities, mostly when wireless communications are highly employed. With that in mind, this research aims to review different utilized communication technologies in smart farming. Moreover, possible cyber attacks are investigated to discover the vulnerabilities of communication technologies considering the most frequent cyber-attacks that have been happened. The study mainly focuses on investigating traditional security threats and further studies on discovering emerging threats online. The paper concludes with recommendations on how to implement protective mechanisms against ransomware attacks, flash floods, and other harmful events. Furthermore, future research directions are recommended to develop innovative technology combinations and cybersecurity measures to mitigate the current global","792":"We investigate the evolution of epidemics over dynamical networks when nodes choose to interact with others in a selfish and decentralized manner. Specifically, we analyze the susceptible-asymptomatic-infected-recovered (SAIR) epidemic in the framework of activity-driven networks with heterogeneous node degrees and time-varying activation rates, and derive both individual and degree-based mean-field approximations of the exact state evolution. We then present a game-theoretic model where nodes choose their activation probabilities in a strategic manner using current state information as feedback, and characterize the quantal response equilibrium (QRE) of the proposed setting. We then consider the activity-driven susceptible-infected-susceptible (SIS) epidemic model, characterize equilibrium activation probabilities and analyze epidemic evolution in closed-loop. Our numerical results provide compelling insights into epidemic evolution under game-theoretic activation. Specifically, for the SAIR epidemic, we show that under suitable conditions, the epidemic can persist, as any decrease in infected proportion is counteracted by an increase in activity rates by the nodes. For the SIS epidemic, we show that in regimes where there is an endemic state, the infected proportion could be significantly smaller under game-theoretic","793":"The ongoing COVID-19 pandemic has affected most of the countries on Earth. It has become a pandemic outbreak with more than 50 million confirmed infections and above 1 million deaths worldwide. In this study, we consider a mathematical model on COVID-19 transmission with the prosocial awareness effect. The proposed model can have four equilibrium states based on different parametric conditions. The local and global stability conditions for awareness-free, disease-free equilibrium are studied. Using Lyapunov function theory and LaSalle invariance principle, the disease-free equilibrium is shown globally asymptotically stable under some parametric constraints. The existence of unique awareness-free, endemic equilibrium and unique endemic equilibrium is presented. We calibrate our proposed model parameters to fit daily cases and deaths from Colombia and India. Sensitivity analysis indicates that the transmission rate and the learning factor related to awareness of susceptibles are very crucial for reduction in disease-related deaths. Finally, we assess the impact of prosocial awareness during the outbreak and compare this strategy with popular control measures. Results indicate that prosocial awareness has competitive potential to flatten the COVID-19 prevalence curve. Overall, this study demonstrates that prosocial awareness can significantly reduce the death toll of COVID-19 by","794":"The environmental performance of shared micromobility services compared to private alternatives has never been assessed using an integrated modal Life Cycle Assessment (LCA) relying on field data. Such an LCA is conducted on three shared micromobility services in Paris - bikes, second-generation e-scooters, and e-mopeds - and their private alternatives. Global warming potential, primary energy consumption, and the three endpoint damages are calculated. Sensitivity analyses on vehicle lifespan, shipping, servicing distance, and electricity mix are conducted. Electric micromobility ranks between active modes and personal ICE modes. Its impacts are globally driven by vehicle manufacturing. Ownership does not affect directly the environmental performance: the vehicle lifetime mileage does. Assessing the sole carbon footprint leads to biased environmental decision-making, as it is not correlated with the three damages: multicriteria LCA is mandatory to preserve the planet. Finally, a major change of paradigm is needed to eco-design modern transportation policies. Our results show that electric micromobility ranks between active and personal COVID-19 modes. Its benefits cannot be fully decarbonized without shifting its engine from fossil fuel powered to cement based thermal generators. Ownership does not affect the overall environmental quality either. In the long","795":"The novel coronavirus disease (COVID-19) has been spreading rapidly across the world and caused significant impact on the public health and economy. However, there is still lack of studies on effectively quantifying the lung infection caused by COVID-19. As a basic but challenging task of the diagnostic framework, segmentation plays a crucial role in accurate quantification of COVID-19 infection measured by computed tomography (CT) images. To this end, we proposed a novel deep learning algorithm for automated segmentation of multiple COVID-19 infection regions. Specifically, we use the Aggregated Residual Transformations to learn a robust and expressive feature representation and apply the soft attention mechanism to improve the capability of the model to distinguish a variety of symptoms of the COVID-19. With a public CT image dataset, we validate the efficacy of the proposed method in comparison with other competing methods. Experimental results demonstrate the outstanding performance of our method for automated segmentation of COVID-19 Chest CT images. Our study provides a promising deep leaning-based segmentation tool to lay a foundation to quantitative diagnosis of COVID-19 lung infections in CT images. We hope it can raise awareness of the potential threat of deep learning-based medical imaging algorithms towards infectious diseases","796":"The rapid outbreak of COVID-19 has caused humanity to come to a stand-still and brought with it a plethora of other problems. COVID-19 is the first pandemic in history when humanity is the most technologically advanced and relies heavily on social media platforms for connectivity and other benefits. Unfortunately, fake news and misinformation regarding this virus is also available to people and causing some massive problems. So, fighting this infodemic has become a significant challenge. We present our solution for the\"Constraint@AAAI2021 - COVID19 Fake News Detection in English\"challenge in this work. After extensive experimentation with numerous architectures and techniques, we use eight different transformer-based pre-trained models with additional layers to construct a stacking ensemble classifier and fine-tuned them for our purpose. We achieved 0.979906542 accuracy, 0.979913119 precision, 0.979906542 recall, and 0.979907901 f1-score on the test dataset of the competition. This method will be used by practitioners to further improve their performance via experimenting with more training data. Our code is publicly available at https:\/\/github.com\/cbasemaster\/constraint@aaiapp.org. It","797":"Suppressing SARS-CoV-2 will likely require the rapid identification and isolation of infected individuals, on an ongoing basis. RT-PCR (reverse transcription polymerase chain reaction) tests are accurate but costly, making regular testing of every individual expensive. The costs are a challenge for all countries and particularly for developing countries. Cost reductions can be achieved by combining samples and testing them in groups. We propose an algorithm for grouping subsamples, prior to testing, based on the geometry of a hypercube. At low prevalence, this testing procedure uniquely identifies infected individuals in a small number of tests. We discuss the optimal group size and explain why, given the highly infectious nature of the disease, parallel searches are preferred. We report proof of concept experiments in which a positive sample was detected even when diluted a hundred-fold with negative samples. Using these methods, the costs of mass testing could be reduced by a factor of ten to a hundred or more. If infected individuals are quickly and effectively quarantined, the prevalence will fall and so will the costs of regularly testing everyone. Such a strategy provides a possible pathway to the longterm elimination of SARS-CoV-2. Field trials of our approach are now under way in Rwanda and initial data from these are reported","798":"The stoichiometric Ni$_{50}$Mn$_{25}$In$_{25}$ Heusler alloy transforms from a stable ferromagnetic austenitic ground state to an incommensurate modulated martensitic ground state with a progressive replacement of In with Mn without any pre-transition phases. The absence of pre-transitions like strain glass in Ni$_{50}$Mn$_{25+x}$In$_{25-x}$ alloys is explained to be the ability of the ferromagnetic cubic structure to accommodate the lattice strain caused by atomic size differences of In and Mn atoms. Beyond the critical value of $x$ = 8.75, the alloys undergo martensitic transformation despite the formation of ferromagnetic and antiferromagnetic clusters and the appearance of a super spin glass state. The outcoupled cluster singles are shown to form a Gaussian mixture on the coupled spinless matter field. As a result, their magnetization distribution curves are compared to those obtained from Molecular Dynamics simulations for several related compounds which also possess a Gaussian mixture structure. It is shown that the typical dimensionality of the Gaussian mixture is sufficient to capture the finite wavevector properties of","799":"There remains an urgent need to identify existing drugs that might be suitable for treating patients suffering from COVID-19 infection. Drugs rarely act at a single molecular target, with off target effects often being responsible for undesirable side effects and sometimes, beneficial synergy between targets for a specific illness. Off target activities have also led to blockbuster drugs in some cases, e.g. Viagra for erectile dysfunction and Minoxidil for male pattern hair loss. Drugs already in use or in clinical trials plus approved natural products constitute a rich resource for discovery of therapeutic agents that can be repurposed for existing and new conditions, based on the rationale that they have already been assessed for safety in man. A key question then is how to rapidly and efficiently screen such compounds for activity against new pandemic pathogens such as COVID-19. Here we show how a fast and robust computational process can be used to screen large libraries of drugs and natural compounds to identify those that may inhibit the main protease of SARS-Cov-2 (3CL pro, Mpro). We show how the resulting shortlist of candidates with strongest binding affinities is highly enriched in compounds that have been independently identified as potential antivirals against COVID-19. The top candidates also include a","800":"The negative effects of misinformation filter bubbles in adaptive systems have been known to researchers for some time. Several studies investigated, most prominently on YouTube, how fast a user can get into a misinformation filter bubble simply by selecting wrong choices from the items offered. Yet, no studies so far have investigated what it takes to burst the bubble, i.e., revert the bubble enclosure. We present a study in which pre-programmed agents (acting as YouTube users) delve into misinformation filter bubbles by watching misinformation promoting content (for various topics). Then, by watching misinformation debunking content, the agents try to burst the bubbles and reach more balanced recommendation mixes. We recorded the search results and recommendations, which the agents encountered, and analyzed them for the presence of misinformation. Our key finding is that bursting of a filter bubble is possible, albeit it manifests differently from topic to topic. Moreover, we observe that filter bubbles do not truly appear in some situations. We also draw a direct comparison with a previous study. Sadly, we did not find much improvements in misinformation occurrences, despite recent pledges by YouTube. Finally, we discuss the reasons for these discrepancies between real and simulated misinformation propagation. To our knowledge, this is the first study investigating the behavior of misinformation filter bubbles through the lens of psychological theories","801":"We show that precise knowledge of epidemic transmission parameters is not required to build an informative model of the spread of disease. We propose a detailed model of the topology of the contact network under various control regimes and demonstrate that this is sufficient to capture the salient dynamical characteristics and to inform decisions. Our model of city-level transmission of an infectious agent (SEIR model) characterises spreading via a (a) scale-free contact network (no control); (b) a random graph (elimination of mass gatherings; and (c) small world lattice (partial to full lockdown --\"social\"distancing). This model exhibits good agreement between simulation and data from the 2020 pandemic spread of COVID-19. Also, estimates of the relevant rate parameters of the SEIR model are obtained and we demonstrate the robustness of our model predictions under uncertainty of those estimates. Finally, the social context and utility of this work is identified. The results exhibit potential for applications in designing targeted interventions focused on mitigating or eliminating future pandemics. They also qualify as the basic building blocks of theory for modeling sequential systems, such as policy iteration and dynamic networks. Thus, our work provides a basis for bridging the gap between epidemiological models of evidence and expert groups decision","802":"Data dependencies have been extended to graphs to characterize topological and value constraints. Existing data dependencies are defined to capture inconsistencies in static graphs. Nevertheless, inconsistencies may occur over evolving graphs and only for certain time periods. The need for capturing such inconsistencies in temporal graphs is evident in anomaly detection and predictive dynamic network analysis. This paper introduces a class of data dependencies called Temporal Graph Functional Dependencies (TGFDs). TGFDs generalize functional dependencies to temporal graphs as a sequence of graph snapshots that are induced by time intervals, and enforce both topological constraints and attribute value dependencies that must be satisfied by these snapshots. (1) We establish the complexity results for the satisfiability and implication problems of TGFDs. (2) We propose a sound and complete axiomatization system for TGFDs. (3) We also present efficient parallel algorithms to detect inconsistencies in temporal graphs as violations of TGFDs. The algorithm exploits data and temporal locality induced by time intervals, and uses incremental pattern matching and load balancing strategies to enable feasible error detection in large temporal graphs. Using real datasets, we experimentally verify that our algorithms achieve lower runtimes compared to existing baselines, while improving the accuracy over error detection using existing graph neural networks substantially boosts. 4) We","803":"Quantum computing was once regarded as a mere theoretical possibility, but recent advances in engineering and materials science have brought practical quantum computers closer to reality. Currently, representatives from industry, academia, and governments across the world are working to build the educational structures needed to produce the quantum workforce of the future. Less attention has been paid to growing quantum computing capacity at the high school level. This article details work at The University of Texas at Austin to develop and pilot the first full-year high school quantum computing class. Over the course of two years, researchers and practitioners involved with the project learned several pedagogical and practical lessons that can be helpful for quantum computing course design and implementation at the secondary level. In particular, we find that the use of classical optics provides a clear and accessible avenue for representing quantum states and gate operators and facilitates both learning and the transfer of knowledge to other Science, Technology, and Engineering (STEM) skills. Furthermore, students found that exploring quantum optical phenomena prior to the introduction of mathematical models helped in the understanding and mastery of the material. Overall, these results demonstrate the feasibility of developing a similar programmability approach for other institutions around the world and their ability to apply new technologies to help facilitate the next generation of quantum computing educators and professionals. As well as","804":"Analysis pipelines commonly use high-level technologies that are popular when created, but are unlikely to be readable, executable, or sustainable in the long term. A set of criteria is introduced to address this problem: Completeness (no execution requirements beyond a minimal Unix-like operating system, no administrator privileges, no network connection, and storage primarily in plain text); modular design; minimal complexity; scalability; verifiable inputs and outputs; version control; linking analysis with narrative; and free and open source software. As a proof of concept, we introduce\"Maneage\"(Managing data lineage), enabling cheap archiving, provenance extraction, and peer verification that has been tested in several research publications. We show that longevity is a realistic requirement that does not sacrifice immediate or short-term reproducibility. The caveats (with proposed solutions) are then discussed and we conclude with the benefits for the various stakeholders. This article is itself a Maneage'd project (project commit 54e4eb2). Copyright may be transferred without notice, after which this version becomes a public good infrastructure for all to participate in enhancing it. Thus, giving non-commercial incentives for people to contribute to improving this document will benefit future collaboration on its downstream effects. Additionally, since these documents provide","805":"The ongoing COVID-19 pandemic has raised concerns for many regarding personal and public health implications, financial security and economic stability. Alongside many other unprecedented challenges, there are increasing concerns over social isolation and mental health. We introduce \\textit{Expressive Interviewing}--an interview-style conversational system that draws on ideas from motivational interviewing and expressive writing. Expressive Interviewing seeks to encourage users to express their thoughts and feelings through writing by asking them questions about how COVID-19 has impacted their lives. We present relevant aspects of the system's design and implementation as well as quantitative and qualitative analyses of user interactions with the system. In addition, we conduct a comparative evaluation with a general purpose dialogue system for mental health that shows our system potential in helping users to cope with COVID-19 issues. Finally, we discuss the possibility of integrating our speech synthesis module with other tools in order to create a more comprehensive assistant system for mental health. Our code and data are available at https:\/\/github.com\/annahaensch\/expressive-interviewing.git.io\/. Video recordings of the interviews are available at http:\/\/youtu.be\/zhbcSvxEKMk. Source code and documentation are also available at https:\/\/","806":"Many parents and people with disabilities (e.g., autism spectrum, depression) may be interested in receiving assistance from their child due to concerns about health, education, and well-being. However, it is challenging to design and evaluate the most effective mobile application(s) for this group given the wide range of factors involved in the design process. In this paper, we present the results of an observational study conducted between February 2020 and July 2021 at the end of which no new mobile app designs were released to the public. Our goal is to systematically collect data on the use of these apps among children with and without disabilities using the same dataset as in the previous work [1]. We focus on two aspects of the design: the choice of third-party developers who created the app, and the privacy policy adopted by the participating entities. To observe the adoption of the app, we captured both the front view and the back view of the screen, recorded the search result, and analyzed the differences between groups. The results show that while third-party developers did indeed create the app, they are not responsible for its success, and the privacy policies adopted by the participating parties do not guarantee such outcomes. Additionally, we found significant differences between genders regarding the experiences gained with different apps.","807":"Rural-urban classifications are essential for analyzing geographic, demographic, environmental, and social processes across the rural-urban continuum. Most existing classifications are, however, only available at relatively aggregated spatial scales, such as at the county scale in the United States. The absence of rurality or urbanness measures at high spatial resolution poses significant problems when the process of interest is highly localized, as with the incorporation of rural towns and villages into encroaching metropolitan areas. Moreover, existing rural-urban classifications are often inconsistent over time, or require complex, multi-source input data (e.g., remote sensing observations or road network data), thus, prohibiting the longitudinal analysis of rural-urban dynamics. Here, we develop a set of distance- and spatial-network-based methods for consistently estimating the remoteness and rurality of places at fine spatial resolution, over long periods of time. We demonstrate the utility of our approach by constructing indices of urbanness for 30,000 places in the United States from 1930 to 2018 and further test the plausibility of our results against a variety of evaluation datasets. We call these indices the place-level urban-rural index (PLURAL) and make the resulting datasets publicly available (https:\/\/doi.","808":"We report findings from a case study of a large agile information systems development (ISD) organization`s sudden transformation to distributed, digital work in the context of the Covid-19 pandemic. It seeks to understand how knowledge creation and sharing changes. The findings show various forms of distance being introduced, digital tool usage, increased task orientation, and variations across teams. To analyze the findings, we use the concepts of large-scale collaborations and sociability. Large-scale collaboration offers a socio-technical perspective on tackling distributed knowledge sharing and creation in the presence of multiple, loosely coupled partners using digital tools for collaboration. We show what the digital tools afford using the concept of sociability. We discuss how distributed digital practices make teams more task-oriented and that creating and maintaining sociability, a key issue for knowledge sharing in agile ISD organizations, require relation oriented communication during practical problem solving using digital tools. We conclude by discussing potential implications for hybrid remote \/ virtual reality deployments and by suggesting design directions for future research within this area. [1] 2]. This article is part of the theme issue \u2018Topics in mathematical software engineering\u2019. In addition to the theoretical aspects, the findings have empirical limitations and can be used to inform managerial strategies as well as policies for","809":"In the era of big data, standard analysis tools may be inadequate for making inference and there is a growing need for more efficient and innovative ways to collect, process, analyze and interpret the massive and complex data. We provide an overview of challenges in big data problems and describe how innovative analytical methods, machine learning tools and metaheuristics can tackle general healthcare problems with a focus on the current pandemic. In particular, we give applications of modern digital technology, statistical methods, data platforms and data integration systems to improve diagnosis and treatment of diseases in clinical research and novel epidemiologic tools to tackle infection source problems, such as finding Patient Zero in the spread of epidemics. We make the case that analyzing and interpreting large amounts of data is a very challenging task that requires a multi-disciplinary effort to continuously create more effective methodologies and powerful tools to transfer data information into knowledge that enables informed decision making. We hope that our efforts will help to transform the traditional approach to collecting and analyzing big data into many meaningful insights and propose new directions in this promising area. In addition, we introduce a series of benchmarks aimed at evaluating the performance of existing approaches, which should serve as guidelines for future development in this important topic. The datasets used in this study are taken from various sources and","810":"We propose a mechanism to allocate slots fairly at congested airports. This mechanism: (a) ensures that the slots are allocated according to the true valuations of airlines, (b) provides fair opportunities to the flights connecting remote cities to large airports, and (c) controls the number of flights in each slot to minimize congestion. The mechanism draws inspiration from economic theory. It allocates the slots based on an affine maximizer allocation rule and charges payments to the airlines such that they are incentivized to reveal their true valuations. The allocation also optimizes the occupancy of every slot to keep them as uncongested as possible. The formulation solves an optimal integral solution in strongly polynomial time. We conduct experiments on the data collected from two major airports in India. We also compare our results with existing allocations and also with the allocations based on the International Air Transport Association (IATA) guidelines. The computational results show that the social utility generated using our mechanism is 20-30% higher than IATA and current allocations. Moreover, we find that when allocating slots more than 150\\%, the social utility is more than 30\\%. Thus, our mechanism can be used to address the lack of fairness in airline travel demand. Finally, we note that there exists no","811":"We examine how six search engines filter and rank information in relation to the queries on the U.S. 2020 presidential primary elections under the default - that is nonpersonalized - conditions. For that, we utilize an algorithmic auditing methodology that uses virtual agents to conduct large-scale analysis of algorithmic information curation in a controlled environment. Specifically, we look at the text search results for\"us elections\",\"donald trump\",\"joe biden\"and\"bernie sanders\"queries on Google, Baidu, Bing, DuckDuckGo, Yahoo, and Yandex, during the 2020 primaries. Our findings indicate substantial differences in the search results between search engines and multiple discrepancies within the results generated for different agents using the same search engine. It highlights that whether users see certain information is decided by chance due to the inherent randomization of search results. We also find that some search engines prioritize different categories of information sources with respect to specific candidates. These observations demonstrate that algorithmic curation of political information can create information inequalities between the search engine users even under nonpersonalized conditions. Such inequalities are particularly troubling considering that search results are highly trusted by the public and can shift the opinions of undecided voters as demonstrated by previous research. Finally, we note","812":"Obesity is a complex disease and its prevalence depends on multiple factors related to the local socioeconomic, cultural and urban context of individuals. Many obesity prevention strategies and policies, however, are horizontal measures that do not depend on context-specific evidence. In this paper we present an overview of BigO (http:\/\/bigoprogram.eu), a system designed to collect objective behavioral data from children and adolescent populations as well as their environment in order to support public health authorities in formulating effective, context-specific policies and interventions addressing childhood obesity. We present an overview of the data acquisition, indicator extraction, data exploration and analysis components of the BigO system, as well as an account of its preliminary pilot application in 33 schools and 2 clinics in four European countries, involving over 4,200 participants. Our analyses provide insights into the types of information collected by BigO system for each of the indicators examined. The system will be made publicly available through our website. https:\/\/www.fil.ion.ucl.ac.uk\/spm\/covid19\/dashboard\/dataDetail\/?pid=%2FStatsPupil&mode=listenetV1. This web platform facilitates collaborative research among researchers in the field of epidemiology, biomedical engineering,","813":"While algorithm audits are growing rapidly in commonality and public importance, relatively little scholarly work has gone toward synthesizing prior work and strategizing future research in the area. This systematic literature review aims to do just that, following PRISMA guidelines in a review of over 500 English articles that yielded 62 algorithm audit studies. The studies are synthesized and organized primarily by behavior (discrimination, distortion, exploitation, and misjudgement), with codes also provided for domain (e.g. search, vision, advertising, etc.), organization (e.g. Google, Facebook, Amazon, etc.), and audit method (e.g. sock puppet, direct scrape, crowdsourcing, etc.). The review shows how previous audit studies have exposed public-facing algorithms exhibiting problematic behavior, such as search algorithms culpable of distortion and advertising algorithms culpable of discrimination. Based on the studies reviewed, it also suggests some behaviors (e.g. discrimination on the basis of intersectional identities), domains (e.g. advertising algorithms), methods (e.g. code auditing), and organizations (e.g. Twitter, TikTok, LinkedIn) that call for future audit attention. The paper concludes by offering the common ingredients of successful audits, and discussing algorithm auditing in the context","814":"Owing to the emergence of large datasets, applying current sequential wrapper-based feature subset selection (FSS) algorithms increases the complexity. This limitation motivated us to propose a wrapper for feature subset selection (FSS) based on parallel and distributed hybrid evolutionary algorithms (EAs) under the Apache Spark environment. The hybrid EAs are based on the BDE and Binary Threshold Accepting (BTA), a point-based EA, which is invoked to enhance the search capability and avoid premature convergence of the PB-DE. Thus, we designed the hybrid variants (i) parallel binary differential evolution and threshold accepting (PB-DETA), where DE and TA work in tandem in every iteration, and (ii) parallel binary threshold accepting and differential evolution (PB-TADE), where TA and DE work in tandem in every iteration under the Apache Spark environment. Both PB-DETA and PB-TADE are compared with the baseline, viz., the parallel version of the binary differential evolution (PB-DE). All three proposed approaches use logistic regression (LR) to compute the fitness function, namely, the area under ROC curve (AUC). The effectiveness of the proposed algorithms is tested over the five large datasets of varying feature space dimension, taken from cyber security","815":"Highly asymmetric optical properties of b-Ga2O3 are investigated by polarization-dependent photoluminescence (PL) excitation spectroscopy for assessing its potential use for a novel ultraviolet emitter and nonlinear optical material. Based on power-dependent excitation and two-photon depth scan, it is demonstrated that the underlying mechanism for the PL occurring far below the bandgap is not caused by intrinsic polaron effects, but by excitonic recombination at extrinsic defect sites. The findings are corroborated by density functional theory calculations describing local electronic structure and momentum distribution in combination with finite-difference time domain reflectometry. They indicate that the magnitude of the PL occurring far below the bandgap is dependent on the detailed balance between interfacial and background recombination paths, which can be tuned by external gating conditions. Furthermore, the appearance of multiple stable states depending on the phase of the reopening stage of the system presents evidence for intertwined charge carriers localized in the corners of nanometer-scale surface defects. These results open up new avenues for the development of low-dimensional topological insulators yielding to optically active sublattices exhibiting enhanced quantum efficiency. Ultimately, the fundamental role of symmetries in particle-hole symmetry generation","816":"Insufficient Social Cost of Carbon (SCC) estimation methods and short-term decision-making horizons have hindered the ability of carbon emitters to properly correct for the negative externalities of climate change, as well as the capacity of nations to balance economic and climate policy. To overcome these limitations, we introduce Retrospective Social Cost of Carbon Updating (ReSCCU), a novel mechanism that corrects for these limitations as empirically measured evidence is collected. To implement ReSCCU in the context of carbon taxation, we propose Retroactive Carbon Pricing (ReCaP), a market mechanism in which polluters offload the payment of ReSCCU adjustments to insurers. To alleviate systematic risks and minimize government involvement, we introduce the Private ReCaP (PReCaP) prediction market, which could see real-world implementation based on the engagement of a few high net-worth individuals or independent institutions. Finally, we present a series of empirical findings that illustrate the effectiveness of our proposed mechanism in relieving uncertainty, improving efficiency, and boosting public confidence in sustainable travel. The paper concludes with a discussion on implications for international governance strategies, including promoting enrollment practices that leverage private data and are subject to confidentiality constraints. We also defend that ReSCCU should","817":"We consider networks of banks with assets and liabilities. Some banks may be insolvent, and a central bank can decide which insolvent banks, if any, to bail out. We view bailouts as an optimization problem where the central bank has given resources at its disposal and an objective it wants to maximize. We show that under various assumptions and for various natural objectives this optimization problem is NP-hard, and in some cases even hard to approximate. Furthermore, we also show that given a fixed central bank bailout objective, banks in the network can make new debt contracts to increase their own market value in the event of a bailout (at the expense of the central bank). These results are illustrated via numerical examples. Our analysis provides theoretical justification for why these problems are so difficult to solve numerically, and why the optimality of a bailout should not be done too quickly. Finally, we present a new design principle for tackling hyperparameter tuning by Markov Decision Processes (MDPs) designed to fit our model's parameters better than existing approaches. MDPs are capable of solving arbitrary high-dimensional optimization problems while using only a small number of variables compared to classical MDPs. In particular, they allow us to optimize portfolios w.r.t. different cost functions","818":"The paper discusses multivariate self-adaptive dynamics of econometric models with a non-Gaussian likelihood representation obtained from the limit distribution of a sequence of independent observations, where each observation's value is assumed to be stationary within a certain time interval (e.g., $T$). We show that such slow-rolling limits result in a wide class of inference problems for the full parameter estimation problem, including classical parameters derived from the minimax concave penalty, and provide a mixture of robust and inconsistent estimators for all other parameters. The asymptotic properties of the resulting estimators are examined via simulation studies and real data analysis. The results suggest that even a minimal variance version of the Bayesian linear regression model would yield reasonable standard errors in most existing econometric methods. Finally, an application to the daily number of deaths due to COVID-19 in the USA is presented. Both the methodology and its implementation, along with any other improvements made to the current method, are also discussed. The PyTorch source code for this work is available at https:\/\/github.com\/pfryz\/minimax-contrast-self-adaptive-model. Under consideration in Theory and Practice of Logic Programming (TPLP).","819":"We develop FinText, a novel, state-of-the-art, financial word embedding from Dow Jones Newswires Text News Feed Database. Incorporating this word embedding in a machine learning model produces a substantial increase in volatility forecasting performance on days with volatility jumps for 23 NASDAQ stocks from 27 July 2007 to 18 November 2016. A simple ensemble model, combining our word embedding and another machine learning model that uses limit order book data, provides the best forecasting performance for both normal and jump volatility days. Finally, we use Integrated Gradients and SHAP (SHapley Additive exPlanations) to make the results more 'explainable' and the model comparisons more transparent. We find that adding our word embedding to the ensemble yields the best performing model for forecasting daily volatility, especially during turbulent periods where uncertainty significantly overlaps with extreme losses. The corresponding out-of-sample accuracy measure for predicting future daily volatility is also highly correlated with the volatility forecast performance. These findings suggest that incorporating social media data can improve the real-time risk assessment of models, such as ours, for well-calibrated trading strategies. Overall, these results demonstrate the feasibility of building a similar integrated workflow around pricing and hedging of financial options and equ","820":"This paper aims at analyzing the political polarization in Twitter discussions about inflation during the COVID-19 pandemic from the perspective of content and context. By characterizing the users with their tweets, we are able to provide analysis on the evolution of topics discussed within specific discursive events involving different issues over time. We have considered information sources such as Twitter to characterize the public discussion around the topic of inflation and its policy implications. To understand the relationship between the discourse about inflation and the real economy, we have also studied the temporal nature of the discussions. Our results indicate that the partisan gap regarding the credibility of reported statistics of economic impact by major governments across Europe is only growing after the onset of the COVID-19 pandemic (mostly through unofficial channels). Finally, we have also studied the user sentiment towards economists' policies, revealing interesting trends. From all these analyses, we conclude that there is a high degree of disagreement among the participants concerning the veracity of targeted fiscal policy measures implemented by the European authorities. These measures seem to favor the interests of conservative voters more than the ones of liberal ones. Overall, our study provides another layer of evidence into social media's role in informing public opinion on economic matters during a crisis. A comparison with pre-pandemic data indicates that","821":"The weaponization of digital communications and social media to conduct disinformation campaigns at immense scale, speed, and reach presents new challenges to identify and counter hostile influence operations (IO). This paper presents an end-to-end framework to automate detection of disinformation narratives, networks, and influential actors. The framework integrates natural language processing, machine learning, graph analytics, and a novel network causal inference approach to quantify the impact of individual actors in spreading IO narratives. We demonstrate its capability on real-world hostile IO campaigns with Twitter datasets collected during the 2017 French presidential elections, and known IO accounts disclosed by Twitter. Our system detects IO accounts with 96% precision, 79% recall, and 96% area-under-the-PR-curve, maps out salient network communities, and discovers high-impact accounts that escape the lens of traditional impact statistics based on activity counts and network centrality. Results are corroborated with independent sources of known IO accounts from U.S. Congressional reports, investigative journalism, and IO datasets provided by Twitter. Our system enables automated discovery of high-impact accounts, along with community detection and analysis of uncertainty quantification regarding these accounts' misinformation status. Equally, our framework can be used to automate hate speech detection, misinformation modeling, and botnet research.","822":"The COVID-19 pandemic has been generating unprecedent disruption on practically every aspect of society throughout the world. In addition to the health and economical impacts, there is an enormous emotional toll associated with the constant stress of daily life with the numerous restrictions in place to combat the pandemic. To better understand the impact of COVID-19 on people's lives, we propose a novel method based on analyzing the user reviews published by Google Trends (GT) users towards building a continuous graph\/network of places where the users live. Using this network, we can analyze both direct and indirect effects produced by COVID-19 through GT data. We apply Italian restriction measures against COVID-19 and show how their economic indicators affect the psychological well-being of citizens residing in these countries. Furthermore, we build a list of main factors contributing to the change in mobility patterns in Italy. Finally, we discuss some possible research directions for future work exploiting the massive text mining available at https:\/\/github.com\/calciu\/C0VID19-reviews. Our results allow researchers and policy makers to better understand the impact of COVID-19 on people's lives using online searches and information posted by Twitter users. This study contributes to the scientific development of COVID-","823":"Medical imaging AI systems such as disease classification and segmentation are increasingly inspired and transformed from computer vision based AI systems. Although an array of adversarial training and\/or loss function based defense techniques have been developed and proved to be effective in computer vision, defending against adversarial attacks on medical images remains largely an uncharted territory due to the following unique challenges: 1) label scarcity in medical images significantly limits adversarial generalizability of the AI system; 2) vastly similar and dominant fore- and background in medical images make it hard samples for learning the discriminating features between different disease classes; and 3) crafted adversarial noises added to the entire medical image as opposed to the focused organ target can make clean and adversarial examples more discriminate than that between different disease classes. In this paper, we propose a novel robust medical imaging AI framework based on Semi-Supervised Adversarial Training (SSAT) and Unsupervised Adversarial Detection (UAD), followed by designing a new measure for assessing systems adversarial risk. We systematically demonstrate the advantages of our robust medical imaging AI system over the existing adversarial defense techniques under diverse real-world settings of adversarial attacks using a benchmark OCT imaging data set. The results show that our method achieves superior performance over all competing defense","824":"Mutations sometimes increase contagiousness for evolving pathogens. During an epidemic, scientists use viral genome data to infer a shared evolutionary history and connect this history to geographic spread. We propose a model that directly relates a pathogen's evolution to its spatial contagion dynamics -- effectively combining the two epidemiological paradigms of phylogenetic inference and self-exciting process modeling -- and apply it to the latest COVID-19 pandemic. The key idea is to formalize the joint dynamic stochastic behavior of our model and the inferred multi-relational structure of the underlying genotype network. Our approach successfully models the current COVID-19 pandemic, and provides insights into managing effective public health policies aimed at curbing the disease. Finally, we find a surprising application of our model to laboratory influenza research. Instead of developing more accurate or complex simulations, we present a simple visual framework which enables us to explore the potential impact of various intervention strategies on the spread of influenza in heterogeneous contact networks. Specifically, we focus on the worldwide airline network and simulate mutations there from scratch with the aim of quantifying the interplay between the risk of infection and each of the thousands of flights that pass through the airport onto both global and regional scale. Our work illustrates how powerful our model","825":"As humanity struggles to contain the global COVID-19 pandemic, privacy concerns are emerging regarding confinement, tracing and testing. The scientific debate concerning privacy of the COVID-19 tracing efforts has been intense, especially focusing on the choice between centralised and decentralised tracing apps. The privacy concerns regarding COVID-19 testing, however, have not received as much attention even though the privacy at stake is arguably higher. COVID-19 tests require the collection of samples. Those samples possibly contain viral material but inevitably also human DNA. Patient DNA is not necessary for the test but it is technically impossible to avoid collecting it. The unlawful preservation, or misuse, of such samples at a massive scale may hence disclose patient DNA information with far-reaching privacy consequences. Inspired by the cryptographic concept of\"Indistinguishability under Chosen Plaintext Attack\", this paper poses the blueprint of novel types of tests allowing to detect viral presence without leaving persisting traces of the patient's DNA. Authors are listed in alphabetical order. We refer to our proposed design as\"Inguistic Trace Test\". In particular, we discuss how the general public can be deceived by fake news and why the sample should be collected only with specific medical equipment (e.g. Smartphones). We further","826":"COVID-19 has created a global pandemic with high morbidity and mortality in 2020. Novel coronavirus (nCoV), also known as Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV2), is responsible for this deadly disease. International Committee on Taxonomy of Viruses (ICTV) has declared that nCoV is highly genetically similar to SARS-CoV epidemic in 2003 (89% similarity). Limited number of clinically validated Human-nCoV protein interaction data is available in the literature. With this hypothesis, the present work focuses on developing a computational model for nCoV-Human protein interaction network, using the experimentally validated SARS-CoV-Human protein interactions. Initially, level-1 and level-2 human spreaders are identified in SARS-CoV-Human interaction network, using Susceptible-Infected-Susceptible (SIS) model. These potential human spreaders are considered as nodes in ICTV structure. A gene-ontology based fuzzy affinity function has been used to construct the nCoV-Human protein interaction network at 99.98% specificity threshold. This also identifies the level-1 human spreaders for COVID-19","827":"Clinical image analysis has been adopted to diagnose COVID-19 and other pathologies in practice. In this study, we propose a novel deep learning architecture, namely CovidCarex, for automated diagnosis of COVID-19 from chest X-ray (CXR) images. We construct a 3D heterogeneous feature representation inspired by the anatomy of the human lungs and related tissues. To facilitate the modeling of complex shapes,we introduce a dense layer with octave transfer learning capability for spatial modeling and also a multi-scale GAN model with multilevel discrimination capability for detailed semantic segmentation. The proposed network combines the advantages of both 2D and 3D architectures and exhibits superior performance compared to the state-of-the-art methods on two public CXR datasets. Our method reaches 90.8% Average Precision which outperforms the baseline method by a large margin. The comprehensive experiments demonstrate the effectiveness of our proposed architecture on the detection of COVID-19 cases. Furthermore, we extract interpretable features for visualizing the judgment based on the learned medical findings. This can provide clinical doctors with a quantitative estimate of disease severity and inform patients about the risk of developing severe symptoms if diagnosed late. All code details are available at https:\/\/github.com","828":"It is a widely known and important problem in network science, representing real-world scenarios, to uncover the underlying spreading dynamics from observations and\/or interventions (e.g., traffic jams or dynamical systems). The goal is to find the generic network patterns for this approach while avoiding detection of the specific networks that are bi-partite, non-bi-partite, etc. Herein, we propose a novel framework, called Multi-scale Feature Guided Search Network (MSF-GNN), which combines multi-scale feature extraction with self-similarity based on the GAN architecture. In particular, MSF-GNN captures both the topological and temporal features of the input network using only a few simple graph neural networks and utilizes them in parallel during the search process. We demonstrate that our method outperforms state-of-the-art methods in terms of prediction accuracy and F1 score on artificial and empirical networks. Moreover, it achieves good performance on the CIFAR10 dataset, managing to discover the underlying spreading dynamics of the credit default network. Finally, we show the applicability of our model by evaluating it on the MIT Complex Networks dataset and detecting the COVID-19 spreading dynamics, obtaining excellent results. Our code is available at https","829":"We present a formal approach based on graphical causal models to identify the\"root causes\"of the change in the probability distribution of variables. After factorizing the joint distribution into conditional distributions of each variable, given its parents (the\"causal mechanisms\"), we attribute the change to changes of these causal mechanisms. This attribution analysis accounts for the fact that mechanisms often change independently and sometimes only some of them change. Through simulations, we study the performance of our distribution change attribution method. We then present a real-world case study identifying the drivers of the difference in the income distribution between men and women. Our results show that our identification procedure can successfully detect the sources of gender differences in income distribution, which are typically hidden by comparing aggregate data. Finally, we provide recommendations for improving the fairness of non-linear classifiers so as to minimize bias when estimating causal effects. The code is available at https:\/\/github.com\/olivesgatech\/fairness_analysis_model_. In addition, we present a novel algorithm for efficiently computing global solutions of the multi-armed bandit problem, including online stochastic approximation bandsit, parameterized complexity theory and statistical property of the proposed algorithm. We believe that this contribution opens up new research directions for understanding the changing dynamics of economic","830":"In this paper, we study the dynamics of COVID-19 in Lebanon using the data collected by the Epidemiological Surveillance Program of Lebanon's Ministry of Public Health. We develop both a compartmental model as well as a stochastic approach to analyze the evolution of the pandemic under different scenarios. We provide simulation results that illustrate the main features of the spread of the virus in Lebanon, such as the increase of infections and deaths due to the pandemic. In particular, our modeling framework can be used to simulate the scenario of the Lebanese Ministry of Public Health for the case of heterogeneous or nonhomogeneous population. The main feature of our model is the fact that it considers the heterogeneity of the population not only on the level of individuals' susceptibility but also on the time of social interactions between them. This constitutes an important improvement compared to existing models where only the mean number of contacts are considered. Our model allows us to test various scenarios of public policy measures and strategies that aim at reducing the effects of the current pandemic. We show that relaxing severe restrictions right now will lead to a high death toll (average of 149,000). However, keeping schools open at present levels does not necessarily reduce the number of infections nor fatalities. Thus, we highlight the importance of coordinating","831":"We study the problem of estimating the parameters (i.e., infection rate and recovery rate) governing the spread of epidemics in networks. Such parameters are typically estimated by measuring various characteristics (such as the number of infected and recovered individuals) of the infected populations over time. However, these measurements also incur certain costs, depending on the population being tested and the times at which the tests are administered. We thus formulate the epidemic parameter estimation problem as an optimization problem, where the goal is to either minimize the total cost spent on collecting measurements, or to optimize the parameter estimates while remaining within a measurement budget. We show that these problems are NP-hard to solve in general, and then propose approximation algorithms with performance guarantees. We validate our algorithms using numerical examples. We additionally consider the scenario of noisy data, and demonstrate empirical results using real-world data from the COVID-19 pandemic. Our experimental results indicate that our proposed algorithms outperform the state-of-the-art methods used to estimate the epidemic parameters, especially when the latter fail to exist. Finally, we discuss the applicability of our method to other types of stochastic processes, including continuous-time SIS\/SIR models, as well as discrete-time difference equations. The code is","832":"Survival regression is used to estimate the relation between time-to-event and feature variables such as the Cox's proportional hazards model, which is useful for identifying high-risk individuals. In this paper, we propose a novel generative machine learning framework, called LinearSVC (LSC), with a new survival regression function. We show that LSC can be regarded as a marriage of ensemble learning with deep learning. Unlike traditional ensembling approaches that require training several models independently from scratch, LSC trains all shared parameters jointly in a single neural network. To illustrate its superiority over other existing methods, we evaluate LSC on three medical imaging datasets including head and neck CT, lung CT, and chest X-ray images. The results validate that LSC can provide more accurate predictions than state-of-the-art alternatives while requiring less fine-tuning data samples. Finally, we demonstrate its applicability by integrating LSC into a low-cost health care AI system for COVID-19 patient triaging. Our work highlights the potential of combining large-scale pre-trained language models with clinical decision support systems. It provides valuable insights for doctors regarding the most effective sub-models for their applications and shows that LSC can facilitate the translation of knowledge from clinical","833":"We analyze the global structure of the world-wide air transportation network, a critical infrastructure with an enormous impact on local, national, and international economies. We find that the world-wide air transportation network is a scale-free small-world network. In contrast to the prediction of scale-free network models, however, we find that the most connected cities are not necessarily the most central, resulting in anomalous values of the centrality. We demonstrate that these anomalies arise because of the multi-community structure of the network. We identify the communities in the air transportation network and show that the community structure cannot be explained solely based on geographical constraints, and that geo-political considerations have to be taken into account. We identify each city's global role based on its pattern of inter- and intra-community connections, which enables us to obtain scale-specific representations of the network. We further study whether the role of cities differs across countries and time, and conclude that there are significant variations in how the global structure shapes the networks associated with different geographies. Our analysis provides strategic insights into the design of effective intervention strategies to mitigate the effects of future pandemics, while also pointing to promising directions for research. The underlying framework opens up new avenues for studying the very first step of","834":"This paper develops a methodology for tracking in real time the impact of the COVID-19 pandemic on economic activity by analyzing high-frequency electricity market data. The approach is validated by several robustness tests and by contrasting our estimates with the official statistics on the recession caused by COVID-19 in different European countries during the first two quarters of 2020. Compared with the standard indicators, our results are much more chronologically disaggregated and up-to-date and, therefore, can inform the current debate on the appropriate policy response to the pandemic. Unsurprisingly, we find that nations that experienced the most severe initial outbreaks also grappled with the hardest financial recessions. However, we detect diffused signs of recovery, with economic activities in most European countries returning to their pre-pandemic level by August 2020. Furthermore, we show how delaying intervention or pursuing 'herd immunity' are not successful strategies, since they increase both economic disruption and mortality. The most effective short-run strategy to minimize the impact of the pandemic appears to be the introduction of early and relatively less stringent non-pharmaceutical interventions. The resulting containment policies are subsequently optimal from a public policy perspective under the assumption of full adherence to the imposed constraints. We illustrate this dynamic aspect","835":"We explore what it would mean for a generative AI model to be socially responsible\u2014that is, to reason about the decisions or actions of other agents with respect to both their own future (what they will do) and the welfare of society (how these policies treat people). We argue that, due to its essential role as a collective decision-making tool, social responsibility is one of the most important features of a complex system. In this paper, we present two tractable models that demonstrate how a social planner can promote responsibility through incentives. The first model suggests a policy that has random effects but is also submodular, such that the planner provides additional income besides the random factor. The second model demonstrates a different kind of agent, where the player believes that the planner is trying to maximize her\/her utility function. We show that if the planner attaches public goods to individuals then the latter becomes more concerned with the former's behavior; otherwise, the planner needs to improve his\/her strategy. These results suggest that whether the human body should be involved in the design of algorithms depends on the perspective of the planner. This means that there could be multiple regimes of optimal social responsibility, each dependent on the particular circumstances. Finally, we discuss some real-world implications of this research","836":"The outbreak of novel coronavirus disease 2019 (COVID-19) has already infected millions of people and is still rapidly spreading all over the globe. Most COVID-19 patients suffer from lung infection, so one important diagnostic method is to screen chest radiography images, e.g., X-Ray or CT images. However, such examinations are time-consuming and labor-intensive, leading to limited diagnostic efficiency. To solve this issue, AI-based technologies, such as deep learning, have been used recently as effective computer-aided means to improve diagnostic efficiency. However, one practical and critical difficulty is the limited availability of annotated COVID-19 data, due to the prohibitive annotation costs and urgent work of doctors to fight against the pandemic. This makes the learning of deep diagnosis models very challenging. To address this, motivated by that typical pneumonia has similar characteristics with COVID-19 and many pneumonia datasets are publicly available, we propose to conduct domain knowledge adaptation from typical pneumonia to COVID-19. There are two main challenges: 1) the discrepancy of data distributions between domains; 2) the task difference between the diagnosis of typical pneumonia and COVID-19. To address them, we propose a new deep domain adaptation method for COVID","837":"Real-time detection of COVID-19 using radiological images has gained priority due to the increasing demand for fast diagnosis of COVID-19 cases. This paper introduces a novel two-phase approach for classifying chest X-ray images. Deep Learning (DL) methods fail to cover these aspects since training and fine-tuning the model's parameters consume much time. In this approach, the first phase comes to train a deep CNN working as a feature extractor, and the second phase comes to use Extreme Learning Machines (ELMs) for real-time detection. The main drawback of ELM-based detectors is to meet the need of large number of hidden-layer nodes to gain a reliable and accurate detector in applying image processing since the detective performance remarkably depends on the setting of initial weights and biases. Therefore, this paper uses Chimp Optimization Algorithm (ChOA) to improve results and increase the reliability of the network while maintaining real-time capability. The designed detector is to be benchmarked on the COVIDX dataset, and the proposed method outperforms all other state-of-the-art models with 98% relative error rate (RER). The advantages of the proposed method are threefold; it can provide better generalization performance than vanilla","838":"Tether, one of the most essential and widely capitalized type of blockchain technology, has revolutionized many aspects of our daily lives by enabling people to transact electronically and privately with any pair of hands on their physical assets. However, tether is also fragmented into different types of tokens such as Ethereum and Flow. This paper studies the dynamic characteristics of tether network infrastructure and its relation to the exchange rate among cryptocurrencies. We specifically investigate the correlation between tether's price direction and its distance from the gravity center of the decentralized universe. Our numerical results show that the market influences the fundamentalist community known as Dark Nexus (derived from Q-turn) significantly more than the mainstream community known as Bitcoin. Besides, the fractionalization of tether networks leads to non-linear variations in terms of both the volume and the number of public keys. We provide a detailed analysis of the free-form conditions for the existence of a stable match between the two communities. For several practical scenarios, we observe that the cost of restraining from export bans can be significantly reduced if there is sufficient control over the supply chain during the pandemic. The time period where the recovery of the key indices takes place after the lockdown may be used to design suitable prevention strategies. During the research process, we develop tools","839":"The recent rapid advancements in artificial intelligence research and deployment have sparked more discussion about the potential ramifications of socially- and emotionally-intelligent AI. The question is not if research can produce such affectively-aware AI, but when it will. What will it mean for society when machines -- and the corporations and governments they serve -- can\"read\"people's minds and emotions? What should developers and operators of such AI do, and what should they not do? The goal of this article is to pre-empt some of the potential implications of these developments, and propose a set of guidelines for evaluating the (moral and) ethical consequences of affectively-aware AI, in order to guide researchers, industry professionals, and policy-makers. We propose a multi-stakeholder analysis framework that separates the ethical responsibilities of AI Developers vis-\\`a-vis the entities that deploy such AI -- which we term Operators. Our analysis produces two pillars that clarify the responsibilities of each of these stakeholders: Provable Beneficence, which rests on proving the effectiveness of the AI, and Responsible Stewardship, which governs responsible collection, use, and storage of data and the decisions made from such data. We end with recommendations for researchers, developers, operators, as well","840":"The classification of dark matter halos as isolated hosts or subhalos is critical to our understanding of structure formation and the galaxy-halo connection. Most commonly, subhalos are defined to reside inside a spherical overdensity boundary such as the virial radius. The resulting host-subhalo relations depend sensitively on the somewhat arbitrary overdensity threshold, but the impact of this dependence is rarely quantified. The recently proposed splashback radius tends to be larger and to include more subhalos than even the largest spherical overdensity boundaries. We systematically investigate the dependence of the subhalo fraction on the radius definition and show that it can vary by factors of unity between different spherical overdensity definitions. Using splashback radii can yet double the abundance of subhalos compared to the virial definition. We also quantify the abundance of flyby (or backsplash) halos, hosts that used to be subhalos in the past. We show that the majority of these objects are mislabeled satellites that are naturally classified as subhalos when we use the splashback radius. We compare our results to self-similar universes to show that the subhalo and flyby fractions are not universal with redshift and cosmology. Finally, we provide updated","841":"Triangulum, M33, is a low mass, relatively undisturbed spiral galaxy that offers a new regime in which to test models of dynamical heating. In spite of its proximity, the dynamical heating history of M33 has not yet been well constrained. In this work we present the TREX Survey, the largest stellar spectroscopic survey across the disk of M33. We present the stellar disk kinematics as a function of age to study the past and ongoing dynamical heating of M33. We measure line of sight velocities for ~4,500 disk stars. Using a subset, we divide the stars into broad age bins using Hubble Space Telescope and Canada-France-Hawaii-Telescope photometric catalogs: massive main sequence stars and helium burning stars (~80 Myr), intermediate mass asymptotic branch stars (~1 Gyr), and low mass red giant branch stars (~4 Gyr). We compare the stellar disk dynamics to that of the gas using existing HI, CO, and Halpha kinematics. We find that the disk of M33 has relatively low velocity dispersion (~16 km\/s), and unlike in the Milky Way and Andromeda galaxies, there is no strong trend in velocity dispersion as a function","842":"For building successful Machine Learning (ML) systems, it is imperative to have high quality data and well tuned learning models. But how can one assess the quality of a given dataset? And how can the strengths and weaknesses of a model on a dataset be revealed? Our new tool PyHard employs a methodology known as Instance Space Analysis (ISA) to produce a hardness embedding of a dataset relating the predictive performance of multiple ML models to estimated instance hardness meta-features. This space is built so that observations are distributed linearly regarding how hard they are to classify. The user can visually interact with this embedding in multiple ways and obtain useful insights about data and algorithmic performance along the individual observations of the dataset. We show in a COVID prognosis dataset how this analysis supported the identification of pockets of hard observations that challenge ML models and are therefore worth closer inspection, and the delineation of regions of strengths and weaknesses of ML models. A case study involving 99,147 diabetes related tweets from 1,336 patients demonstrates the potential usefulness of ISA for improving the accuracy of vision tasks and highlight features of ML algorithms that are capable of explaining the decisions made by ISA. Overall, we argue that further improvements could be achieved if more patient-specific datasets are available to improve","843":"Finder networks in general, and Apple's Find My network in particular, can pose a grave threat to users' privacy and even health if these networks are abused for stalking. Apple has partially addressed the issue of release mechanisms, but there is still a gap in understanding regarding how and when they will be released. We report on measurements taken from one of the largest Android malware detectors currently in use, MaMaDroid, which analyzes the control flow graph of the application. We find that similar to Apple's DoS attacks, it can only handle basic types of flows (e.g., HTTP\/2-based) with its default parameters, and very few additional operations are needed to reach most state-of-the-art levels of performance. However, we also witness instances where the attacker changes malicious samples such that those samples will be misclassified as benign; this behavior is particularly evident during times where the diversity of CFG patterns is high enough that some classifiers cannot distinguish between different sources. Our findings indicate that there is a broad lack of research towards making inference about the provenance of these files, and hence do not provide us with any solution to protect against abuse by tracking devices. We suggest instead that existing solutions should be improved to make them more robust against","844":"The present ongoing global pandemic caused by SARS-CoV-2 virus is creating havoc across the world. The absence of any vaccine as well as any definitive drug to cure, has made the situation very grave. Therefore only few effective tools are available to contain the rapid pace of spread of this disease, named as COVID-19. On 24th March, 2020, the the Union Government of India made an announcement of unprecedented complete lockdown of the entire country effective from the next day. No exercise of similar scale and magnitude has been ever undertaken anywhere on the globe in the history of entire mankind. This study aims to scientifically analyze the implications of this decision using a kinetic model covering more than 96% of Indian territory. This model was further constrained by large sets of realistic parameters pertinent to India in order to capture the ground realities prevailing in India, such as: (i) true state wise population density distribution, (ii) accurate state wise infection distribution for the zeroth day of simulation (20th March, 2020), (iii) realistic movements of average clusters, (iv) rich diversity in movements patterns across different states, (v) migration patterns across different geographies, (vi) different migration patterns for pre- and post-COVID-19 outbreak,","845":"Coronaviruses are enveloped, non-segmented positive-sense RNA viruses that have the largest genome among RNA viruses. The genome contains a large replicase ORF encodes nonstructural proteins (NSPs), structural and accessory genes. NSP15 is a nidoviral RNA uridylate-specific endoribonuclease (NendoU) has C-terminal catalytic domain. The endoribonuclease activity of NSP15 interferes with the innate immune system of the host cell. Here, we screened Selleckchem Natural product database of compounds against the NSP15, Thymopentin and Oleuropein showed highest binding energies. The binding of these molecules was further validated by Molecular dynamic simulation and found very stable complexes. These drugs might serve as effective counter molecules in the reduction of virulence of this virus. Future validation of both these inhibitors are worth consideration for patients being treated for COVID -19. In addition, thrombosiscentroid and vascular occlusion dataset are also included in the study. We propose that these inhibitors may be helpful to treat early stage COVID-19. Overall, the proposed approach based on molecular docking and simulation can help","846":"Software development has become significantly more social than ever before. Contemporary software builds on a complex foundation of diverse software artifacts -- ranging from programming to libraries and tools -- which are used in conjunction with machine learning techniques. A wide range of developers use these software artifacts to build programs and systems for their daily needs. However, it is not clear what happens to the thousands of programs and systems developed by them over time. In this paper we explore the problem of program repair vs. creation (i.e., adding missing dependencies into existing ones) using Graph-of-Things (GoT), a new paradigm of large-scale data analysis found outside the box. We present a framework where graphs provide a visual representation of partially completed software projects and automatically suggest ways to compose them, enabling GoT users to create new apps from existing components. Furthermore, we demonstrate how our approach strikes a balance between automation and manual exploration, and provide empirical evidence about its benefits for both. We also discuss the many challenges that plague medical imaging applications, including integrating multiple sources of patient data, achieving coverage beyond 90%+, and providing direct assistance to clinicians via personalized recommendations. Finally, we show how our overall solution yields up to 50 times better average recommendation scores than state-of-the-art methods, and","847":"The increasing availability of curated citation data provides a wealth of resources for analyzing and understanding the intellectual influence of scientific publications. In the field of statistics, current studies of citation data have mostly focused on the interactions between statistical journals and papers, limiting the measure of influence to mainly within statistics itself. In this paper, we take the first step towards understanding the impact statistics has made on other scientific fields in the era of Big Data. By collecting comprehensive bibliometric data from the Web of Science database for selected statistical journals, we investigate the citation trends and compositions of citing fields over time to show that their diversity has been increasing. Furthermore, we use the local clustering technique involving personalized PageRank with conductance for size selection to find the most relevant statistical research area for a given external topic of interest. We provide theoretical guarantees for the procedure and, through a number of case studies, show the results from our citation data align well with our knowledge and intuition about these external topics. Overall, we have found that the statistical theory and methods recently invented by the statistics community have made increasing impact on other scientific fields. This study opens up new avenues for enhancing the interpretability of scientific theories and lays the foundation for future efforts seeking to make them more discoverable to the general public. Contact: t","848":"The 3rd-generation (3G) mobile communication system has been rolled out in many countries and with its rapid adoption, it has become a new reality for people to experience indoor mobility with their mobile devices. In this context, WiFi6, as an extension of the 2nd-generation (2D) Internet architecture, is expected to be widely used both at home and during travel. In fact, according to the Utopian domain expert network log, the majority of IoT devices consume Wi-Fi 6 or some other signal types than the background noise. The research community has proposed numerous deep learning models to classify these signals either as background noise or by proximity using Convolutional Neural Networks (CNNs). However, existing studies have shown performance degradation when they are deployed on portable networks like smartphones. Due to the lack of large-scale publicly available datasets, we attempt to improve the quality of CNN inference on such smartphone-based networks through a transfer learning approach. Specifically, we propose the Embedding Unmanned Aerial Vehicle (EUAV) simulator which can simulate the real-world environment from user perspectives and test the efficacy of various state-of-the-art deep learning architectures under realistic conditions. Furthermore, we provide a comprehensive empirical analysis of different architectural choices","849":"We consider two large datasets consisting of all games played among top-tier European soccer clubs in the last 60 years, and among professional American basketball teams in the past 70 years. We leverage game data to build networks of pairwise interactions between the head coaches of the teams, and measure their career performance in terms of network centrality metrics. We identify Arsene Wenger, Sir Alex Ferguson, Jupp Heynckes, Carlo Ancelotti, and Jose Mourinho as the top 5 European soccer coaches of all time. In American basketball, the first 5 positions of the all-time ranking are occupied by Red Auerbach, Gregg Popovich, Phil Jackson, Don Nelson, and Lenny Wilkens. We further establish rankings by decade and season. We develop a simple methodology to monitor performance throughout a coach's career, and to dynamically compare the performance of two or more coaches at a given time. The manuscript is accompanied by the website coachscore.luddy.indiana.edu where complete results of our analysis are accessible to the interested readers. Our website serves as a blueprint for other researchers aiming to apply similar methods to study different aspects of football and basketball careers. All analyses considered are focused on the global rather than local level, although relevant countries may be subject to additional","850":"In this work, the problem of localizing ground devices (GDs) is studied comparing the performance of four range-free (RF) localization algorithms that use a mobile anchor (MA). All the investigated algorithms are based on the so-called heard\/not-heard (HnH) method, which allows the GDs to detect the MA at the border of their antenna communication radius. Despite the simplicity of this method, its efficacy in terms of accuracy is poor because it relies on the antenna radius that continuously varies under different conditions. Usually, the antenna radius declared by the manufacturer does not fully characterize the actual antenna radiation pattern. In this paper, the radiation pattern of the commercial DecaWave DWM1001 Ultra-Wide-Band (UWB) antennas is observed in a real test-bed at different altitudes for collecting more information and insights on the antenna radius. The compared algorithms are then tested using both the observed and the manufacturer radii. The experimental accuracy is close to the expected theoretical one only when the antenna pattern is actually omnidirectional. However, typical antennas have strong pattern irregularities that decrease the accuracy. For improving the performance, we propose range-based (RB) variants of the compared algorithms in which, instead of using the observed or the","851":"We are presenting COVID-19Base, a knowledgebase highlighting the biomedical entities related to COVID-19 disease based on literature mining. To develop COVID-19Base, we mine the information from publicly available scientific literature and related public resources. We considered seven topic-specific dictionaries, including human genes, human miRNAs, human lncRNAs, diseases, Protein Databank, drugs, and drug side effects, are integrated to mine all scientific evidence related to COVID-19. We have employed an automated literature mining and labeling system through a novel approach to measure the effectiveness of drugs against diseases based on natural language processing, sentiment analysis, and deep learning. To the best of our knowledge, this is the first knowledgebase dedicated to COVID-19, which integrates such large variety of related biomedical entity types through literature mining. Proper investigation of the mined biomedical entities along with the identified interactions among those, reported in COVID-19Base, would help the research community to discover possible ways for the therapeutic treatment of COVID-19. In addition, the dataset used in this study contains enough samples to investigate successful bio-entities (such as ADMET and Drug Side Effects) at least from the perspective of text mining. This study also reveals","852":"This study adopts three neural network-based models (i.e., BERT, Albert, and Julia) to identify informative COVID-19 tweets. We first train these models on a random dataset of Tweets while covering topics from different domains (i.e., Education, Health, and Other). Then we conduct two clustering experiments to find out the most relevant sub-topics. Finally, we integrate the topic-aware clusters into a third model which was trained on the official Twitter data with extra parameters. Our evaluation shows that this third model achieved the highest F1 score for the sub-topics of INFORMATIVE label. In addition, our results show that using more effective words can improve the performance of language models. Overall, the proposed approach contributes to the research by proposing a novel method to extract topical structure from real-world text. This may facilitate the early screening of potential informative texts during crises such as the ongoing COVID-19 pandemic. Lastly, we also demonstrate how our findings contribute to improving the accessibility of balanced and unbiased information on social media. The code and datasets are available at https:\/\/github.com\/zliucr\/informative_topic_model.git.io\/. Keywords: Informative CO","853":"Visual information displays are typically composed of multiple visualizations that are used to facilitate an understanding of the underlying data. A common example are dashboards, which are frequently used in domains such as finance, process monitoring and business intelligence. However, users may not be aware of existing guidelines and lack expert design knowledge when composing such multi-view visualizations. In this paper, we present semantic snapping, an approach to help non-expert users design effective multi-view visualizations from sets of pre-existing views. When a particular view is placed on a canvas, it is\"aligned\"with the remaining views -- not with respect to its geometric layout, but based on aspects of the visual encoding itself, such as how data dimensions are mapped to channels. Our method uses an on-the-fly procedure to detect and suggest resolutions for conflicting, misleading, or ambiguous designs, as well as to provide suggestions for alternative presentations. With this approach, users can be guided to avoid common pitfalls encountered when composing visualizations. Our provided examples and case studies demonstrate the usefulness and validity of our approach. An accompanying user study demonstrates the effectiveness of our approach under different conditions than usual. All code and examples are available online. https:\/\/github.com\/annahaensch\/semantic-","854":"Bot Detection is an essential asset in a period where Online Social Networks(OSN) is a part of our lives. This task becomes more relevant in crises, as the Covid-19 pandemic, where there is an incipient risk of proliferation of social bots, producing a possible source of misinformation. In order to address this issue, it has been compared different methods to detect automatically social bots on Twitter using Data Selection. The techniques utilized to elaborate the bot detection models include the utilization of features as the tweets metadata or the Digital Fingerprint of the Twitter accounts. In addition, it was analyzed the presence of bots in tweets from different periods of the first months of the Covid-19 outbreak, using the bot detection technique which best fits the scope of the task. Moreover, this work includes also analysis over aspects regarding the discourse of bots and humans, such as sentiment or hashtag utilization. As a result, it can be stated that Twitter bot detection is a complex task, but one with high relevance for both politics and public health issues, due to the inherent characteristics of such types of users. Therefore, automation of this process is required. Previous research indicates that natural language processing techniques can help us better understand the presence of bots in tweets, achieving a higher precision than data selection","855":"Precisely defining the terminology is the first step in scientific communication. Developing neural text generation models for definition generation can circumvent the labor-intensity curation, further accelerating scientific discovery. Unfortunately, the lack of large-scale terminology definition dataset hinders the process toward definition generation. In this paper, we present Graphex, a massive parallel corpus generated by two state-of-the-art deep learning architectures, including BERT. We employ GAPEX to define terminologies on PubMed abstracts and generate a total of 34k terminology definition pairs using UMLS, which covers 843 types of cells. Experiments show that Gaphex is able to produce accurate definitions with high precision (63.1\\% in terms of P@5), compared to existing approaches such as DefiniBandit and spaCyber. Finally, we demonstrate the utility of Gapehex via case studies from medicine to COVID-19 research. All code and data are available at https:\/\/github.com\/allenai\/GAPEX.git. Our website contains interactive notebooks, scripts, and leaderboards under development status. All collected datasets are publicly available at https:\/\/gapsex.shinyapps.io\/datasets\/gapextreme","856":"Here we propose and implement a generalized mathematical model to find the time evolution of population in infectious diseases and apply the model to study the recent COVID-19 pandemic. Our approach consists of setting up a differential equation system (DE) such that, after some nonlinearity, the DE can be reasonably solved by means of ordinary differential equations (ODEs). The parameters of the ODE are identified by employing a simple least-squares method. We start with the linear case and then extend it to the nonlinear regime using a quadratic polynomial approximation. For both cases, we obtain similar results within $1\\%$ accuracy. At last, we present numerical simulations to demonstrate the actual behavior of the model. In particular, for the SEIR model, we show how the peak infection rate and final death ratio change as a function of $\\alpha$. Furthermore, we explain how the decrease in these quantities indicates the increase in the number of infected individuals during the lockdown period. From our work, we conclude that a better understanding of the spread dynamics of epidemics in health facilities would require monitoring of the real data more closely than what is foreseen by usual models. Finally, we discuss how the lack of efficiency in finding the optimal parameter values implies that predictions","857":"This work describes a simple agent model for the spread of an epidemic outburst, with special emphasis on mobility and geographical considerations, which we characterize via statistical mechanics and numerical simulations. As the mobility is decreased, a percolation phase transition is found separating a free-propagating phase in which the outburst spreads without finding spatial barriers and a localized phase in which the outburst dies off. Interestingly, the number of infected agents is subject to maximal fluctuations at the transition point, building upon the unpredictability of the evolution of an epidemic outburst. Our model also lends itself to testing vaccination schedules. Indeed, it has been suggested that if a vaccine is available but scarce it is convenient to carefully select the vaccination program to maximize the chances of halting the outburst. We discuss and evaluate several schemes, with special interest on how the percolation transition point can be shifted, allowing for higher mobility without epidemiological impact. Finally, we consider the case of an outbreak that persists in time indefinitely, and study both quarantine and containment strategies, focusing on integrating the vaccine into the routine healthcare system. Again, our model demonstrates the importance of acting decisively when closing down, and the benefits of using vaccines once the herd immunity is achieved. If there is no vaccine, then investing efforts should aim at helping people survive","858":"Learning visual concepts from raw images without strong supervision is a challenging task. In this work, we show the advantages of prototype representations for understanding and revising the latent space of neural concept learners. For this purpose, we introduce interactive Concept Swapping Networks (iCSNs), a novel framework for learning concept-grounded representations via weak supervision and implicit prototype representations. iCSNs learn to bind conceptual information to specific prototype slots by swapping the latent representations of paired with the semantic structure of the concepts being prototyped. This semantically grounded and discrete latent representation allows human users to understand and revise their knowledge with no coding or fine-tuning required. We support this claim by conducting experiments on our novel data set\"Elementary Concept Reasoning\"(ECR) tasks, which requires participants to reason about the relationships between elements. Our results demonstrate the effectiveness of iCSNs in helping cognitive scientists reconstruct complex ideas based on few prior beliefs, while improving the accuracy of different reconstruction methods. We also discuss potential challenges with iCSNs and encourage future research to further investigate them. One of the most interesting observations from our experimental results is that iCSNs do not seem to require auxiliary pretraining, only 3D residual networks are needed for accurately predicting element associations.\"Smart\"and\"f","859":"The joint EDBT\/ICDT conference (International Conference on Extending Database Technology\/International Conference on Database Theory) is a well established conference series on data management, with annual meetings in the second half of March that attract 250 to 300 delegates. Three weeks before EDBT\/ICDT 2020 was planned to take place in Copenhagen, the rapidly developing Covid-19 pandemic led to the decision to cancel the face-to-face event. In the interest of the research community, it was decided to move the conference online while trying to preserve as much of the real-life experience as possible. As far as we know, we are one of the first conferences that moved to a fully synchronous online experience due to the COVID-19 outbreak. With fully synchronous, we mean that participants jointly listened to presentations, had live Q&A, and attended other live events associated with the conference. In this report, we share our decisions, experiences, and lessons learned. We also present three key themes that describe our reflections on moving the conference online: The changing purposes and opportunities of running virtual conferences, the challenges and pitfalls encountered, and the way we dealt with miscommunication. This article is part of the theme issue \u2018Data management for digital contact trace\u2019","860":"The coronavirus disease 2019 (COVID-19) has changed the world since the World Health Organization declared its outbreak on 30th January 2020, recognizing the outbreak as a pandemic on 11th March 2020. As often said in the news media, the objective is\"to flatten the curve\", or\"push the peak down\", or similar wording, of the epidemic's spread. Central to the official advice for managing the COVID-19 pandemic provided by the Centers for Disease Control and Prevention (CDC) is the importance of understanding the dynamics of the infection, and how it can be effectively contained with non-pharmaceutical interventions. The CDC advises against the use of face masks, social distancing, contact tracing, and other personal protective equipment (PPE), which can make people more susceptible to the disease through respiratory droplets. To push the peak down, governments have used temporary ad-hoc policy measures, such as stay-at-home orders, and have encouraged the use of PPE, including mask filters. While these actions help reduce the transmission rate of the disease, they also create an economic impact. In this paper, we present a framework to both estimate the cost of these policies and quantify their effectiveness at all times. We start by","861":"This article reports from a case study of a Language Learning Bauhaus VR hackathon with Goethe Institute. It was organized as an educational and research project to tap into the dynamics of transdisciplinary teams challenged with a specific requirement. In our case, it was to build a Bauhaus-themed German Language Learning VR App. We constructed this experiment to simulate how representatives of different disciplines may work together towards a very specific purpose under time pressure. So, each participating team consisted of members of various expert-fields: software development (Unity or Unreal), design, psychology and linguistics. The results of this study cast light on the recommended cycle of design thinking and customer-centered design in VR. Especially in interdisciplinary rapid prototyping conditions, where stakeholders initially do not share competences. They also showcase educational benefits of working in transdisciplinary environments. This study, combined with our previous work about human factors in rapid software development and co-design, including hackathons, allowed us to formulate recommendations for organizing content creation VR hackathons for specific purposes. We also provide guidelines on how to prepare the participants to work in rapid prototyping VR environments and benefit from such experiences in the long term. Our data set, consisting of three parts, is made publicly available. Part I","862":"Fighting the ongoing COVID-19 infodemic has been declared as one of the most important focus areas by the World Health Organization since the onset of the COVID-19 pandemic. While the information that is consumed and disseminated consists of promoting fake cures, rumors, and conspiracy theories to spreading xenophobia and panic, at the same time there is information (e.g., containing advice, promoting cure) that can help different stakeholders such as policy-makers. Social media platforms enable the infodemic and there has been an effort to curate the content on such platforms, analyze and debunk them. While a majority of the research efforts consider one or two aspects (e.g., detecting factuality) of such information, in this study we focus on a multifaceted approach, including an API,\\url{https:\/\/app.swaggerhub.com\/apis\/yifan2019\/Tanbih\/0.8.0\/} and a demo system,\\url{https:\/\/covid19.tanbih.org}, which we made freely and publicly available. We believe that this will facilitate researchers and different stakeholders. A screencast of the API services and demo is available.\\url{https:\/\/youtu.be","863":"The widespread availability of large amounts of soccer data in the era of social media has provided an opportunity to develop novel approaches to analyze and visualize the impact of the coronavirus pandemic on the game world. In this study, we consider two simple measures for quantifying the impact of COVID-19 on a sports team's performance -- winning probability (WP) and average plus-minus player ratings over time -- and investigate their relationship with other important factors such as the market value ($M$). We use longitudinal high-resolution panel regression models to exploit the multivariate relationships between the two outcome measures and the covariates associated with the home teams' performance throughout the 2020 season. Results from our statistical analyses show that the positive effect of COVID-19 on a players' performance was observable across all five major premier European football leagues, where the difference between the WP and average plus-minus player rating of COVID-19 ranged from 5% to 19%. The results also suggest that the negative effect of COVID-19 on a players' performance decreased significantly by more than $10\\%$ but only for one of the five major leagues. All together, these findings demonstrate the potential of aggregate human judgment to complement more traditional simulation-based methods for estimating the impact","864":"There has been great concern in the UK and abroad about the role of social distancing after the Covid-19 outbreak. But one group which has always been more active on the social network side are people from the BAME (Black And Minority Ethnic) community. These are also the people who are at greater risk of dying from Covid-19, as they have a lower chance of recovery without any medical intervention. In this study we examine the online discussion around the UN Conference of The Parties on Climate Change (COP) using social media data from the BAME and minority ethnic communities in the United Kingdom and the USA respectively. We find that during COP26 there was a notable shift between the two groups with the majority of the US population being vaccinated for the first time. This observation then raises the question:\"What if the vaccine coverage were not high?\". While we cannot eliminate all of the uncertainty surrounding the number of vaccines administered, our results highlight the importance of updating the parameters when the amount of new information becomes available. If the information source did indeed provide reliable numbers, then religion and ethnicity can be equated by focusing only on the current fraction of the population. To further explore the topic, we use a sample stratification framework to perform a multinomial analysis based","865":"The classification of dark matter halos as isolated hosts or subhalos is critical for our understanding of structure formation and the galaxy-halo connection. Most commonly, subhalos are defined to reside inside a spherical overdensity boundary such as the virial radius. The resulting host-subhalo relations depend sensitively on the somewhat arbitrary overdensity threshold, but the impact of this dependence is rarely quantified. The recently proposed splashback radius tends to be larger and to include more subhalos than even the largest spherical overdensity boundaries. We systematically investigate the dependence of the subhalo fraction on the radius definition and show that it can vary by factors of unity between different spherical overdensity definitions. Using splashback radii can yet double the abundance of subhalos compared to the virial definition. We also quantify the abundance of flyby (or backsplash) halos, hosts that used to be subhalos in the past. We show that the majority of these objects are mislabeled satellites that are naturally classified as subhalos when we use the splashback radius. We compare our results to self-similar universes to show that the subhalo and flyby fractions are not universal with redshift and cosmology. Finally, we provide updated","866":"All pandemics are local; so learning about the impacts of pandemics on public health and related societal issues at granular levels has become more important than ever. COVID-19 is affecting everyone in the globe and mask wearing is one of the few precautions against it. To quantify people's perception of mask effectiveness and to prevent the spread of COVID-19 for small areas, we use Understanding America Study's (UAS) survey data on COVID-19 as our primary data source. Our data analysis shows that direct survey-weighted estimates for small areas could be highly unreliable. In this paper we develop a synthetic estimation method to estimate proportions of mask effectiveness for small areas using a logistic model that combines information from multiple data sources. We select our working model using an extensive data analysis facilitated by a new variable selection criterion for survey data and benchmarking ratios. We propose a Jackknife method to estimate variance of our proposed estimator. From our data analysis. it is evident that our synthetic data outperform direct survey-weighted estimator with respect to commonly used evaluation measures. We further explore how our synthetic data influence the results of direct testing in a hypothetical epidemic scenario where only 70% of the population wear masks. The outcome of such simulation study suggests","867":"Situations in which no scientific consensus has been reached due to either insufficient, inconclusive or contradicting findings place strain on governments and public organizations which are forced to take action under circumstances of uncertainty. In this chapter, we focus on the case of COVID-19, its effects on children and the public debate around the reopening of schools. The aim is to better understand the relationship between policy interventions in the face of an uncertain and rapidly changing knowledge landscape and the subsequent use of scientific information in public debates related to the policy interventions. Our approach is to combine scientific information from journal articles and preprints with their appearance in the popular media, including social media. First, we provide a picture of the different scientific areas and approaches, by which the effects of COVID-19 on children are being studied. Second, we identify news media and social media attention around the COVID-19 scientific output related to children and schools. We focus on policies and media responses in three countries: Spain, South Africa and the Netherlands. These countries have followed very different policy actions with regard to the reopening of schools and represent very different policy approaches to the same problem. We analyse the activity in (social) media around the debate between COVID-19, children and school closures by focusing","868":"The COVID-19 pandemic has significantly altered many aspects of people's lives and behaviors as well as global mobility patterns, including urban traffic patterns. Although several studies are available on the impact of the pandemic on traffic patterns, few analyses exist in the context of spatial systems, which have been found to be highly impacted by the pandemic. In this study, we examine the impact of the coronavirus disease (COVID-19) on spatio-temporal pattern nevertheless interacts with socioeconomic characteristics of individuals across the United States. We utilize cluster analysis and multiple correspondence analysis (MCA) to examine how the spread of COVID-19 influences social distancing measures and travel patterns. Our findings show that the 2020 coronavirus pandemic disproportionately impacts lower income groups and races, while there was a more notable decline in commuting time for high income groups. Among other conclusions, we also find that the increase in vehicle miles driven at the end of the year 2020 leads to a less decrease in future hospitalizations and deaths, suggesting that fewer trips into areas with higher proportions of seniors may be safer. Finally, we provide insights into how our results can inform government agencies tasked with managing transportation during the pandemic. As one of the first studies to analyze the effect of a","869":"The COVID-19 pandemic has been a scourge upon humanity, claiming the lives of more than 5.1 million people worldwide; the global economy contracted by 3.5% in 2020. This paper presents a COVID-19 calculator, synthesizing existing published calculators and data points, to measure the positive U.S. socio-economic impact of a COVID-19 AI\/ML pre-screening solution (algorithm&application). The COVID-19 calculator also includes two other categories\u2014(i) \u201cIsolate Me\u201d and (ii) \u201cDo Not Look Like Us\u201d. These sub-categories represent approximately 90% of the U.S. population living below the poverty line. The median income level for a household with one person who is socially assistive is about $3,400 per year. If we exclude the households who do not provide social assistance then about $2,200 per year would be required to support the most fragile populations during this time. Our results indicate that addressing the needs of the most vulnerable requires 675,000 total hours of labor per year which will further exacerbate the dire situation with respect to their health outcomes. We conclude that there is no simple way to solve the enormous challenge of","870":"The shortage of high-quality teachers is one of the biggest educational problems faced by underdeveloped areas. With the development of information and communication technologies (ICTs), China has begun a remote co-teaching intervention program using ICTs for rural classes, forming a unique co-teaching classroom. We conducted semi-structured interviews with nine remote urban teachers and twelve local rural teachers. We identified the remote co-teaching classes standard practices and co-teachers collaborative work process. We also found that remote teachers high-quality class directly impacted local teachers and students. Furthermore, interestingly, local teachers were also actively involved in making indirect impacts on their students by deeply coordinating with remote teachers and adapting the resources offered by the remote teachers. We conclude by summarizing and discussing the challenges faced by teachers, lessons learned from the current program, and related design implications to achieve a more adaptive and sustainable ICT4D program design. Our code is available online at https:\/\/github.com\/JizhiziLi\/adaptive-covid19-teacher-program.git.io\/. This paper discusses how our findings are applicable to other countries, including those currently engaged in the similar programs or even implement them differently. Finally, we provide some possible future","871":"In this paper we propose two algorithms for solving nonparametric regression problems): polynomial regression (PR) and Gaussian process regression (GP). We define these models by their corresponding target functionals, which are obtained from the minimization of a loss function. Our goal is to find a set of optimal hyperparameters for each functional so as to achieve a desired coverage probability without any additional computational cost compared to state of the art in numerical methods. In order to estimate the number of parameters required to reach a given target value, we propose to use Bayesian optimization with Gaussian process priors on the objective function. This approach allows us to both determine the number of parameters needed by our model and also to adaptively guide the choice of the prior distribution. An extensive simulation study shows that our two algorithms deliver superior performance to standard Bayesian optimization approaches and also outperform existing nonparametric regression methods such as Lasso and Elastic Net. Finally, we demonstrate the applicability of our two algorithms on two real data sets - the average length of hospital stay for COVID-19 in South Korea and the mean severity of pneumonia in Italy. We hope that our work enables researchers to efficiently solve healthcare prediction tasks. All code is available at https:\/\/github.com\/h","872":"The study introduces a new analysis scheme to analyze trace data and visualize students' self-regulated learning strategies in a mastery-based online learning modules platform. The pedagogical design of the platform resulted in fewer event types and less variability in student trace data. This paper presents three neural network models for predicting if a student has started studying using the module's quizzes, which were built on either machine learning or deep learning algorithms. In addition, we present how these neural networks can be used to analyze pupil-level video frames without collecting any extra parameters from the camera. The proposed method interacts with the existing state-of-the-art mouse trajectory dataset to capture features relevant to determining whether a student has started studying using videos. We also propose two loss functions to address the overfitting problem and reduce the overall model's complexity. On average, the 3 models achieve 0.78 Spearman correlation coefficient (SCC) performance between predictions within 5% error intervals while only requiring one minute of computing time each to predict a single test score once the entire cluster is completed. To our knowledge, this is the first work to analyze students' self-regulation learning strategy in such a fine-grained manner and thus providing a valuable tool for educators to improve their teaching style. Furthermore,","873":"This paper addresses the problem of single view 3D human reconstruction. Recent implicit function based methods have shown impressive results, but they fail to recover fine face details in their reconstructions. This largely degrades user experience in applications like 3D telepresence. In this paper, we focus on improving the quality of face in the reconstruction and propose a novel Jointly-aligned Implicit Face Function (JIFF) that combines the merits of the implicit function based approach and model based approach. We employ a 3D morphable face model as our shape prior and compute space-aligned 3D features that capture detailed face geometry information. Such space-aligned 3D features are combined with pixel-aligned 2D features to jointly predict an implicit face function for high quality face reconstruction. We further extend our pipeline and introduce a coarse-to-fine architecture to predict high quality texture for our detailed face model. Extensive evaluations have been carried out on public datasets and our proposed JIFF has demonstrates superior performance (both quantitatively and qualitatively) over existing state-of-the-arts. Qualitative results indicate that the MIMI can yield remarkable quality improvements without any trade-off between computation time and model accuracy. A total of four new datasets related to COVID-19 imaging","874":"Due to privacy concerns of users and law enforcement in data security and privacy, it becomes more and more difficult to share data among organizations. Data federation brings new opportunities to the world of work by offering abstract data interfaces. With the development of cloud computing, organizations store data on the edge to achieve elasticity and scalability for data processing. The existing approaches generally only consider one aspect, which is either execution time or monetary cost, and do not consider data partitioning for hard constraints. In this paper, we propose an approach to enable collaborative reasoning with the help of true-data and a practical framework for data sharing. We first formalize the problem as a multi-objective, non-linear constrained optimization problem. To solve it, we develop a novel algorithm, namely FedNI, based on neural network inference to guide the solution of the transformed optimization problem. Furthermore, we provide a theoretical analysis of the convergence rate of FedNI from the perspective of the Rademacher theory. Finally, we conduct experiments to evaluate the effectiveness of our proposed approach in both optimizing the utility of real-world data and meeting the accuracy requirement in a standard database environment. Our experiment results demonstrate the outstanding performance of our approach (up to 2 orders of magnitude in terms of average F1 score)","875":"Network embedding techniques are powerful to capture structural regularities in networks and to identify similarities between their local fabrics. However, conventional network embedding models are developed for static structures, commonly consider nodes only and they are seriously challenged when the network is varying in time. Temporal networks may provide an advantage in the description of real systems, but they code more complex information, which could be effectively represented only by a handful of methods so far. Here, we propose a new method of event embedding of temporal networks, called weg2vec, which builds on temporal and structural similarities of events to learn a low dimensional representation of a temporal network. This projection successfully captures latent structures and similarities between events involving different nodes at different times and provides ways to predict the final outcome of spreading processes unfolding on the temporal structure. We apply this approach to a variety of problems, such as identifying political leaders who coordinate events during the same period where others (e.g., followers of the leader) also organize into groups and execute different training tasks. Experimental results show that our method outperforms several baseline methods for event embeddings, while it achieves comparable performance with regard to forecasting accuracy for predictions spanning over two weeks. These findings demonstrate the potential of using our method as an auxiliary tool for modeling dynamics","876":"A novel Coronavirus disease (COVID-19) has been emerged in Wuhan, China since late December 2019. It consists of a viral pneumonia with serious respiratory symptoms that infected people have difficulty breathing and coughing, leading to severe pneumonia in humans. If diagnosed at the early stage, however, most people can be effectively treated with antimalarial therapy. The current outbreak presents a challenge for researchers; it requires a fast diagnosis method to contain the further spread of this epidemic. In this study we propose a deep learning model based on chest CT images to diagnose COVID-19 patients. We first prepare a dataset of 5,000 Chest CT images from 8 hospitals between January 1st and April 30th, 2020. A public convolutional neural network (CNN) model is then trained using the proposed dataset to predict whether a patient is infected with COVID-19 or not. After tuning parameters, the CNN model shows promising results when tested on the test set of 2,600 images. The overall accuracy of 92.8 % is achieved while the sensitivity of 95.7 % and specificity of 93.9 % are obtained by selecting suitable values of the hyperparameters. Moreover, we also achieve an average F1 score of 0.93 which indicates the","877":"Cloud computing enables remote execution of users tasks. The pervasive adoption of cloud computing in smart cities services and applications requires timely execution of tasks adhering to Quality-of-Service (QoS). However, the increasing use of computing servers exacerbates the issues of high energy consumption, operating costs, and environmental pollution. Maximizing the performance and minimizing the energy in a cloud data center is challenging. In this paper, we propose a performance and energy optimization bi-objective algorithm to tradeoff the contradicting performance and energy objectives. An evolutionary algorithm-based multi-objective optimization is for the first time proposed using system performance counters. The performance of the proposed model is evaluated using a realistic cloud dataset in a cloud computing environment. Our experimental results achieve higher performance and lower energy consumption compared to a state of the art algorithm. Furthermore, the bitrate required by the proposed video streaming method is almost 30% faster than the rate of real world videos. The superiority of the proposed approach will be more evident with its low latency design and efficient implementation. https:\/\/github.com\/srvCodes\/energy_efficient_cloud_data_center_operating_systems_and_resources_in_austria\/. Further, https:\/\/youtu.be\/P","878":"AI and Machine Learning can offer powerful tools to help in the fight against Covid-19. In this paper we present a study and a concrete tool based on machine learning to predict the prognosis of hospitalised patients with Covid-19. In particular we address the task of predicting the risk of death of a patient at different times of the hospitalisation, on the base of some demographic information, chest X-ray scores and several laboratory findings. Our machine learning models use ensembles of decision trees trained and tested using data from more than 2000 patients. An experimental evaluation of the models shows good performance in solving the addressed task. We also illustrate how the model can be easily modified by adding further layers before the output classification step, resulting in a significant gain of 15% in accuracy compared to the best possible baseline. Such results are very important to identify clinically relevant features for the early mortality prediction during the hospitalization period, which could lead to better treatment and care planning. Finally, we provide an online web service for sharing our predictions with healthcare professionals via email. It is made available under https:\/\/anonymous.4open.science\/r\/0e5ad6b7-ac02f1a2d3dbdb\/s\/covid19","879":"The ongoing COVID-19 pandemic continues to affect communities around the world. To date, almost 6 million people have died as a consequence of COVID-19, and more than one-quarter of a billion people are estimated to have been infected worldwide. The design of appropriate and timely mitigation strategies to curb the effects of this and future disease outbreaks requires close monitoring of their spatio-temporal trajectories. We present machine learning methods to anticipate sharp increases in COVID-19 activity in US counties in real-time. Our methods leverage Internet-based digital traces -- e.g., disease-related Internet search patterns from the general population and clinicians, disease-relevant Twitter microblogs, and outbreak trajectories from neighboring locations -- to monitor potential changes in population-level health trends. Motivated by the need for finer spatial-resolution epidemiological insights to improve local decision-making, we build upon previous retrospective research efforts originally conceived at the state level and in the early months of the pandemic. Our approaches -- tested in real-time and in an out-of-sample manner on a subset of 97 counties distributed across the US -- frequently anticipated sharp increases in COVID-19 activity 1-6 weeks before the onset of local outbreaks (defined as the time when","880":"Augmented reality technology is one of the leading technologies in the context of Industry 4.0. The promising potential application of augmented reality in industrial production systems has received much attention, which led to the concept of industrial augmented reality. On the one hand, this technology provides a suitable platform that facilitates the registration of information and access to them to help make decisions and allows concurrent training for the user while executing the production processes. This leads to increased work speed and accuracy of the user as a process operator and consequently offers economic benefits to the companies. Moreover, recent advances in the internet of things, smart sensors, and advanced algorithms have increased the possibility of widespread and more effective use of augmented reality. Currently, many research pieces are being done to expand the application of augmented reality and increase its effectiveness in industrial production processes. This research demonstrates the influence of augmented reality in Industry 4.0 while critically reviewing the industrial augmented reality history. Afterward, the paper discusses the critical role of industrial augmented reality by analyzing some use cases and their prospects. With a systematic analysis, this paper discusses the main future directions for industrial augmented reality applications in industry 4.0. The article investigates various areas of application for this technology and its impact on improving production conditions. Finally, the challenges that this technology faces and","881":"While digital divide studies primarily focused on access to information and communications technology (ICT) in the past, its influence on other associated dimensions such as privacy is becoming critical with a far-reaching impact on the people and society. For example, the various levels of government legislation and compliance on information security measures around the world have created a new era of digital divide in the privacy preservation domain. In this article, the concept\"digital privacy divide (DPD)\"is introduced to describe the perceived gap in the privacy preservation of individuals based on the geopolitical location of different countries. To better understand DPD phenomenon, we created an online questionnaire and collected answers from more than 700 respondents from four different countries (the United States, Germany, Bangladesh, and India) who come from two distinct cultural orientations as per Hofstede's individualist vs. collectivist society. However, our results revealed some interesting findings. DPD does not depend on Hofstede's cultural orientation of the countries. For example, individuals residing in Germany and Bangladesh share similar privacy concerns, while there is a significant similarity among individuals residing in the United States and India. Moreover, while most respondents acknowledge the importance of privacy legislation to protect their digital privacy, they do not mind their governments to allow domestic companies and organizations collecting","882":"We review and introduce FiberStars, a novel method for learning hierarchicallystructured prefixes -- lightweight modules prepended to a larger component -- in a continual fashion. We consider both cases of fixed and increasing network dimension and we show that our approach provides better generalization than standard hierarchical learning methods. Finally, we demonstrate the utility of FiberStars for learning from complex data by applying it to the global health science network. The results show that our approach has scalability benefits while maintaining effectiveness. Specifically, compared with conventional hierarchical training, Factor-star clustering algorithm can leverage more than 100% of the information contained in the factor structure without any degradation in the learned embeddings. In addition, because the factors are interpreted as modular building blocks from which two or more layers can be directly connected, this interpretation allows us to meaningfully extract the most important variables in determining the dependencies between the different layers of the architecture. To further improve the performance of our model, we suggest incorporating a back-propagation mechanism into the learning process. This can enable the fact-checkers to efficiently detect missing connections and help mitigate the propagation of errors. Our experimental results indicate that our proposed framework achieves state-of-the-art performances on three real-world datasets for knowledge graph completion (","883":"Recently the first laser spectroscopy measurement of the radioactive RaF molecule has been reported [Nature 581, 396 (2020)]. This and similar molecules are considered to search for the New Physics effects. The radium nucleus is of interest as it is octupole-deformed and has close levels of opposite parity. The preparation of such experiments can be simplified if there are reliable theoretical predictions. It is shown that the accurate prediction of these hyperfine structures would require taking into account the finite magnetization distribution inside the ion. For atoms, this effect is known as the Bohr-Weisskopf (BW) effect. Its magnitude depends on the model of the nuclear magnetism distribution which is usually not well known. We show that it is possible to express the nuclear magnetism distribution contribution to the hyperfine structure constant in terms of one magnetization distribution dependent parameter: BW matrix element for $1s$-state of the corresponding hydrogen-like ion. This parameter can be extracted from the accurate experimental and theoretical electronic structure data for an ion, atom, or molecule without the explicit treatment of any nuclear magnetism distribution model. This approach can be applied to predict the hyperfine structure of atoms and \\textit{molecules} and allows one to separate the","884":"Different retail and e-commerce companies are facing the challenge of assembling large numbers of time-critical picking orders that include both small-line and multi-line orders. To reduce unproductive picker working time as in traditional picker-to-parts warehousing systems, different solutions are proposed in the literature and in practice. For example, in a mixed-shelves storage policy, items of the same stock keeping unit are spread over several shelves in a warehouse; or automated guided vehicles (AGVs) are used to transport the picked items from the storage area to packing stations instead of human pickers. This is the first paper to combine both solutions, creating what we call AGV-assisted mixed-shelves picking systems. We model the new integrated order batching and routing problem in such systems as an extended multi-depot vehicle routing problem with both three-index and two-commodity network flow formulations. Due to the complexity of the integrated problem, we develop a novel variable neighborhood search algorithm to solve the integrated problem more efficiently. We test our methods with different sizes of instances, and conclude that the improved computational performance of our methods enables them to be applied into real-life problems. Our variable neighborhood search algorithm provides optimal solutions within an acceptable computational","885":"This paper addresses the problem of automated diagnosis of disease classes from chest X-ray images. To build this classifier, we propose a convolutional neural network called CNN+LSTM with Attention mechanism. The proposed model learns both visual and semantic representations of the input image during the training process. For the sake of improving the performance of the model, three techniques are incorporated into the loss function: one more token per line, a new approach to combine low-level features and dense features, and another technique for highlighting different regions which contain important clues. By adding these mechanisms, the learned classifier has better generalization ability than standard methods. We conduct extensive experiments on four public datasets comprising two large benchmark datasets and two smaller datasets. The results show that our method achieves superior performances compared with state-of-the-art methods. Our code is available at https:\/\/github.com\/hz-zhu\/covid19_weak_supervised_methods.git. This version contains supplementary material, which is available to authorized users. Copyright may be transferred without notice, after which this version may no longer be accessible online. The source code can be found at https:\/\/github.com\/hz-zhu\/Covid19_weak_super","886":"How will the coronavirus disease 2019 (COVID-19) pandemic develop in the coming months and years? Based on an expert survey, we examine key aspects that are likely to influence the COVID-19 pandemic in Europe. The challenges and developments will strongly depend on the progress of national and global vaccination programs, the emergence and spread of variants of concern (VOCs), and public responses to non-pharmaceutical interventions (NPIs). In the short term, many people remain unvaccinated, VOCs continue to emerge and spread, and mobility and population mixing are expected to increase. Therefore, lifting restrictions too much and too early risk another damaging wave. This challenge remains despite the reduced opportunities for transmission given vaccination progress and reduced indoor mixing in summer 2021. In autumn 2021, increased indoor activity might accelerate the spread again, whilst a necessary reintroduction of NPIs might be too slow. The incidence may strongly rise again, possibly filling intensive care units, if vaccination levels are not high enough. A moderate, adaptive level of NPIs will thus remain necessary. These epidemiological aspects combined with economic, social, and health-related consequences provide a more holistic perspective on the future of the COVID-19 pandemic. We expect that this review","887":"The knowledge of the carrier dynamics in nanostructures is of fundamental importance for the development of (opto)electronic devices. This is true for semiconducting nanostructures as well as for plasmonic nanoparticles (NPs). Indeed, improvement of photocatalytic efficiencies by combining semiconductor and plasmonic nanostructures is one of the reasons why their ultrafast dynamics are intensively studied. In this work, we will review our activity on ultrafast spectroscopy in nanostructures carried out in the recently established EuroFEL Support Laboratory. We have investigated the dynamical plasmonic responses of metal NPs both in solution and in 2D and 3D arrays on surfaces, with particular attention being paid to the effects of the nanoparticle shape and to the conversion of absorbed light into heat on a nano-localized scale. We will summarize the results obtained on the carrier dynamics in nanostructured perovskites with emphasis on the hot-carrier dynamics and in semiconductor nanosystems such as ZnSe and Si nanowires, with particular attention to the band-gap bleaching dynamics. Subsequently, the study of semiconductor-metal NP hybrids, such as","888":"Computing Education Research (CER) is critical for supporting the increasing number of students who need to learn computing skills. To systematically advance knowledge, publications must be clear enough to support replications, meta-analyses, and theory-building. The goal of this study is to characterize the reporting of empiricism in CER literature by identifying whether publications include information to support replications, meta-analyses, and theory building. The research questions are: RQ1) What percentage of papers in CER venues have not reported formal testing procedures? RQ2) What are the characteristics of the empirical evaluations? RQ3) Do the papers with empirical evaluation follow reporting norms (both for inclusion and for labeling of key information)? We conducted an SLR of 427 papers published during 1950-2019 from five CER venues: SIGCSE TS, ICER, ITiCSE, TOCE, and CSE. We developed and applied the CER Empiricism Assessment Rubric. Over 80% of papers had some form of empirical evaluation. Quantitative evaluation methods were the most frequent. Papers most frequently reported results on interventions around pedagogical techniques, curriculum, community, or tools. There was a split in papers that had some type of comparison between","889":"The COVID-19 pandemic has affected people's lives around the world on an unprecedented scale. We intend to investigate hoarding behaviors in response to the pandemic using large-scale social media data. First, we collect hoarding-related tweets shortly after the outbreak of the coronavirus. Next, we analyze the hoarding and anti-hoarding patterns of over 42,000 unique Twitter users in the United States from March 1 to April 30, 2020, and dissect the hoarding-related tweets by age, gender, and geographic location. We find the percentage of females in both hoarding and anti-hoarding groups is higher than that of the general Twitter user population. Furthermore, using topic modeling, we investigate the opinions expressed towards the hoarding behavior by categorizing these topics according to demographic and geographic groups. We also calculate the anxiety scores for the hoarding and anti-hoarding related tweets using a lexical approach. By comparing their anxiety scores with the baseline Twitter anxiety score, we reveal further insights. The LIWC anxiety mean for the hoarding-related tweets is significantly greater than the baseline Twitter anxiety mean. Interestingly, beer has the highest calculated anxiety score compared to other hoarded items mentioned in the tweets. Moreover, our analysis shows that the","890":"Due to the different losses caused by various photovoltaic (PV) array faults, accurate diagnosis of fault types is becoming increasingly important. Compared with a single one, multiple PV stations collect sufficient fault samples, but their data is not allowed to be shared directly due to potential conflicts of interest. Therefore, federated learning can be exploited to train a collaborative fault diagnosis model. However, the modeling efficiency is seriously affected by the model update mechanism since each PV station has a different computing capability and amount of data. Moreover, for the safe and stable operation of the PV system, the robustness of collaborative modeling must be guaranteed rather than simply being processed on a central server. To address these challenges, a novel asynchronous decentralized federated learning (ADFL) framework is proposed. Each PV station not only trains its local model but also participates in collaborative fault diagnosing by exchanging model parameters to improve the generalization without losing accuracy. The global model is aggregated distributedly to avoid central node failure. By designing the asynchronous update scheme, the communication overhead and training time are greatly reduced. Both the experiments and numerical simulations are carried out to verify the effectiveness of the proposed method. Results show that the proposed method achieves better performance than the state-of-the-art baseline. Furthermore","891":"The outbreak of COVID-19 disease caused more than 100,000 deaths so far in the USA alone. It is necessary to conduct an initial screening of patients with the symptoms of COVID-19 disease to control the spread of this diseases. However, it is becoming laborious to conduct the tests with the available testing kits due to the growing number of patients. Some studies proposed CT scan or chest X-ray images as an alternative solution. Therefore, it is essential to use every available resource, instead of either a CT scan or chest X-ray to conduct a large number of tests simultaneously. As a result, this study aims to develop a deep learning-based model that can detect COVID-19 patients with better accuracy both on CT scan and chest X-ray image dataset. In this work, eight different deep learning approaches such as VGG16, InceptionResNetV2, ResNet50, DenseNet201, VGG19, MobilenetV2, NasNetMobile, and ResNet15V2 have been tested on two dataset-one dataset includes 400 CT scan images, and another dataset includes 400 chest X-ray images studied. Besides, Local Interpretable Model-agnostic Explanations (LIME) is used to explain","892":"In this work we study the inflation mechanism driven by the Barrow Holographic dark energy (BHDE) in the early universe. BHDE is based on the Barrow relation for horizon entropy, which in turn is inspired from the shape of the COVID-19 virus. It was shown by Barrow that the quantum gravitational effects may instigate complex fractal features in the structure of a black hole. Since the length scale during the inflation is expected to be small, the energy density obtained from the application of the holographic principle in the early universe will be large enough to support the inflationary scenario. Using the Granda-Oliveros IR cut-off we have studied the inflationary scenario with the universe filled with BHDE. Various analytic solutions for the model were found out including the slow-roll parameters, scalar spectral index and tensor-to-scalar ratio. Since inflation is generally attributed to the presence of scalar fields, we have explored a correspondence between BHDE and scalar field models. Both canonical scalar field and the Tachyonic scalar field have been considered for this purpose. The evolution of the potential generated from the fields are plotted and found to be consistent with the observations. From the work we","893":"Existing tensor completion formulation mostly relies on partial observations from a single tensor. However, tensors extracted from real-world data are often more complex due to: (i) Partial observation: Only a small subset (e.g., 5%) of tensor elements are available. (ii) Coarse observation: Some tensor modes only present coarse and aggregated patterns (e.g., monthly summary instead of daily reports). In this paper, we are given a subset of the tensor and some aggregated\/coarse observations (along one or more modes) and seek to recover the original fine-granular tensor with low-rank factorization. We formulate a coupled tensor completion problem and propose an efficient Multi-resolution Tensor Completion model (MTC) to solve the problem. Our MTC model explores tensor mode properties and leverages the hierarchy of resolutions to recursively initialize an optimization setup, and optimizes on the coupled system using alternating least squares. MTC ensures low computational and space complexity. We evaluate our model on two COVID-19 related spatio-temporal tensors. The experiments show that MTC could provide 65.20% and 75.79% percentage of fitness (PoF) in tens","894":"Stochastic simulations such as large-scale, spatiotemporal, age-structured epidemic models are computationally expensive at fine-grained resolution. We propose Spatiotemporal Neural Processes (STNP), a neural latent variable model to mimic the spatiotemporal dynamics of stochastic simulators. To further speed up training, we use a Bayesian active learning strategy to proactively query the simulator, gather more data, and continuously improve the model. Our model can automatically infer the latent processes which describe the intrinsic uncertainty of the simulator. This also gives rise to a new acquisition function based on latent information gain. Theoretical analysis demonstrates that our approach reduces sample complexity compared with random sampling in high dimension. Empirically, we demonstrate that our framework can faithfully imitate the behavior of a complex infectious disease simulator with a small number of examples, enabling rapid simulation and scenario exploration. Finally, we show that STNP can be used to efficiently simulate age-structured epidemic models using a moderate amount of real-world data. Additional code is available at https:\/\/github.com\/zalandoresearch\/stochastik.git.io\/. Video tutorials demonstrating the usage of our model are available at https:\/\/youtu.be\/zh","895":"We propose a general methodology to measure labour market dynamics, inspired by the search and matching framework, based on the estimate of the transition rates between job roles. We show how to estimate instantaneous transition rates starting from discrete time observations provided in longitudinal datasets, allowing for any number of categories. This approach allows us to analyse different aspects of the labour market, such as the evolution of the skill level of workers, or the impact of policy measures aimed at curbing the spread of COVID-19. First, we illustrate the potential of this kind of data source, using Italian labour market data. Then, we demonstrate how to use it with machine learning methods, in particular deep neural network algorithms, to estimate the transition rates. The results indicate that the effective reproduction number can be larger than one million years, and that the migration trend seems to be increasing over time. Finally, we highlight some limitations of the existing procedure, pointing out possible future improvements. Overall, this paper provides a novel alternative source of information about the labour market, which can complement more traditional sources, such as official statistics, labor demand estimates, and stock exchange data. It also establishes itself as a reference for future methodological developments in the field. An interactive dashboard version of the paper is available online. [English] (","896":"Starting with the exact factorization of the molecular wavefunction, this paper presents the results from the numerical implementation in nonadiabatic molecular dynamics of the recently proposed bohmion method. Within the context of quantum hydrodynamics, we introduce a regularized nuclear Bohm potential admitting solutions comprising a train of $\\delta$-functions which provide a finite-dimensional sampling of the hydrodynamic flow paths. The bohmion method inherits all the basic conservation laws from its underlying variational structure and captures electronic decoherence. After reviewing the general theory, the method is applied to the well-known Tully models, which are used here as benchmark problems. In the present case of study, we show that the new method accurately reproduces both electronic decoherence and nuclear population dynamics. We also illustrate how the different methods can be combined for more accurate predictions in many-body systems. Perspectives toward increasingly complex and computationally expensive stages of the development of nonhydrated molecular dynamics simulations are presented. Finally, the common challenge of estimating the diffusion coefficient in molecular flows is discussed. The results obtained and insights gained from this study would contribute to the development of novel high-fidelity approaches for studying numerous physical processes at atomistic scales. All calculations reported","897":"Older adults have been hit disproportionally hard by the COVID-19 pandemic. One critical way for older adults to minimize the negative impact of COVID-19 and future pandemics is to stay informed about its latest information, which has been increasingly presented through online interactive visualizations (e.g., live dashboards and websites). Thus, it is imperative to understand how older adults interact with and comprehend online COVID-19 interactive visualizations and what challenges they might encounter to make such visualizations more accessible to older adults. We adopted a user-centered approach by inviting older adults to interact with COVID-19 interactive visualizations while at the same time verbalizing their thought processes using a think-aloud protocol. By analyzing their think-aloud verbalisms, we identified four types of thought processes representing how older adults comprehended the visualizations and uncovered the challenges they encountered. Furthermore, we also identified the challenges they encountered with seven common types of interaction techniques adopted by the visualizations. Based on the findings, we present design guidelines for making interactive visualizations more accessible to older adults. To facilitate further research in this area, we are sharing our codebook and data consisting of 2,593 responses from 1,486 participants. This paper presents only the","898":"Operational networks commonly rely on machine learning models for many tasks, including detecting anomalies, inferring application performance, and forecasting demand. Yet, unfortunately, model accuracy can degrade due to concept drift, whereby the relationship between the features and the target prediction changes due to reasons ranging from software upgrades to seasonality to changes in user behavior. Mitigating concept drift is thus an essential part of operationalizing machine learning models, and yet despite its importance, concept drift has not been extensively explored in the context of networking -- or regression models in general. Thus, it is not well-understood how to detect or mitigate it for many common network management tasks that currently rely on machine learning models. As we show, concept drift cannot always be mitigated by periodic retraining models using newly available data, and doing so can even degrade model accuracy. In this paper, we characterize concept drift in a large cellular network for a metropolitan area in the United States. We find that concept drift occurs across key performance indicators (KPIs), regardless of model, training set size, and time interval -- thus necessitating practical approaches to detect, explain, and mitigate it. To do so, we develop Local Error Approximation of Features (LEAF). LEAF detects drift; explains features and time","899":"As COVID-19 is rapidly spreading across the globe, short-term modeling forecasts provide time-critical information for decisions on containment and mitigation strategies. A main challenge for short-term forecasts is the assessment of key epidemiological parameters and how they change as first governmental intervention measures are showing an effect. By combining an established epidemiological model with Bayesian inference, we analyze the time dependence of the effective growth rate of new infections. For the case of COVID-19 spreading in Germany, we detect change points in the effective growth rate that correlate well with the times of publicly announced interventions. Thereby, we can (a) quantify the effects of recent governmental measures to mitigating the disease spread, and (b) incorporate the corresponding change points to forecast future scenarios and case numbers. Our code is freely available and can be readily adapted to any country or region. We hope that this analysis will help gaining better insights into the effectiveness of public health policies by monitoring the changing progression of the pandemic. All collected data, including the daily number of newly infected cases, are now available online at https:\/\/github.com\/spang-lab\/covid19_monitoring_policy.git.io\/. Contact details for reaching out to the community are provided. Finally,","900":"The COVID-19 pandemic has influenced virtually all aspects of our lives. Across the world, countries have applied various mitigation strategies for the epidemic, based on social, political, and technological instruments. We postulate that one should {identify the relevant requirements} before committing to a particular mitigation strategy. One way to achieve it is through an overview of what is considered relevant by the general public, and referred to in the media. To this end, we have collected a number of news clips that mention the possible goals and requirements for a mitigation strategy. The snippets are sorted thematically into several categories, such as health-related goals, social and political impact, civil rights, ethical requirements, and so on. In a forthcoming companion paper, we will present a digest of the requirements, derived from the news clips, and a preliminary take on their formal specification. A third part is dedicated to verifying these requirements, and deriving answers. Our work is informed by extensive research on the domain, including a thorough review of the main guidelines, policies, and datasets. Finally, we highlight some potential challenges and open issues. All along with this document, our website contains a collection of new tools for studying (and being) satisfied with respect to various mitigating strategies, required to be","901":"SARS-CoV-2 (Severe Acute Respiratory Syndrome Coronavirus 2) has created an unprecedented global health crisis with worldwide 6-digital infection rates and thousands death tolls daily. Envelope protein E is one of the most important viral proteins and its inhibition opens up new possibilities for the treatment of patients infected by SARS-CoV-2. In this work, we propose the use of two evolutionary algorithms, namely Covide Search and Deep Reinforcement Learning (CRL), for finding potential candidates that can inhibit the main protease of SARS-CoV-2 virus. We have applied both methods to find small molecules that are potential CoV-2 inhibitors. The results from our computational efforts show that fortunellin (acacetin 7-O-neohesperidoside), a natural product found in the fruits of Citrus japonica var. margarita (kumquat), and ellagic acid (OA) were among the top candidates. The chemical structures of the phytochemicals tested suggest that fortunellin may represent O-glycosylated timelike on the membrane of ACE2 associated with viral entry into the human host. This hypothesis needs further validation to explore","902":"The worldwide spread of coronavirus disease (COVID-19) has become a threatening risk for global public health. It is of great importance to rapidly and accurately screen patients with COVID-19 from community acquired pneumonia (CAP). In this study, we propose a machine learning method to distinguish patients with COVID-19 from CAP using chest X-ray images. The automatic analysis and diagnosis of COVID-19 are made possible by our proposed deep learning model. We used 828 Chest X-ray images from two publically available datasets to achieve our goal. These images were all from patients confirmed as having had SARS-CoV-2 laboratory tests or who had suffered from viral pneumonia during their travel to work. For the classification model, we used the logistic regression algorithm. Our experimental results show that the proposed approach can help identify those with COVID-19 pathology with high accuracy. This may be helpful in helping healthcare systems allocate resources more effectively and accelerate the battle pace against COVID-19. To further improve the performance of the proposed framework, it could be applied to detect other pathologies based on the characteristics of other infections in the lungs. Moreover, the proposed framework can be adjusted to consider various application scenarios. As well as, it can be","903":"This work proposes a model for detecting COVID-19 positive subjects from their cough sounds. We have built a machine learning model, based on a Convolutional Neural Network (CNN), to classify the chest sounds of COVID-19 positive and non-COVID-19 individuals. The proposed model has been trained in both a binary classification manner with a single input sound file and by using all six senses with no additional features. Our results show that the CNN model successfully detects the presence of COVID-19 disease with high accuracy from the test set. Further, we propose a multi-input method for effectively improving the performance of our model. This leads us to achieve an accuracy of 98.4% for two-class classification and 99.3% for three-class classification. In addition, we provide an extensive analysis of the models' misclassified items. These experiments prove that the proposed model can be used as a supplementary tool for automatic screening of COVID-19 patients. Finally, we obtain an accuracy of 97.9% for four-class classification and 98.8% for five-class classification. Thus, this research demonstrates the feasibility of detecting COVID-19 positive subjects from their cough sounds. Although further studies are required to confirm this finding, it","904":"In this paper, we present Tracer Tokens, a hardware token of privacy-preserving contact tracing utilizing Exposure Notification \\cite{GAEN} protocol. Through subnetworks, we show that any disease spread by proximity can be traced such as seasonal flu, cold, regional strains of COVID-19, or Tuberculosis. Further, we show this protocol to notify $n^n$ users in parallel, providing a speed of information unmatched by current contact tracing methods. To demonstrate this effectiveness, we set up a simple channel where packets are broadcasted over multiple hops and then receive all messages passing through the endpoints at the same time. Our results show that there is a tradeoff between throughput and delay which stems from the overhead incurred by sending $\\bullet$ bits of information through channels for which the capacity of the buffers is not enough to absorb all the additional information. We propose alternative strategies to minimize this problem and test them through their effect on the performance of the technique. Finally, we provide theoretical guarantees for two important classes of random walkers, namely centralised and decentralized models of epidemic propagation. By implementing these techniques, we aim to reduce the number of infected individuals at any given point in time. This constitutes an improvement over known state of the art","905":"Contacts between people are the absolute drivers of contagious respiratory infections. For this reason, limiting and tracking contacts is a key strategy for the control of COVID-19. Digital contact tracing has been proposed as an automated solution to scale up traditional contact tracing. However, the required penetration of contact tracing apps within a population to achieve a desired target in the control of the epidemic is currently under discussion within the research community. In order to understand the effects of digital contact tracing, several mathematical models have been proposed. In this article, we survey the main ones and we propose a compartmental SEIR model with which it is possible, differently from the models in the related literature, to derive closed-form conditions regarding the control of the epidemic as a function of the contact tracing apps penetration and the testing efficiency. Closed-form conditions are crucial for the understandability of models, and thus for decision makers (including digital contact tracing designers) to correctly assess the dependencies within the epidemic. With our model, we find that digital contact tracing alone can rarely tame an epidemic: for unrestrained COVID-19, this would require a testing turnaround of around 1 day and app uptake above 80% of the population, which are very difficult to achieve in practice. However, digital contact tracing can still","906":"The COVID-19 pandemic has caused severe public health consequences in over 200 countries and territories around the world. These effects are only expected to increase with the continuous rise in infection cases after the outbreak of this virus was identified in China at the end of 2019. To prevent future outbreaks, accurate modeling of social distancing patterns between cities or regions by considering human mobility data can be very useful. In this study, we propose a novel spatio-temporal model for infectious disease spread using mobility data, which includes intra-city dynamics (e.g., traffic flow) among other factors. We develop a hierarchical Bayesian framework to quantify the uncertainty of each factor contributing to the change in mobility patterns across time. The proposed model is able to deal with heterogeneity in both spatial and temporal levels. By proposing a spatially heterogeneous compartmental model, we also investigate the impact of city size on the delay of infections' onset. Results show that larger cities seem to begin exponential spreading earlier and thus obtain lower migration rates during the first wave of infection. Furthermore, the dynamic transmission rate $\\beta$ seems to have a strong influence on urbanization and GDP, since the correlation between these two parameters is strongly correlated with the presence of airports. As a result, we suggest a precaution","907":"We propose novel techniques for learning model representations of temporal interaction graphs (TIG) using convex optimization methods including TensorFlow, Keras, and StyleGAN. Our goal is to leverage these neural network architectures to learn effective embeddings for dynamic graph structures which can be further adapted to perform reasoning tasks on top of them. We apply our learned embeddings to the state-of-the-art TIG dataset in order to outperform baseline models with respect to several existing metrics designed to measure the quality of generated embeddings. Moreover, we demonstrate the effectiveness of our proposed approach by also performing competitive empirical results against strong baselines. Code available at https:\/\/github.com\/jbr-ai-labs\/convex-embedding-based-interaction-graph-learning-using-case-study-contribution.gitlab.org\/ will be used to track which methods are able to generate useful embeddings for this problem. All code is available there as well as instructions for reproducibility purposes. Experiments are performed via simulation studies and illustrated by means of social media data sets. Code written during the research paper \\url{https:\/\/github.com\/jbr-ai-labs\/ConvXEL","908":"Asynchronous programming models (APM) are gaining more and more traction, allowing applications to expose the available concurrency to a runtime system tasked with coordinating the execution. While MPI has long provided support for multi-threaded communication and non-blocking operations, it falls short of adequately supporting APMs as correctly and efficiently handling MPI communication in different models is still a challenge. Meanwhile, new low-level implementations of light-weight, cooperatively scheduled execution contexts (fibers, aka user-level threads (ULT)) are meant to serve as a basis for higher-level APMs and their integration in MPI implementations has been proposed as a replacement for traditional POSIX thread support to alleviate these challenges. In this paper, we first establish a taxonomy in an attempt to clearly distinguish different concepts in the parallel software stack. We argue that the proposed tight integration of fiber implementations with MPI is neither warranted nor beneficial and instead is detrimental to the goal of MPI being a portable communication abstraction. We propose MPI Continuations as an extension to the MPI standard to provide callback-based notifications on completed operations, leading to a clear separation of concerns by providing a loose coupling mechanism between MPI and APMs. We show that this interface is flexible and interacts well","909":"We have designed a computational model of a virus spread near the outbreak threshold. Using computer simulation we studied the Susceptible - Infected - Recovered (SIR) process where in consequence of a force of habit that is manifested by the population mobility patterns, the recovered persons create the spatio-temporal patterns as the barriers to a virus transmission. The results show a spontaneous stopping of the virus spread without a need to infect the whole population, a non-trivial random noise of daily count of infected cases, and power laws of a cumulative count of infected cases. Outbreak evolution strongly depends on the initial conditions thus we concluded that the model has the features of chaotic systems that makes it difficult to predict its behaviors. We also found that long range correlations behind the average macroscopic dynamics of the system exist which confirms the presence of structural constraints. In addition, we illustrated how the lack of predictability can be addressed by introducing a suitable regularization term. As a result, our model showed how adaptive strategies could be used to control the epidemic spread while guaranteeing the population safety. Finally, we investigated how different factors affect the optimal controls for both short and long terms interventions. They reveal that if there is no predictor or even a consensus about the basic principles of infection spreading","910":"Since March 2020, companies nationwide have started work from home (WFH) due to the rapid increase of confirmed COVID-19 cases in an attempt to help prevent the coronavirus from spreading and rescue the economy from the pandemic. Many organizations have conducted surveys to understand people's opinions towards WFH. However, the findings are limited due to small sample size and the dynamic topics over time. This study aims to address these limitations by conducting a large-scale social media analysis on Twitter data with respect to both WFH and COVID-19. We conduct several experiments to identify popular words and phrases during the different phases of the pandemic and highlight the differences between them based on the sentiments shared online. Further, we investigate how the sentiment polarity of the language used in WFH tweets changed along with the increasing number of deaths reported daily. Finally, we discuss the implications of our findings for future research on effective public health communication strategies. Our code and datasets are available at https:\/\/github.com\/anyleopeace\/scrapping-based-analysis-of-social-media-data-with-respect-to-pandemic-related issues. They can be found at https:\/\/github.com\/anyleopeace\/covid","911":"In this paper, we present a game-theoretic model describing the voluntary social distancing during the spread of an epidemic. The payoffs of the agents depend on the social distancing they practice and on the probability of getting infected. We consider two types of agents, the vulnerable agents who have a small cost if they get infected, and the non-vulnerable agents who have a higher cost. For the modeling of the epidemic outbreak, a variant of the SIR model is considered, involving populations of susceptible, infected, and recovered persons of vulnerable and non-vulnerable types. The Nash equilibria of this social distancing game are studied. We then analyze the case where the players, desiring to achieve a low social inequality, pose a bound on the variance of the payoffs. In this case, we introduce a notion of Generalized Nash Equilibrium (GNE) for games with a continuum of players and characterize the GNE. We then present some numerical results. It turns out that inequality constraints result in a slower spread of the epidemic and an improved cost for the vulnerable players. Furthermore, it is possible that inequality constraints are beneficial for non-vulnerable players as well. As a byproduct of our work, we provide a mathematical analysis of","912":"In this paper we present an approach for learning a single model that universally segments 33 anatomical structures, including vertebrae, pelvic bones, and abdominal organs. Our model building has to address the following challenges. Firstly, while it is ideal to learn such a model from a large-scale, fully-annotated dataset, it is practically hard to curate such a dataset. Thus, we resort to learn from a union of multiple datasets, with each dataset containing the images that are partially labeled. Secondly, along the line of partial labelling, we contribute an open-source, large-scale vertebra segmentation dataset for the benefit of spine analysis community, CTSpine1K, boasting over 1,000 3D volumes and over 11K annotated vertebrae. Thirdly, in a 3D medical image segmentation task, due to the limitation of GPU memory, we always train a model using cropped patches as inputs instead a whole 3D volume, which limits the amount of contextual information to be learned. To this, we propose a cross-patch transformer module to fuse more information in adjacent patches, which enlarges the aggregated receptive field for improved segmentation performance. This is especially important for segmenting, say, the elongated spine. Based on","913":"After steep drops and then rebounds in transportation-related CO$_2$ emissions over the first half of 2020, a second wave of COVID-19 this fall has caused further -- but less substantial -- emissions reductions. Here, we use near-real-time estimates of daily emissions to explore differences in human behavior and restriction policies over the course of 2020. In particular, we find that social distancing measures have led to sharp reductions in traffic volume only at modest levels of nitrogen dioxide (NO$_2$) emission. However, unlike NO$_2$, there has been little impact on changes in other organic pollutants like aerosolized organic carbon (AOC), which are primarily emitted from fossil fuel consumption. Therefore, our findings indicate that society can achieve significant gains in limiting future climate change by simply reducing mobility across counties with similar demographic distributions. Furthermore, the benefits of such localized policy innovations are likely to be more pronounced for low-income communities of color, which have a higher risk of dying from Covid-19. Finally, our results suggest that efforts aimed at curbing COVID-19 transmission should focus on supporting economic vitality rather than protecting public health. Overall, we emphasize the importance of considering long-term effects of shifting societal activities and providing support to","914":"The High Energy Particle Physics (HEP) faces challenges over the coming decades with a need to attract young people into the field and STEM careers, as well as a need to recognize, promote and sustain those in the field who are making important contributions to the research effort across many specialties needed to deliver the science. Such skills can also serve as attractors for students who may not want to pursue a PhD in HEP but use them as a springboard to other STEM careers. This paper reviews the challenges and develops strategies to correct the disparities to help transform the particle physics field into a stronger and more diverse ecosystem of talent and expertise, with the expectation of long-lasting scientific and societal benefits. We begin by outlining the gaps in secondary educational offerings for people from different socio-economic backgrounds, and discuss how these barriers could be addressed with novel approaches to revitalizing the particle physics workforce. We then outline a number of possible directions for future leadership and engagement activities within the realm of today's quantum technology practices and opportunities. In particular, we focus on the critical role of human factors in any aspect of the COVID-19 pandemic, including its effects on team performance, public outreach, and education and training of personnel. Finally, we note emerging trends and encourage further study","915":"This paper presents preliminary summary results from a longitudinal study of participants in seven U.S. states during the COVID-19 pandemic. In addition to standard socio-economic characteristics, we collect data on various economic preference parameters: time, risk, and social preferences, and risk perception biases. We pay special attention to predictors that are both important drivers of social distancing and are potentially malleable and susceptible to policy levers. We note three important findings: (1) demographic characteristics exert the largest influence on social distancing measures and mask-wearing, (2) we show that individual risk perception and cognitive biases exert a critical role in influencing the decision to adopt social distancing measures, (3) we identify important demographic groups that are most susceptible to changing their social distancing behaviors. These findings can help inform the design of policy interventions regarding targeting specific demographics groups, which can help reduce the transmission speed of the COVID-19 virus. Our work summarizes what is currently known about the determinants of social distancing and helps derive future insights into the potential impact of state-level policies on reducing health disparities. A full sample of our study participants is available at https:\/\/github.com\/annahaensch\/ODyN. This dataset is made publicly","916":"Analysing and understanding the transmission dynamics of COVID-19 is mandatory to be able to design the best social and medical policies, foresee their outcomes and deal with all the subsequent socio-economic effects. We address this important problem from a computational and machine learning perspective. More specifically, we want to statistically estimate all the relevant parameters for the new coronavirus COVID-19, such as the reproduction number, fatality rate or length of infectiousness period, based on Romanian patients, as well as be able to predict future outcomes. This endeavor is important, since it is well known that these factors vary across the globe, and might be dependent on many causes, including social, medical, age and genetic factors. We use a recently published improved version of SEIR, which is the classic, established model for infectious diseases. We want to infer all the parameters of the model, which govern the evolution of the pandemic in Romania, based on the only reliable, true measurement, which is the number of deaths. Once the model parameters are estimated, we are able to predict all the other relevant measures, such as the number of exposed and infectious people. To this end, we propose a self-supervised approach to train a deep convolutional network to guess the","917":"While digital trace data from sources like search engines hold enormous potential for tracking and understanding human behavior, these streams of data lack information about the actual experiences of those individuals generating the data. Moreover, most current methods ignore or under-utilize human processing capabilities that allow humans to solve problems not yet solvable by computers (human computation). We demonstrate how behavioral research, linking digital and real-world behavior, along with human computation, can be utilized to improve the performance of studies using digital data streams. This study looks at the use of search data to track prevalence of Influenza-Like Illness (ILI). We build a behavioral model of flu search based on survey data linked to users online browsing data. We then utilize human computation for classifying search strings. Leveraging these resources, we construct a tracking model of ILI prevalence that outperforms strong historical benchmarks using only a limited stream of search data and lends itself to tracking ILI in smaller geographic units. While this paper only addresses searches related to ILI, the method we describe has potential for tracking a broad set of phenomena in near real-time. Our results show that it is possible to track ILI prevalence across the globe with relatively few accesses to high quality data. Since our method does not require additional computer","918":"Jakarta is a metropolitan city and among the most dense city in Indonesia. Jakarta has 12 major cities with population over 13 million people. However, there are only 2 main hospitals in Jakarta, both of which provide about 100% high-quality medical services. To improve the quality of public health in this city, we need to understand its very first case. We planned to conduct an investigation on the cause of outbreak of COVID-19 in Jakarta by using the data from the National Ministry of Health of Java. The study was carried out based on the analysis of logistic growth equations and the modeling of the pandemic propagation model. Our results show that the total number of infected cases in Jakarta will reach 3.52 x 10^5 persons after the peak period. We also found that as the exponent value increases, the distance between the normalized patterns of the two parts of the country becomes larger. The result shows that despite being a relatively new city, Jakarta has already become a good metric for the assessment of the quality of public health. Furthermore, compared to some other countries, India has taken the second best performance regarding prevention and control of the disease. This study indicates that the problem of lack of uniformity of the quarantine policy throughout","919":"We develop a stochastic epidemic model progressing over dynamic networks, where infection rates are heterogeneous and may vary with individual-level covariates. The joint dynamics are modeled as a continuous-time Markov chain such that disease transmission is constrained by the contact network structure, and network evolution is in turn influenced by individual disease statuses. To accommodate partial epidemic observations commonly seen in real-world data, we propose a likelihood-based inference method based on the stochastic EM algorithm, introducing key innovations that include efficient conditional samplers for imputing missing infection and recovery times which respect the dynamic contact network. Experiments on both synthetic and real datasets demonstrate that our inference method can accurately and efficiently recover model parameters and provide valuable insight at the presence of unobserved disease episodes in epidemic data. In addition to its practical applications, this framework also provides a theoretical basis for exploring important properties of the underlying dynamical process via topological analysis and clustering analyses. Our code has been made publicly available at https:\/\/github.com\/susceptible-infected-recovered-dead (SIRD) model, which will be updated upon new information regarding the COVID-19 outbreak or other parameters related to infectious diseases. This work demonstrates how standard methods of dealing with imperfect","920":"We present a framework for allowing autonomous robots deployed for extended periods of time in public spaces to adapt their own behaviour online from user interactions. The robot behaviour planning is embedded in a Reinforcement Learning (RL) framework, where the objective is maximising the level of overall user engagement during the interactions. We use the Upper-Confidence-Bound Value-Iteration (UCBVI) algorithm, which gives a helpful way of managing the exploration\/exploitation trade-off for real-time interactions. An engagement model trained end-to-end generates the reward function in real-time during policy execution. We test this approach in a public museum in Lincoln (UK), where the robot is deployed as a tour guide for the visitors. Results show that after a couple of months of exploration, the robot policy learned to maintain the engagement of users for longer, with an increase of 22.8% over the initial static policy in the number of items visited during the tour and a 30% increase in the probability of completing the tour. This work is a promising step toward behavioural adaptation in long-term scenarios for robotics applications in social settings. Additional research is needed to investigate generalised policies beyond the current limitations of fixed interaction lengths. Our code is available at https:\/\/github.","921":"The Coronavirus disease 2019 (COVID-19) has affected several million people. With the outbreak of the epidemic, many researchers are devoting themselves to the COVID-19 screening system. The standard practices for rapid risk screening of COVID-19 are the CT imaging or RT-PCR (real-time polymerase chain reaction). However, these methods demand professional efforts of the acquisition of CT images and saliva samples, a certain amount of waiting time, and most importantly prohibitive examination fee in some countries. Recently, some literatures have shown that the COVID-19 patients usually accompanied by ocular manifestations consistent with the conjunctivitis, including conjugate, epiphora, or increased secretions. After more than four months study, we found that the confirmed cases of COVID-19 present the consistent ocular pathological symbols; and we propose a new screening method of analyzing the eye-region images, captured by common CCD and CMOS cameras, could reliably make a rapid risk screening of COVID-19 with very high accuracy. We believe a system implementing such an algorithm should assist the triage management or the clinical diagnosis. To further evaluate our algorithm and approved by the Ethics Committee of Shanghai public health clinic center of Fud","922":"We demonstrate an approach to replicate and forecast the spread of the SARS-CoV2 (Severe Acute Respiratory Syndrome) epidemic using the toolkit of probabilistic programming languages such as Bayesian inference with Gaussian processes priors. We first establish the connection between the stochastic partial differential equations (SPDEs) and GARCH models by adapting the framework of Guided Backward Induction (GBI), which learns from data how values for the parameters in the SPDE model are deduced from the observations; we then derive the posterior distribution on the estimated illness reproduction number, infection fatality rate, and probability of severe cases for different countries based on historical data. The results show that our method can effectively track the spread of the disease, and provide valuable insights at both early stages and more advanced forecasting stages. Our code is available in R and Julia at https:\/\/github.com\/spang-lab\/covid19_estimates.git.io\/forecasting_model_based_analysis_of_data\/project_home.html. Additionally, we present an extensive overview of the tools and resources used to facilitate this project. We believe that this demonstrates some of the value of complex epidemiological models and provides","923":"Importance: Alternative methods for hospital utilization forecasting, essential information in hospital crisis planning, are necessary in a novel pandemic when traditional data sources such as disease testing are limited. Objective: Determine whether mandatory daily employee symptom attestation data can be used as syndromic surveillance to forecast COVID-19 hospitalizations in the communities where employees live. Design: Retrospective cohort study. Setting: Large academic hospital network of 10 hospitals accounting for a total of 2,384 beds and 136,000 discharges in New England. Participants: 6,841 employees working on-site of Hospital 1 from April 2, 2020 to November 4, 2020, who live in the 10 hospitals' service areas. Interventions: Mandatory, daily employee self-reported symptoms were collected using an automated text messaging system. Main Outcomes: Mean absolute error (MAE) and weighted mean absolute percentage error (WMAPE) of 7 day forecasts of daily COVID-19 hospital census at each hospital. Results: 6,841 employees, with a mean age of 40.8 (SD = 13.6), 8.8 years of service (SD = 10.4), and 74.8% were female (n = 5,120), living in the 10 hospitals' service areas","924":"Recent works in Reinforcement Learning (RL) combine model-free (Mf)-RL algorithms with model-based (MB)-RL approaches to get the best from both: asymptotic performance of Mf-RL and high sample-efficiency of MB-RL. Inspired by these works, we propose a hierarchical framework that integrates online learning for the Mb-trajectory optimization with off-policy methods for the Mf-RL. In particular, two loops are proposed, where the Dynamic Mirror Descent based Model Predictive Control (DMD-MPC) is used as the inner loop to obtain an optimal sequence of actions. These actions are in turn used to significantly accelerate the outer loop Mf-RL. We show that our formulation is generic for a broad class of MPC based policies and objectives, and includes some of the well-known Mb-Mf approaches. Based on the framework we define two algorithms to increase sample efficiency of Off Policy RL and to guide end to end RL algorithms for online adaption respectively. Thus we finally introduce two novel algorithms: Dynamic-Mirror Descent Model Predictive RL(DeMoRL), which uses the method of elite fractions for the inner loop and Soft Actor-Critic (SAC) as","925":"We study the influence of partisan content from national and international media on Twitter users' engagement with U.S. politics online during the COVID-19 pandemic. We collect over 67.7 million impeachment related tweets posted by 3,698,890 unique Twitter accounts between January 25th to June 7th, 2021. We find that the average follower count significantly decreased below one before the advent of the COVID-19 pandemic (mostly within 0.5% of the pre-pandemic mean), but increased steadily thereafter. The median retweeting time of these accounts was also highly correlated with their reported affiliation, where the latter ranged from hours to decades. Our results suggest that this phenomenon may have been caused by a combination of factors such as a decrease in both the number of followers and the duration of their tweets (which would thus increase the visibility of the accounts). To our best knowledge, this is the first large-scale analysis to explore how polarization affects collective attention towards political issues via social media platforms. Further, we highlight the outsized role of polarized communication networks in promoting mutual distrust and hostility between Democrats and Republicans, which could have direct implications for advancing offline election strategies. Overall, we provide an empirical demonstration of the effects of extreme polarization on public opinion evolving","926":"The existence of ultralight bosons such as axion-like particles, which are viable candidates for dark matter, can be tested by their effect on large scale gravitational wave (GW) signals. The standard assumption among many models is that they arise from the interaction between two components - one composed by a scalar field and another by a vector field. Here we present a new approach to describe this scenario with a single scalar field interacting with both GW photons and their environment. We show that such a light gravitino could successfully drive the global dynamics of the universe towards the steady state during the early phase of expansion, providing a completely different picture of the Universe compared to previous models. As for the magnitude of the GW signal, it follows the same power law as usual in the case where the exponent of the linear coupling of the scalar field equals $3$. To understand how these conclusions are valid, we have also performed simulations considering a broad range of parameters and found that the nature of the GW signal strongly depends on the parameter values. This suggests that the detection of the inspiral of compact objects (such as sperms) through the use of advanced LIGO detectors will allow to constrain the parameter space of our model. In addition, we discuss the","927":"We study a multi-group SAIRS-type epidemic model with vaccination. The role of asymptomatic and symptomatic infectious individuals is explicitly considered in the transmission pattern of the disease among the groups in which the population is divided. This is a natural extension of the homogeneous mixing SAIRS model with vaccination studied in Ottaviano et. al (2021). We provide a global stability analysis for the model. We determine the value of the basic reproduction number $\\mathcal{R}_0$ and prove that the disease-free equilibrium is globally asymptotically stable if $\\mathcal{R}_0<1$. In the case of the SAIRS model without vaccination, we prove the global asymptotic stability of the disease-free equilibrium also when $\\mathcal{R}_0=1$. Moreover, if $\\mathcal{R}_0>1$, the disease-free equilibrium is unstable and a unique endemic equilibrium exists. First, we investigate the local asymptotic properties of the endemic equilibrium and subsequently its global stability, for two different variations of the original model. Last, we perform numerical simulations to compare the epidemic spreading on different networks topologies. These results strongly suggest that the network structure","928":"We investigate networks formed by keywords in tweets and study their community structure. Based on datasets of tweets mined from over seven hundred political figures in the U.S. and Canada, we hypothesize that such Twitter keyword networks exhibit a small number of communities. Our results are further reinforced by considering via so-called pseudo-tweets generated randomly and using AI-based language generation software. We speculate as to the possible origins of the small community hypothesis and further attempts at validating it. Finally, we provide our proposed methodology for future research purposes. Impressively enough, the method seems to start raising participant awareness of automated accounts, which can be used to efficiently manage online discussions about health issues, policy changes, etc. The dataset is available at https:\/\/github.com\/annahaensch\/covid19-data-set.git.io\/. This data set is made publicly available at https:\/\/github.com\/annahaensch\/Covid19-data-set.git.io\/ with the goal of promoting media literacy through an accessible means of communication. It is expected that this paper will help catalyze studies focusing on the propagation of information (aka diffusion) between social media platforms. The dataset is already large and representative, providing a","929":"This article presents preliminary summary results from a longitudinal study of participants in seven U.S. states during the COVID-19 pandemic. In addition to standard socio-economic characteristics, we collect data on various economic preference parameters: time, risk, and social preferences, and risk perception biases. We pay special attention to predictors that are both important drivers of social distancing and are potentially malleable and susceptible to policy levers. We note three important findings: (1) demographic characteristics exert the largest influence on social distancing measures and mask-wearing, (2) we show that individual risk perception and cognitive biases exert a critical role in influencing the decision to adopt social distancing measures, (3) we identify important demographic groups that are most susceptible to changing their social distancing behaviors. These findings can help inform the design of public policies regarding targeting specific demographics groups, which can help reduce the transmission speed of the COVID-19 virus. Overall, this study advances our understanding of human behavior amid unprecedented circumstances, and provides important insights for guiding future interventions as well as significant evidence for policymakers to consider adapting their policies to accommodate these new normals. Our code is available at https:\/\/github.com\/annahaensch\/covid19_analysis_","930":"In spite of the fact that efficient compression methods for dense two-dimensional flow fields would be very useful for modern video codecs, hardly any research has been performed in this area so far. Our paper addresses this problem by proposing the first lossy diffusion-based codec for this purpose. It keeps only a few flow vectors on a coarse grid. Additionally stored edge locations ensure the accurate representation of discontinuities. In the decoding step, the missing information is recovered by homogeneous diffusion inpainting that incorporates the stored edges as reflecting boundary conditions. In spite of the simple nature of this codec, our experiments show that it achieves remarkable quality for compression ratios up to 800 : 1.000. The best results are obtained with a temporal mode setting and almost perfect recovery is observed for all practical scenarios. With its encoding parameters fixed at specific values, the performance of the proposed codec drops little when compared with conventional codecs. Moreover, we also demonstrate that using auxiliary storage space allows the adaptation of the coding model to accommodate varying degrees of degradation. As a result, the bitrate can be dynamically adjusted according to the network condition and thus achieving more robustness against noise than previous work. Finally, we also discuss the construction of the proposed codec under very similar settings. To allow for a","931":"In this paper, we propose a game-theoretic solution to the parking problem, by exploiting a strategic-reasoning approach for multi-agent systems. Precisely, cars are modeled by agents interacting among them in a multi-player game setting, whose aim is to get a free slot parking-place satisfying their own constraints. The overall assignment is then given as a Nash equilibrium solution. We come up with an algorithm (and its implementation in a tool) that works in quadratic time. We give evidence of the benefits of our approach by running our tool on a large hospital parking space. We also describe how our approach can be used to obtain inexpensive and timely insights from real-world data. A key ingredient of our method is a new superregular random walk, whose joint distribution of steps is made explicit. Our results show that our approach is able to achieve better performance than standard methods in terms of both computation time and quality. We conclude by using our tools to analyze the state of the art in home office parking problems, and discussing some future research directions. \\url{https:\/\/github.com\/gbriel21\/parking_problem}} In addition, we present a comprehensive empirical study aiming at understanding whether our environment facilitates the adoption of cycling or","932":"On 26 January 2021, India witnessed a national embarrassment from the demographic least expected from - farmers. People across the nation watched in horror as a pseudo-patriotic mob of farmers stormed capital Delhi and vandalized the national pride- Red Fort. Investigations that followed the event revealed the existence of a social media trail that led to the likes of such an event. Consequently, it became essential and necessary to archive this trail for social media analysis - not only to understand the bread-crumbs that are dispersed across the trail but also to visualize the role played by misinformation and fake news in this case. In this paper, we propose the tractor2twitter dataset which contains around 0.05 million tweets that were posted before, during, and after this event. Also, we benchmark our dataset with an Explainable AI ML classifier for classification of each tweet into either of the three categories - disinformation, misinformation, and opinion. The insights extracted from our corpus provide a rare look at the relationship between real-world events and social media behavior. We hope that this data will be useful for researchers, policy makers, and peacekeepers who are involved in detecting or mitigating misinformation about these events. Our dataset is available at https:\/\/github.com\/annahaensch\/tractor2twitter","933":"Blended learning (BL) is a recent tread among many options that can best fit learners' needs, regardless of time and place. This study aimed to discover students' perceptions of BL and the challenges faced by them while using technology. This quantitative study used data gathered from 300 students enrolled in four public universities in the Sindh province of Pakistan. the finding shows that students were compatible with the use of technology, and it has a positive effect on their academic experience. The study also showed that the use of technology encourages peer collaboration. The challenges found include: neither teacher support nor a training program was provided to the students for the course which needed to shift from a traditional face to face paradigm to a blended format, a lake of space lies with skills in a laboratory assistants for the courses with a blended format and as shortage of high tech computer laboratories \/ computer units to run these courses. Therefore, it is recommended that the authorities must develop and incorporate a comprehensive mechanism for the effective implementation of BL in the learning teaching-learning process heads of the departments should also provide additional computing infrastructure to their departments. The findings of this research would help researchers and policymakers plan the better deployment of existing technologies and improve the quality of education through a more efficient and sustainable approach. They will also help educational institutions","934":"The economic downturn and the air travel crisis triggered by the recent coronavirus pandemic pose a substantial threat to the new consumer class of many emerging economies. In Brazil, considerable improvements in social inclusion have fostered the emergence of hundreds of thousands of first-time fliers over the past decades. We apply a two-step regression methodology in which the first step consists of identifying air transport markets characterized by greater social inclusion, using indicators of the local economies' income distribution, credit availability, and access to the Internet. In the second step, we inspect the drivers of the plunge in air travel demand since the pandemic began, differentiating markets by their predicted social inclusion intensity. After controlling for potential endogeneity stemming from the spread of COVID-19 through air travel, our results suggest that short and low-density routes are among the most impacted airline markets and that business-oriented routes are more impacted than leisure ones. Finally, we estimate that a market with 1 per cent higher social inclusion is associated with a 0.153 per cent to 0.166 per cent more pronounced decline in demand during the pandemic. Therefore, markets that have benefited from greater social inclusion in the country may be the most vulnerable to the current crisis. These findings should help policymakers design effective intervention strategies","935":"The novel coronavirus disease, named COVID-19, emerged in China in December 2019, and has rapidly spread around the world. It is clearly urgent to fight COVID-19 at global scale. The development of methods for identifying drug uses based on phenotypic data can improve the efficiency of drug development. However, there are still many difficulties in identifying drug applications based on cell picture data. This work reported one state-of-the-art machine learning method to identify drug uses based on the cell image features of 1024 drugs generated in the LINCS program. Because the multi-dimensional features of the image are affected by non-experimental factors, the characteristics of similar drugs vary greatly, and the current sample number is not enough to use deep learning and other methods are used for learning optimization. As a consequence, this study is based on the supervised ITML algorithm to convert the characteristics of drugs. The results show that the characteristics of ITML conversion are more conducive to the recognition of drug functions. The analysis of feature conversion shows that different features play important roles in identifying different drug functions. For the current COVID-19, Chloroquine and Hydroxychloroquine achieve antiviral effects by inhibiting endocytosis, etc., and were","936":"We address a multicoupled dynamics on complex networks with tunable structural segregation. Specifically, we work on a networked epidemic spreading under a vaccination campaign with agents in favor and against the vaccine. Our results show that such coupled dynamics exhibits a myriad of phenomena such as nonequilibrium transitions accompanied by bistability. Besides we observe the emergence of an intermediate optimal segregation level where the community structure enhances negative opinions over vaccination but counterintuitively hinders - rather than favoring - the global disease spreading. Thus, our results hint vaccination campaigns should avoid policies that end up segregating excessively anti-vaccine groups so that they effectively work as echo chambers in which individuals look to confirmation without jeopardising the safety of the whole population. This strategy could be pursued via inducing exogenous biases in the acceptance of the coronavirus vaccine from various social media platforms. Finally, we study the impact of proactive versus reactive strategies regarding the public health policy towards COVID-19 vaccines. We find that while reacting to the pandemic triggers an increase in the number of pro-vaccine nodes, this effect is not permanent and varies depending on the specific nation. Therefore, we argue that a targeted approach to identify influential spreaders can help mitigate the effects of the vaccination campaign while minimizing its associated risks.","937":"The VirtualCube system is a 3D video conference system that attempts to overcome some limitations of conventional technologies. The key ingredient is VirtualCube, an abstract representation of a real-world cubicle instrumented with RGBD cameras for capturing the 3D geometry and texture of a user. We design VirtualCube so that the task of data capture is standardized and significantly simplified, and everything can be built using off-the-shelf hardware. We use VirtualCubes as input into a voxelized (deformable) virtual conferencing environment, and we provide each VirtualCube user with a surrounding display showing life-size videos of remote participants. To achieve real-time rendering of remote participants, we develop the V-Cube View algorithm, which uses multi-view stereo for more accurate depth estimation and Lumi-Net rendering for better rendering quality. The entire system consists of three components: 1) the VR headset with its integrated camera, 2) a view frustum that contains both the subject's face and body parts, and 3) the render client application. All these components are standard software or require no specialized hardware. We conduct experiments on 30 participants performing various tasks involving physical distance between pairs of subjects, simulated air turbulence, and varying head positions while taking pictures","938":"Actually, after one year it is recognized that the evolution of COVID-19 is different in each country or region around the world. In this paper, we do a revision to the date about COVID-19 evolution in Mexico, we explain where the main epicenter and states with most high impact on the dynamics are located, what the general public is concerned about and how these concerns can be addressed. Finally, we give a practical perspective as this evolution is a complex system. We expect that the research in this paper helps to understand why Mexico is one of the countries with the most high mortality impact by a new virus (in Latin America), why the government is reacting so negatively to contain its expansion, and how the measures implemented by the government can help to mitigate the negative effect of the pandemic. On the other hand, the study introduces a new concept of epidemic network centrality, which could be useful to optimize vaccines, mitigation policies and administrative decisions for a better restructuring of the economy. All the data used, our development pipeline and analysis, and the methods used are available under an open access policy at https:\/\/github.com\/annahaensch\/covid19-mexico.html. This platform is made possible by the collaboration of the Epidem","939":"High-quality data can improve learning models and make them more robust, but it poses major challenges for which traditional data annotation strategies are inadequate. Recently, federated learning (FL) has received high interest as a promising privacy-preserving machine learning paradigm and FL appeared to be particularly attractive for healthcare applications. However, FL's applicability is limited because it requires that each client reaches the global minimum with full supervision from all clients, which is impossible in realistic scenarios due to the heterogeneity of medical datasets. To address this problem, we propose a new supervised method for semi-supervised FL learning named FedMix, where a mix of labeled and unlabeled data is used to guide the training of FL model. Although conventional FL algorithm cannot solve the mixed-data recognition challenge successfully, we have shown that the trained FL model can achieve better performance on the unseen test set than the baseline model based on fully supervised data. A comprehensive experiments demonstrated that our proposed FedMix improves the generalization ability of FL network by 5.3% compared to the state-of-the-art methods among different FL settings. The source code will be released at https:\/\/github.com\/XiaoyuanGuo\/FedMix.git. Besides, we collected a total of 77,","940":"Machine-generated conversation (e.g., videoconferencing) is now being widely used to replace traditional offline and face-to-face communication. However, there exists little research on automatically generating speech with high quality from different domains, including religious texts. In this paper, we present the first study on extracting audio with high quality from the Qur'an QA 2022 shared task. We use two popular voice cloning models and external sources as reference templates for training and testing our models, respectively. Our experiments show that both models can produce very good results when compared to state-of-the-art recordings, but they still have room for improvement. For example, the VGPT 2021 leaderboard score is still approximately 15% higher than the F1 score based on the test set. The Muon2Vec model has achieved 93% accuracy under the same dataset, showing that it can be used as a general-purpose element of conversational QA. Finally, we show that creating speech using the pre-trained models produces better result than random or more fine-tuned models. This shows that deterministic models such as Blender and GPT-3 are able to generate speech with high fidelity even with the inherent difficulty of understanding Arabic language. It also","941":"The electric power grid is a critical societal resource connecting multiple infrastructural domains such as agriculture, transportation, and manufacturing. The electrical grid as an infrastructure is shaped by human activity and public policy in terms of demand and supply requirements. Further, the grid is subject to changes and stresses due to solar weather, climate, hydrology, and ecology. The emerging interconnected and complex network dependencies make such interactions increasingly dynamic causing potentially large swings, thus presenting new challenges to manage the coupled human-natural system. This paper provides a survey of models and methods that seek to explore the significant interconnected impact of the electric power grid and interdependent domains. We also provide relevant critical risk indicators (CRIs) across diverse domains that may influence electric power grid risks, including climate, ecology, hydrology, finance, space weather, and agriculture. We discuss the convergence of indicators from individual domains to explore possible systemic risk, i.e., holistic risk arising from cross-domains interconnections. Our study provides an important first step towards data-driven analysis and predictive modeling of risks in the coupled interconnected systems. Further, we propose a compositional approach to risk assessment that incorporates diverse domain expertise and information, data science, and computer science to identify domain-specific CRIs and their union in","942":"Distance correlation is a new class of multivariate dependence coefficients applicable to random vectors of arbitrary and not necessarily equal dimension. Distance covariance and distance correlation are analogous to product-moment covariance and correlation, but generalize and extend these classical bivariate measures of dependence. As such, we obtain a quantum circuit for estimating both covariance and correlations from a single random vector of size $d$ in time (a proxy for the number of dimensions). The resulting algorithm allows us to efficiently compute all pairwise distances between any two randomly chosen vectors. We validate our approach on simulated data as well as real financial market data; the latter proves highly correlated with daily returns of S&P 500 index options. Finally, we apply it to a COVID-19 dataset tracking social distancing behavior during the pandemic. Our results show that distance correlation can be effectively computed from a single seed matrix of parameters strictly related to the traded prices, and therefore it becomes increasingly important when the underlying correlation structure among different samples is strongly dissimilar. This offers novel insight into the limits of quantum information processing in diverse contexts. All calculations are reproducible via the freely available R package covid19.1. Furthermore, because our method requires no additional memory or components besides the one used by the currently","943":"Kiosks are a popular self-service option in many fast-food restaurants, they save time for the visitors and save labor for the fast-food chains. In this paper, we propose an effective design of a kiosk shopping cart recommender system that combines a language model as a vectorizer and a neural network-based classifier. The model performs better than other models in offline tests and exhibits performance comparable to the best models in A\/B\/C tests. On top of these results, the proposed model can be easily implemented in real-world applications. Specifically, it reduces the labour cost by around 50% (as compared to 30% with conventional methods) and increases the revenue by around 90% (as compared to 80% with conventional methods). It also provides significantly improved recommendation quality! Thanks to its ability to generalize beyond the training data, the model has great potential for unseen situations where no training data is available. Therefore, this research presents a complete redesign of the kiosky shopping cart recommender system architecture combining languages model and deep learning based classifiers. We also provide a comprehensive study of the effectiveness of the adopted approach including online evaluation through R2R testing. Both the code and the trained models are available at https:\/\/github.com\/","944":"Score-based algorithms for tuberculosis (TB) verbal screening perform poorly, causing misclassification that leads to missed cases and unnecessary costly laboratory tests for false positives. We compared score-based classification defined by clinicians to machine learning classification such as SVM-RBF, logistic regression, and XGBoost. We restricted our analyses to data from adults, the population most affected by TB, and investigated the difference between untuned and unweighted classifiers to the cost-sensitive ones. Predictions were compared with the corresponding GeneXpert MTB\/Rif results. After adjusting the weight of the positive class to 40 for XGBoost, we achieved 96.64% sensitivity at 85.06% specificity. As such, the sensitivity of our identifier increased by 1.26% while the specificity increased by 13.19% in absolute value compared to the traditional score-based method defined by our clinicians. Our approach further demonstrated that only 2000 data points were sufficient to enable the model to converge. The results indicate that even with limited data we can actually devise a better method to identify TB suspects from verbal screening. This may be especially useful for countries where verbal testing is not widely available or fails. Lastly, we provided our code for free use enabling future research","945":"In India, post-pandemic vaccination has been a controversial issue. People have demanded that immunization should be increased to combat the pandemic and protect themselves from infection. The government has partially released vaccines under Randomized Control Techniques (RCTs). However, people are not willing to take the COVID-19 vaccine immediately or in general. Therefore, it becomes essential to consider the context of the ongoing Covid-19 Pandemic with the aim of determining the effective RCTs for curbing the disease spread until the desired herd immunity is achieved. In this paper, we develop a deep learning model to predict the transmission dynamics of Covid-19 across communities after lockdown. We use publicly available data about Covid-19 collected by the Government on its Facebook page to build our model. We train it to learn the community-specific social distancing measures based on the community-wise demographic characteristics. We then introduce different classification techniques to classify the remaining susceptible population, which can help us determine the number of vaccinated required to effectively curb the disease. With the performance of the model, we also suggest possible alternative scenarios such as lifting the restrictions when the hospital capacity is limited. Importantly, our model predicts that even considering the worst case scenario of high contact rate, the","946":"We use the stellar evolution code MESA-binary and follow the evolution of six exoplanets to determine their potential role in the future evolution of their parent star on the red giant branch (RGB) and on the asymptotic giant branch (AGB). We limit this study to planets with orbits that have semi-major axis of 1AU0.25, and having a parent star of mass M>1Mo. We find that the star HIP 75458 will engulf its planet HIP75458 b during its RGB phase. The planet will remove the envelope and terminate the RGB evolution, leaving a bare helium core of mass 0.4Mo that will evolve to form a helium white dwarf. Only in one system out of six, the planet beta Pic c will enter the envelope of its parent star during the AGB phase. For that to occur, we have to reduce the wind mass-loss rate by a factor of about four from its commonly used value. This strengthens an early conclusion, which was based on exoplanets with circular orbits, that states that to have a non-negligible fraction of AGB stars that engulf planets we should consider lower wind mass-loss rates of isolated AGB stars (before they are spun-up by a companion).","947":"In a circular economy, tracking the flow of second-life components for quality control and resilience has become a critical challenge. Tokenization can enhance the transparency of the flow of second-life components. However, simple tokenization does not correspond to real economic models and lacks the ability to finely manage complex business processes. In this paper, we propose a blockchain-based platform with a hierarchical architecture to enable intelligent third-party service providers (e.g., governments, hospitals, security forces, etc.) to perform automatic initial design of cryptocurrency assets and thus achieve better efficiency than conventional approaches. The proposed framework will also allow different parties to cooperatively adopt the same standardize the exchange of information among these services. We show that our approach is able to improve the utilization rate of existing cryptocurrencies by increasing their partitioning ratio. With the increase of the number of users on each blockchain, the self-sovereign Identity (SSI) becomes more efficient. To tackle the challenges related to the distributed ledger technology, we develop a side chain-based transaction protocol to facilitate responsible collection, processing, and presentation of SSI. Furthermore, we introduce a bidirectional escrow mechanism to protect the integrity of the supply chain from multiple dishonest participants. Extensive experiments conducted over a large scale","948":"Opiates, or hubs and spans, have been widely used as a generalization of network centrality for ranking nodes in a network. However, existing definitions of opiates do not accurately reflect the complex topological dependence present among genes in each individual biological sample. In this paper, we propose a novel method to define the relative importance of all kinds of edges with respect to the average number of contacts given by any pair of gene sets (of arbitrary size) -- these can be defined as the effective number of motifs. We use our approach to obtain simple, yet powerful representations of the relationships between genes that characterize different tissues, and also those associated with diseases such as COVID-19. From these results, we propose a sampling algorithm that leverages the information provided by the Effective Number of Networks (the ENN) to significantly improve the accuracy of osteosarcoma cancer detection from clinical expression data. Finally, we apply our method to a broader range of real-world datasets, including both protein-protein interaction and metabolic networks. Our results reveal new insights into the ways that various types of weight on proteins and other network indicators relate to their binding properties. This offers potential guidance for further exploring the function of opiates in biology and medicine. The","949":"This paper addresses novel consensus problems for multi-agent systems operating in an unreliable environment where adversaries are spreading. The dynamics of the adversarial spread processes follows the susceptible-infected-recovered (SIR) model, where the infection induces faulty behaviors in the agents and affects their state values. Such a problem setting serves as a model of opinion dynamics in social networks where consensus is to be formed at the time of pandemic and infected individuals may deviate from their true opinions. To ensure resilient consensus among the noninfectious agents, the difficulty is that the number of infectious agents changes over time. We assume that a local policy maker announces the local level of infection in real-time, which can be adopted by the agent for its preventative measures. It is demonstrated that this problem can be formulated as resilient consensus in the presence of the socalled mobile malicious models, where the mean subsequence reduced (MSR) algorithms are known to be effective. We characterize sufficient conditions on the network structures for different policies regarding the announced infection levels and the strength of the epidemic. Numerical simulations are carried out for random graphs to verify the effectiveness of our approach. Our results indicate that the robustness of our strategy is better than one of the main approaches used in the literature for","950":"Let $G$ be a bipartite graph where every node has a strict ranking of its neighbors. For every node, its preferences over neighbors extend naturally to preferences over matchings. Matching $N$ is more popular than matching $M$ if the number of nodes that prefer $N$ to $M$ is more than the number that prefer $M$ to $N$. A maximum matching $M$ in $G$ is a\"popular max-matching\"if there is no maximum matching in $G$ that is more popular than $M$. Such matchings are relevant in applications where the set of admissible solutions is the set of maximum matchings and we wish to find a best maximum matching as per node preferences. It is known that a popular max-matching always exists in $G$. Here we show a compact extended formulation for the popular max-matching polytope. So when there are edge costs, a min-cost popular max-matching in $G$ can be computed in polynomial time. This is in contrast to the min-cost popular matching problem which is known to be NP-hard. We also consider Pareto-optimality, which is a relaxation of popularity, and show that computing","951":"Air pollution has been on continuous rise with increase in industrialization and vehicular traffic volume worldwide. The impact of COVID-19 on air pollution has also been significant with increased focus on industrializing and reviving the economy. In this paper, we analyze the impact of COVID-19 on air pollution from multiple source domains at different spatial scales using NASA's satellite dataset on aerosol optical depth (AOD) measurements for various major pollutants including carbon monoxide (C), ozone (O3), nitrogen dioxide (NO2), and sulphur dioxide (SO2). We find that during COVID-19 lockdown measures, overall NO2 concentration dropped by 40$\\%$ compared to pre-lockdown period as well as did not change significantly when accompanied by AOD changes. However, analyzing performance across various sectors, we find that C-prefix ($>$0.5$\\mu$m) dust formation region under BPRF (Bureau of Public Roads Fitters) becomes more polluted than other regions while both NO2 and SO2 concentrations are observed to be reduced. Interestingly, our results show that despite the drop in overall pollutant emissions, there is a significant reduction in the emission prefixes. Our analysis provides a quantitative measure of how much any","952":"We present a high-contrast imaging search for Pa$\\beta$ line emission from protoplanets in the PDS~70 system with Keck\/NIRC2 at $0.04<z<1.12$, conducted during and after the COVID-19 pandemic. We applied the high-resolution spectral differential imaging technique to the NIR HST data but were unable to detect the Pa$\\beta$ line at the level predicted using the parameters of \\cite{Hashimoto2020}. This lack of Pa$\\beta$ detection suggests the MUSE-based study may have overestimated the line width of HST\/NIRC2, which turned out to be much narrower than implied by high-contrast near-infrared studies. The narrowness of the beam current distribution on the surface of aqueous samples could also explain the failure of our Pa$\\beta$ detection method; it is possible that the sample was simply imaged along another angle or a different wavelength range. We consider the broader problem of detecting Pa$\\beta$ lines from low-contrast near-infrared images as a test example for the theory of continuum quantum mechanics. Our results suggest that any PAUSE-based study of this system must take","953":"Despite substantial advances in global measles vaccination, measles disease burden remains high in many low- and middle-income countries. A key public health strategy for controling measles in such high-burden settings is to conduct supplementary immunization activities (SIAs) in the form of mass vaccination campaigns, in addition to delivering scheduled vaccination through routine immunization (RI) programs. To achieve balanced implementations of RI and SIAs, robust measurement of sub-national RI-specific coverage is crucial. In this paper, we develop a space-time smoothing model for estimating RI-specific coverage of the first dose of measles-containing-vaccines (MCV1) at sub-national level using complex survey data. The application that motivated this work is estimation of the RI-specific MCV1 coverage in Nigeria's 36 states and the Federal Capital Territory. Data come from four Demographic and Health Surveys, three Multiple Indicator Cluster Surveys, and two National Nutrition and Health Surveys conducted in Nigeria between 2003 and 2018. Our method incorporates information from the SIA calendar published by the World Health Organization and accounts for the impact of SIAs on the overall MCV1 coverage, as measured by cross-sectional surveys. The model can be used to analyze data from multiple surveys with","954":"In this paper, we present our case study at UiPathogen@home using Robotic Process Automation (RPA), and specifically, a path planning algorithm for creating a robotic process that moves people through a crowd safely. The robot is designed to also navigate through the crowd by following social distancing guidelines suggested by the Centers for Disease Control and Prevention (CDC). We demonstrate both the generality of RPA and show its effectiveness in comparison with other well-known path planning algorithms under different simulation frameworks. Our results indicate that if time permits, people can be transported into crowded scenes without risking infection transmission. In addition, our policy analysis indicates that the CDC's guideline of one meter per second of exposure duration may not be sufficient to develop effective policies unless it is actually applied. Finally, we note that existing robots are not suitable for processing paths in complex environments such as classrooms or shopping malls. To facilitate future research, we have made our code publicly available. https:\/\/github.com\/gbriel21\/covid19_weak_force_fencing_mechanism\/. This project may serve as a basis for further research in the field of path planning and robotics. It may also provide a testbed for the design of new agents and collision detection systems.","955":"We study a class of non-standard epidemic models under general assumptions, e.g., heterogeneous contact rates encapsulating changes in behavior and\/or enforcement of control measures. We show that the large-population dynamics are deterministic and relate to the Kermack-McKendrick PDE. Our assumptions are minimalistic in the sense that the only important requirement is that the basic reproduction number of each subpopulation be finite, and allow us to tackle both Markovian and non-Markovian dynamics. The novelty of our approach lies in studying the\"infection graph\"of the population. We show local convergence of this random graph to a Poisson (Galton-Watson) marked tree, recovering Markovian backward-in-time dynamics in the limit as we trace back the transmission chain leading to a focal infection. This effectively models the process of contact tracing in a large population. It is expressed in terms of the Doob $h$-transform of a certain renewal process encoding the time of infection along the chain. Our results provide a mathematical formulation relating a fundamental epidemiological quantity, the generation time distribution, to the successive time of infections along this transmission chain. Beyond its practical applications, the method applies to purely mathematical formulations of problems in","956":"During a pandemic in which aerosol and droplet transmission is possible, the demand of masks that meet medical or workplace standards can prevent most individuals or organizations from obtaining suitable protection. Cloth masks are widely believed to impede droplet and aerosol transmission but most are constructed from materials with unknown filtration efficiency, airflow resistance and water resistance. Further, there has been no clear guidance on the most important performance metrics for the materials used by the general public (as opposed to high-risk healthcare settings). Here we provide data on a range of common fabrics that might be used to construct masks. None of the materials were suitable for masks meeting the N95 NIOSH standard, but many could provide useful filtration (>90%) of 3 micron particles (a plausible challenge size for human generated aerosols), with low pressure drop. These were: nonwoven sterile wraps, dried baby wipes and some double-knit cotton materials. Decontamination of N95 mask using isopropyl alcohol produces the expected increase in particle penetration, but for 3 mm particles, filtration efficiency is still well above 95%. Tightly woven thin fabrics, despite having the visual appearance of a good particle barrier, had remarkably low filtration efficiencies and high pressure drop. These","957":"The first goal of this study is to quantify the magnitude and spatial variability of air quality changes in the US during COVID-19. We focus on two federally regulated pollutants, nitrogen dioxide (NO2), and fine particulate matter (PM2.5). Observed concentrations at all available ground monitoring sites (240 and 480 for NO2 and PM2.5, respectively) were compared between April 2020 and April of the prior five years, 2015-2019, as the baseline. Large statistically significant decreases in NO2 concentrations were found at more than 65% of the monitoring sites, with an average drop of 2 ppb when compared to the mean of the previous five years. The same patterns are confirmed by satellite-derived NO2 column totals from NASA OMI. PM2.5 concentrations from the ground monitoring sites, however, were more likely to be higher. The second goal of this study is to explain the different responses of these two pollutants during the COVID-19 pandemic. The hypothesis put forward is that the shelter-in-place measures affected peoples' driving patterns most dramatically, thus passenger vehicle NO2 emissions were reduced. Commercial vehicles and electricity demand for all purposes remained relatively unchanged, thus PM2.5 concentrations did not drop significantly. To establish a","958":"We give some numerical observations on the total number of infected by the SARS-CoV-2 in Italy. The analysis is based on a tanh formula involving two parameters. A polynomial correlation between the parameters gives an upper bound for the time of the peak of new infected. A numerical indicator of the temporal variability of the upper bound is introduced. The result and the possibility to extend the analysis to other countries are discussed in the conclusions. From the mathematical point of view, our results can be interpreted as a non-linear variation of the equation of state of neutron stars with mass $\\sim 10^8\\,M_\\odot$ and specific effective coupling equations. According to the real data from Jan. 2020 to March 2021, we find that the evolution of the COVID-19 pandemic in Italy has been strongly differentiated from those of other countries. Finally, it is also observed that the upper bound of the temporal variability of the factor loading matrix (FGM) becomes very close to one value, corresponding to the lower limit of the rotation frequency of the star. This suggests the importance of the lockdown measures to reduce the spread of the virus. To this end, several indexes were proposed in the literature. However, there is no clear evidence of a","959":"The global spread of 2019-nCoV, a new virus belonging to the coronavirus family, forced national and local governments to apply different sets of measures aimed at containing the outbreak. Los Angeles has been one of the first cities in the United States to declare the state of emergency on March 4th, progressively issuing stronger policies involving (among the others) social distancing, the prohibition of crowded private and public gatherings and closure of leisure premises. These interventions highly disrupt and modify daily activities and habits, urban mobility and micro-level interactions between citizens. One of the many social phenomena that could be influenced by such measures is crime. Exploiting publicly available weekly crime data for 18 metropolitan areas in the United States, this work investigates whether correlations between city-level activity and reported crime cases might be leveraged as tools for crisis management. The correlation analysis builds upon a mixed-method approach combining machine learning and Bayesian statistical inference. It specifically considers possible temporal shifts in the relationship between two variables across time and space and test whether the resulting change points can signify significant changes in the effective reproduction number of criminal actions. Results show that the 2019-nCoV prevalence rate and Rt value increase consistently with the rise in non-violent felonies during the second half of March","960":"We study a class of individual-based, fixed-population size epidemic models under general assumptions, e.g., heterogeneous contact rates encapsulating changes in behavior and\/or enforcement of control measures. We show that the large-population dynamics are deterministic and relate to the Kermack-McKendrick PDE. Our assumptions are minimalistic in the sense that the only important requirement is that the basic reproduction number of the epidemic $R_0$ be finite, and allow us to tackle both Markovian and non-Markovian dynamics. The novelty of our approach lies in modeling simultaneously the social structure and governmental interventions (such as quarantine or stay-at-home orders) during the course of an epidemic, and the behavioral responses of the population towards intervention policies. We provide two possible states for the model: one with constant parameters throughout the entire simulation period, and another with time-varying parameter estimates. In the first state, we show that the system evolves rapidly to extinction, while in the second state it decays exponentially within a certain time interval before its total resolution is reached. These results demonstrate the ability of our framework to accurately describe the COVID-19 pandemic both through analytical approaches and numerical simulations. An application to the initial","961":"The COVID-19 pandemic has prompted technological measures to control the spread of the disease. Private contact tracing (PCT) is one of the promising techniques for the purpose. However, the recently proposed Bluetooth-based PCT has several limitations in terms of functionality and flexibility. The existing systems are only able to detect direct contact (i.e., human-human contact), but cannot detect indirect contact (i.e., human-object, such as the disease transmission through surface). Moreover, the rule of risky contact cannot be flexibly changed with the environmental situation and the nature of the virus. In this paper, we propose a secure and efficient trajectory-based PCT system using trusted hardware. We formalize trajectory-based PCT as a generalization of the well-studied Private Set Intersection (PSI), which is mostly based on cryptographic primitives and thus insufficient. We solve the problem by leveraging trusted hardware such as Intel SGX and designing a novel algorithm to achieve a secure, efficient and flexible PCT system. Our experiments on real-world data show that the proposed system can achieve high performance and scalability. Specifically, our system (one single machine with Intel SGX) can process thousands of queries on 100 million records of trajectory data in","962":"The COVID-19 pandemic has highlighted the importance of heterogeneity in clinical phenotypes and states across patients, affecting coexisting morbidity and mortality rates as well as emphasizing the need for early interventions. We posit that this phenomenon is due to pathophysiological differences between children and adults, which are characterized by differences in transmissibility, severity, and treatment modalities. In particular, we focus on the interplay between these factors and outline a heterogeneous risk profile in children vis-a-vis adult populations. To further understand this aspect of the disease spectrum, we analyze RNA sequencing datasets from healthy children and corresponding cohorts of children with acute respiratory distress syndrome (ARDS) using machine learning methods. Furthermore, we compare the features extracted from these two groups of patients with respect to the previously defined three pediatric biomarkers - neutrophils, lymphocyte counts, and high-sensitivity C-reactive protein (hs-CRP). Finally, we discuss the limitations of our study and provide recommendations for future research directions. Our dataset is available at https:\/\/github.com\/lanagarmire\/covid-neutrophil-countries-in-the-loop\/. This data set was curated from multiple online sources and consists of 93 studies spanning 14 countries","963":"Precise and high-resolution carbon dioxide (CO$_2$) emission data is of great importance of achieving the carbon neutrality around the world. Here we present for the first time the near-real-time Global Gridded Daily CO$_2$ Emission Datasets (called GACELA) from fossil fuel and cement production with a global spatial-resolution of 0.1$^\\circ$ by 0.3$^\\circ$ and a temporal-resolution of 1-day. Gridded fossil emissions are computed for different sectors based on the daily national CO$_2$ emissions from near real time dataset (Carbon Monitor), the spatial patterns of point source emission dataset Global Carbon Grid (GID), Emission Database for Global Atmospheric Research (EDGAR) and spatiotemporal patters of satellite nitrogen dioxide (NO$_2$) retrievals. Our study on the global CO$_2$ emissions responds to the growing and urgent need for high-quality, fine-grained near-real-time CO2 emissions estimates to support global emissions monitoring across various spatial scales. We show the spatial patterns of emission changes between January 2019 and December 2020 for 1500 cities globally and in 46 countries, where","964":"The World Health Organization (WHO) announced that COVID-19 was a pandemic disease on the 11th of March as there were 118K cases in several countries and territories. Numerous researchers worked on forecasting the number of confirmed cases since anticipating the growth of the cases helps governments adopting knotty decisions to ease the lockdowns orders for their countries. These orders help several people who have lost their jobs and support gravely impacted businesses. Our research aims to investigate the relation between Google search trends and the spreading of the novel coronavirus (COVID-19) over countries worldwide, to predict the number of cases. We perform a correlation analysis on the keywords of the related Google search trends according to the number of confirmed cases reported by the WHO. After that, we applied several machine learning techniques (Multiple Linear Regression, Non-negative Integer Regression, Deep Neural Network), to forecast the number of confirmed cases globally based on historical data as well as the hybrid data (Google search trends). Our results show that Google searches are highly associated with the number of reported confirmed cases, where the Deep Learning approach outperforms other forecasting techniques. We believe that it is not only a promising approach for forecasting the confirmed cases of COVID-19, but also for similar forecasting problems that are","965":"Background: A major difficulty in predicting the results of COVID-19 experiments is due to its complexity as well as the number of experimental design tests. To reduce the number of tests, researchers have developed mathematical and statistical methods to predict the outcome of these experiments. Methods: We present a general framework for using existing data to predict the outcome of COVID-19 experiments. In our approach, we use machine learning techniques to learn the parameters of a compartmental model from the available experimental data. Then, we utilize the learned parameters to generate new synthetic data with the desired outcome. Results: We propose a computational method to estimate the risk of death during the incubation period of COVID-19. Furthermore, we simulate several scenarios that help policy makers to take informed decisions. Conclusion: The proposed approach can be used by other researchers to forecast the spread of COVID-19 and may lead to encouraging results. Specifically, based on the logistic model, we suggest a probability distribution function (PDF) estimator of the mortality rate that will maximize the likelihood of observing the true mortality curve. Additionally, we provide a generative simulation study to demonstrate how the PDF estimator performs when applied to observational data. Although we do not claim that this simple data-driven technique guarantees perfect predictive","966":"Methanol, one of the simplest complex organic molecules in the Interstellar Medium (ISM), has been shown to be present and extended in cold environments such as starless cores. We aim at studying methanol emission across several starless cores and investigate the physical conditions at which methanol starts to be efficiently formed, as well as how the physical structure of the cores and their surrounding environment affect its distribution. Methanol and C$^{18}$O emission lines at 3 mm have been observed with the IRAM 30m telescope within the large program\"Gas phase Elemental abundances in Molecular CloudS\"(GEMS) towards 66 positions across 12 starless cores in the Taurus Molecular Cloud. A non-LTE radiative transfer code was used to compute the column densities in all positions. We then used state-of-the-art chemical models to reproduce our observations. We have computed N(CH$_3$OH)\/N(C$^{18}$O) column density ratios for all the observed offsets, and two different behaviours can be recognised: the cores where the ratio peaks at the dust peak, and the cores where the ratio peaks with a slight offset with respect to the dust peak ($\\sim $10000 AU). We suggest that","967":"Child Sexual Abuse Media (CSAM) is any visual record of a sexually-explicit activity involving minors. CSAM impacts victims differently from the actual abuse because the distribution never ends, and images are permanent. Machine learning-based solutions can help law enforcement quickly identify CSAM and block digital distribution. However, collecting CSAM imagery to train machine learning models has many ethical and legal constraints, creating a barrier to research development. With such restrictions in place, the development of CSAM machine learning detection systems based on file metadata uncovers several opportunities. Metadata is not a record of a crime, and it does not have legal restrictions. Therefore, investing in detection systems based on metadata can increase the rate of discovery of CSAM and help thousands of victims. We propose a framework for training and evaluating deployment-ready machine learning models for CSAM identification. Our framework provides guidelines to evaluate CSAM detection models against intelligent adversaries and models' performance with open data. We apply the proposed framework to the problem of CSAM detection based on file paths. In our experiments, the best-performing model is based on convolutional neural networks and achieves an accuracy of 0.97. Our evaluation shows that the CNN model is robust against offenders actively trying to evade detection by evaluating the model","968":"We present a framework for a controlled Markov chain where the state of the chain is only given at chosen observation times and of a cost. Optimal strategies therefore involve the choice of observation times as well as the subsequent control values. We show that the corresponding value function satisfies a dynamic programming principle, which leads to a system of quasi-variational inequalities (QVIs). Next we give an extension where the model parameters are not known a priori but are inferred from the costly observations by Bayesian updates. We then prove a comparison principle for a larger class of QVIs, which implies uniqueness of solutions to our proposed problem. We utilise penalty methods to obtain arbitrarily accurate solutions. Finally, we perform numerical experiments on three applications which illustrate our framework. An example demonstrates an application in modelling infectious disease outbreaks in Australia via a modified Susceptible-Infected-Recovered (SIR) model, while the second example shows that our methodology can be used to efficiently compute good investment strategies with low risk. The third example illustrates the applicability of our framework to analyse observed data related to COVID-19 epidemics in Italy. Our code is made publicly available. https:\/\/github.com\/milojsk\/covid19_model_f","969":"The COVID-19 pandemic has prompted social media users to create various narrative about it. Using misinformation and false information shared online for several months, we collected tweets containing both factual statements and rumors regarding the coronavirus from January 20th until June 7th, 2020. We found that the quantity of actual tweets was much higher than the number of rumor tweets. We also found that most of the tweets with factual content were made for fun and did not relate to serious issues, such as taking antidotes. The findings suggest that even mainstream social media accounts can spread misleading information on topics that are not involved in their organizations. They could do so by using language models that learn from the abundant text data available on Twitter. Moreover, automatic fact-checking does not help propagate the misinformation further. This study focuses on investigating how well deep learning models and linguistic features affect the perceived accuracy of COVID-19 vaccine Tweets. Our results show that different socio-linguistic groups have different levels of trust towards the vaccines, which leads to alter the perception of vaccination campaigns. To our best knowledge, this is the first large-scale quantitative study exploring the effect of both language use and network structure on public opinion regarding COVID-19 vaccines. Altogether, our work highlights the","970":"The classic searches for supersymmetry have not given any strong indication for new physics. Therefore CMS is designing dedicated searches to target the more difficult and specific supersymmetry scenarios. This contribution present three such recent searches based on 13 TeV proton-proton collisions recorded with the CMS detector in 2016, 2017 and 2018: a search for heavy gluinos cascading via heavy next-to-lightest neutralino in final states with boosted Z bosons and missing transverse momentum; a search for compressed supersymmetry in final states with soft taus; and a search for compressed, long-lived charginos in hadronic final states with disappearing tracks. The results of these searches are presented in terms of the number of signal candidates per channel, the most stringent being at about 22% higher than the previous best result from one of the eight analysed channels. These results are used to provide some guidance for future studies and also apply to the latest versions of the Standard Model and its extension as well as to other models proposed in the literature. For the moment, we consider only the phenomenological side of the theory. We find no significant evidence either for new physics or for a variation of the existing model parameters. Furthermore, we interpret the most sensitive regions in the $(\\","971":"\\'{E}.\\~Ghys proved that the linking numbers of modular knots and the\"missing\"trefoil $K_{2,3}$ in $S^3$ coincide with the values of a highly ubiquitous function called the Rademacher symbol for ${\\rm SL}_2{\\mathbb Z}$. In this paper we replace ${\\rm SL}_2{\\mathbb Z}=\\Gamma_{2,3}$ by the triangle group $\\Gamma_{p,q}$ for any coprime pair $(p,q) \\in S^3$. We invoke the theory of harmonic Maass forms for $\\Gamma_{p,q}$ to introduce the notion of the Rademacher symbol $\\psi_{p,q}$, and provide several characterizations as well as examples demonstrating that all properties of $\\psi_{p,q}$ are cohomological. Among other things, we generalize Ghys's theorem for modular knots around any\"missing\"torus knot $K_{p,q}$ in $S^3$ and in a lens space. For certain configurations of $(p,q)$ the tensor product $\\psi_{p,q}\\ot","972":"Coronaviruses are a famous family of viruses that causes illness in human or animals. The new type of corona virus COVID-19 disease was firstly discovered in Wuhan-China. However, recently, the virus has been widely spread in most of the world countries and is reported as a pandemic. Further, nowadays, all the world countries are striving to control the coronavirus disease COVID-19. There are many mechanisms to detect the coronavirus disease COVID-19 including clinical analysis of chest CT scan images and blood test results. The confirmed COVID-19 patient manifests as fever, tiredness, and dry cough. Particularly, several techniques can be used to detect the initial results of the virus such as medical detection Kits. However, such devices are incurring huge cost and it takes time to install them and use. Therefore, in this paper, a new framework is proposed to detect coronavirus disease COVID-19 using onboard smartphone sensors. The proposal provides a low-cost solution, since most of the radiologists have already held smartphones for different daily-purposes. People can use the framework on their smartphones for the virus detection purpose. Nowadays, smartphones are powerful with existing computation-rich processors, memory space","973":"Many popular sports involve matches between two teams or players where each team have the possibility of scoring points throughout the match. While the overall match winner and result is interesting, it conveys little information about the underlying scoring trends throughout the match. Modeling approaches that accommodate a finer granularity of the score difference throughout the match is needed to evaluate in-game strategies, discuss scoring streaks, teams strengths, and other aspects of the game. We propose a latent Gaussian process to model the score difference between two teams and introduce the Trend Direction Index as an easily interpretable probabilistic measure of the current trend in the match as well as a measure of post-game trend evaluation. In addition we propose the Excitement Trend Index - the expected number of monotonicity changes in the running score difference - as a measure of overall game excitement. Our proposed methodology is applied to all 1143 matches from the 2019-2020 National Basketball Association (NBA) season. We show how the trends can be interpreted in individual games and how the excitement score can be used to cluster teams according to how exciting they are to watch. The applicability of our method is demonstrated by testing for factors which induce the existence of non-linear trends in the running score differences; this includes both offensive and defensive play","974":"Contact tracing is a very powerful method to implement and enforce social distancing to avoid spreading of infectious diseases. The traditional approach of contact tracing is time consuming, manpower intensive, dangerous and prone to error due to fatigue or lack of skill. Due to this there is an emergence of mobile based applications for contact tracing. These applications primarily utilize a combination of GPS based absolute location and Bluetooth based relative location remitted from user's smartphone to infer various insights. These applications have eased the task of contact tracing; however, they also have severe implication on user's privacy, for example, mass surveillance, personal information leakage and additionally revealing the behavioral patterns of the user. This impact on user's privacy leads to trust deficit in these applications, and hence defeats their purpose. In this work we discuss the various scenarios which a contact tracing application should be able to handle. We highlight the privacy handling of some of the prominent contact tracing applications. Additionally, we describe the various threat actors who can disrupt its working, or misuse end user's data, or hamper its mass adoption. Finally, we present privacy guidelines for contact tracing applications from different stakeholder's perspective. To best of our knowledge, this is the first generic work which provides privacy guidelines for contact tracing applications. Although specific contact tracing applications are","975":"The COVID-19 (coronavirus disease 2019) pandemic affected more than 186 million people with over 4 million deaths worldwide by June 2021. The magnitude of which has strained global healthcare systems. Chest Computed Tomography (CT) scans have a potential role in the diagnosis and prognostication of COVID-19. Designing a diagnostic system, which is cost efficient and convenient to operate on resource constrained devices like mobile phones would enhance the clinical usage of chest CT scans and provide swift, mobile, and accessible diagnostic capabilities. This work proposes developing Android application as a black-box detector for COVID-19 using machine learning techniques and provides a preliminary exploration into the feasibility of detecting COVID-19 from CT scans. Results show that the detection accuracy of our proposed model is comparable to the state-of-the-art methods reported on the same datasets. We further study the implementation of the developed algorithm on smartphones through a low-power app and evaluate it through four different popular smartphone applications. Promising results are obtained on both CPUs and GPUs. Furthermore, we successfully implement the model on one of the most powerful smartphones, an iPhone XR device. Finally, we discuss two important points for achieving better performance on the HTC Vive headset. Our codebase","976":"Symptom checkers have emerged as an important tool for collecting symptoms and diagnosing patients, minimizing the involvement of clinical personnel. We developed a machine-learning-backed system, SmartTriage, which goes beyond conventional symptom checking through a tight bi-directional integration with the electronic medical record (EMR). Conditioned on EMR-derived patient history, our system identifies the patient's chief complaint from a free-text entry and then asks a series of discrete questions to obtain relevant symptomatology. The patient-specific data are used to predict detailed ICD-10-CM codes as well as medication, laboratory, and imaging orders. Patient responses and clinical decision support (CDS) predictions are then inserted back into the EMR. To train the machine learning components of SmartTriage, we employed novel data sets of over 25 million primary care encounters and 1 million patient free-text reason-for-visit entries. These data sets were used to construct: (1) a long short-term memory (LSTM) based patient embedding model; (2) a fine-tuned transformer network for CDS prediction; and (3) a feed-forward network for question sequencing. In total, our system supports 337 patient chief complaints,","977":"We propose a highly data-efficient classification and active learning framework for classifying chest X-rays. It is based on (1) unsupervised representation learning of a Convolutional Neural Network and (2) the Gaussian Process method. The unsupervised representation learning employs self-supervision that does not require class labels, and the learned features are proven to achieve label-efficient classification. GP is used to obtain high-quality kernel functions by minimizing the distance between the posteriors of a CNN and its kernel function. Our novel framework combines these two elements in sequence to achieve highly data-efficient classifications. Moreover, both elements can be independently combined with any other element. Thus, our model has the potential to solve the unsupervised pneumonia problem, which is one of the major challenges of medical image analysis. We demonstrate our approach on three datasets where it outperforms several state-of-the-art methods. Further analyses show the importance of both elements in the feature fusion process. This suggests that combining them separately yields more accurate predictions than using them together. As such, our code is publicly available at https:\/\/github.com\/ankurmallick\/covid19_active_learning. In addition, we make our dataset available to the","978":"We reflect on two museum visiting experiences that adopted the strategy of interpersonalization in which one visitor creates an experience for another. In the Gift app, visitors create personal mini-tours for specific others. In Never let me go, one visitor controls the experience of another by sending them remote instructions as they follow them around the museum. By reflecting on the design of these experiences and their deployment in museums we show how interpersonalization can deliver engaging social visits in which visitors make their own interpretations. We contrast the approach to previous research in customization and algorithmic personalization. We reveal how these experiences relied on intimacy between pairs of visitors but also between visitors and the museum. We propose that interpersonalization requires museums to step back to make space for interpretation, but that this then raises the challenge of how to reintroduce the museum's own perspective. Finally, we articulate strategies and challenges for applying this approach. Our main contribution is a framework for understanding how to develop interpretable experiences that adhere to the needs of stakeholders while balancing the tension between traditional approaches to development with new technologies such as digital badges. We conclude by discussing some potential changes to current practices and highlighting some future research directions. However, our work provides little advice regarding how to implement this process or whether it is appropriate at all. Each","979":"The quality of sleep has a deep impact on people's physical and mental health. People with insufficient sleep are more likely to report physical and mental distress, activity limitation, anxiety, and pain. Moreover, in the past few years, there has been an explosion of applications and devices for activity monitoring and health tracking. Signals collected from these wearable devices can be used to study and improve sleep quality. In this paper, we utilize the relationship between physical activity and sleep quality to find ways of assisting people improve their sleep using machine learning techniques. People usually have several behavior modes that their bio-functions can be divided into. Performing time series clustering on activity data, we find cluster centers that would correlate to the most evident behavior modes for a specific subject. Activity recipes are then generated for good sleep quality for each behavior mode within each cluster. These activity recipes are supplied to an activity recommendation engine for suggesting a mix of relaxed to intense activities to subjects during their daily routines. The recommendations are further personalized based on the subjects' lifestyle constraints, i.e. their age, gender, body mass index (BMI), resting heart rate, etc, with the objective of the recommendation being the improvement of that night's quality of sleep. This would in turn serve a longer-","980":"Predicting the evolution of mortality rates plays a central role for life insurance and pension funds.Various stochastic frameworks have been developed to model mortality patterns taking into account the main stylized facts driving these patterns. However, relying on the prediction of one specific model can be too restrictive and lead to some well documented drawbacks including model misspecification, parameter uncertainty and overfitting. To address these issues we first consider mortality modelling in a Bayesian Negative-Binomial framework to account for overdispersion and the uncertainty about the parameter estimates in a natural and coherent way. Model averaging techniques are then considered as a response to model misspecifications. In this paper, we propose two methods based on leave-future-out validation which are compared to the standard Bayesian model averaging (BMA) based on marginal likelihood. An intensive numerical study is carried out over a large range of simulation setups to compare the performances of the proposed methodologies. An illustration is then proposed on real-life mortality datasets which includes a sensitivity analysis to a Covid-type scenario. Overall, we found that both methods based on out-of-sample criterion outperform the standard BMA approach in terms of prediction performance and robustness. The advantages of using either methods with a smaller number","981":"This article explores how non-governmental organizations (NGOs) in Hong Kong can be considered as 'infomediaries' (UNDP, 2003) in their use of information and communication technologies (ICTs) to support resilience-building across a growing population of migrant domestic workers (MDWs). It also acknowledges MDWs effective existing self-organizing community networks, including religious groups and labour unions. This study maps how NGO infomediaries are currently supporting MDW communities. It posits that NGOs are uniquely capable of developing ICTs grounded in local legal, psychological, and cultural contexts to improve MDW community resilience. The study finds that the fragmented nature of technology use between NGO infomediaries and the competition between NGOs for funding hinders NGO infomediaries' ability to support building lasting resilience within the MDW community. Recommendations from this study seek to align NGO infomediary tool development more closely with the MDW community in Hong Kong's existing communicative ecologies. It considers NGOs as infomediaries capable of adapting and streamlining various linkages across grassroots MDW social organizations, local community leaders, and governments that impact the MDW community. This study is a tool for NGO infomediaries to understand the types of resilience networks that they are uniquely capable","982":"Recent works suggest that striking a balance between maximizing idea stimulation and minimizing idea redundancy can elevate creativity in self-organizing social networks. We explore whether dispersing the visibility of idea generators can help achieve such a trade-off. We employ popularity signals (follower counts) of participants as an external source of variation in network structures, which we control across four randomized study conditions. We observe that popularity signals influence inspiration-seeking ties, partly by biasing people's perception of their peers' creativity. Networks that partially disperse the ideators' visibility using this external signal show reduced idea-redundancy and elevated creativity. However, extreme dispersal leads to inferior creativity by narrowing the range of idea stimulation. Our work holds future-of-work implications for elevating creativity. Impressively, our results occur amidst COVID-19 pandemic discussions, raising the possibility that workplace interactions might impact people's creative performances. Our findings have broad implications for shaping online creativity studies and real-world creative systems. They also apply to other settings where characteristics of users are known to affect creativity. SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1007\/s10584-021-03279-7. ELECTRONIC SUPPLEMENTARY MATERIAL","983":"According to the characteristics of art, art education according to Bloom's classification, should take place at high and close to the level of creativity. E-learning related to the subject of art is different from ordinary E-learning. In this paper, a general circular model for E-learning for art education is presented, a case study which has been done in Iran among students and artists. Art in the general sense, in addition to the same and Comprehensive features, has characteristics specific to different countries and cultures, and consequently there are differences in art education. But within the framework of the general circular model, some similarities exist between the different cultural aspects of art and scientific studies about art education. The main results of the research show that art learning can be better conducted with E-learning than with conventional methods, and that it has advantages over both methodologies and approaches. Furthermore, based on the findings, strategies for future research and development of art education can be proposed. This will also help policymakers to develop their work program and strategy for the effective incorporation of art in all activities possible to us. For example, the appropriate placement of canvases in front of walls may be one of the most effective ways to prevent infection or disease transmission by visualizing the presence of humans. Similarly, placing","984":"Machine learning techniques including neural networks are popular tools for materials and chemical scientists with applications that may provide viable alternative methods in the analysis of structure and energetics of systems ranging from crystals to biomolecules. However, efforts are less abundant for prediction of dynamics. Here we explore the ability of three well established recurrent neural network architectures for forecasting the energetics of a macromolecular polymer-lipid aggregate solvated in ethyl acetate at ambient conditions. Data models generated from recurrent neural networks are trained and tested on nanoseconds-long time series of the intra-macromolecules potential energy and their interaction energy with the solvent generated from Molecular Dynamics and containing half million points. Our exhaustive analyses convey that the three recurrent neural network investigated generate data models with limited capability of reproducing the energetic fluctuations and yielding short or long term energetics forecasts with underlying distribution of points inconsistent with the input series distributions. We propose an in silico experimental protocol consisting on forming an ensemble of artificial network models trained on an ensemble of series with additional features from time series containing pre-clustered time patterns of the original series. The forecast process improves by predicting a band of forecasted time series with a spread of values consistent with the molecular dynamics energy fluctuation span","985":"The human-robot collaboration (HRC) plays an important role in industrial automation and manufacturing. HRC is expected to involve co-locating, interacting, and sharing physical artifacts such as work items, tools, and facilities. The current study on hrc focuses on the safety issues of its users and provides recommendations for improving the user experience. In this paper, we present the results of a systematic literature review on the existing research topics of hc and discuss future directions with potential research advances. We also share our insights about building effective hc systems and develop recommended practices for safe use. As key highlights, we find that social proximity between humans and robots is essential for achieving trusted AI in hc. Using social distance can cause undesired robot failures, but it reduces the productivity loss of the human operator when there exists a failure case in the mission. From both perspectives, we see that taking advantage of established technologies by using them in the context of hc will help achieve higher success rates, better learning experiences, and more efficient communication and collaboration. We hope that this study contributes to the community effort toward establishing best-practices for robotic designers, developers, and operators of hc integrated systems. This contribution comes from the perspective of experienced researchers who have collected empirical data from","986":"In the U.S., approximately 15-17% of children 2-8 years of age are estimated to have at least one diagnosed mental, behavioral or developmental disorder. However, such disorders often go undiagnosed, and their ability to evaluate and treat disorders in the first years of life is limited. To analyze infant developmental changes, previous studies have shown advanced ML models excel at classifying infant and\/or parent vocalizations collected using cell phone, video, or audio-only recording device like LENA. In this study, we pilot test the audio component of a new infant wearable multi-modal device that we have developed called LittleBeats (LB). LB audio pipeline is advanced in that it provides reliable labels for both speaker diarization and vocalization classification tasks, compared with other platforms that only record audio and\/or provide speaker diarization labels. We leverage wav2vec 2.0 to obtain superior and more nuanced results with the LB family audio stream. We use a bag-of-audio-words method with wav2vec 2.0 features to create high-level visualizations to understand family-infant vocalization interactions. We demonstrate that our high quality visualizations capture major types of family vocalization interactions, in categories indicative","987":"While almost any kind of face mask offers some protection against particles and pathogens of different sizes, the most efficient ones make use of a layered structure where one or more layers are electrically charged. This electret layer is essential to efficient filtration of difficult-to-capture small particles, yet the exact nature of electrostatic capture with respect to both the charge on the particles and the electret fibres as well as the effect of immediate environment remains unclear. Here, we explore in detail the electrostatic interaction between the surface of a single charged electret fibre and a model of SARS-CoV-2 virus. Using Poisson-Boltzmann electrostatics coupled to a detailed spike protein charge regulation model, we show how pH and salt concentration drastically change both the scale and sign of the interaction. Furthermore, the configuration of the few spike proteins closest to the electret fibre turns out to be as important for the strength of the interaction as their total number on the virus, a direct consequence of spike protein charge regulation. The results of our work elucidate the details of virus electrostatics and contribute to the general understanding of efficient virus filtration mechanisms. Our findings enable further studies on viral electrostatics and may also serve as a basis for","988":"We describe our proposed methodology to approach the predict+optimise challenge introduced in the IEEE CIS 3rd Technical Challenge. The predictive model employs an ensemble of LightGBM models and the prescriptive analysis employs mathematical optimisation to efficiently prescribe solutions that minimise the average cost over multiple scenarios. Our solutions ranked 1st in the optimisation and 2nd in the prediction challenge of the competition. We further discuss the applicability of the proposed model in other real-world contexts including the COVID-19 pandemic. For this, we also present an extended discussion on the generalisations of the model. Finally, we examine the impact of the number of environments and hyperparameters used by the competition with the presence of stochasticity. We find that, while increasing the number of environments increases the uncertainty of the forecast, lowering the upper bound of the forecast favours extreme values of the optimal policy. Therefore, it is essential to balance between the extremes of good and bad cases of the model. This points out the need for careful planning when deploying such a sensitive resource-intensive model in practice. To conclude, we offer some practical advice on how to further improve this type of forecasting model. All the source codes are publicly available at https:\/\/github.com\/cbas","989":"We study in this work an instance of the multi-armed bandit (MAB) problem, specifically where several causal MABs operate chronologically in the same dynamical system. Practically the reward distribution of each bandit is governed by the same non-trivial dependence structure, which is a dynamic causal model. Dynamic because we allow for each causal MAB to depend on the preceding MAB and in doing so are able to transfer information between agents. Our contribution, the Chronological Causal Bandit Problem (CCBP), is useful in discrete decision-making settings where the causal effects are changing across time and can be informed by earlier interventions in the same system. In this paper, we present some early findings of the CCBP as demonstrated on a toy problem. We also provide an algorithm that performs best for the given parameter values investigated. Furthermore, we investigate how our problem might behave in an online setting when the number of interactions or the amount of capital budgeted towards different parameters scales linearly with the size of the population. To do so, we use a matching scheme for solving an approximate dynamic Bayesian inference problem. A key innovation of the CCBP is that it allows for informative prior distributions on the causal effects themselves through a hierarchical mixture model.","990":"Representation learning over temporal networks has drawn considerable attention in recent years. Efforts are mainly focused on modeling structural dependencies and temporal evolving regularities in Euclidean space which, however, underestimates the inherent complex and hierarchical properties in many real-world temporal networks, leading to sub-optimal embeddings. To explore these properties of a complex temporal network, we propose a hyperbolic temporal graph network (HTGN) that fully takes advantage of the exponential capacity and hierarchical awareness of hyperbolic geometry. More specially, HTGN maps the temporal graph into hyperbolic space, and incorporates hyperbolic graph neural network and hyperbolic gated recurrent neural network, to capture the evolving behaviors and implicitly preserve hierarchical information simultaneously. Furthermore, in the hyperbolic space, we propose two important modules that enable HTGN to successfully model temporal networks: (1) hyperbolic temporal contextual self-attention (HTA) module to attend to historical states and (2) hyperbolic temporal consistency (HTC) module to ensure stability and generalization. Experimental results on multiple real-world datasets demonstrate the superiority of HTGN for temporal graph embedding, as it consistently outperforms competing methods by significant margins in various temporal link prediction tasks. Specifically, HT","991":"The unprecedented cessation of human activities during the COVID-19 pandemic has affected global energy use and CO2 emissions from fossil fuel use and cement production. Here we show that the decrease in global fossil CO2 emissions during the first quarter of 2020 was of 5.8% (542 Mt CO2 with a 20% 1-{\\sigma} uncertainty). Unlike other emerging estimates, ours show the temporal dynamics of emissions based on actual emissions data from power generation (for 29 countries) and industry (for 73 countries), on near real time activity data for road transportation (for 132 countries), aviation and maritime traffic, and on heating degree days for commercial and residential sectors emissions (for 206 countries). These dynamic estimates cover all of the human induced CO2 emissions from fossil fuel combustion and cement production. The largest share of COVID-related decreases in emissions are due to decreases in industry (157.9 Mt CO2, -7.1% compared to 2019), followed by road transportation (145.7 Mt CO2, -8.3%), power generation (131.6 Mt CO2, -3.8%), residential (47.8 Mt CO2, -3.6%), fishing and maritime transport (35.5Mt CO2, -13","992":"In this paper we propose a cloud-based platform for analyzing the massive and complex data traffic of mobile phones. The proposed platform makes use of machine learning techniques to analyze the mobility patterns and the demographic information of users in addition to other useful features such as hotspots, topics, time zones, etc. Then, the spatio-temporal population flows are analyzed using recurrent neural networks with fixed weights and infinite sizes. We also implement some comparison functions like unweighted averaging and pairwise clustering. Extensive experimental results show the superiority of our approach over some baselines including VGG16, CNNs, and LSTM models. Furthermore, it takes less computing power on average than existing systems while still performing similarly accurate predictions. For example, our network achieves 99% accuracy on Twitter but only 0.4% on Weibo (the largest twitter thread). The obtained computational savings by using different hyperparameters can be very valuable to lower the training costs of neural network based models. All collected data, including the tweets and Weibo posts, will be available to the research community soon. \\url{https:\/\/github.com\/VinAIResearch\/P2P-MobileNet} source code is publicly available at https:\/\/github.com\/vin","993":"The COVID-19 epidemic has been characterized by its generation of large volumes of viral genomic data at an incredible pace due to recent advances in high-throughput sequencing technologies, the rapid global spread of SARS-CoV2, and its persistent threat to public health. However, distinguishing the most epidemiologically relevant information encoded in these vast amounts of data requires substantial effort across the research and public health communities. Studies of SARS-CoV2 genomes have been critical in tracking the spread of variants and understanding its epidemic dynamics, and may prove crucial for controlling future epidemics and alleviating significant public health burdens. Together, genomic data and bioinformatics methods enable broad-scale investigations of the spread of SARS-CoV2 at the local, national, and global scales and allow researchers the ability to efficiently track the emergence of novel variants, reconstruct epidemic dynamics, and provide important insights into drug and vaccine development and disease control. Here we discuss the tremendous opportunities that genomics offers to unlock effective use of SARS-CoV2 genome data for efficient public health surveillance and guiding timely responses to COVID-19. In particular, we show how genomics can be used to infer the latest year for the origin of SARS-CoV2, which","994":"We propose a physics-inspired mathematical model underlying the temporal evolution of competing virus variants that relies on the existence of (quasi) fixed points capturing the large time scale invariance of the dynamics. To motivate our result we first modify the time-honoured compartmental models of the SIR type to account for the existence of competing variants and then show how their evolution can be naturally re-phrased in terms of flow equations ending at quasi fixed points. As the natural next step we employ (near) scale invariance to organise the time evolution of the competing variants within the effective description of the epidemic Renormalization Group framework. We test the resulting theory against the time evolution of COVID-19 virus variants that validate the theory empirically. Our results are found to match well with the numerical simulations of the Effective Infection Centre in Stockholm, where the model was used to extract the daily infection rate. The source code for this paper is available at https:\/\/github.com\/felipemaiapolo\/covid19_model_contribution. This model is released under the Swedish Region of Uppsala (SRU) license via the GitHub repository https:\/\/github.com\/sfu-db\/Covid19_Effective_I","995":"This paper considers the estimation of the trajectory of coronavirus COVID-19 adhering to respiratory droplets projected horizontally, considering the geographical altitude. The size of viruses and respiratory droplets is the factor that determines the trajectory of the microorganisms in a viscous medium such as air; For this purpose, a graphical comparison of the diameters and masses of the microdroplets produced when breathing, speaking, singing, coughing, and sneezing has been made. In addition to determining the effect of the size of the microdroplets on the spread of the virus, the results of the present study may be used by health care authorities to determine the best course of action in case of future pandemics. Using the data obtained for the macroscopic air flow through a facile filter, a piezoelectric sensor was built to detect the smaller droplets responsible for the transmission of the virus. Subsequently, the objective function of the research model is determined with respect to the concentration of the pathogen in water and the social distance. The results indicate that for all the tested settings, the value of R2 = 0.9927 means that the effective reproduction number for the smaller droplets is much greater than 1.0 e \u2212 04 than what","996":"We propose a partial identification method for estimating disease prevalence from serology studies. Our data are results from antibody tests in some population sample tested during an epidemic, where the test parameters, such as the true\/false positive rates, are unknown. Our method scans the entire parameter space, and rejects parameter values using the joint data density as the test statistic. The proposed method is conservative for marginal inference, in general, but its key advantage over more standard approaches comes from being valid in finite samples even when the underlying model is not point identified. Moreover, our method requires only independence of serological test results, and does not rely on asymptotic arguments, normality assumptions, or other approximations. We use recent Covid-19 serology studies in the US, and show that the parameter confidence set is generally wide, and cannot support definite conclusions. Specifically, recent serology studies from California suggest a prevalence anywhere in the range 0%-2% (at the time of study), and are therefore inconclusive. However, this range could be narrowed down to 0.7%-1.5% if the actual false positive rate of the antibody test was indeed near its empirical estimate (~0.5%). In another study from New York State, Covid-19 prevalence","997":"We propose a two-step framework for predicting the implied volatility surface over time without static arbitrage. In the first step, we select features to represent the surface and predict them over time. In the second step, we use the predicted features to construct the implied volatility surface using a deep neural network (DNN) model by incorporating constraints that prevent static arbitrage. We consider three methods to extract features from the implied volatility data: principal component analysis, variational autoencoder and sampling the surface, and we predict these features using LSTM. Using a long time series of implied volatility data for S\\&P500 index options to train our models, we find two feature construction methods, sampling the surface and variational autoencoders combined with DNN for surface construction, are the best performers in out-of-sample prediction. In particular, they outperform a classical method substantially. Furthermore, the DNN model for surface construction not only removes static arbitrage, but also significantly reduces the prediction error compared with a standard interpolation method. Our framework can also be used to simulate the dynamics of the implied volatility surface without static arbitrage. To this end, we developed a simple R program called \\texttt{sk} which makes it convenient to apply","998":"In this paper, we present the first results from a project to develop a real-time robot for children's digital information search using a single video call. In addition, we adapt a existing speech recognition algorithm to provide robots with a low false positive rate when there is a need to exclude potentially harmful content from the child interaction graph. The proposed method was evaluated on 37 videos collected from 13 trending challenges on the ForYou Page, a social media platform, and shows promising results according to the test requirements. The best model achieved a true positive rate of 0.931 and a specificity of.91. These results are significantly better than those obtained from previous work and show the potential of incorporating further improvements into our framework. We also discuss how the findings of this work may impact the design of public policy related to online safety and trustworthiness of robots in general. Our dataset will be made publicly available at https:\/\/github.com\/robi56\/call_for_action.git.io\/targeted_opportunity\/. This data will be used to facilitate research within the area of human-AI hybrid systems (e.g. facial-, voice-and-language input) as well as industrial automation (e.g. where necessary). It is expected that the","999":"The recent outbreak of COVID-19 has resulted in a record number of pandemic related scientific papers published on the Internet along with the corresponding research tools. This paper aims to characterize and analyze the trends of these publications from the perspective of their topic. Specifically, we attempt to answer the question: how does the average subject or article title vary across time and space? In addition, we aim to detect changes in collaboration patterns at the institution level as well as country-level. To achieve this goal, we propose an unsupervised text mining approach using metadata about scientific collaborations for detecting topics evolving over time. We provide results that show not only the volume of individual contributions but also how they have changed over time. Furthermore, we identify different forms of collaboration and classify them based on their role in initiating and ending each contribution. Finally, we add up numerical scores which indicate areas where particular groups of researchers are collaborating. The resulting dataset can be used by other scholars who want to explore the nature of scientific collaboration and the change of collaboration patterns during a specific time period. It is also helpful for policy makers and health leaders to know how the international community is collectively coping with this unprecedented global crisis. For example, it is observed that countries which were involved in the early stages of the pand"}}